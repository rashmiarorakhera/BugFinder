assignee,components,created,creator,description,fixVersions,issuetype,key,priority,reporter,resolution,resolution.date,status.description,status.name,subtask,summary,updated,versions,watches
,[],2022-04-01T08:48:51.000+0000,Adarsh Shukla,"Hi Team,

We want to understand how to handle the zookeeper log4j vulnerability? is the team planning to release a new version of zookeeper which resolves the log4j vulnerability issue?

 

Regards,

Adarsh",[],Bug,ZOOKEEPER-4509,Major,Adarsh Shukla,Information Provided,2022-04-01T09:02:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,log4j vulnerability,2022-04-05T05:42:04.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",3.0
Kezhu Wang,"[<JIRA Component: name='java client', id='12312381'>]",2022-04-01T07:37:35.000+0000,Kezhu Wang,"The observable behavior is that client will not get expired event from watcher. The cause if twofold:
1. `updateLastSendAndHeard` is called in reconnection so the session timeout don't decrease.
2. There is not break out from `ClientCnxn.SendThread.run` after session timeout.",[],Bug,ZOOKEEPER-4508,Major,Kezhu Wang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper client run to endless loop in ClientCnxn.SendThread.run if all server down,2022-04-01T09:10:57.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.8.0', id='12349587'>]",1.0
,[],2022-03-29T12:43:54.000+0000,Edwin Hobor,"*CVE-2020-36518* vulnerability affects jackson-databind in Zookeeper (see [https://github.com/advisories/GHSA-57j2-w4cx-62h2]).

Upgrading to jackson-databind version *2.13.2.1* should address this issue.","[<JIRA Version: name='3.7.1', id='12350030'>, <JIRA Version: name='3.6.4', id='12350076'>, <JIRA Version: name='3.9.0', id='12351304'>, <JIRA Version: name='3.8.1', id='12351398'>]",Bug,ZOOKEEPER-4505,Major,Edwin Hobor,Fixed,2022-03-31T18:24:08.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,CVE-2020-36518 - Upgrade jackson databind to 2.13.2.1,2022-03-31T18:30:08.000+0000,[],1.0
Mohammad Arshad,[],2022-03-29T10:01:19.000+0000,Mohammad Arshad,"*Problem and Analysis:*
After integrating ZooKeeper 3.6.3 we observed deadlock in HDFS HA functionality as shown in below thread dumps.
{code:java}
""main-EventThread"" #33 daemon prio=5 os_prio=0 tid=0x00007f9c017f1000 nid=0x101b waiting for monitor entry [0x00007f9bda8a6000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.hadoop.ha.ActiveStandbyElector.processWatchEvent(ActiveStandbyElector.java:603)
	- waiting to lock <0x00000000c17986c0> (a org.apache.hadoop.ha.ActiveStandbyElector)
	at org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef.process(ActiveStandbyElector.java:1193)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:626)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:582)
{code}
{code:java}
""main"" #1 prio=5 os_prio=0 tid=0x00007f9c00060000 nid=0xea3 waiting on condition [0x00007f9c06404000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000c1b383c8> (a java.util.concurrent.Semaphore$NonfairSync)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:838)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:999)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1306)
	at java.util.concurrent.Semaphore.acquire(Semaphore.java:467)
	at org.apache.zookeeper.ZKUtil.deleteInBatch(ZKUtil.java:122)
	at org.apache.zookeeper.ZKUtil.deleteRecursive(ZKUtil.java:64)
	at org.apache.zookeeper.ZKUtil.deleteRecursive(ZKUtil.java:76)
	at org.apache.hadoop.ha.ActiveStandbyElector$1.run(ActiveStandbyElector.java:386)
	at org.apache.hadoop.ha.ActiveStandbyElector$1.run(ActiveStandbyElector.java:383)
	at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1103)
	at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1095)
	at org.apache.hadoop.ha.ActiveStandbyElector.clearParentZNode(ActiveStandbyElector.java:383)
	- locked <0x00000000c17986c0> (a org.apache.hadoop.ha.ActiveStandbyElector)
	at org.apache.hadoop.ha.ZKFailoverController.formatZK(ZKFailoverController.java:290)
	at org.apache.hadoop.ha.ZKFailoverController.doRun(ZKFailoverController.java:227)
	at org.apache.hadoop.ha.ZKFailoverController.access$000(ZKFailoverController.java:66)
	at org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:186)
	at org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:182)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:360)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1741)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:498)
	at org.apache.hadoop.ha.ZKFailoverController.run(ZKFailoverController.java:182)
	at org.apache.hadoop.hdfs.tools.DFSZKFailoverController.main(DFSZKFailoverController.java:220)
{code}
org.apache.hadoop.ha.ActiveStandbyElector#clearParentZNode is instance synchronized and calls ZKUtil.deleteRecursive(zk, pathRoot)

ZKUtil.deleteRecursive is making async delete API call with MultiCallback as it callback.
As processWatchEvent is being processed, pathRoot or one of the child paths must have set watcher for delete notification.

When delete API is called, notification comes first to client then the actual delete response.
In this case both notification and delete response are processed through callback and through common waitingEvents queue one by one.

First notification is processed, but it cannot complete as it cannot take lock on processWatchEvent() method as lock is already taken by another thread while calling clearParentZNode()
As delete notification cannot be processed, MultiCallback is not taken from queue for processing. It stays there in the queue forever.

 

*Why this problem was not happening with earlier versions (3.5.x)?*

In earlier ZK versions, ZKUtil.deleteRecursive was using sync delete API. So delete response was processed directly not though the callback. 
Sot both clearParentZNode and processWatchEvent were completing independently. 


*Proposed Fix:*
There are two approaches to fix this problem. 
1. We can fix the problem in HDFS, modify the HDFS code to avoid the deadlock. But we may get similar bugs in other projects.
2. Fix the problem in ZK. Make the API behavior same as the old behavior(use sync API to delete the ZK node) and provide new overloaded API with new behavior(use async API to delete the ZK node)

I propose to fix the problem with 2nd approach.","[<JIRA Version: name='3.7.1', id='12350030'>, <JIRA Version: name='3.6.4', id='12350076'>, <JIRA Version: name='3.9.0', id='12351304'>, <JIRA Version: name='3.8.1', id='12351398'>]",Bug,ZOOKEEPER-4504,Critical,Mohammad Arshad,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZKUtil#deleteRecursive causing deadlock in HDFS HA functionality,2022-04-05T08:29:58.000+0000,[],5.0
,[],2022-03-28T12:51:25.000+0000,May,"Here is the bug triggering process:
 
 # A cluster with three nodes: zk1, zk2 and zk3. zk3 is the leader.
 # client create a znode ""/bug"" with value ""bad""
 # client update znode ""/bug"" to value ""good""
 # zk1 crashes before receiving proposal for leader for the request in step 3.
 # ""/bug"" is modified to ""good""
 # zk1 was restarted
 # another client connects to zk1, reads ""/bug""  and gets ""bad""
 # zk1 finish synchronization with current leader, and then modify ""/bug"" to ""good"".

The problem is that zk1 should be accessed by a client when it finish synchronization with current leader in case of a client reads bad data.

 

****************************************************************************************

The actual testing scenario is as following:

I have a cluster of 5 nodes: C1ZK1(172.30.0.2), C1ZK2(172.30.0.3), C1ZK3(172.30.0.4), C1ZK4(172.30.0.5), C1ZK5(172.30.0.6)

 
 # 2022-03-24 22:51:40,246 [Client1] - INFO -  build connection with zookeeper (client1 actuallly builds connection with  C1ZK1)
 # 2022-03-24 22:51:40,479 crash C1ZK4 before creating file ""/home/zkuser/evaluation/zk-3.6.3/zkData/version-2/log.100000001"" (I think this crash does not matter):
{code:java}
java.io.FileOutputStream.<init>(FileOutputStream.java:213), java.io.FileOutputStream.<init>(FileOutputStream.java:162), org.apache.zookeeper.server.persistence.FileTxnLog.append(FileTxnLog.java:287), org.apache.zookeeper.server.persistence.FileTxnSnapLog.append(FileTxnSnapLog.java:582), org.apache.zookeeper.server.ZKDatabase.append(ZKDatabase.java:641), org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:181), org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:0{code}

 # 2022-03-24 22:51:40,761 [Client1] - INFO - created znode ""/bug"" ""hello""
 # 2022-03-24 22:51:40,869 [Client1] - INFO - set znode ""/bug"" ""nice""
 # 2022-03-24 22:51:40,915 [Client1] - INFO - read znode ""/bug"" is ""nice""
 # 2022-03-24 22:51:40,996 [Client1] - INFO - deleted znode ""/bug""
 # Client1 requests to create ephemeral znode ""/eph""
 # 2022-03-24 22:51:45,033 crash C1ZK1 before:
{code:java}
org.apache.zookeeper.server.quorum.QuorumPacket.serialize(QuorumPacket.java:68), org.apache.jute.BinaryOutputArchive.writeRecord(BinaryOutputArchive.java:126), org.apache.zookeeper.server.quorum.Learner.writePacketNow(Learner.java:194), org.apache.zookeeper.server.quorum.Learner.writePacket(Learner.java:186), org.apache.zookeeper.server.quorum.SendAckRequestProcessor.processRequest(SendAckRequestProcessor.java:46), org.apache.zookeeper.server.SyncRequestProcessor.flush(SyncRequestProcessor.java:246), org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:169), org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:0) {code}

 # 2022-03-24 22:51:49,451  restart C1ZK1 before C1ZK2 write to file ""/home/zkuser/evaluation/zk-3.6.3/zkData/version-2/log.100000001"":
{code:java}
java.io.FileOutputStream.writeBytes(FileOutputStream.java), java.io.FileOutputStream.write(FileOutputStream.java:326), java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82), java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140), org.apache.zookeeper.server.persistence.FileTxnLog.append(FileTxnLog.java:293), org.apache.zookeeper.server.persistence.FileTxnSnapLog.append(FileTxnSnapLog.java:582), org.apache.zookeeper.server.ZKDatabase.append(ZKDatabase.java:641), org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:181), org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:0) {code}

 # 2022-03-24 22:51:56,744 [Client2] - INFO -  build connection with zookeeper
 # 2022-03-24 22:51:56,876 [Client2] - INFO - cannot read ephemeral znode ""/eph"", got ""KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /eph""
 # When we connect with every alive node in the cluster and read data respectively, we get
{code:java}
2022-03-24 22:52:14,663 [ZKChecker] - INFO - server C1ZK3:11181 and server C1ZK1:11181 have different number of znodes:[/zookeeper/quota, /zookeeper] and [/zookeeper/quota, /eph, /zookeeper] {code}

 # Then I killed all the nodes in the cluster

 

The file ""log.100000001"" in C1ZK1 is :
{code:java}
ZooKeeper Transactional Log File with dbid 0 txnlog format version 2
22-3-24 下午10时51分40秒 session 0x3035dd9dccf0000 cxid 0x0 zxid 0x100000001 createSession 15000
 2,137198550422-3-24 下午10时51分40秒 session 0x3035dd9dccf0000 cxid 0x1 zxid 0x100000002 create '/bug,#68656c6c6f,v{s{31,s{'world,'anyone}}},F,1
 2,662048746122-3-24 下午10时51分40秒 session 0x3035dd9dccf0000 cxid 0x2 zxid 0x100000003 setData '/bug,#6e696365,1
 2,558865945422-3-24 下午10时51分40秒 session 0x3035dd9dccf0000 cxid 0x4 zxid 0x100000004 delete '/bug
 2,291011889122-3-24 下午10时51分40秒 session 0x3035dd9dccf0000 cxid 0x5 zxid 0x100000005 create '/eph,#657068656d,v{s{31,s{'world,'anyone}}},T,2
 2,7205004542EOF reached after 5 txns. {code}
And txn logs in other nodes are:
{code:java}
ZooKeeper Transactional Log File with dbid 0 txnlog format version 2
22-3-24 下午10时51分40秒 session 0x3035dd9dccf0000 cxid 0x0 zxid 0x100000001 createSession 15000
 2,137198550422-3-24 下午10时51分40秒 session 0x3035dd9dccf0000 cxid 0x1 zxid 0x100000002 create '/bug,#68656c6c6f,v{s{31,s{'world,'anyone}}},F,1
 2,662048746122-3-24 下午10时51分40秒 session 0x3035dd9dccf0000 cxid 0x2 zxid 0x100000003 setData '/bug,#6e696365,1
 2,558865945422-3-24 下午10时51分40秒 session 0x3035dd9dccf0000 cxid 0x4 zxid 0x100000004 delete '/bug
 2,291011889122-3-24 下午10时51分40秒 session 0x3035dd9dccf0000 cxid 0x5 zxid 0x100000005 create '/eph,#657068656d,v{s{31,s{'world,'anyone}}},T,2
 2,720500454222-3-24 下午10时51分57秒 session 0x5035dda2c040000 cxid 0x0 zxid 0x200000001 createSession 15000
 2,720500454222-3-24 下午10时51分57秒 session 0x5035dda2c040000 cxid 0x1 zxid 0x200000002 closeSession v{}
 2,7205004542EOF reached after 7 txns. {code}",[],Bug,ZOOKEEPER-4503,Major,May,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,A restarted node can be accessed before it finishing synchronization with leader,2022-04-02T03:00:28.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>]",2.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2022-03-28T08:34:21.000+0000,xiongjianbo,"After a three-node ZooKeeper cluster runs for a period of time, OutOfMemory occurs. Check the memory stack and find that the number of SyncRequestProcessor objects is over 400.

 

 

Check the previous modification records. It is found that the Learner.shutdown logic is modified this time.

[https://github.com/apache/zookeeper/pull/1619]

After the modification, the original invoking logic is changed.

Before modification: Learner.shutdown() -> LearnerZooKeeperServer.shutdown() -> ZooKeeperServer.shutdown()

After modification: Learner.shutdown() -> ZooKeeperServer.shutdown(boolean)

Finally, LearnerZooKeeperServer.syncProcessor.shutdown() was never called.

 

 

Analysis by MAT:

401 instances of {*}""org.apache.zookeeper.server.SyncRequestProcessor""{*}, loaded by *""sun.misc.Launcher$AppClassLoader @ 0xfc000000""* occupy *48,494,208 (40.02%)* bytes.

*Keywords*
org.apache.zookeeper.server.SyncRequestProcessor
sun.misc.Launcher$AppClassLoader @ 0xfc000000

 

 

Follower shutdown logs:

03-15 09:20:23,917 WARN (QuorumPeer[myid=1](plain=disabled)(secure=192.168.18.23:26310)) (Follower:129) Exception when following the leader
java.io.EOFException: null
        at java.io.DataInputStream.readInt(DataInputStream.java:393) ~[?:1.8.0_322]
        at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:96) ~[zookeeper-jute-3.6.3.jar:3.6.3]
        at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:86) ~[zookeeper-jute-3.6.3.jar:3.6.3]
        at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:134) ~[zookeeper-jute-3.6.3.jar:3.6.3]
        at org.apache.zookeeper.server.quorum.Learner.readPacket(Learner.java:221) ~[zookeeper-3.6.3-h0.gdd.pub.r65.jar:3.6.3]
        at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:125) [zookeeper-3.6.3-h0.gdd.pub.r65.jar:3.6.3]
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1480) [zookeeper-3.6.3-h0.gdd.pub.r65.jar:3.6.3]
03-15 09:20:23,918 INFO (QuorumPeer[myid=1](plain=disabled)(secure=192.168.18.23:26310)) (Follower:143) Disconnected from leader (with address: /192.168.18.24:26311). Was connected for 35683ms. Sync state: true
03-15 09:20:23,918 INFO (QuorumPeer[myid=1](plain=disabled)(secure=192.168.18.23:26310)) (Follower:292) shutdown Follower
03-15 09:20:23,919 INFO (QuorumPeer[myid=1](plain=disabled)(secure=192.168.18.23:26310)) (ZooKeeperServer:812) shutting down
03-15 09:20:23,919 INFO (QuorumPeer[myid=1](plain=disabled)(secure=192.168.18.23:26310)) (RequestThrottler:244) Shutting down
03-15 09:20:23,919 INFO (RequestThrottler) (RequestThrottler:205) Draining request throttler queue
03-15 09:20:23,919 INFO (RequestThrottler) (RequestThrottler:181) RequestThrottler shutdown. Dropped 0 requests
03-15 09:20:23,920 INFO (QuorumPeer[myid=1](plain=disabled)(secure=192.168.18.23:26310)) (FollowerRequestProcessor:148) Shutting down
03-15 09:20:23,920 INFO (QuorumPeer[myid=1](plain=disabled)(secure=192.168.18.23:26310)) (CommitProcessor:617) Shutting down
03-15 09:20:23,920 INFO (FollowerRequestProcessor:1) (FollowerRequestProcessor:112) FollowerRequestProcessor exited loop!
03-15 09:20:23,920 INFO (CommitProcessor:1) (CommitProcessor:406) CommitProcessor exited loop!
03-15 09:20:23,928 INFO (QuorumPeer[myid=1](plain=disabled)(secure=192.168.18.23:26310)) (FinalRequestProcessor:661) shutdown of request processor complete
03-15 09:20:24,298 INFO (QuorumPeer[myid=1](plain=disabled)(secure=192.168.18.23:26310)) (QuorumPeer:864) Peer state changed: looking
03-15 09:20:24,298 WARN (QuorumPeer[myid=1](plain=disabled)(secure=192.168.18.23:26310)) (QuorumPeer:1526) PeerState set to LOOKING
03-15 09:20:24,298 INFO (QuorumPeer[myid=1](plain=disabled)(secure=192.168.18.23:26310)) (QuorumPeer:1396) LOOKING",[],Bug,ZOOKEEPER-4502,Critical,xiongjianbo,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Memory leaks: SyncRequestProcessor objects leak when leader election occurred,2022-04-02T11:14:40.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>]",3.0
,[],2022-03-24T08:51:56.000+0000,Margarita Stoilova,"Version 9.4.43 is vulnerable for PRISMA-2021-0182. 

 org.eclipse.jetty_jetty-server package versions before 9.4.44 are vulnerable to DoS (Denial of Service). ",[],Bug,ZOOKEEPER-4501,Major,Margarita Stoilova,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Upgrade Jetty from 9.4.43 to 9.4.44,2022-03-24T08:51:56.000+0000,"[<JIRA Version: name='3.8.0', id='12349587'>]",1.0
,[],2022-03-22T16:31:58.000+0000,Houston Putman,"Currently the validateACL() method in Zookeeper.java looks like this:

 
{code:java}
if (acl == null || acl.isEmpty() || acl.contains(null)) {
  throw new KeeperException.InvalidACLException();
} {code}
However the {{contains()}} method for an {{ImmutableCollection}} validates that the argument is non-null: (Note, {{contains()}}{{{} calls {{indexOf()}}{}}}{{{}){}}}

 

 
{code:java}
@Override
public int indexOf(Object o) {
    Objects.requireNonNull(o);
    for (int i = 0, s = size(); i < s; i++) {
        if (o.equals(get(i))) {
            return i;
        }
    }
    return -1;
} {code}
Therefore if you try to pass an immutable collection for ACLs, you will get a null-pointer exception.

 ",[],Bug,ZOOKEEPER-4500,Major,Houston Putman,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ACL Lists cannot be validated when they are immutable,2022-03-22T19:27:39.000+0000,[],2.0
,"[<JIRA Component: name='server', id='12312382'>]",2022-03-22T09:31:46.000+0000,Keyi Zhong,,[],Bug,ZOOKEEPER-4499,Major,Keyi Zhong,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,The status of all nodes will be abnormal when only the inbound direction of the ZooKeeper port is faulty,2022-03-22T09:31:46.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>]",1.0
,[],2022-03-17T06:38:18.000+0000,May,"# client connects to node ZK1;
# client creates an ephemeral znode ""/eph""
# client closes the session;
# ZK1 crashes before sending closing session request  to leader
# the ephemeral znode ""/eph"" leaves in the cluster

Since ZK1 is down, the cluster should clean up sessions that connect to ZK1.",[],Bug,ZOOKEEPER-4497,Major,May,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Crash before closing session makes an ephemeral znode leave in ZooKeeper,2022-03-28T12:42:22.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>]",1.0
,"[<JIRA Component: name='scripts', id='12312384'>]",2022-03-16T13:10:52.000+0000,Youhei Sakurai,"ZooKeeper scripts for Windows don't allow to override ZOOCFG, ZOOCFGDIR, and ZOO_LOG_DIR differently from the ones for Unix/Linux.

Here is my proposal: [https://github.com/sakurai-youhei/zookeeper/commit/2ae3499fcbef3122b040ec7aa309e9f65dbb4ce5] If this looks good to go, I will open a PR after adding Jira ID to the commit message.

Any comments will be greatly appreciated. Thanks.",[],Bug,ZOOKEEPER-4496,Minor,Youhei Sakurai,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"ZOOCFG, ZOOCFGDIR, and ZOO_LOG_DIR are not overridable on Windows",2022-03-16T13:10:52.000+0000,"[<JIRA Version: name='3.8.0', id='12349587'>]",1.0
,[],2022-03-16T11:25:50.000+0000,Ramya Rohidas,Some critical and high vulnerabilities are on high priority for us to be fixed. Attaching the twistlock scan for your reference.,[],Bug,ZOOKEEPER-4495,Critical,Ramya Rohidas,Invalid,2022-03-23T04:10:41.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Twistlock CVEs - Zookeeper latest version 3.6.3,2022-03-23T04:10:41.000+0000,[],2.0
,[],2022-03-14T02:45:47.000+0000,Prem Kumar,"Hi Team,

I have upgraded my zk cluster from 3.4.13 to 3.6.3 and it was successful. when i am trying to rollback from 3.6.3 to 3.4.13, i am facing some issues and it was not successful.

Can someone help me on this issue?

 

*Logs:*

2022-02-10 12:50:37,532 [myid:3] - INFO [main:QuorumPeer@1178] - minSessionTimeout set to 4000
2022-02-10 12:50:37,533 [myid:3] - INFO [main:QuorumPeer@1189] - maxSessionTimeout set to 40000
2022-02-10 12:50:37,540 [myid:3] - INFO [main:QuorumPeer@1467] - QuorumPeer communication is not secured!
2022-02-10 12:50:37,541 [myid:3] - INFO [main:QuorumPeer@1496] - quorum.cnxn.threads.size set to 20
2022-02-10 12:50:37,551 [myid:3] - INFO [main:FileSnap@86] - Reading snapshot /zk/version-2/snapshot.18000000bd
2022-02-10 12:50:37,641 [myid:3] - ERROR [main:QuorumPeer@692] - Unable to load database on disk
*java.io.IOException: Unsupported Txn with type=%d15*
at org.apache.zookeeper.server.util.SerializeUtils.deserializeTxn(SerializeUtils.java:89)
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:641)
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.init(FileTxnLog.java:556)
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.<init>(FileTxnLog.java:531)
at org.apache.zookeeper.server.persistence.FileTxnLog.read(FileTxnLog.java:358)
at org.apache.zookeeper.server.persistence.FileTxnSnapLog.fastForwardFromEdits(FileTxnSnapLog.java:193)
at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:176)
at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:217)
at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:645)
at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:635)
at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:170)
at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:114)
at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:81)
2022-02-10 12:50:37,643 [myid:3] - ERROR [main:QuorumPeerMain@92] - Unexpected exception, exiting abnormally
java.lang.RuntimeException: Unable to run quorum server
at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:693)
at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:635)
at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:170)
at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:114)
at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:81)
*Caused by: java.io.IOException: Unsupported Txn with type=%d15*
at org.apache.zookeeper.server.util.SerializeUtils.deserializeTxn(SerializeUtils.java:89)
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:641)
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.init(FileTxnLog.java:556)
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.<init>(FileTxnLog.java:531)
at org.apache.zookeeper.server.persistence.FileTxnLog.read(FileTxnLog.java:358)
at org.apache.zookeeper.server.persistence.FileTxnSnapLog.fastForwardFromEdits(FileTxnSnapLog.java:193)
at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:176)
at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:217)
at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:645)",[],Bug,ZOOKEEPER-4493,Major,Prem Kumar,Not A Bug,2022-03-14T05:02:25.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZK downgrade from 3.6.3 to 3.4.13 is not working,2022-03-14T05:02:25.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2022-03-06T14:14:04.000+0000,Antonio Antonucci,"Hi everybody, I am having an issue zookeeper 3.6.3

Two things to highlight:
 * 1) The issue I am having is:

INFO Refusing session request for client /192.168.1.163:42230 as it has seen zxid 0xa25 our last zxid is 0x23 client must try another server (org.apache.zookeeper.server.ZooKeeperServer)

>> Initially the zookeeper datadir was configured as per default under /tmp.

>> Following this error I have moved the datadir under /opt/zookeeper, deleted all the logs directory content and rebooted the server. 

>> Soon after the rebooting, the errror message was:

INFO Refusing session request for client /192.168.1.163:41738 as it has seen zxid 0xa25 our last zxid is 0x0 client must try another server (org.apache.zookeeper.server.ZooKeeperServer)

>> However once I started Kafka, the error went back to the original ""... it has seen zxid 0xa25 our last zxid is 0x23.. ""

>> I don't know how to sort this out
 * 2) I have tried to run the ZkWorkarounderMultiThreaded java file (proposed in ZOOKEEPER-832), however I am getting the following errors:

(base) *oo0v0oo@kafka-n1*:*~/zkworkaround/zk-work_around/src*$  java -cp ""/opt/kafka_2.13-3.1.0/libs/*"" ZkWorkarounderMultiThreaded.java 192.168.1.160:2181 /kafka-n1 1 1

0 [pool-1-thread-1] INFO org.apache.zookeeper.ZooKeeper  - Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT

1 [pool-1-thread-1] INFO org.apache.zookeeper.ZooKeeper  - Client environment:host.name=kafka-n1

1 [pool-1-thread-1] INFO org.apache.zookeeper.ZooKeeper  - Client environment:java.version=17.0.1

1 [pool-1-thread-1] INFO org.apache.zookeeper.ZooKeeper  - Client environment:java.vendor=Private Build

1 [pool-1-thread-1] INFO org.apache.zookeeper.ZooKeeper  - Client environment:java.home=/usr/lib/jvm/java-17-openjdk-amd64

1 [pool-1-thread-1] INFO org.apache.zookeeper.ZooKeeper  - Client environment:java.class.path=/opt/kafka_2.13-3.1.0/libs/connect-basic-auth-extension-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/kafka_2.13-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/jackson-core-2.12.3.jar:/opt/kafka_2.13-3.1.0/libs/kafka-streams-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/netty-transport-native-epoll-4.1.68.Final.jar:/opt/kafka_2.13-3.1.0/libs/hk2-utils-2.6.1.jar:/opt/kafka_2.13-3.1.0/libs/zookeeper-3.6.3.jar:/opt/kafka_2.13-3.1.0/libs/jetty-security-9.4.43.v20210629.jar:/opt/kafka_2.13-3.1.0/libs/jetty-util-ajax-9.4.43.v20210629.jar:/opt/kafka_2.13-3.1.0/libs/kafka-clients-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/jersey-container-servlet-2.34.jar:/opt/kafka_2.13-3.1.0/libs/activation-1.1.1.jar:/opt/kafka_2.13-3.1.0/libs/hk2-locator-2.6.1.jar:/opt/kafka_2.13-3.1.0/libs/kafka-metadata-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/netty-buffer-4.1.68.Final.jar:/opt/kafka_2.13-3.1.0/libs/jackson-module-scala_2.13-2.12.3.jar:/opt/kafka_2.13-3.1.0/libs/jetty-servlet-9.4.43.v20210629.jar:/opt/kafka_2.13-3.1.0/libs/maven-artifact-3.8.1.jar:/opt/kafka_2.13-3.1.0/libs/plexus-utils-3.2.1.jar:/opt/kafka_2.13-3.1.0/libs/zstd-jni-1.5.0-4.jar:/opt/kafka_2.13-3.1.0/libs/jersey-container-servlet-core-2.34.jar:/opt/kafka_2.13-3.1.0/libs/connect-api-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/jackson-jaxrs-base-2.12.3.jar:/opt/kafka_2.13-3.1.0/libs/kafka-storage-api-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/javax.servlet-api-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/zookeeper-jute-3.6.3.jar:/opt/kafka_2.13-3.1.0/libs/connect-json-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/jline-3.12.1.jar:/opt/kafka_2.13-3.1.0/libs/netty-handler-4.1.68.Final.jar:/opt/kafka_2.13-3.1.0/libs/kafka-raft-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/javassist-3.27.0-GA.jar:/opt/kafka_2.13-3.1.0/libs/jakarta.inject-2.6.1.jar:/opt/kafka_2.13-3.1.0/libs/metrics-core-4.1.12.1.jar:/opt/kafka_2.13-3.1.0/libs/kafka-streams-scala_2.13-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/jose4j-0.7.8.jar:/opt/kafka_2.13-3.1.0/libs/jersey-hk2-2.34.jar:/opt/kafka_2.13-3.1.0/libs/jaxb-api-2.3.0.jar:/opt/kafka_2.13-3.1.0/libs/connect-transforms-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/kafka-shell-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/snappy-java-1.1.8.4.jar:/opt/kafka_2.13-3.1.0/libs/netty-codec-4.1.68.Final.jar:/opt/kafka_2.13-3.1.0/libs/jackson-dataformat-csv-2.12.3.jar:/opt/kafka_2.13-3.1.0/libs/audience-annotations-0.5.0.jar:/opt/kafka_2.13-3.1.0/libs/kafka-log4j-appender-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/slf4j-log4j12-1.7.30.jar:/opt/kafka_2.13-3.1.0/libs/log4j-1.2.17.jar:/opt/kafka_2.13-3.1.0/libs/scala-reflect-2.13.6.jar:/opt/kafka_2.13-3.1.0/libs/scala-logging_2.13-3.9.3.jar:/opt/kafka_2.13-3.1.0/libs/paranamer-2.8.jar:/opt/kafka_2.13-3.1.0/libs/jakarta.xml.bind-api-2.3.2.jar:/opt/kafka_2.13-3.1.0/libs/rocksdbjni-6.22.1.1.jar:/opt/kafka_2.13-3.1.0/libs/lz4-java-1.8.0.jar:/opt/kafka_2.13-3.1.0/libs/argparse4j-0.7.0.jar:/opt/kafka_2.13-3.1.0/libs/jetty-util-9.4.43.v20210629.jar:/opt/kafka_2.13-3.1.0/libs/jakarta.activation-api-1.2.1.jar:/opt/kafka_2.13-3.1.0/libs/osgi-resource-locator-1.0.3.jar:/opt/kafka_2.13-3.1.0/libs/jetty-server-9.4.43.v20210629.jar:/opt/kafka_2.13-3.1.0/libs/aopalliance-repackaged-2.6.1.jar:/opt/kafka_2.13-3.1.0/libs/trogdor-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/netty-transport-native-unix-common-4.1.68.Final.jar:/opt/kafka_2.13-3.1.0/libs/kafka-streams-test-utils-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/jakarta.validation-api-2.0.2.jar:/opt/kafka_2.13-3.1.0/libs/jopt-simple-5.0.4.jar:/opt/kafka_2.13-3.1.0/libs/netty-transport-4.1.68.Final.jar:/opt/kafka_2.13-3.1.0/libs/jersey-common-2.34.jar:/opt/kafka_2.13-3.1.0/libs/jetty-http-9.4.43.v20210629.jar:/opt/kafka_2.13-3.1.0/libs/jakarta.annotation-api-1.3.5.jar:/opt/kafka_2.13-3.1.0/libs/slf4j-api-1.7.30.jar:/opt/kafka_2.13-3.1.0/libs/commons-lang3-3.8.1.jar:/opt/kafka_2.13-3.1.0/libs/jackson-annotations-2.12.3.jar:/opt/kafka_2.13-3.1.0/libs/netty-resolver-4.1.68.Final.jar:/opt/kafka_2.13-3.1.0/libs/jackson-databind-2.12.3.jar:/opt/kafka_2.13-3.1.0/libs/jersey-server-2.34.jar:/opt/kafka_2.13-3.1.0/libs/metrics-core-2.2.0.jar:/opt/kafka_2.13-3.1.0/libs/jackson-jaxrs-json-provider-2.12.3.jar:/opt/kafka_2.13-3.1.0/libs/netty-common-4.1.68.Final.jar:/opt/kafka_2.13-3.1.0/libs/kafka-server-common-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/reflections-0.9.12.jar:/opt/kafka_2.13-3.1.0/libs/kafka-streams-examples-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/jackson-datatype-jdk8-2.12.3.jar:/opt/kafka_2.13-3.1.0/libs/jakarta.ws.rs-api-2.1.6.jar:/opt/kafka_2.13-3.1.0/libs/jersey-client-2.34.jar:/opt/kafka_2.13-3.1.0/libs/kafka-tools-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/scala-java8-compat_2.13-1.0.0.jar:/opt/kafka_2.13-3.1.0/libs/hk2-api-2.6.1.jar:/opt/kafka_2.13-3.1.0/libs/scala-collection-compat_2.13-2.4.4.jar:/opt/kafka_2.13-3.1.0/libs/jackson-module-jaxb-annotations-2.12.3.jar:/opt/kafka_2.13-3.1.0/libs/scala-library-2.13.6.jar:/opt/kafka_2.13-3.1.0/libs/jetty-io-9.4.43.v20210629.jar:/opt/kafka_2.13-3.1.0/libs/jetty-client-9.4.43.v20210629.jar:/opt/kafka_2.13-3.1.0/libs/commons-cli-1.4.jar:/opt/kafka_2.13-3.1.0/libs/kafka-storage-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/javax.ws.rs-api-2.1.1.jar:/opt/kafka_2.13-3.1.0/libs/jetty-continuation-9.4.43.v20210629.jar:/opt/kafka_2.13-3.1.0/libs/connect-file-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/connect-runtime-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/jetty-servlets-9.4.43.v20210629.jar:/opt/kafka_2.13-3.1.0/libs/connect-mirror-client-3.1.0.jar:/opt/kafka_2.13-3.1.0/libs/connect-mirror-3.1.0.jar

1 [pool-1-thread-1] INFO org.apache.zookeeper.ZooKeeper  - Client environment:java.library.path=/usr/java/packages/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib

1 [pool-1-thread-1] INFO org.apache.zookeeper.ZooKeeper  - Client environment:java.io.tmpdir=/tmp

1 [pool-1-thread-1] INFO org.apache.zookeeper.ZooKeeper  - Client environment:java.compiler=<NA>

1 [pool-1-thread-1] INFO org.apache.zookeeper.ZooKeeper  - Client environment:os.name=Linux

1 [pool-1-thread-1] INFO org.apache.zookeeper.ZooKeeper  - Client environment:os.arch=amd64

1 [pool-1-thread-1] INFO org.apache.zookeeper.ZooKeeper  - Client environment:os.version=5.13.0-30-generic

1 [pool-1-thread-1] INFO org.apache.zookeeper.ZooKeeper  - Client environment:user.name=oo0v0oo

1 [pool-1-thread-1] INFO org.apache.zookeeper.ZooKeeper  - Client environment:user.home=/home/oo0v0oo

1 [pool-1-thread-1] INFO org.apache.zookeeper.ZooKeeper  - Client environment:user.dir=/home/oo0v0oo/zkworkaround/zk-work_around/src

1 [pool-1-thread-1] INFO org.apache.zookeeper.ZooKeeper  - Client environment:os.memory.free=40MB

1 [pool-1-thread-1] INFO org.apache.zookeeper.ZooKeeper  - Client environment:os.memory.max=982MB

1 [pool-1-thread-1] INFO org.apache.zookeeper.ZooKeeper  - Client environment:os.memory.total=64MB

6 [pool-1-thread-1] INFO org.apache.zookeeper.ZooKeeper  - Initiating client connection, connectString=192.168.1.160:2181 sessionTimeout=3000 watcher=ZkWorkarounderMultiThreaded$1@18d72986

9 [pool-1-thread-1] INFO org.apache.zookeeper.common.X509Util  - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation

17 [pool-1-thread-1] INFO org.apache.zookeeper.ClientCnxnSocket  - jute.maxbuffer value is 1048575 Bytes

23 [pool-1-thread-1] INFO org.apache.zookeeper.ClientCnxn  - zookeeper.request.timeout value is 0. feature enabled=false

57 [pool-1-thread-1-SendThread(192.168.1.160:2181)] DEBUG org.apache.zookeeper.SaslServerPrincipal  - Canonicalized address to kafka-n1.homenet.telecomitalia.it

59 [pool-1-thread-1-SendThread(192.168.1.160:2181)] INFO org.apache.zookeeper.ClientCnxn  - Opening socket connection to server kafka-n1.homenet.telecomitalia.it/192.168.1.160:2181.

59 [pool-1-thread-1-SendThread(192.168.1.160:2181)] INFO org.apache.zookeeper.ClientCnxn  - SASL config status: Will not attempt to authenticate using SASL (unknown error)

71 [pool-1-thread-1-SendThread(192.168.1.160:2181)] INFO org.apache.zookeeper.ClientCnxn  - Socket connection established, initiating session, client: /192.168.1.160:54046, server: kafka-n1.homenet.telecomitalia.it/192.168.1.160:2181

72 [pool-1-thread-1-SendThread(192.168.1.160:2181)] DEBUG org.apache.zookeeper.ClientCnxn  - Session establishment request sent on kafka-n1.homenet.telecomitalia.it/192.168.1.160:2181

75 [pool-1-thread-1-SendThread(192.168.1.160:2181)] INFO org.apache.zookeeper.ClientCnxn  - Session establishment complete on server kafka-n1.homenet.telecomitalia.it/192.168.1.160:2181, session id = 0x100009333020014, negotiated timeout = 6000

81 [pool-1-thread-1-SendThread(192.168.1.160:2181)] DEBUG org.apache.zookeeper.ClientCnxn  - Reading reply session id: 0x100009333020014, packet:: clientPath:null serverPath:null finished:false header:: 1,1  replyHeader:: 1,101,-110  request:: '/kafka-n1,,v\{s{31,s{'world,'anyone}}},0  response::  

105 [pool-1-thread-1-SendThread(192.168.1.160:2181)] DEBUG org.apache.zookeeper.ClientCnxn  - Reading reply session id: 0x100009333020014, packet:: clientPath:null serverPath:null finished:false header:: 2,1  replyHeader:: 2,102,0  request:: '/kafka-n1/ZkWorkarounderMultiThreaded@2b344318,,v\{s{31,s{'world,'anyone}}},0  response:: '/kafka-n1/ZkWorkarounderMultiThreaded@2b344318 

109 [pool-1-thread-1-SendThread(192.168.1.160:2181)] DEBUG org.apache.zookeeper.ClientCnxn  - Reading reply session id: 0x100009333020014, packet:: clientPath:null serverPath:null finished:false header:: 3,3  replyHeader:: 3,102,0  request:: '/kafka-n1/ZkWorkarounderMultiThreaded@2b344318,T  response:: s\{102,102,1646588048633,1646588048633,0,0,0,0,0,0,102} 

111 [pool-1-thread-1-SendThread(192.168.1.160:2181)] DEBUG org.apache.zookeeper.ClientCnxn  - Got notification session id: 0x100009333020014

112 [pool-1-thread-1-SendThread(192.168.1.160:2181)] DEBUG org.apache.zookeeper.ClientCnxn  - Got WatchedEvent state:SyncConnected type:NodeDataChanged path:/kafka-n1/ZkWorkarounderMultiThreaded@2b344318 for session id 0x100009333020014

114 [pool-1-thread-1-SendThread(192.168.1.160:2181)] DEBUG org.apache.zookeeper.ClientCnxn  - Reading reply session id: 0x100009333020014, packet:: clientPath:null serverPath:null finished:false header:: 4,5  replyHeader:: 4,103,0  request:: '/kafka-n1/ZkWorkarounderMultiThreaded@2b344318,,0  response:: s\{102,103,1646588048633,1646588048641,1,0,0,0,0,0,102} 

118 [pool-1-thread-1-SendThread(192.168.1.160:2181)] DEBUG org.apache.zookeeper.ClientCnxn  - Reading reply session id: 0x100009333020014, packet:: clientPath:null serverPath:null finished:false header:: 5,3  replyHeader:: 5,103,0  request:: '/kafka-n1/ZkWorkarounderMultiThreaded@2b344318,T  response:: s\{102,103,1646588048633,1646588048641,1,0,0,0,0,0,102} 

120 [pool-1-thread-1-SendThread(192.168.1.160:2181)] DEBUG org.apache.zookeeper.ClientCnxn  - Got notification session id: 0x100009333020014

120 [pool-1-thread-1-SendThread(192.168.1.160:2181)] DEBUG org.apache.zookeeper.ClientCnxn  - Got WatchedEvent state:SyncConnected type:NodeDeleted path:/kafka-n1/ZkWorkarounderMultiThreaded@2b344318 for session id 0x100009333020014

121 [pool-1-thread-1-SendThread(192.168.1.160:2181)] DEBUG org.apache.zookeeper.ClientCnxn  - Reading reply session id: 0x100009333020014, packet:: clientPath:null serverPath:null finished:false header:: 6,2  replyHeader:: 6,104,0  request:: '/kafka-n1/ZkWorkarounderMultiThreaded@2b344318,1  response:: null

121 [pool-1-thread-1] DEBUG org.apache.zookeeper.ZooKeeper  - Closing session: 0x100009333020014

121 [pool-1-thread-1] DEBUG org.apache.zookeeper.ClientCnxn  - Closing client for session: 0x100009333020014

122 [pool-1-thread-1-SendThread(192.168.1.160:2181)] DEBUG org.apache.zookeeper.ClientCnxn  - Reading reply session id: 0x100009333020014, packet:: clientPath:null serverPath:null finished:false header:: 7,-11  replyHeader:: 7,105,0  request:: null response:: null

123 [pool-1-thread-1] DEBUG org.apache.zookeeper.ClientCnxn  - Disconnecting client for session: 0x100009333020014

123 [pool-1-thread-1-SendThread(192.168.1.160:2181)] WARN org.apache.zookeeper.ClientCnxn  - An exception was thrown while closing send thread for session 0x100009333020014.

EndOfStreamException: Unable to read additional data from server sessionid 0x100009333020014, likely server has closed socket

 at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:77)

 at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)

 at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)

228 [pool-1-thread-1] INFO org.apache.zookeeper.ZooKeeper  - Session: 0x100009333020014 closed

228 [pool-1-thread-1-EventThread] INFO org.apache.zookeeper.ClientCnxn  - EventThread shut down for session: 0x100009333020014

>> I don't literally know how to sort this out.

>> Can somebody help me please?",[],Bug,ZOOKEEPER-4489,Blocker,Antonio Antonucci,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Refusing session request for client: zxid misalignment,2022-04-05T03:34:04.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>]",2.0
,[],2022-03-04T12:21:00.000+0000,caoguangjie,"|h2. CVE-2022-20621 Detail
h3. Current Description
Jenkins Metrics Plugin 4.0.2.8 and earlier stores an access key unencrypted in its global configuration file on the Jenkins controller where it can be viewed by users with access to the Jenkins controller file system.

https://nvd.nist.gov/vuln/detail/CVE-2022-20621

|",[],Bug,ZOOKEEPER-4486,Major,caoguangjie,Invalid,2022-03-04T14:19:39.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,The metrics-core-3.2.5.jar on which zookeeper depends has the open-source vulnerability CVE-2022-20621,2022-03-04T14:19:39.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>]",1.0
,[],2022-03-03T22:48:15.000+0000,Ricardo R,"The class org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider responds to TRACE http method, Because of that we have some issuses with the security scan.",[],Bug,ZOOKEEPER-4485,Major,Ricardo R,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,PrometheusMetricsProvider class shouldn't respond to TRACE Method,2022-03-03T22:48:28.000+0000,[],1.0
,[],2022-03-03T04:07:19.000+0000,Debanjan Bhowmick,"We have found this below list of CRITICAL Security vulnerabilties present in the official zookeper image -


||Vulnerability ID||Component||Infected versions||Fixed versions||
|CVE-2021-33574|debian:bullseye:libc6:2.31-13+deb11u2|N/A|N/A|
|XRAY-179837|io.netty:netty-codec:4.1.59.Final|< 4.1.66.Final|4.1.66.Final|
|CVE-2022-23307|log4j:log4j:1.2.17|All Versions|N/A|
|CVE-2019-17571|log4j:log4j:1.2.17|≤ 1.2.17|N/A|
|CVE-2022-23305|log4j:log4j:1.2.17|1.1.0 ≤ Version ≤ 1.2.17|N/A|
|CVE-2022-23219|debian:bullseye:libc6:2.31-13+deb11u2|N/A|N/A|
|CVE-2022-23218|debian:bullseye:libc6:2.31-13+deb11u2|N/A|N/A|


Can you please help us with the fix or update us on the release of security patches and also their respective timelines.

 ",[],Bug,ZOOKEEPER-4484,Critical,Debanjan Bhowmick,Invalid,2022-03-03T07:14:20.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Critical Security Vulnerabilities in Apache Zookeper image,2022-03-07T08:34:07.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",2.0
,"[<JIRA Component: name='security', id='12329414'>]",2022-02-25T02:51:11.000+0000,Gaurav Jetly,"There are vulnerabilities with  4.1.63 version of  io.netty_netty-codec. Please help upgrade the dependency to a vulnerability free version.

Associated vulnerabilities:
CVE-2021-37136
CVE-2021-37137
CVE-2021-43797",[],Bug,ZOOKEEPER-4481,Major,Gaurav Jetly,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Upgrade vulnerable dependencies,2022-02-25T02:51:11.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>]",1.0
Mate Szalay-Beko,"[<JIRA Component: name='kerberos', id='12329415'>]",2022-02-23T07:25:51.000+0000,Vincent Grivel,"Zookeeper refresh thread for Kerberos have the same problem in the reLogin() [https://github.com/apache/zookeeper/blob/release-3.5.5/zookeeper-server/src/main/java/org/apache/zookeeper/Login.java#L413]  function as describe in https://issues.apache.org/jira/browse/KAFKA-12730
{quote}The refresh thread for Kerberos performs re-login by logging out and then logging in again. If login fails, we retry after a backoff. Every iteration of the loop performs loginContext.logout() and loginContext.login(). If login fails, we end up with two consecutive logouts. This used to work, but from Java 9 onwards, this results in a NullPointerException due to [https://bugs.openjdk.java.net/browse/JDK-8173069]. We should check if logout is required before attempting logout.
{quote}
 

A NPE is throw if multiple logout() is invoke multiple times: 
{code:java}
2022-02-14 18:38:11,899 ERROR org.apache.zookeeper.Login: Failed to refresh TGT: refresh thread exiting now.
javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input(s)
    at java.base/java.util.Objects.requireNonNull(Objects.java:246)
    at java.base/javax.security.auth.Subject$SecureSet.remove(Subject.java:1172)
    at java.base/java.util.Collections$SynchronizedCollection.remove(Collections.java:2043)
    at jdk.security.auth/com.sun.security.auth.module.Krb5LoginModule.logout(Krb5LoginModule.java:1202)
    at java.base/javax.security.auth.login.LoginContext.invoke(LoginContext.java:732)
    at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:665)
    at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:663)
    at java.base/java.security.AccessController.doPrivileged(Native Method)
    at java.base/javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:663)
    at java.base/javax.security.auth.login.LoginContext.logout(LoginContext.java:613)
    at org.apache.zookeeper.Login.reLogin(Login.java:413)
    at org.apache.zookeeper.Login.access$500(Login.java:49)
    at org.apache.zookeeper.Login$1.run(Login.java:240)
    at java.base/java.lang.Thread.run(Thread.java:834)
    at java.base/javax.security.auth.login.LoginContext.invoke(LoginContext.java:821)
    at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:665)
    at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:663)
    at java.base/java.security.AccessController.doPrivileged(Native Method)
    at java.base/javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:663)
    at java.base/javax.security.auth.login.LoginContext.logout(LoginContext.java:613)
    at org.apache.zookeeper.Login.reLogin(Login.java:413)
    at org.apache.zookeeper.Login.access$500(Login.java:49)
    at org.apache.zookeeper.Login$1.run(Login.java:240)
    at java.base/java.lang.Thread.run(Thread.java:834) {code}","[<JIRA Version: name='3.5.10', id='12349434'>, <JIRA Version: name='3.7.1', id='12350030'>, <JIRA Version: name='3.6.4', id='12350076'>, <JIRA Version: name='3.9.0', id='12351304'>, <JIRA Version: name='3.8.1', id='12351398'>]",Bug,ZOOKEEPER-4477,Minor,Vincent Grivel,Fixed,2022-03-01T15:41:05.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Single Kerberos ticket renewal failure can prevent all future renewals since Java 9,2022-03-02T09:32:51.000+0000,"[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>]",4.0
Kezhu Wang,[],2022-02-22T10:43:10.000+0000,Kezhu Wang,"Currently, {{NodeChildrenChanged}} events are sent to all persistent watchers unconditionally in client. This requires server to never deliver {{NodeChildrenChanged}} for node's descendants. It can not be true. So we need to filter {{NodeChildrenChanged}} for persistent recursive watchers.

Splits from ZOOKEEPER-4466.",[],Bug,ZOOKEEPER-4475,Major,Kezhu Wang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Persistent recursive watcher got NodeChildrenChanged event,2022-03-09T03:16:43.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>]",1.0
,"[<JIRA Component: name='contrib', id='12312700'>]",2022-02-22T03:08:41.000+0000,qiang Liu,"create root node using zoo inspector will fail with an exception, 
{code:java}
java.lang.IllegalArgumentException: Invalid path string ""//test"" caused by empty node name specified @1
{code}
and what's worse is the UI will be updated, even creation failed","[<JIRA Version: name='3.9.0', id='12351304'>, <JIRA Version: name='3.8.1', id='12351398'>]",Bug,ZOOKEEPER-4473,Major,qiang Liu,Fixed,2022-03-04T13:58:27.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zooInspector create root node fail with path validate,2022-03-04T13:59:42.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",2.0
Kezhu Wang,"[<JIRA Component: name='server', id='12312382'>]",2022-02-21T04:05:06.000+0000,Kezhu Wang,"{{AddWatchMode.PERSISTENT}} was divided as data and child watch in server side. When remove {{WatcherType.Children}}, child part of {{AddWatchMode.PERSISTENT}} is removed but not its data part. This could introduce trick usage of persistent data watch while there is no official api for this. It is better forbid this by dedicate {{WatcherType.Children}} to standard child watch only.

I [commits|https://github.com/kezhuw/zookeeper/commit/f7a996646074114830bdc2361e8ff679d08c00bc] a modified {{RemoveWatchesTest.testRemoveAllChildWatchesOnAPath}} in my local repo to reproduce this.

I think it is better to support {{removeWatches}} for two persistent watchers too. But it might be a separate issue.",[],Bug,ZOOKEEPER-4471,Major,Kezhu Wang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Remove WatcherType.Children break persistent watcher's child events,2022-02-22T02:40:44.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>]",1.0
Norbert Kalmár,"[<JIRA Component: name='build', id='12312383'>]",2022-02-17T17:05:27.000+0000,Kevin Beck,"Downloads cannot be verified because the PGP keys on the download site have expired.

Keys are located here:

[https://downloads.apache.org/zookeeper/KEYS]

 

kbeck@KBECK-C02CF67PMD6R Downloads % gpg --verify apache-zookeeper-3.5.9-bin.tar.gz.asc apache-zookeeper-3.5.9-bin.tar.gz
gpg: Signature made Wed Jan  6 11:55:29 2021 PST
gpg:                using RSA key 3D296268A36FACA1B7EAF110792D43153B5B5147
gpg: Good signature from ""Norbert Kalmar <nkalmar@apache.org>"" [expired]
gpg: Note: This key has expired!
Primary key fingerprint: 3D29 6268 A36F ACA1 B7EA  F110 792D 4315 3B5B 5147
kbeck@KBECK-C02CF67PMD6R Downloads %",[],Bug,ZOOKEEPER-4470,Blocker,Kevin Beck,Done,2022-03-28T13:46:43.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Keys for validating downloads of apache-zookeeper-3.5.9 have expired,2022-03-28T13:46:43.000+0000,"[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.6.2', id='12347809'>]",3.0
Kezhu Wang,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2022-02-11T04:09:58.000+0000,Kezhu Wang,"I used to think watchers of different modes are orthogonal. I found there are not, when I wrote tests for unfinished rust client. And I wrote [test cases|https://github.com/kezhuw/zookeeper/commit/79b05a95d2669a4acd16a4d544f24e2083a264f2#diff-8d31d27ea951fbc1f4fbda48d45748318f7124502839d825b77ad3fb8551bf43L152] in java and confirmed.

I copied test case here for evaluation. You also clone from [my fork|https://github.com/kezhuw/zookeeper/tree/watch-overlapping-path-with-different-modes-test-case].

{code:java}
    // zookeeper-server/src/test/java/org/apache/zookeeper/test/PersistentRecursiveWatcherTest.java

    @Test
    public void testPathOverlapWithStandardWatcher() throws Exception {
        try (ZooKeeper zk = createClient(new CountdownWatcher(), hostPort)) {
            CountDownLatch nodeCreated = new CountDownLatch(1);
            zk.addWatch(""/a"", persistentWatcher, PERSISTENT_RECURSIVE);
            zk.exists(""/a"", event -> nodeCreated.countDown());

            zk.create(""/a"", new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
            zk.create(""/a/b"", new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
            zk.delete(""/a/b"", -1);
            zk.delete(""/a"", -1);

            assertEvent(events, Watcher.Event.EventType.NodeCreated, ""/a"");
            assertEvent(events, Watcher.Event.EventType.NodeCreated, ""/a/b"");
            assertEvent(events, Watcher.Event.EventType.NodeDeleted, ""/a/b"");
            assertEvent(events, Watcher.Event.EventType.NodeDeleted, ""/a"");

            assertTrue(nodeCreated.await(5, TimeUnit.SECONDS));
        }
    }

    @Test
    public void testPathOverlapWithPersistentWatcher() throws Exception {
        try (ZooKeeper zk = createClient(new CountdownWatcher(), hostPort)) {
            zk.addWatch(""/a"", persistentWatcher, PERSISTENT_RECURSIVE);
            zk.addWatch(""/a/b"", event -> {}, PERSISTENT);
            zk.create(""/a"", new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
            zk.create(""/a/b"", new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
            zk.create(""/a/b/c"", new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
            zk.delete(""/a/b/c"", -1);
            zk.delete(""/a/b"", -1);
            zk.delete(""/a"", -1);
            assertEvent(events, Watcher.Event.EventType.NodeCreated, ""/a"");
            assertEvent(events, Watcher.Event.EventType.NodeCreated, ""/a/b"");
            assertEvent(events, Watcher.Event.EventType.NodeCreated, ""/a/b/c"");
            assertEvent(events, Watcher.Event.EventType.NodeDeleted, ""/a/b/c"");
            assertEvent(events, Watcher.Event.EventType.NodeDeleted, ""/a/b"");
            assertEvent(events, Watcher.Event.EventType.NodeDeleted, ""/a"");
        }
    }
{code}

I skimmed the code and found two possible causes:
# {{ZKWatchManager.materialize}} materializes all persistent watchers(include recursive ones) for {{NodeChildrenChanged}} event.
# {{WatcherModeManager}} trackes only one watcher mode.",[],Bug,ZOOKEEPER-4466,Major,Kezhu Wang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Watchers of different modes interfere on overlapping pathes,2022-02-22T10:50:10.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7', id='12349953'>, <JIRA Version: name='3.6.4', id='12350076'>]",3.0
,"[<JIRA Component: name='contrib', id='12312700'>]",2022-02-10T01:20:25.000+0000,qiang Liu,"zooinspect logback config file logback.xml currently use a pattern of this

 
{code:java}
<pattern>%5p [%t] (%F:%L) - %m%n</pattern> {code}
which not escape the '(' and ')', cause logback to ignore parts after ')'.

 

 

according to logback documents, '(' and ')' is used for grouping, need escape by '\' if used as normal char

[https://logback.qos.ch/manual/layouts.html#grouping|https://logback.qos.ch/manual/layouts.html#grouping] 

 

will create pr update it to 
{code:java}
<pattern>%5p [%t] \(%F:%L\) - %m%n</pattern> {code}
 

 ","[<JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-4465,Major,qiang Liu,Fixed,2022-02-15T11:31:12.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zooinspector logback pattern config add escape for '(' and ')',2022-02-15T18:54:34.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2022-01-28T08:13:16.000+0000,Alan Bateman,"In OpenJDK, Project Loom has significantly re-implemented java.lang.Thread. One part of this is using some of the bits in the thread identifier for non-exposed purposes. Sadly, Thread::getId is not final and it's possible that sub-classes of Thread have overridden getId to have different semantics. The JDK can defend against this but there may be 3rd party libraries that make use of Thread::getId. A corpus search of Maven central found only one class: org.apache.zookeeper.server.quorum.QuorumPeer. Does this project know why getId has been overridden to return something that is not the thread identifier?",[],Bug,ZOOKEEPER-4460,Major,Alan Bateman,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,QuorumPeer overrides Thread.getId with different semantics,2022-01-28T08:13:16.000+0000,[],1.0
,"[<JIRA Component: name='server', id='12312382'>]",2022-01-28T07:14:54.000+0000,Anoop Negi,"Information logs in zkServer.sh script being redirected to *stderr* (2) stream
{code:java}
/opt/apache-zookeeper-3.7.0-bin/bin
bash-4.4$ cat zkServer.sh | grep ""&2""
  echo ""ZooKeeper JMX enabled by default"" >&2
    echo ""ZooKeeper remote JMX Port set to $JMXPORT"" >&2
    echo ""ZooKeeper remote JMX authenticate set to $JMXAUTH"" >&2
    echo ""ZooKeeper remote JMX ssl set to $JMXSSL"" >&2
    echo ""ZooKeeper remote JMX log4j set to $JMXLOG4J"" >&2
      echo ""ZooKeeper remote JMX Hostname set to $JMXHOSTNAME"" >&2
    echo ""JMX disabled by user request"" >&2
echo ""Using config: $ZOOCFG"" >&2
    echo ""Usage: $0 [--config <conf-dir>] {start|start-foreground|stop|version|restart|status|print-cmd}"" >&2
bash-4.4$ {code}
and in container logs we could see these logs are coming as *stderr*
{code:java}
2022-01-19T08:16:34.393866602+01:00 stderr F ZooKeeper JMX enabled by default
2022-01-19T08:16:34.395496162+01:00 stderr F Using config: /var/lib/zookeeper/config-dynamic/zoo.cfg
 {code}
*Expectation:*

Information logs should coming in stdout.",[],Bug,ZOOKEEPER-4459,Major,Anoop Negi,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Info logs in zkServer.sh redirected to stderr,2022-01-28T07:18:55.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",1.0
,"[<JIRA Component: name='security', id='12329414'>]",2022-01-27T16:06:13.000+0000,Anisha K J,"Hello everyone,

I work for a product which uses apache/zookeeper 3.6.3.  We scanned our product with a security scanner which reported below security issues. After analysis we found that this vulnerability is coming from zookeeper 3.6.3 because of direct dependency on  jetty-io-9.4.39.v20210325.jar
|Jetty: Java based HTTP/1.x, HTTP/2, Servlet, WebSocket Server|9.4.39.v20210325|CVE-2021-34429 |MEDIUM|5.3|MEDIUM|
|Jetty: Java based HTTP/1.x, HTTP/2, Servlet, WebSocket Server|9.4.39.v20210325|CVE-2021-34428 |LOW|3.5|LOW|
|Jetty: Java based HTTP/1.x, HTTP/2, Servlet, WebSocket Server|9.4.39.v20210325|CVE-2021-28169 |MEDIUM|5.3|MEDIUM|

Could you please let us know is there any plan to update jetty in coming versions",[],Bug,ZOOKEEPER-4457,Major,Anisha K J,Duplicate,2022-02-01T23:37:05.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Jetty used by zookeeper 3.6.3 is vulnerable to CVE-2021-34429 ,2022-02-01T23:37:05.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>]",1.0
,"[<JIRA Component: name='security', id='12329414'>]",2022-01-27T15:56:05.000+0000,Anisha K J,"Hello everyone,

I work for a product which uses apache/zookeeper 3.6.3.  We scanned our product with a security scanner which reported CVE-2019-17571, CVE-2021-37137, CVE-2021-37136

After analysis we found that this vulnerability is coming from zookeeper 3.6.3 because of direct dependency on netty-buffer-4.1.63.Final.jar

 Could you please let us know is there any plan to update netty in coming versions",[],Bug,ZOOKEEPER-4456,Major,Anisha K J,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Netty used by zookeeper 3.6.3 is vulnerable to CVE-2021-43797 ,2022-02-01T23:26:12.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>]",2.0
Enrico Olivelli,"[<JIRA Component: name='security', id='12329414'>]",2022-01-25T14:24:38.000+0000,Dominique Mongelli,"Some log4j 1.x vulnerabilities have been disclosed recently:   
 * CVE-2022-23302: [https://nvd.nist.gov/vuln/detail/CVE-2022-23302]    
 * CVE-2022-23305 : [https://nvd.nist.gov/vuln/detail/CVE-2022-23305]    
 * CVE-2022-23307 : [https://nvd.nist.gov/vuln/detail/CVE-2022-23307]

We would like to know if zookeeper is affected by these vulnerabilities ?","[<JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>, <JIRA Version: name='3.6.4', id='12350076'>]",Bug,ZOOKEEPER-4452,Major,Dominique Mongelli,Fixed,2022-03-29T07:42:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Log4j 1.X CVE-2022-23302/5/7 vulnerabilities,2022-03-29T07:43:01.000+0000,[],5.0
,[],2022-01-25T11:31:00.000+0000,Nagalakshmi Nagaraj,"Even the latest version of Zookeeper (3.7.0) is still using the vulnerable version of log4j

log4j-1.2.15.jar

log4j-1.2.16.jar

 

We require apache Zookeeper tar with the Fix version of log4j (2.17.1)",[],Bug,ZOOKEEPER-4451,Critical,Nagalakshmi Nagaraj,Duplicate,2022-01-25T12:19:30.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,vulnerable version of log4j (1.2.15/12.16) is being used in Zookeeper ,2022-01-25T12:19:30.000+0000,[],1.0
Mohammad Arshad,"[<JIRA Component: name='audit', id='12336158'>]",2022-01-25T11:11:06.000+0000,Dilip anand,"Hello Team,

 

We are currently using Zookeeper of 3.4.6 and found the below log4j security vulnarbilty. 

 

The sad part is zookeeper is using too old log4j jar file and the fixed version of log4j is 2.16.0.

 

Can we get the ""log4j"" fixed version of zookeeper as soon as possible to include it in the production setup? 

 

Nessus scan report::

---------------------

Path : /opt/zookeeper/zookeeper-3.4.10/bin/../lib/log4j-1.2.16.jar Installed version : 1.2.16 Fixed version : 2.16.0

Path : /opt/zookeeper/zookeeper-3.4.10/contrib/rest/lib/log4j-1.2.15.jar Installed version : 1.2.15 Fixed version : 2.16.0

Path : /opt/zookeeper/zookeeper-3.4.10/lib/log4j-1.2.16.jar Installed version : 1.2.16 Fixed version : 2.16.0

 

Regards,

Anandaa",[],Bug,ZOOKEEPER-4450,Major,Dilip anand,Duplicate,2022-01-25T12:20:07.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper 3.7.0 is using Vulnerable log4j of 1.2.17,2022-01-25T12:20:07.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",1.0
,[],2022-01-24T09:30:00.000+0000,lanzhiwang,"Deploy kafka and zookeeper in k8s, and use the following configuration to output monitoring metrics

 
{code:java}
-javaagent:/opt/kafka/libs/jmx_prometheus_javaagent-0.12.0.jar=9404:/opt/kafka/custom-config/metrics-config.yml {code}
 

 

The metrics configuration is as follows:

 
{code:java}
$ cat /opt/kafka/custom-config/metrics-config.yml
{
    ""lowercaseOutputName"":true,
    ""rules"":[
       
{             ""name"":""zookeeper_$2"",             ""pattern"":""org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\d+)><>(\\w+)"",             ""type"":""GAUGE""         }
,
        {
            ""labels"":
{                 ""replicaId"":""$2""             }
,
            ""name"":""zookeeper_$3"",
            ""pattern"":""org.apache.ZooKeeperService<name0=ReplicatedServer_id(d+), name1=replica.(\\d+)><>(w+)"",
            ""type"":""GAUGE""
        },
        {
            ""labels"":
{                 ""memberType"":""$3"",                 ""replicaId"":""$2""             }
,
            ""name"":""zookeeper_$4"",
            ""pattern"":""org.apache.ZooKeeperService<name0=ReplicatedServer_id(d+), name1=replica.(d+), name2=(w+)><>(Packets.*)"",
            ""type"":""COUNTER""
        },
        {
            ""labels"":
{                 ""memberType"":""$3"",                 ""replicaId"":""$2""             }
,
            ""name"":""zookeeper_$4"",
            ""pattern"":""org.apache.ZooKeeperService<name0=ReplicatedServer_id(d+), name1=replica.(d+), name2=(\\w+)><>(w+)"",
            ""type"":""GAUGE""
        },
        {
            ""labels"":
{                 ""memberType"":""$3"",                 ""replica"":""$2""             }
,
            ""name"":""zookeeper_$5"",
            ""pattern"":""org.apache.ZooKeeperService<name0=(.).name1=(.).name2=(.).name3=(.)><>(PacketsReceived)"",
            ""type"":""COUNTER""
        },
        {
            ""labels"":
{                 ""memberType"":""$3"",                 ""replica"":""$2""             }
,
            ""name"":""zookeeper_$5"",
            ""pattern"":""org.apache.ZooKeeperService<name0=(.).name1=(.).name2=(.).name3=(.)><>(PacketsSent)"",
            ""type"":""COUNTER""
        },
        {
            ""labels"":
{                 ""memberType"":""$3"",                 ""replicaId"":""$2""             }
,
            ""name"":""zookeeper_$4_$5"",
            ""pattern"":""org.apache.ZooKeeperService<name0=ReplicatedServer_id(d+), name1=replica.(d+), name2=(w+), name3=(\\w+)><>(w+)"",
            ""type"":""GAUGE""
        },
       
{             ""name"":""zookeeper_$2"",             ""pattern"":""org.apache.ZooKeeperService<name0=StandaloneServer_port(\\d+)><>(\\w+)"",             ""type"":""GAUGE""         }
,
       
{             ""name"":""zookeeper_$2"",             ""pattern"":""org.apache.ZooKeeperService<name0=StandaloneServer_port(\\d+), name1=InMemoryDataTree><>(\\w+)"",             ""type"":""GAUGE""         }
    ]
} {code}
 

 

At this time, there are some very strange monitoring metrics in promethues

!image-2022-01-24-17-32-33-873.png!

 

The jmx exporter version used is 0.12.0

What is the problem at this time, have you encountered it?","[<JIRA Version: name='3.5.8', id='12346950'>]",Bug,ZOOKEEPER-4449,Critical,lanzhiwang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zookeeper monitoring metrics error,2022-01-24T09:32:44.000+0000,"[<JIRA Version: name='3.5.8', id='12346950'>]",1.0
,"[<JIRA Component: name='security', id='12329414'>]",2022-01-21T14:21:05.000+0000,Margarita Stoilova,"Netty component needs to be upgraded to version higher than 4.1.7.1.Final to address CVE-2021-43797.

[https://nvd.nist.gov/vuln/detail/CVE-2021-43797]

 ",[],Bug,ZOOKEEPER-4448,Major,Margarita Stoilova,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,CVE-2021-43797 in netty 4.1.59.Final in zookeeper 3.7.0,2022-02-01T23:26:29.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2022-01-19T13:20:16.000+0000,Phang Khar Yeow,"The issue is due to [https://github.com/apache/zookeeper/blob/branch-3.6/zookeeper-server/src/test/java/org/apache/zookeeper/server/TxnLogCountTest.java]
{code:java}
import static org.junit.jupiter.api.Assertions.assertEquals; {code}
Cause the compilation error, since Junit 4 still in used.
{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.1:testCompile (default-testCompile) on project zookeeper: Compilation failure: Compilation failure: 

  

  
    
    [ERROR] /home/runner/work/zookeeper/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/TxnLogCountTest.java:[21,36] package org.junit.jupiter.api does not exist

  

  
    
    [ERROR] /home/runner/work/zookeeper/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/TxnLogCountTest.java:[21,1] static import only from classes and interfaces

  

  
    
    [ERROR] /home/runner/work/zookeeper/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/TxnLogCountTest.java:[49,9] cannot find symbol

  

  
    
    [ERROR]   symbol:   method assertEquals(int,int)

  

  
    
    [ERROR]   location: class org.apache.zookeeper.server.TxnLogCountTest

  

  
    
    [ERROR] /home/runner/work/zookeeper/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/TxnLogCountTest.java:[52,9] cannot find symbol

  

  
    
    [ERROR]   symbol:   method assertEquals(int,int)

  

  
    
    [ERROR]   location: class org.apache.zookeeper.server.TxnLogCountTest

  

  
    
    [ERROR] -> [Help 1] {code}
 

 ","[<JIRA Version: name='3.6.4', id='12350076'>]",Bug,ZOOKEEPER-4446,Minor,Phang Khar Yeow,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,branch-3.6 txnLogCountTest use wrong version of Junit Assert import,2022-01-26T08:33:03.000+0000,"[<JIRA Version: name='3.6.4', id='12350076'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2022-01-19T13:20:07.000+0000,Phang Khar Yeow,"The issue is due to [https://github.com/apache/zookeeper/blob/branch-3.6/zookeeper-server/src/test/java/org/apache/zookeeper/server/TxnLogCountTest.java]
{code:java}
import static org.junit.jupiter.api.Assertions.assertEquals; {code}
Cause the compilation error, since Junit 4 still in used.
{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.1:testCompile (default-testCompile) on project zookeeper: Compilation failure: Compilation failure: 

  

  
    
    [ERROR] /home/runner/work/zookeeper/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/TxnLogCountTest.java:[21,36] package org.junit.jupiter.api does not exist

  

  
    
    [ERROR] /home/runner/work/zookeeper/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/TxnLogCountTest.java:[21,1] static import only from classes and interfaces

  

  
    
    [ERROR] /home/runner/work/zookeeper/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/TxnLogCountTest.java:[49,9] cannot find symbol

  

  
    
    [ERROR]   symbol:   method assertEquals(int,int)

  

  
    
    [ERROR]   location: class org.apache.zookeeper.server.TxnLogCountTest

  

  
    
    [ERROR] /home/runner/work/zookeeper/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/TxnLogCountTest.java:[52,9] cannot find symbol

  

  
    
    [ERROR]   symbol:   method assertEquals(int,int)

  

  
    
    [ERROR]   location: class org.apache.zookeeper.server.TxnLogCountTest

  

  
    
    [ERROR] -> [Help 1] {code}
 

 ","[<JIRA Version: name='3.6.4', id='12350076'>]",Bug,ZOOKEEPER-4445,Minor,Phang Khar Yeow,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,branch-3.6 txnLogCountTest use wrong version of Junit Assert import,2022-01-21T17:23:49.000+0000,"[<JIRA Version: name='3.6.4', id='12350076'>]",2.0
,[],2022-01-18T12:02:48.000+0000,Andreas Weber,"Hi folks, I've got an issue with 3.6.3.
I can't provide a simple test, because it seems to depend on timing in a cluster environment, but I tried to reduce the scenario as far as possible:
 * zookeeper cluster with 5 nodes, all of them Followers (no Observers)
 * start some parallel threads which do some writes in the cluster (e.g. create/delete znodes)
 * kill one of the zookeeper processes, let's say on node X (where node X is not the Leader)
 * restart zookeeper process on node X
 * wait a few seconds
 * kill zookeeper process on node X again
 * restart zookeeper process on node X again

In some cases (every 3-4 runs) I see the following in the log of node X:

After first restart of node X:
{noformat}
 WARN  persistence.FileTxnLog           - Current zxid 4294968525 is <= 4294969524 for 15
 WARN  persistence.FileTxnLog           - Current zxid 4294968526 is <= 4294969524 for 15
 WARN  persistence.FileTxnLog           - Current zxid 4294968527 is <= 4294969524 for 15
 ... (this kind of WARN is repeated some hundred times)
 WARN  quorum.SendAckRequestProcessor   - Closing connection to leader, exception during packet send java.net.SocketException: Socket closed ...
 ... (this kind of WARN is repeated some hundred times)
{noformat}
After second restart of node X:
{noformat}
 ERROR persistence.FileTxnSnapLog       - 4294970146(highestZxid) > 4294969147(next log) for type 2
 WARN  server.DataTree                  - Message:Digests are not matching. Value is Zxid. Value:4294969147
 ERROR server.DataTree                  - First digest mismatch on txn: 360466402305310720,3870,4294969147,1639258399998,2
, ...
, expected digest is 2,1365261838770
, actual digest is 1098406565142, 
 ERROR persistence.FileTxnSnapLog       - 4294970146(highestZxid) > 4294969148(next log) for type 2
 ERROR persistence.FileTxnSnapLog       - 4294970146(highestZxid) > 4294969149(next log) for type 5
 ERROR persistence.FileTxnSnapLog       - 4294970146(highestZxid) > 4294969150(next log) for type 2
 ... (this kind of ERROR is repeated some hundred times)
{noformat}
And afterwards (in the actual application), zookepeer on node X seems to have a different view of the cluster state and doesn't get synchronized, at least for a few hours.
This e.g. leads to phantom reads of znodes that were deleted a long time ago.
(The resulting behaviour looks a little bit similar as described in ZOOKEEPER-3911.)

This does not happen with zookeeper 3.6.2 !
(at least I can't reproduce it with this version)",[],Bug,ZOOKEEPER-4444,Major,Andreas Weber,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Follower doesn't get synchronized after process restart,2022-02-01T12:45:27.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>]",2.0
Chris Nauroth,[],2022-01-13T11:25:33.000+0000,Abhisek Swain,Please update the dependency of log4j to 2.17.1 to avoid security vulnerabilities.,"[<JIRA Version: name='3.6.3', id='12348703'>]",Bug,ZOOKEEPER-4442,Major,Abhisek Swain,Invalid,2022-01-13T21:24:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Support for log4j 2.17.1,2022-01-21T17:22:23.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2022-01-12T12:40:35.000+0000,Anoop Negi,"We have three(3) node zookeeper cluster running as a pod on Kubernetes cluster,
Zookeeper version is 3.7.0, While upgrading zookeeper from Plain-text+Secure mode to only secure mode we are facing issue( i.e. disabling Plain-Text channel)

1. To disable plain-text we are removing <clientport> from the dynamic configuration file to enable only secure communication but after upgrade zookeeper ensemble failed to form. leader election continuous failing and getting notification timeout
{code:java}
#server configuration
server.1=server1zookeeper.svc.cluster.local:2888:3888:participant
server.2=server2zookeeper.svc.cluster.local:2888:3888:participant
server.3=server3zookeeper.svc.cluster.local:2888:3888:participant

#secure port enabled
secureClientPort=2281
{code}
{code:java}
 
2021-05-19T08:00:06.900+0000 [myid:] - WARN [QuorumConnectionThread-[myid=3]-3:QuorumCnxManager@400] - Cannot open channel to 1 at election address server1zookeeper/192.168.57.156:3888 java.net.SocketTimeoutException: connect timed out at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?] at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399) ~[?:?] at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242) ~[?:?] at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224) ~[?:?] at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:?] at java.net.Socket.connect(Socket.java:609) ~[?:?] at org.apache.zookeeper.server.quorum.QuorumCnxManager.initiateConnection(QuorumCnxManager.java:383) [zookeeper-3.7.0.jar:3.7.0] at org.apache.zookeeper.server.quorum.QuorumCnxManager$QuorumConnectionReqThread.run(QuorumCnxManager.java:457) [zookeeper-3.7.0.jar:3.7.0] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?] at java.lang.Thread.run(Thread.java:834) [?:?]
{code}
{code:java}
2021-05-19T07:47:56.894+0000 [myid:] - INFO  [QuorumPeer[myid=1](plain=disabled)(secure=0.0.0.0:2281):FastLeaderElection@979] - Notification time out: 60000

{code}
2. We also tried to perform reconfiguration from CLI using zkCli.sh but this also not working, we tried to use ""reconfig -member"" and provided servers details but zookeeper ensemble not updating and getting error. created DigestAuthenticationProvider user to allow reconfig
{code:java}
[zk: zookeeper:2281(CONNECTED) 0]
[zk: zookeeper:2281(CONNECTED) 0]
[zk: zookeeper:2281(CONNECTED) 0] config
server.1=server1zookeeper.svc.cluster.local:2888:3888:participant
server.2=server2zookeeper.svc.cluster.local:2888:3888:participant
server.3=server3zookeeper.svc.cluster.local:2888:3888:participant
version=1700000000
[zk: zookeeper:2281(CONNECTED) 1]
[zk: zookeeper:2281(CONNECTED) 1]
[zk: zookeeper:2281(CONNECTED) 1] addauth digest zookeeper:admin
[zk: zookeeper:2281(CONNECTED) 2]
[zk: zookeeper:2281(CONNECTED) 2]
[zk: zookeeper:2281(CONNECTED) 2] reconfig -members server.1=server1zookeeper.svc.cluster.local:2888:3888:participant;0.0.0.0:2181,server.2=server2zookeeper.svc.cluster.local:2888:3888:participant;0.0.0.0:2181,server.3=server3zookeeper.svc.cluster.local:2888:3888:participant;0.0.0.0:2181
2021-05-19T08:16:43.376+0000 [myid:zookeeper:2281] - WARN  [main-SendThread(zookeeper:2281):ClientCnxn$SendThread@1242] - Client session timed out, have not heard from server in 20000ms for session id 0x30169d99fdf0000
2021-05-19T08:16:43.377+0000 [myid:zookeeper:2281] - WARN  [main-SendThread(zookeeper:2281):ClientCnxn$SendThread@1285] - Session 0x30169d99fdf0000 for sever zookeeper/10.107.240.229:2281, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
org.apache.zookeeper.ClientCnxn$SessionTimeoutException: Client session timed out, have not heard from server in 20000ms for session id 0x30169d99fdf0000
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1243) [zookeeper-3.7.0.jar:3.7.0]WATCHER::WatchedEvent state:Disconnected type:None path:null
2021-05-19T08:16:43.390+0000 [myid:] - INFO  [nioEventLoopGroup-2-1:ClientCnxnSocketNetty$ZKClientHandler@469] - channel is disconnected: [id: 0xa97b55e0, L:/192.168.220.12:47114 ! R:zookeeper/10.107.240.229:2281]
2021-05-19T08:16:43.392+0000 [myid:] - INFO  [nioEventLoopGroup-2-1:ClientCnxnSocketNetty@249] - channel is told closing
KeeperErrorCode = ConnectionLoss
{code}
Kindly suggest the way to perform upgrade with desire changes and should also work with rollback.",[],Bug,ZOOKEEPER-4440,Critical,Anoop Negi,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper Upgrade failed when disabling Plain-text communication and ensemble failed to form,2022-02-04T12:06:00.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",2.0
,[],2022-01-07T11:00:24.000+0000,Shivakumar,"|packages|Package Version|CVSS|Fix version|
|jetty-server|9.4.39.v20210325|5.5|fixed in 9.4.44|
|jetty-servlet|9.4.39.v20210325|5.5|fixed in 9.4.44|
|org.eclipse.jetty_jetty-servlet|9.4.39.v20210325|5.5|fixed in 9.4.44|
|com.fasterxml.jackson.core_jackson-databind|2.10.5.1|7.5|fixed in 2.14, 2.13.1, 2.12.6|
|jetty-io. (CVE-2021-34429)|9.4.39.v20210325|5.3|fixed in 11.0.6, 10.0.6, 9.4.43|

Our security scan detected the above vulnerabilities

upgrade to correct versions for fixing vulnerabilities",[],Bug,ZOOKEEPER-4439,Major,Shivakumar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Upgrade zk vulnerabilities ,2022-02-09T10:23:07.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>]",2.0
Ananya Singh,[],2021-12-30T06:48:04.000+0000,Ananya Singh,,"[<JIRA Version: name='3.5.10', id='12349434'>]",Bug,ZOOKEEPER-4433,Major,Ananya Singh,Fixed,2022-02-08T15:51:59.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Backport ZOOKEEPER-2872 for branch-3.5,2022-02-08T16:13:08.000+0000,"[<JIRA Version: name='3.5.9', id='12348201'>]",1.0
Chris Nauroth,[],2021-12-27T14:05:24.000+0000,lekshmi anandakrishnan,"Please confirm whether apache zookeeper 3.6.2 has any impact on the below log4j CVE`s. Apache zookeeper uses log4j 1.2.17 and since Log4J 1.X version is an end of life in Aug 2015 which is vulnerable already.

 
|CVE ID|Title|
|CVE-2021-4104 (1.X)|Apache Log4j 1.2 Remote Code Execution Vulnerability|
|CVE-2021-45105 (2.X)|Apache Log4j Remote Code Execution (RCE) Vulnerability (Log4Shell)|
|CVE-2021-45046 (2.X)|Apache Log4j Remote Code Execution (RCE) Vulnerability (CVE-2021-45046)|
|CVE-2021-44228(2.X)|Apache Log4j Remote Code Execution (RCE) Vulnerability (Log4Shell)|",[],Bug,ZOOKEEPER-4431,Critical,lekshmi anandakrishnan,Invalid,2022-01-08T04:23:55.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Log4j vulnerabilities in Apache zookeeper,2022-01-18T21:15:44.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",4.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-12-15T16:20:46.000+0000,Ryan Ruel,"In a production environment with some connectivity problems it was found the ZooKeeper server was using over 1000 threads with name ""SyncThread"" (that were never being freed).

Looking through the server logs indicates that these nodes were experiencing connection timeouts to the leader.

A test environment (described below in the ""environment"" field of this ticket) showed that these connection timeouts are what seem to be leaking these threads.",[],Bug,ZOOKEEPER-4428,Major,Ryan Ruel,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"ZooKeeper Server leaks ""SyncThread"" threads when leadership connection times out and is reestablished ",2021-12-15T16:23:15.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-12-12T05:47:23.000+0000,Haoze Wu,"When `Leader$LearnerCnxAcceptor$LearnerCnxAcceptorHandler` is accepting a new socket connection, it may throw an IOException at line 510 or 514 or 515 or 517.

The scenario of IOException at line 510 is discussed in ZOOKEEPER-4203 and [https://github.com/apache/zookeeper/pull/1596] . It triggers a concurrency bug. However, If the IOException occurs at line 514 or 515 or 517, actually we can avoid this complicated process. We can simply catch the IOException and proceed to accept the next socket connection, without exiting the LearnerCnxAcceptorHandler thread. The exceptions in those operations (e.g., `setSoTimeout`) only indicates that the socket connection has some issues, but the `ServerSocket` can still work well. Therefore, the `LearnerCnxAcceptorHandler` can proceed to accept more socket connections with this `ServerSocket`.
{code:java}
//zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Leader.java
            private void acceptConnections() throws IOException {
                Socket socket = null;
                boolean error = false;
                try {
                    socket = serverSocket.accept();  // line 510


                    // start with the initLimit, once the ack is processed
                    // in LearnerHandler switch to the syncLimit
                    socket.setSoTimeout(self.tickTime * self.initLimit); // line 514
                    socket.setTcpNoDelay(nodelay);  // line 515

                    BufferedInputStream is = new BufferedInputStream(socket.getInputStream());  // line 517
                    LearnerHandler fh = new LearnerHandler(socket, is, Leader.this);
                    fh.start();
                } catch (SocketException e) {
                    error = true;
                    if (stop.get()) {
                        LOG.warn(""Exception while shutting down acceptor."", e);
                    } else {
                        throw e;
                    }
                } catch (SaslException e) {
                    LOG.error(""Exception while connecting to quorum learner"", e);
                    error = true;
                } catch (Exception e) {
                    error = true;
                    throw e;
                } finally {
                    // Don't leak sockets on errors
                    if (error && socket != null && !socket.isClosed()) {
                        try {
                            socket.close();
                        } catch (IOException e) {
                            LOG.warn(""Error closing socket: "" + socket, e);
                        }
                    }
                }
            }
{code}
We propose that the following implementation is better. The advantage is that those IOException in the socket will not force the `Leader$LearnerCnxAcceptor$LearnerCnxAcceptorHandler` to exit, and thus avoid the overhead of re-election and potential concurrency bugs such as ZOOKEEPER-4203.
{code:java}
            private void acceptConnections() throws IOException {
                Socket socket = null;
                boolean error = false;
                try {
                    socket = serverSocket.accept();
                    BufferedInputStream is;

                    try {
                        // start with the initLimit, once the ack is processed
                        // in LearnerHandler switch to the syncLimit
                        socket.setSoTimeout(self.tickTime * self.initLimit);
                        socket.setTcpNoDelay(nodelay);

                        is = new BufferedInputStream(socket.getInputStream());
                    } catch (IOException e) {
                        error = true;
                        return;  // close the socket at the finally block
                    }

                    LearnerHandler fh = new LearnerHandler(socket, is, Leader.this);
                    fh.start();
                } catch (SocketException e) {
                    error = true;
                    if (stop.get()) {
                        LOG.warn(""Exception while shutting down acceptor."", e);
                    } else {
                        throw e;
                    }
                } catch (SaslException e) {
                    LOG.error(""Exception while connecting to quorum learner"", e);
                } catch (Exception e) {
                    error = true;
                    throw e;
                } finally {
                    // Don't leak sockets on errors
                    if (error && socket != null && !socket.isClosed()) {
                        try {
                            socket.close();
                        } catch (IOException e) {
                            LOG.warn(""Error closing socket: "" + socket, e);
                        }
                    }
                }
            }
{code}
This code pattern has been adopted by other communities. For example, in Kafka [https://github.com/apache/kafka/blob/2cd96f0e64f8a4f4b74e8049a6c527a990cb4777/core/src/main/scala/kafka/network/SocketServer.scala#L714-L740] :
{code:java}
  /**
   * Accept a new connection
   */
  private def accept(key: SelectionKey): Option[SocketChannel] = {
    val serverSocketChannel = key.channel().asInstanceOf[ServerSocketChannel]
    val socketChannel = serverSocketChannel.accept()
    try {
      connectionQuotas.inc(endPoint.listenerName, socketChannel.socket.getInetAddress, blockedPercentMeter)
      configureAcceptedSocketChannel(socketChannel)
      Some(socketChannel)
    } catch {
      case e: TooManyConnectionsException =>
        info(...)
        close(endPoint.listenerName, socketChannel)
        None
      case e: ConnectionThrottledException =>
        // ...
        None
      case e: IOException =>
        error(...)
        close(endPoint.listenerName, socketChannel)
        None
    }
  }

  /**
   * Close `channel` and decrement the connection count.
   */
  def close(listenerName: ListenerName, channel: SocketChannel): Unit = {
    if (channel != null) {
      // ...
      closeSocket(channel)
    }
  }

  protected def closeSocket(channel: SocketChannel): Unit = {
    CoreUtils.swallow(channel.socket().close(), this, Level.ERROR)
    CoreUtils.swallow(channel.close(), this, Level.ERROR)
  } {code}",[],Bug,ZOOKEEPER-4424,Major,Haoze Wu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Re-throwing IOException in Leader$LearnerCnxAcceptor$LearnerCnxAcceptorHandler#acceptConnections is not always needed,2021-12-12T06:12:55.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",2.0
,[],2021-12-10T05:05:35.000+0000,Ananya Singh,,[],Bug,ZOOKEEPER-4421,Major,Ananya Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Backport ZOOKEEPER-3781 for branch-3.5 ,2021-12-10T15:13:59.000+0000,"[<JIRA Version: name='3.5.9', id='12348201'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-11-27T05:15:54.000+0000,Haoze Wu,"We were doing some testing in ZooKeeper 3.6.2, and found that sometimes a follower may take longer time (about 10s) to join the quorum. In our cluster, usually we expect that the leader and followers are ready to provide service in 1 or 2 seconds.

*Analysis*

We did some investigations and found that the root cause is the exception thrown from `Learner.createSocket` ([https://github.com/apache/zookeeper/blob/release-3.6.2/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Learner.java#L410-L423]).

`Learner.createSocket` is used in `Learner$LeaderConnector.connectToLeader`.

The code snippet ([https://github.com/apache/zookeeper/blob/release-3.6.2/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Learner.java#L344-L407]) is as follows:
{code:java}
        private Socket connectToLeader() throws IOException, X509Exception, InterruptedException {
            Socket sock = createSocket();               // line 345
            // ...
            int remainingTimeout;
            long startNanoTime = nanoTime();
            for (int tries = 0; tries < 5 && socket.get() == null; tries++) {
                try {
                    remainingTimeout = connectTimeout - (int) ((nanoTime() - startNanoTime) / 1_000_000);
                    if (remainingTimeout <= 0) {
                        LOG.error(...);
                        throw new IOException(""connectToLeader exceeded on retries."");
                    }
                    sockConnect(sock, address, Math.min(connectTimeout, remainingTimeout));
                    // ...
                    break;
                } catch (IOException e) {
                    remainingTimeout = connectTimeout - (int) ((nanoTime() - startNanoTime) / 1_000_000);
                    if (remainingTimeout <= leaderConnectDelayDuringRetryMs) {
                        LOG.error(...);
                        throw e;
                    } else if (tries >= 4) {
                        LOG.error(...);
                        throw e;
                    } else {
                        LOG.warn(...);
                        sock = createSocket();          // line 400
                    }
                }
                Thread.sleep(leaderConnectDelayDuringRetryMs);
            }
            return sock;
        }
{code}
The retry (5 times) in this code implies that sometimes exceptions are expected to appear, and the program should proceed to try again. However, `Learner.createSocket` is invoked in either line 345 or line 400. And it may also throw exceptions. But it’s not within the try block. Therefore, this exception will turn to `Follower.followLeader` (the caller of `Learner$LeaderConnector.connectToLeader`). And then this server will change from FOLLOWING state to LOOKING state.

`Learner.createSocket` may throw exceptions in line 417 or 421:
{code:java}
    /**
     * Creating a simple or and SSL socket.
     * This can be overridden in tests to fake already connected sockets for connectToLeader.
     */
    protected Socket createSocket() throws X509Exception, IOException {
        Socket sock;
        if (self.isSslQuorum()) {
            sock = self.getX509Util().createSSLSocket();    // line 417
        } else {
            sock = new Socket();
        }
        sock.setSoTimeout(self.tickTime * self.initLimit);  // line 421
        return sock;
    } {code}
The reason for throwing the exception may be [https://stackoverflow.com/questions/22423063/java-exception-on-sslsocket-creation] or [https://stackoverflow.com/questions/7064788/when-does-java-net-socket-setsotimeout-throws-socketexception.]

We reproduced this bug by intentionally injecting an IOException right before the invocation of `Learner.createSocket` in line 345. We only grant one single injection in a testing run of a ZooKeeper cluster (3 nodes). Our analysis proves to be true. In terms of the symptom, sometimes the affected follower takes more time to join the quorum; sometimes the leader election is also affected, and the whole cluster does the re-election, meaning that every node is affected.

*Fix*

We propose that the `Learner.createSocket` should be moved to the try-catch block. We have created a pull request.",[],Bug,ZOOKEEPER-4419,Major,Haoze Wu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,The potential exception in Learner$LeaderConnector.connectToLeader may cause unnecessary re-election or service delay,2021-12-02T04:17:39.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-11-23T09:41:42.000+0000,Yong-Hao Zou,"The server exit with the following logs
{code:java}
// code placeholder
2021-11-23 17:02:45,602 [myid:] - INFO  [main:QuorumPeerConfig@174] - Reading configuration from: zoo2
2021-11-23 17:02:45,629 [myid:] - WARN  [main:VerifyingFileFactory@65] - zoo2 is relative. Prepend ./ to indicate that you're sure!
2021-11-23 17:02:45,649 [myid:] - WARN  [main:VerifyingFileFactory@65] - data2 is relative. Prepend ./ to indicate that you're sure!
2021-11-23 17:02:45,661 [myid:] - INFO  [main:QuorumPeerConfig@444] - clientPortAddress is 0.0.0.0:2182
2021-11-23 17:02:45,661 [myid:] - INFO  [main:QuorumPeerConfig@448] - secureClientPort is not set
2021-11-23 17:02:45,662 [myid:] - INFO  [main:QuorumPeerConfig@464] - observerMasterPort is not set
2021-11-23 17:02:45,662 [myid:] - INFO  [main:QuorumPeerConfig@481] - metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider
2021-11-23 17:02:45,790 [myid:2] - INFO  [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3
2021-11-23 17:02:45,790 [myid:2] - INFO  [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 0
2021-11-23 17:02:45,791 [myid:2] - INFO  [main:DatadirCleanupManager@101] - Purge task is not scheduled.
2021-11-23 17:02:45,806 [myid:2] - INFO  [main:ManagedUtil@44] - Log4j 1.2 jmx support found and enabled.
2021-11-23 17:02:46,060 [myid:2] - INFO  [main:QuorumPeerMain@152] - Starting quorum peer, myid=2
2021-11-23 17:02:46,144 [myid:2] - INFO  [main:ServerMetrics@62] - ServerMetrics initialized with provider org.apache.zookeeper.metrics.impl.DefaultMetricsProvider@2a18f23c
2021-11-23 17:02:46,198 [myid:2] - INFO  [main:DigestAuthenticationProvider@47] - ACL digest algorithm is: SHA1
2021-11-23 17:02:46,199 [myid:2] - INFO  [main:DigestAuthenticationProvider@61] - zookeeper.DigestAuthenticationProvider.enabled = true
2021-11-23 17:02:46,213 [myid:2] - INFO  [main:ServerCnxnFactory@169] - Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory
2021-11-23 17:02:46,218 [myid:2] - WARN  [main:ServerCnxnFactory@309] - maxCnxns is not configured, using default value 0.
2021-11-23 17:02:46,221 [myid:2] - INFO  [main:NIOServerCnxnFactory@652] - Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 24 worker threads, and 64 kB direct buffers.
2021-11-23 17:02:46,251 [myid:2] - INFO  [main:NIOServerCnxnFactory@660] - binding to port 0.0.0.0/0.0.0.0:2182
2021-11-23 17:02:46,268 [myid:2] - INFO  [main:QuorumPeer@796] - zookeeper.quorumCnxnTimeoutMs=-1
2021-11-23 17:02:46,280 [myid:2] - INFO  [main:X509Util@77] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2021-11-23 17:02:46,285 [myid:2] - INFO  [main:FileTxnSnapLog@124] - zookeeper.snapshot.trust.empty : false
2021-11-23 17:02:46,294 [myid:2] - INFO  [main:QuorumPeer@1739] - Local sessions disabled
2021-11-23 17:02:46,297 [myid:2] - INFO  [main:QuorumPeer@1750] - Local session upgrading disabled
2021-11-23 17:02:46,299 [myid:2] - INFO  [main:QuorumPeer@1717] - tickTime set to 200
2021-11-23 17:02:46,299 [myid:2] - INFO  [main:QuorumPeer@1761] - minSessionTimeout set to 400
2021-11-23 17:02:46,299 [myid:2] - INFO  [main:QuorumPeer@1772] - maxSessionTimeout set to 4000
2021-11-23 17:02:46,299 [myid:2] - INFO  [main:QuorumPeer@1797] - initLimit set to 5
2021-11-23 17:02:46,299 [myid:2] - INFO  [main:QuorumPeer@1984] - syncLimit set to 2
2021-11-23 17:02:46,299 [myid:2] - INFO  [main:QuorumPeer@1999] - connectToLearnerMasterLimit set to 0
2021-11-23 17:02:46,377 [myid:2] - INFO  [main:ZookeeperBanner@42] - 
2021-11-23 17:02:46,377 [myid:2] - INFO  [main:ZookeeperBanner@42] -   ______                  _                                          
2021-11-23 17:02:46,377 [myid:2] - INFO  [main:ZookeeperBanner@42] -  |___  /                 | |                                         
2021-11-23 17:02:46,377 [myid:2] - INFO  [main:ZookeeperBanner@42] -     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __   
2021-11-23 17:02:46,378 [myid:2] - INFO  [main:ZookeeperBanner@42] -    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__|
2021-11-23 17:02:46,378 [myid:2] - INFO  [main:ZookeeperBanner@42] -   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |    
2021-11-23 17:02:46,378 [myid:2] - INFO  [main:ZookeeperBanner@42] -  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_|
2021-11-23 17:02:46,382 [myid:2] - INFO  [main:ZookeeperBanner@42] -                                               | |                     
2021-11-23 17:02:46,382 [myid:2] - INFO  [main:ZookeeperBanner@42] -                                               |_|                     
2021-11-23 17:02:46,382 [myid:2] - INFO  [main:ZookeeperBanner@42] - 
2021-11-23 17:02:46,391 [myid:2] - INFO  [main:Environment@98] - Server environment:zookeeper.version=3.7.0-e3704b390a6697bfdf4b0bef79e3da7a4f6bac4b, built on 2021-03-17 09:46 UTC
2021-11-23 17:02:46,392 [myid:2] - INFO  [main:Environment@98] - Server environment:host.name=pc
2021-11-23 17:02:46,393 [myid:2] - INFO  [main:Environment@98] - Server environment:java.version=1.8.0_312
2021-11-23 17:02:46,393 [myid:2] - INFO  [main:Environment@98] - Server environment:java.vendor=Temurin
2021-11-23 17:02:46,393 [myid:2] - INFO  [main:Environment@98] - Server environment:java.home=/home/zyh/jdk8/jre
2021-11-23 17:02:46,393 [myid:2] - INFO  [main:Environment@98] - Server environment:java.class.path=/home/zyh/zookeeper/lib/zookeeper.jar:/home/zyh/zookeeper/lib/netty-handler-4.1.59.Final.jar:/home/zyh/zookeeper/lib/zookeeper-3.7.0.jar:/home/zyh/zookeeper/lib/commons-cli-1.4.jar:/home/zyh/zookeeper/lib/netty-transport-native-epoll-4.1.59.Final.jar:/home/zyh/zookeeper/lib/netty-transport-4.1.59.Final.jar:/home/zyh/zookeeper/lib/jetty-util-9.4.38.v20210224.jar:/home/zyh/zookeeper/lib/slf4j-log4j12-1.7.30.jar:/home/zyh/zookeeper/lib/jetty-io-9.4.38.v20210224.jar:/home/zyh/zookeeper/lib/jetty-server-9.4.38.v20210224.jar:/home/zyh/zookeeper/lib/netty-common-4.1.59.Final.jar:/home/zyh/zookeeper/lib/jackson-annotations-2.10.5.jar:/home/zyh/zookeeper/lib/jetty-http-9.4.38.v20210224.jar:/home/zyh/zookeeper/lib/netty-buffer-4.1.59.Final.jar:/home/zyh/zookeeper/lib/slf4j-api-1.7.30.jar:/home/zyh/zookeeper/lib/jetty-security-9.4.38.v20210224.jar:/home/zyh/zookeeper/lib/audience-annotations-0.12.0.jar:/home/zyh/zookeeper/lib/javax.servlet-api-3.1.0.jar:/home/zyh/zookeeper/lib/zookeeper-jute-3.7.0.jar:/home/zyh/zookeeper/lib/netty-transport-native-unix-common-4.1.59.Final.jar:/home/zyh/zookeeper/lib/simpleclient-0.9.0.jar:/home/zyh/zookeeper/lib/zookeeper-prometheus-metrics-3.7.0.jar:/home/zyh/zookeeper/lib/simpleclient_common-0.9.0.jar:/home/zyh/zookeeper/lib/metrics-core-4.1.12.1.jar:/home/zyh/zookeeper/lib/jetty-servlet-9.4.38.v20210224.jar:/home/zyh/zookeeper/lib/jline-2.14.6.jar:/home/zyh/zookeeper/lib/netty-codec-4.1.59.Final.jar:/home/zyh/zookeeper/lib/log4j-1.2.17.jar:/home/zyh/zookeeper/lib/netty-resolver-4.1.59.Final.jar:/home/zyh/zookeeper/lib/jackson-databind-2.10.5.1.jar:/home/zyh/zookeeper/lib/simpleclient_hotspot-0.9.0.jar:/home/zyh/zookeeper/lib/zookeeper.jar:/home/zyh/zookeeper/lib/snappy-java-1.1.7.7.jar:/home/zyh/zookeeper/lib/jackson-core-2.10.5.jar:/home/zyh/zookeeper/lib/simpleclient_servlet-0.9.0.jar:/home/zyh/zookeeper/lib/jetty-util-ajax-9.4.38.v20210224.jar:/home/zyh/zookeeper/conf
2021-11-23 17:02:46,393 [myid:2] - INFO  [main:Environment@98] - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2021-11-23 17:02:46,394 [myid:2] - INFO  [main:Environment@98] - Server environment:java.io.tmpdir=/tmp
2021-11-23 17:02:46,394 [myid:2] - INFO  [main:Environment@98] - Server environment:java.compiler=<NA>
2021-11-23 17:02:46,395 [myid:2] - INFO  [main:Environment@98] - Server environment:os.name=Linux
2021-11-23 17:02:46,396 [myid:2] - INFO  [main:Environment@98] - Server environment:os.arch=amd64
2021-11-23 17:02:46,396 [myid:2] - INFO  [main:Environment@98] - Server environment:os.version=4.15.0-162-generic
2021-11-23 17:02:46,396 [myid:2] - INFO  [main:Environment@98] - Server environment:user.name=zyh
2021-11-23 17:02:46,396 [myid:2] - INFO  [main:Environment@98] - Server environment:user.home=/home/zyh
2021-11-23 17:02:46,396 [myid:2] - INFO  [main:Environment@98] - Server environment:user.dir=/home/zyh/distributed-system-test/zookeeper_test/bin
2021-11-23 17:02:46,396 [myid:2] - INFO  [main:Environment@98] - Server environment:os.memory.free=463MB
2021-11-23 17:02:46,397 [myid:2] - INFO  [main:Environment@98] - Server environment:os.memory.max=7138MB
2021-11-23 17:02:46,397 [myid:2] - INFO  [main:Environment@98] - Server environment:os.memory.total=481MB
2021-11-23 17:02:46,397 [myid:2] - INFO  [main:ZooKeeperServer@138] - zookeeper.enableEagerACLCheck = false
2021-11-23 17:02:46,397 [myid:2] - INFO  [main:ZooKeeperServer@151] - zookeeper.digest.enabled = true
2021-11-23 17:02:46,397 [myid:2] - INFO  [main:ZooKeeperServer@155] - zookeeper.closeSessionTxn.enabled = true
2021-11-23 17:02:46,397 [myid:2] - INFO  [main:ZooKeeperServer@1499] - zookeeper.flushDelay=0
2021-11-23 17:02:46,398 [myid:2] - INFO  [main:ZooKeeperServer@1508] - zookeeper.maxWriteQueuePollTime=0
2021-11-23 17:02:46,398 [myid:2] - INFO  [main:ZooKeeperServer@1517] - zookeeper.maxBatchSize=1000
2021-11-23 17:02:46,399 [myid:2] - INFO  [main:ZooKeeperServer@260] - zookeeper.intBufferStartingSizeBytes = 1024
2021-11-23 17:02:46,417 [myid:2] - INFO  [main:WatchManagerFactory@42] - Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2021-11-23 17:02:46,417 [myid:2] - INFO  [main:WatchManagerFactory@42] - Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2021-11-23 17:02:46,418 [myid:2] - INFO  [main:ZKDatabase@133] - zookeeper.snapshotSizeFactor = 0.33
2021-11-23 17:02:46,418 [myid:2] - INFO  [main:ZKDatabase@153] - zookeeper.commitLogCount=500
2021-11-23 17:02:46,451 [myid:2] - INFO  [main:QuorumPeer@2063] - Using insecure (non-TLS) quorum communication
2021-11-23 17:02:46,452 [myid:2] - INFO  [main:QuorumPeer@2069] - Port unification disabled
2021-11-23 17:02:46,452 [myid:2] - INFO  [main:QuorumPeer@179] - multiAddress.enabled set to false
2021-11-23 17:02:46,452 [myid:2] - INFO  [main:QuorumPeer@204] - multiAddress.reachabilityCheckEnabled set to true
2021-11-23 17:02:46,452 [myid:2] - INFO  [main:QuorumPeer@191] - multiAddress.reachabilityCheckTimeoutMs set to 1000
2021-11-23 17:02:46,452 [myid:2] - INFO  [main:QuorumPeer@2524] - QuorumPeer communication is not secured! (SASL auth disabled)
2021-11-23 17:02:46,452 [myid:2] - INFO  [main:QuorumPeer@2549] - quorum.cnxn.threads.size set to 20
2021-11-23 17:02:46,465 [myid:2] - INFO  [main:SnapStream@61] - zookeeper.snapshot.compression.method = CHECKED
2021-11-23 17:02:46,466 [myid:2] - INFO  [main:FileTxnSnapLog@479] - Snapshotting: 0x0 to data2/version-2/snapshot.0
2021-11-23 17:02:46,472 [myid:2] - INFO  [main:ZKDatabase@290] - Snapshot loaded in 19 ms, highest zxid is 0x0, digest is 1371985504
2021-11-23 17:02:46,473 [myid:2] - INFO  [main:QuorumPeer@1157] - currentEpoch not found! Creating with a reasonable default of 0. This should only happen when you are upgrading your installation
2021-11-23 17:02:46,487 [myid:2] - INFO  [main:QuorumPeer@1177] - acceptedEpoch not found! Creating with a reasonable default of 0. This should only happen when you are upgrading your installation
2021-11-23 17:02:46,549 [myid:2] - INFO  [main:QuorumPeer@2566] - Using 400ms as the quorum cnxn socket timeout
2021-11-23 17:02:46,568 [myid:2] - INFO  [main:QuorumCnxManager$Listener@924] - Election port bind maximum retries is 3
2021-11-23 17:02:46,572 [myid:2] - INFO  [main:FastLeaderElection@89] - zookeeper.fastleader.minNotificationInterval=200
2021-11-23 17:02:46,573 [myid:2] - INFO  [main:FastLeaderElection@91] - zookeeper.fastleader.maxNotificationInterval=60000
2021-11-23 17:02:46,619 [myid:2] - INFO  [main:ZKAuditProvider@42] - ZooKeeper audit is disabled.
2021-11-23 17:02:46,629 [myid:2] - INFO  [ListenerHandler-/127.0.1.1:3889:QuorumCnxManager$Listener$ListenerHandler@1066] - 2 is accepting connections now, my election bind port: /127.0.1.1:3889
2021-11-23 17:02:46,631 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):QuorumPeer@1430] - LOOKING
2021-11-23 17:02:46,632 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):FastLeaderElection@945] - New election. My id = 2, proposed zxid=0x0
2021-11-23 17:02:46,644 [myid:2] - INFO  [ListenerHandler-/127.0.1.1:3889:QuorumCnxManager$Listener$ListenerHandler@1071] - Received connection request from /127.0.0.1:34944
2021-11-23 17:02:46,677 [myid:2] - WARN  [QuorumConnectionThread-[myid=2]-1:QuorumCnxManager@401] - Cannot open channel to 1 at election address /127.0.1.1:3888
java.net.ConnectException: Connection refused (Connection refused)
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
    at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
    at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    at java.net.Socket.connect(Socket.java:607)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager.initiateConnection(QuorumCnxManager.java:384)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$QuorumConnectionReqThread.run(QuorumCnxManager.java:458)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
2021-11-23 17:02:46,689 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@390] - Notification: my state:LOOKING; n.sid:2, n.state:LOOKING, n.leader:2, n.round:0x1, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x0
2021-11-23 17:02:46,694 [myid:2] - WARN  [NIOWorkerThread-1:NIOServerCnxn@380] - Close of session 0x0
java.io.IOException: ZooKeeperServer not running
    at org.apache.zookeeper.server.NIOServerCnxn.readLength(NIOServerCnxn.java:554)
    at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:339)
    at org.apache.zookeeper.server.NIOServerCnxnFactory$IOWorkRequest.doWork(NIOServerCnxnFactory.java:508)
    at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:154)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
2021-11-23 17:02:46,703 [myid:2] - INFO  [QuorumConnectionThread-[myid=2]-2:QuorumCnxManager@514] - Have smaller server identifier, so dropping the connection: (myId:2 --> sid:3)
2021-11-23 17:02:46,704 [myid:2] - INFO  [QuorumConnectionThread-[myid=2]-1:QuorumCnxManager@514] - Have smaller server identifier, so dropping the connection: (myId:2 --> sid:5)
2021-11-23 17:02:46,707 [myid:2] - INFO  [QuorumConnectionThread-[myid=2]-3:QuorumCnxManager@514] - Have smaller server identifier, so dropping the connection: (myId:2 --> sid:4)
2021-11-23 17:02:46,715 [myid:2] - WARN  [SendWorker:3:QuorumCnxManager$SendWorker@1287] - Exception when using channel: for id 3 my id = 2
java.net.SocketException: Broken pipe (Write failed)
    at java.net.SocketOutputStream.socketWrite0(Native Method)
    at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)
    at java.net.SocketOutputStream.write(SocketOutputStream.java:134)
    at java.io.DataOutputStream.writeInt(DataOutputStream.java:198)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.send(QuorumCnxManager.java:1229)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:1280)
2021-11-23 17:02:46,723 [myid:2] - WARN  [SendWorker:3:QuorumCnxManager$SendWorker@1295] - Send worker leaving thread id 3 my id = 2
2021-11-23 17:02:46,723 [myid:2] - INFO  [ListenerHandler-/127.0.1.1:3889:QuorumCnxManager$Listener$ListenerHandler@1071] - Received connection request from /127.0.0.1:34954
2021-11-23 17:02:46,725 [myid:2] - WARN  [RecvWorker:3:QuorumCnxManager$RecvWorker@1403] - Interrupting SendWorker thread from RecvWorker. sid: 3. myId: 2
2021-11-23 17:02:46,737 [myid:2] - INFO  [ListenerHandler-/127.0.1.1:3889:QuorumCnxManager$Listener$ListenerHandler@1071] - Received connection request from /127.0.0.1:34962
2021-11-23 17:02:46,738 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@390] - Notification: my state:LOOKING; n.sid:5, n.state:LOOKING, n.leader:5, n.round:0x1, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x0
2021-11-23 17:02:46,750 [myid:2] - INFO  [ListenerHandler-/127.0.1.1:3889:QuorumCnxManager$Listener$ListenerHandler@1071] - Received connection request from /127.0.0.1:34972
2021-11-23 17:02:46,750 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@390] - Notification: my state:LOOKING; n.sid:2, n.state:LOOKING, n.leader:5, n.round:0x1, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x0
2021-11-23 17:02:46,754 [myid:2] - INFO  [QuorumConnectionThread-[myid=2]-2:QuorumCnxManager@514] - Have smaller server identifier, so dropping the connection: (myId:2 --> sid:3)
2021-11-23 17:02:46,754 [myid:2] - INFO  [ListenerHandler-/127.0.1.1:3889:QuorumCnxManager$Listener$ListenerHandler@1071] - Received connection request from /127.0.0.1:34982
2021-11-23 17:02:46,756 [myid:2] - WARN  [SendWorker:3:QuorumCnxManager$SendWorker@1287] - Exception when using channel: for id 3 my id = 2
java.net.SocketException: Broken pipe (Write failed)
    at java.net.SocketOutputStream.socketWrite0(Native Method)
    at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)
    at java.net.SocketOutputStream.write(SocketOutputStream.java:134)
    at java.io.DataOutputStream.writeInt(DataOutputStream.java:198)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.send(QuorumCnxManager.java:1229)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:1280)
2021-11-23 17:02:46,756 [myid:2] - WARN  [RecvWorker:3:QuorumCnxManager$RecvWorker@1397] - Connection broken for id 3, my id = 2
java.io.EOFException
    at java.io.DataInputStream.readInt(DataInputStream.java:392)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:1385)
2021-11-23 17:02:46,759 [myid:2] - WARN  [RecvWorker:3:QuorumCnxManager$RecvWorker@1403] - Interrupting SendWorker thread from RecvWorker. sid: 3. myId: 2
2021-11-23 17:02:46,756 [myid:2] - WARN  [RecvWorker:4:QuorumCnxManager$RecvWorker@1397] - Connection broken for id 4, my id = 2
java.net.SocketException: Connection reset
    at java.net.SocketInputStream.read(SocketInputStream.java:210)
    at java.net.SocketInputStream.read(SocketInputStream.java:141)
    at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
    at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
    at java.io.DataInputStream.readInt(DataInputStream.java:387)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:1385)
2021-11-23 17:02:46,763 [myid:2] - WARN  [RecvWorker:4:QuorumCnxManager$RecvWorker@1403] - Interrupting SendWorker thread from RecvWorker. sid: 4. myId: 2
2021-11-23 17:02:46,764 [myid:2] - WARN  [SendWorker:3:QuorumCnxManager$SendWorker@1295] - Send worker leaving thread id 3 my id = 2
2021-11-23 17:02:46,765 [myid:2] - WARN  [SendWorker:4:QuorumCnxManager$SendWorker@1283] - Interrupted while waiting for message on queue
java.lang.InterruptedException
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2088)
    at org.apache.zookeeper.util.CircularBlockingQueue.poll(CircularBlockingQueue.java:105)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager.pollSendQueue(QuorumCnxManager.java:1448)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager.access$900(QuorumCnxManager.java:99)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:1272)
2021-11-23 17:02:46,766 [myid:2] - WARN  [SendWorker:4:QuorumCnxManager$SendWorker@1295] - Send worker leaving thread id 4 my id = 2
2021-11-23 17:02:46,766 [myid:2] - WARN  [RecvWorker:1:QuorumCnxManager$RecvWorker@1403] - Interrupting SendWorker thread from RecvWorker. sid: 1. myId: 2
2021-11-23 17:02:46,767 [myid:2] - WARN  [SendWorker:1:QuorumCnxManager$SendWorker@1295] - Send worker leaving thread id 1 my id = 2
2021-11-23 17:02:46,774 [myid:2] - INFO  [ListenerHandler-/127.0.1.1:3889:QuorumCnxManager$Listener$ListenerHandler@1071] - Received connection request from /127.0.0.1:34986
2021-11-23 17:02:46,774 [myid:2] - WARN  [ListenerHandler-/127.0.1.1:3889:QuorumCnxManager@630] - Exception reading or writing challenge
java.io.EOFException
    at java.io.DataInputStream.readFully(DataInputStream.java:197)
    at java.io.DataInputStream.readLong(DataInputStream.java:416)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager.handleConnection(QuorumCnxManager.java:602)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager.receiveConnection(QuorumCnxManager.java:555)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$Listener$ListenerHandler.acceptConnections(QuorumCnxManager.java:1080)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$Listener$ListenerHandler.run(QuorumCnxManager.java:1034)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
2021-11-23 17:02:46,777 [myid:2] - INFO  [ListenerHandler-/127.0.1.1:3889:QuorumCnxManager$Listener$ListenerHandler@1071] - Received connection request from /127.0.0.1:35004
2021-11-23 17:02:46,803 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@390] - Notification: my state:LOOKING; n.sid:4, n.state:LOOKING, n.leader:5, n.round:0x1, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x0
2021-11-23 17:02:46,817 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@390] - Notification: my state:LOOKING; n.sid:1, n.state:LOOKING, n.leader:1, n.round:0x1, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x0
2021-11-23 17:02:46,843 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@390] - Notification: my state:LOOKING; n.sid:1, n.state:LOOKING, n.leader:5, n.round:0x1, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x0
2021-11-23 17:02:46,844 [myid:2] - WARN  [RecvWorker:1:QuorumCnxManager$RecvWorker@1397] - Connection broken for id 1, my id = 2
java.io.IOException: Received packet with invalid packet: 0
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:1387)
2021-11-23 17:02:46,844 [myid:2] - WARN  [RecvWorker:1:QuorumCnxManager$RecvWorker@1403] - Interrupting SendWorker thread from RecvWorker. sid: 1. myId: 2
2021-11-23 17:02:46,851 [myid:2] - WARN  [SendWorker:1:QuorumCnxManager$SendWorker@1283] - Interrupted while waiting for message on queue
java.lang.InterruptedException
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2088)
    at org.apache.zookeeper.util.CircularBlockingQueue.poll(CircularBlockingQueue.java:105)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager.pollSendQueue(QuorumCnxManager.java:1448)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager.access$900(QuorumCnxManager.java:99)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:1272)
2021-11-23 17:02:46,853 [myid:2] - WARN  [SendWorker:1:QuorumCnxManager$SendWorker@1295] - Send worker leaving thread id 1 my id = 2
2021-11-23 17:02:46,962 [myid:2] - INFO  [ListenerHandler-/127.0.1.1:3889:QuorumCnxManager$Listener$ListenerHandler@1071] - Received connection request from /127.0.0.1:35016
2021-11-23 17:02:46,963 [myid:2] - WARN  [ListenerHandler-/127.0.1.1:3889:QuorumCnxManager@630] - Exception reading or writing challenge
java.net.SocketException: Invalid argument (Read failed)
    at java.net.SocketInputStream.socketRead0(Native Method)
    at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
    at java.net.SocketInputStream.read(SocketInputStream.java:171)
    at java.net.SocketInputStream.read(SocketInputStream.java:141)
    at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
    at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
    at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
    at java.io.DataInputStream.readFully(DataInputStream.java:195)
    at java.io.DataInputStream.readLong(DataInputStream.java:416)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager.handleConnection(QuorumCnxManager.java:602)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager.receiveConnection(QuorumCnxManager.java:555)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$Listener$ListenerHandler.acceptConnections(QuorumCnxManager.java:1080)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$Listener$ListenerHandler.run(QuorumCnxManager.java:1034)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
2021-11-23 17:02:47,044 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):QuorumPeer@901] - Peer state changed: following
2021-11-23 17:02:47,045 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):QuorumPeer@1512] - FOLLOWING
2021-11-23 17:02:47,049 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):Learner@127] - leaderConnectDelayDuringRetryMs: 100
2021-11-23 17:02:47,049 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):Learner@128] - TCP NoDelay set to: true
2021-11-23 17:02:47,049 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):Learner@129] - learner.asyncSending = false
2021-11-23 17:02:47,049 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):Learner@130] - learner.closeSocketAsync = false
2021-11-23 17:02:47,055 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):BlueThrottle@141] - Weighed connection throttling is disabled
2021-11-23 17:02:47,056 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):ZooKeeperServer@1300] - minSessionTimeout set to 400
2021-11-23 17:02:47,057 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):ZooKeeperServer@1309] - maxSessionTimeout set to 4000
2021-11-23 17:02:47,058 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):ResponseCache@45] - getData response cache size is initialized with value 400.
2021-11-23 17:02:47,058 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):ResponseCache@45] - getChildren response cache size is initialized with value 400.
2021-11-23 17:02:47,059 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):RequestPathMetricsCollector@109] - zookeeper.pathStats.slotCapacity = 60
2021-11-23 17:02:47,060 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):RequestPathMetricsCollector@110] - zookeeper.pathStats.slotDuration = 15
2021-11-23 17:02:47,060 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):RequestPathMetricsCollector@111] - zookeeper.pathStats.maxDepth = 6
2021-11-23 17:02:47,060 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):RequestPathMetricsCollector@112] - zookeeper.pathStats.initialDelay = 5
2021-11-23 17:02:47,060 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):RequestPathMetricsCollector@113] - zookeeper.pathStats.delay = 5
2021-11-23 17:02:47,060 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):RequestPathMetricsCollector@114] - zookeeper.pathStats.enabled = false
2021-11-23 17:02:47,064 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):ZooKeeperServer@1536] - The max bytes for all large requests are set to 104857600
2021-11-23 17:02:47,064 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):ZooKeeperServer@1550] - The large request threshold is set to -1
2021-11-23 17:02:47,064 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):AuthenticationHelper@66] - zookeeper.enforce.auth.enabled = false
2021-11-23 17:02:47,064 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):AuthenticationHelper@67] - zookeeper.enforce.auth.schemes = []
2021-11-23 17:02:47,065 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):ZooKeeperServer@361] - Created server with tickTime 200 minSessionTimeout 400 maxSessionTimeout 4000 clientPortListenBacklog -1 datadir data2/version-2 snapdir data2/version-2
2021-11-23 17:02:47,066 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):Follower@77] - FOLLOWING - LEADER ELECTION TOOK - 433 MS
2021-11-23 17:02:47,068 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):QuorumPeer@915] - Peer state changed: following - discovery
2021-11-23 17:02:47,071 [myid:2] - WARN  [LeaderConnector-/127.0.1.1:2892:Learner$LeaderConnector@445] - Unexpected exception, tries=0, remaining init limit=1000, connecting to /127.0.1.1:2892
java.net.ConnectException: Connection refused (Connection refused)
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
    at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
    at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    at java.net.Socket.connect(Socket.java:607)
    at org.apache.zookeeper.server.quorum.Learner.sockConnect(Learner.java:302)
    at org.apache.zookeeper.server.quorum.Learner$LeaderConnector.connectToLeader(Learner.java:419)
    at org.apache.zookeeper.server.quorum.Learner$LeaderConnector.run(Learner.java:377)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
2021-11-23 17:02:47,172 [myid:2] - WARN  [LeaderConnector-/127.0.1.1:2892:Learner$LeaderConnector@445] - Unexpected exception, tries=1, remaining init limit=899, connecting to /127.0.1.1:2892
java.net.ConnectException: Connection refused (Connection refused)
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
    at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
    at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    at java.net.Socket.connect(Socket.java:607)
    at org.apache.zookeeper.server.quorum.Learner.sockConnect(Learner.java:302)
    at org.apache.zookeeper.server.quorum.Learner$LeaderConnector.connectToLeader(Learner.java:419)
    at org.apache.zookeeper.server.quorum.Learner$LeaderConnector.run(Learner.java:377)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
2021-11-23 17:02:47,273 [myid:2] - INFO  [LeaderConnector-/127.0.1.1:2892:Learner$LeaderConnector@381] - Successfully connected to leader, using address: /127.0.1.1:2892
2021-11-23 17:02:47,288 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):QuorumPeer@915] - Peer state changed: following - synchronization
2021-11-23 17:02:47,291 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):Learner@562] - Getting a diff from the leader 0x0
2021-11-23 17:02:47,291 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):QuorumPeer@920] - Peer state changed: following - synchronization - diff
2021-11-23 17:02:47,295 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):Learner@734] - Learner received NEWLEADER message
2021-11-23 17:02:47,296 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):QuorumPeer@1867] - Dynamic reconfig is disabled, we don't store the last seen config.
2021-11-23 17:02:47,369 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):QuorumPeer@920] - Peer state changed: following - synchronization
2021-11-23 17:02:47,381 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):CommitProcessor@491] - Configuring CommitProcessor with readBatchSize -1 commitBatchSize 1
2021-11-23 17:02:47,381 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):CommitProcessor@452] - Configuring CommitProcessor with 12 worker threads.
2021-11-23 17:02:47,382 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):FollowerRequestProcessor@59] - Initialized FollowerRequestProcessor with zookeeper.follower.skipLearnerRequestToNextProcessor as false
2021-11-23 17:02:47,386 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):RequestThrottler@75] - zookeeper.request_throttler.shutdownTimeout = 10000
2021-11-23 17:02:47,731 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@390] - Notification: my state:FOLLOWING; n.sid:4, n.state:LOOKING, n.leader:4, n.round:0x2, n.peerEpoch:0x1, n.zxid:0x0, message format version:0x2, n.config version:0x0
2021-11-23 17:02:47,732 [myid:2] - INFO  [ListenerHandler-/127.0.1.1:3889:QuorumCnxManager$Listener$ListenerHandler@1071] - Received connection request from /127.0.0.1:35042
2021-11-23 17:02:47,736 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@390] - Notification: my state:FOLLOWING; n.sid:3, n.state:LOOKING, n.leader:3, n.round:0x2, n.peerEpoch:0x1, n.zxid:0x0, message format version:0x2, n.config version:0x0
2021-11-23 17:02:47,741 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@390] - Notification: my state:FOLLOWING; n.sid:3, n.state:LOOKING, n.leader:4, n.round:0x2, n.peerEpoch:0x1, n.zxid:0x0, message format version:0x2, n.config version:0x0
2021-11-23 17:02:47,798 [myid:2] - WARN  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):Follower@131] - Exception when following the leader
java.net.SocketTimeoutException: Read timed out
    at java.net.SocketInputStream.socketRead0(Native Method)
    at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
    at java.net.SocketInputStream.read(SocketInputStream.java:171)
    at java.net.SocketInputStream.read(SocketInputStream.java:141)
    at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
    at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
    at java.io.DataInputStream.readInt(DataInputStream.java:387)
    at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:96)
    at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:86)
    at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:134)
    at org.apache.zookeeper.server.quorum.Learner.readPacket(Learner.java:225)
    at org.apache.zookeeper.server.quorum.Learner.syncWithLeader(Learner.java:625)
    at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:109)
    at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1514)
2021-11-23 17:02:47,798 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):Follower@145] - Disconnected from leader (with address: /127.0.1.1:2892). Was connected for 524ms. Sync state: false
2021-11-23 17:02:47,799 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):Follower@289] - shutdown Follower
2021-11-23 17:02:47,799 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):QuorumPeer@915] - Peer state changed: looking
2021-11-23 17:02:47,799 [myid:2] - WARN  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):QuorumPeer@1560] - PeerState set to LOOKING
2021-11-23 17:02:47,799 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):QuorumPeer@1430] - LOOKING
2021-11-23 17:02:47,799 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):FastLeaderElection@945] - New election. My id = 2, proposed zxid=0x0
2021-11-23 17:02:47,801 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@390] - Notification: my state:LOOKING; n.sid:2, n.state:LOOKING, n.leader:2, n.round:0x2, n.peerEpoch:0x1, n.zxid:0x0, message format version:0x2, n.config version:0x0
2021-11-23 17:02:47,804 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@390] - Notification: my state:LOOKING; n.sid:3, n.state:FOLLOWING, n.leader:5, n.round:0x1, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x0
2021-11-23 17:02:47,805 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@390] - Notification: my state:LOOKING; n.sid:1, n.state:LOOKING, n.leader:5, n.round:0x1, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x0
2021-11-23 17:02:47,806 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@390] - Notification: my state:LOOKING; n.sid:5, n.state:LEADING, n.leader:5, n.round:0x1, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x0
2021-11-23 17:02:47,806 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@390] - Notification: my state:LOOKING; n.sid:1, n.state:FOLLOWING, n.leader:5, n.round:0x1, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x0
2021-11-23 17:02:47,806 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):QuorumPeer@901] - Peer state changed: following
2021-11-23 17:02:47,807 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):QuorumPeer@1512] - FOLLOWING
2021-11-23 17:02:47,807 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):ZooKeeperServer@1300] - minSessionTimeout set to 400
2021-11-23 17:02:47,807 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):ZooKeeperServer@1309] - maxSessionTimeout set to 4000
2021-11-23 17:02:47,807 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):ResponseCache@45] - getData response cache size is initialized with value 400.
2021-11-23 17:02:47,807 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):ResponseCache@45] - getChildren response cache size is initialized with value 400.
2021-11-23 17:02:47,807 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):RequestPathMetricsCollector@109] - zookeeper.pathStats.slotCapacity = 60
2021-11-23 17:02:47,807 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):RequestPathMetricsCollector@110] - zookeeper.pathStats.slotDuration = 15
2021-11-23 17:02:47,808 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@390] - Notification: my state:FOLLOWING; n.sid:1, n.state:FOLLOWING, n.leader:5, n.round:0x1, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x0
2021-11-23 17:02:47,813 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):RequestPathMetricsCollector@111] - zookeeper.pathStats.maxDepth = 6
2021-11-23 17:02:47,814 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):RequestPathMetricsCollector@112] - zookeeper.pathStats.initialDelay = 5
2021-11-23 17:02:47,814 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):RequestPathMetricsCollector@113] - zookeeper.pathStats.delay = 5
2021-11-23 17:02:47,814 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):RequestPathMetricsCollector@114] - zookeeper.pathStats.enabled = false
2021-11-23 17:02:47,814 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):ZooKeeperServer@1536] - The max bytes for all large requests are set to 104857600
2021-11-23 17:02:47,814 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):ZooKeeperServer@1550] - The large request threshold is set to -1
2021-11-23 17:02:47,815 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):AuthenticationHelper@66] - zookeeper.enforce.auth.enabled = false
2021-11-23 17:02:47,815 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):AuthenticationHelper@67] - zookeeper.enforce.auth.schemes = []
2021-11-23 17:02:47,815 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):ZooKeeperServer@361] - Created server with tickTime 200 minSessionTimeout 400 maxSessionTimeout 4000 clientPortListenBacklog -1 datadir data2/version-2 snapdir data2/version-2
2021-11-23 17:02:47,815 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):Follower@77] - FOLLOWING - LEADER ELECTION TOOK - 16 MS
2021-11-23 17:02:47,815 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):QuorumPeer@915] - Peer state changed: following - discovery
2021-11-23 17:02:47,817 [myid:2] - INFO  [LeaderConnector-/127.0.1.1:2892:Learner$LeaderConnector@381] - Successfully connected to leader, using address: /127.0.1.1:2892
2021-11-23 17:02:47,819 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):QuorumPeer@915] - Peer state changed: following - synchronization
2021-11-23 17:02:47,820 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):Learner@562] - Getting a diff from the leader 0x0
2021-11-23 17:02:47,821 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):QuorumPeer@920] - Peer state changed: following - synchronization - diff
2021-11-23 17:02:47,821 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):Learner@734] - Learner received NEWLEADER message
2021-11-23 17:02:47,821 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):QuorumPeer@1867] - Dynamic reconfig is disabled, we don't store the last seen config.
2021-11-23 17:02:47,824 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):QuorumPeer@920] - Peer state changed: following - synchronization
2021-11-23 17:02:47,825 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):CommitProcessor@491] - Configuring CommitProcessor with readBatchSize -1 commitBatchSize 1
2021-11-23 17:02:47,825 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):CommitProcessor@452] - Configuring CommitProcessor with 12 worker threads.
2021-11-23 17:02:47,825 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):FollowerRequestProcessor@59] - Initialized FollowerRequestProcessor with zookeeper.follower.skipLearnerRequestToNextProcessor as false
2021-11-23 17:02:47,829 [myid:2] - WARN  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):MBeanRegistry@110] - Failed to register MBean InMemoryDataTree
2021-11-23 17:02:47,829 [myid:2] - WARN  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):LearnerZooKeeperServer@104] - Failed to register with JMX
javax.management.InstanceAlreadyExistsException: org.apache.ZooKeeperService:name0=ReplicatedServer_id2,name1=replica.2,name2=Follower,name3=InMemoryDataTree
    at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
    at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
    at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
    at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
    at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
    at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
    at org.apache.zookeeper.jmx.MBeanRegistry.register(MBeanRegistry.java:106)
    at org.apache.zookeeper.server.quorum.LearnerZooKeeperServer.registerJMX(LearnerZooKeeperServer.java:102)
    at org.apache.zookeeper.server.ZooKeeperServer.startupWithServerState(ZooKeeperServer.java:709)
    at org.apache.zookeeper.server.ZooKeeperServer.startupWithoutServing(ZooKeeperServer.java:692)
    at org.apache.zookeeper.server.quorum.Learner.syncWithLeader(Learner.java:757)
    at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:109)
    at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1514)
2021-11-23 17:02:47,870 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):Learner@718] - Learner received UPTODATE message
2021-11-23 17:02:47,870 [myid:2] - INFO  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):QuorumPeer@915] - Peer state changed: following - broadcast
2021-11-23 17:02:48,009 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@390] - Notification: my state:FOLLOWING; n.sid:4, n.state:LOOKING, n.leader:4, n.round:0x2, n.peerEpoch:0x1, n.zxid:0x0, message format version:0x2, n.config version:0x0
2021-11-23 17:02:48,113 [myid:2] - WARN  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):Follower@172] - Got zxid 0x100000001 expected 0x1
2021-11-23 17:02:48,114 [myid:2] - INFO  [SyncThread:2:FileTxnLog@284] - Creating new log file: log.100000001
2021-11-23 17:02:48,127 [myid:2] - INFO  [CommitProcessor:2:LearnerSessionTracker@116] - Committing global session 0x1000b301f680000
2021-11-23 17:02:48,155 [myid:2] - WARN  [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):Follower@172] - Got zxid 0x100000002 expected 0x100000003
2021-11-23 17:02:48,162 [myid:2] - WARN  [SyncThread:2:FileTxnLog@275] - Current zxid 4294967298 is <= 4294967298 for 1
2021-11-23 17:02:48,724 [myid:2] - ERROR [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):FollowerZooKeeperServer@104] - Committing zxid 0x100000003 but next pending txn 0x100000002
2021-11-23 17:02:48,726 [myid:2] - ERROR [QuorumPeer[myid=2](plain=[0:0:0:0:0:0:0:0]:2182)(secure=disabled):ServiceUtils@42] - Exiting JVM with code 12
 {code}",[],Bug,ZOOKEEPER-4418,Major,Yong-Hao Zou,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Committing zxid 0x100000003 but next pending txn 0x100000002,2021-11-23T09:41:42.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-11-18T13:19:39.000+0000,Anil Kumar Ravva,"Netty related jars have downgraded in latest(3.7.0) to lower version compared the previous versions of zookeeper(3.6.3). 

Please find the details below:

Current Zookeeper Version 3.7.0: netty 4.1.59 version jars have been used

Old Zookeeper version apache-zookeeper-3.6.3 : netty 4.1.63 have been used.

Is there any reason behind this change?

There are so many security advances have been made as part of netty upgrade, All those were missed here. 

 

Is there any plan of adding this jar in the future release.",[],Bug,ZOOKEEPER-4417,Critical,Anil Kumar Ravva,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper 3.7.0 : Netty related jars to be upgraded to 4.1.68 for Security upgrade,2021-11-22T04:24:33.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",3.0
,[],2021-11-17T08:08:04.000+0000,May,"There is a ZooKeeper cluster with three nodes: zk1, zk2 and zk3.

1. zk1 was stopped for a while;
2. restart zk1, and it starts to follow the current leader;
3. zk1 receives snapshot from leader;
4. zk1 receives UPTODATE message from leader;
5. zk1 takes the snapshot of the current data state;
6. zk1 creates the {{currentEpoch.tmp}} file;
7. zk1 crashes before writing current epoch to {{currentEpoch.tmp}} file;
8. restart zk1, and it fails due to ""Unable to load database on disk"" error:

{code:java}
java.io.IOException: Found null in /home/zk-3.6.3/zkData/version-2/currentEpoch.tmp
        at java.lang.Throwable.fillInStackTrace(Throwable.java)
        at java.lang.Throwable.fillInStackTrace(Throwable.java:784)
        at java.lang.Throwable.<init>(Throwable.java:266)
        at java.lang.Exception.<init>(Exception.java:66)
        at java.io.IOException.<init>(IOException.java:58)
        at org.apache.zookeeper.server.quorum.QuorumPeer.readLongFromFile(QuorumPeer.java:2116)
        at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:1118)
        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:1079)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:227)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:136)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:90)
{code}
",[],Bug,ZOOKEEPER-4416,Major,May,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Null currentEpoch.tmp fails the server,2021-11-19T06:58:17.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-11-15T04:52:02.000+0000,Santosh Kumar Sahu,"We are trying to add TLSv1.3 support in Zookeeper, currently by default TLSv1.2 is supported.

Following are the configuration
{code:java}
ssl.protocol=TLSv1.3
ssl.enabledProtocols=TLSv1.3,TLSv1.2
serverCnxnFactory=org.apache.zookeeper.server.NettyServerCnxnFactory
sslQuorumReloadCertFiles=true
quorumListenOnAllIPs=true
secureClientPort=2281
sslQuorum=false
portUnification=true
ssl.quorum.clientAuth=need
ssl.quorum.hostnameVerification=true
ssl.quorum.keyStore.location=/opt/zookeeper/cert/cert1.pem
ssl.quorum.trustStore.location=/opt/zookeeper/cert/cacert.pem
ssl.trustStore.location=/opt/zookeeper/cert/ca/clientcacert.pem
ssl.keyStore.location=/opt/zookeeper/cert/cert1.pem
ssl.clientAuth=need

{code}
by setting  ""{*}ssl.enabledProtocols=TLSv1.3,TLSv1.2{*}"", only TLSv1.2 communication is working but for TLSv1.3 following error coming

 
{code:java}
2021-10-07T12:24:44.121+0000 [myid:] - ERROR [nioEventLoopGroup-4-2:NettyServerCnxnFactory$CertificateVerifier@434] - Unsuccessful handshake with session 0                          x0
2021-10-07T12:24:44.123+0000 [myid:] - WARN  [nioEventLoopGroup-4-2:NettyServerCnxnFactory$CnxnChannelHandler@273] - Exception caught
io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: The client supported protocol versions [TLSv1.3] are not accepted by server p                          references [TLS12]
        at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:471) ~[netty-codec-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276) ~[netty-codec-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.50.Final.jar:4.1.5                          0.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.50.Final.jar:4.1.5                          0.Final]
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.50.Final.jar:4.1.50.                          Final]
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.50.Final.jar:4.1.50.Final                          ]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.50.Final.jar:4.1.5                          0.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.50.Final.jar:4.1.5                          0.Final]
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [netty-common-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-common-4.1.50.Final.jar:4.1.50.Final]
        at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: javax.net.ssl.SSLHandshakeException: The client supported protocol versions [TLSv1.3] are not accepted by server preferences [TLS12]
        at sun.security.ssl.Alert.createSSLException(Alert.java:131) ~[?:?]
        at sun.security.ssl.Alert.createSSLException(Alert.java:117) ~[?:?]
        at sun.security.ssl.TransportContext.fatal(TransportContext.java:336) ~[?:?]
        at sun.security.ssl.TransportContext.fatal(TransportContext.java:292) ~[?:?]
        at sun.security.ssl.TransportContext.fatal(TransportContext.java:283) ~[?:?]
        at sun.security.ssl.ClientHello$ClientHelloConsumer.negotiateProtocol(ClientHello.java:916) ~[?:?]
        at sun.security.ssl.ClientHello$ClientHelloConsumer.onClientHello(ClientHello.java:832) ~[?:?]
        at sun.security.ssl.ClientHello$ClientHelloConsumer.consume(ClientHello.java:813) ~[?:?]
        at sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:392) ~[?:?]
        at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:443) ~[?:?]
        at sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:1074) ~[?:?]
        at sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:1061) ~[?:?]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:?]
        at sun.security.ssl.SSLEngineImpl$DelegatedTask.run(SSLEngineImpl.java:1008) ~[?:?]
        at io.netty.handler.ssl.SslHandler.runAllDelegatedTasks(SslHandler.java:1542) ~[netty-handler-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.ssl.SslHandler.runDelegatedTasks(SslHandler.java:1556) ~[netty-handler-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1440) ~[netty-handler-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1267) ~[netty-handler-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1314) ~[netty-handler-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[netty-codec-4.1.50.Final.jar:4.1.50.                          Final]
        at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[netty-codec-4.1.50.Final.jar:4.1.50.Final]
        ... 17 more

{code}
error""The client supported protocol versions [TLSv1.3] are not accepted by server preferences""

 

 

Zookeeper using {*}netty 4.1.50  which support TLSv1.3{*}( netty 4.1.31 onwards support TLSv1.3  ref: [https://netty.io/news/2018/10/30/4-1-31-Final.html])

when trying to openssl with -tls1_3 to connect with zookeeper over TLS port it failed with following error coming
{code:java}
openssl s_client --connect zookeeper1:2281 --cert /run/secret/client/clicert.pem --key /run/secret/client/cliprivkey.pem --CAfile /run/secret/ca/cacert.pem -tls1_3
CONNECTED(00000003)
140629337047680:error:1409442E:SSL routines:ssl3_read_bytes:tlsv1 alert protocol version:ssl/record/rec_layer_s3.c:1544:SSL alert number 70
---
no peer certificate available
---
No client certificate CA names sent
---
SSL handshake has read 7 bytes and written 318 bytes
Verification: OK
---
New, (NONE), Cipher is (NONE)
Secure Renegotiation IS NOT supported
Compression: NONE
Expansion: NONE
No ALPN negotiated
Early data was not sent
Verify return code: 0 (ok)

{code}
 

and if *ssl.enabledProtocols=TLSv1.3*  (only TLSv1.3) then TLSv1.2 also not working and following error coming in logs
{code:java}
 at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [netty-common-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-common-4.1.50.Final.jar:4.1.50.Final]
        at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: javax.net.ssl.SSLHandshakeException: No appropriate protocol (protocol is disabled or cipher suites are inappropriate)
        at sun.security.ssl.HandshakeContext.<init>(HandshakeContext.java:170) ~[?:?]
        at sun.security.ssl.ServerHandshakeContext.<init>(ServerHandshakeContext.java:62) ~[?:?]
        at sun.security.ssl.TransportContext.kickstart(TransportContext.java:222) ~[?:?]
        at sun.security.ssl.SSLEngineImpl.readRecord(SSLEngineImpl.java:491) ~[?:?]
        at sun.security.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:454) ~[?:?]
        at sun.security.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:433) ~[?:?]
        at javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:637) ~[?:?]
        at io.netty.handler.ssl.SslHandler$SslEngineType$3.unwrap(SslHandler.java:282) ~[netty-handler-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1372) ~[netty-handler-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1267) ~[netty-handler-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1314) ~[netty-handler-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[netty-codec-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[netty-codec-4.1.50.Final.jar:4.1.50.Final]
        ... 17 more

{code}
error "" No appropriate protocol (protocol is disabled or cipher suites are inappropriate)""

I wonder if TLSv1.3 is really supported in zookeeper or not, if yes then from which version onwards?

so, would need help to enable TLSv1.3 support,

let us know if any further information required.",[],Bug,ZOOKEEPER-4415,Blocker,Santosh Kumar Sahu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper 3.7.0 : The client supported protocol versions [TLSv1.3] are not accepted by server preferences,2022-03-02T13:34:17.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",4.0
,"[<JIRA Component: name='java client', id='12312381'>]",2021-11-09T13:03:25.000+0000,tian,"set Negotiated session timeout: 40000

I have thousands of zk clients under a zk cluster, when switch failure, some of  clients did not send suspend event to state listner after 204988ms, but indeed session was expired in server as expected in server after 40000ms
{code:java}
// code placeholder
[10.10.10.2][my-app][2021-11-05 13:55:38.701][ WARN][????][16.195.152:2181)][o.a.z.ClientCnxn                        ] : [][][]Client session timed out, have not heard from server in 204988ms for sessionid 0x405005d85480c3d 
[10.10.10.2][my-app][2021-10-30 15:55:39.275][ WARN][????][16.195.152:218)][o.a.z.ClientCnxn                        ] : [][][]Unable to reconnect to ZooKeeper service, session 0x405005d85480c3d has expired 
[10.10.10.2][my-app][2021-10-30 15:55:39.275][ WARN][????][ain-EventThread][o.a.c.ConnectionState                   ] : [][][]Session expired event received{code}
why after {color:#ff0000}204988ms this client found client session timeout ? socket read timeout = 2/3 * sessiont time out , it  blocked too long to discover network failure. what may cause this scene?{color}",[],Bug,ZOOKEEPER-4412,Critical,tian,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,client blocked too long before session timeout,2021-11-09T13:27:35.000+0000,"[<JIRA Version: name='3.4.14', id='12343587'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-11-02T03:04:43.000+0000,TangHq,"Hi,

 

When zookeeper is running, there will be a warning message in the log and fter zookeeper has been running for a period of time, the connection between the client and the server will be lost after about one or two days.

 

There are my log report:

2021-10-29 09:27:32,358 [myid:] - INFO [SessionTracker:ZooKeeperServer@628] - Expiring session 0x10013a65a780163, timeout of 20000ms exceeded
2021-10-29 09:27:47,827 [myid:] - WARN [NIOWorkerThread-84:NIOServerCnxn@371] - Unexpected exception
EndOfStreamException: Unable to read additional data from client, it probably closed the socket: address = /10.*.*.*:34658, session = 0x10013a65a780164
 at org.apache.zookeeper.server.NIOServerCnxn.handleFailedRead(NIOServerCnxn.java:170)
 at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:333)
 at org.apache.zookeeper.server.NIOServerCnxnFactory$IOWorkRequest.doWork(NIOServerCnxnFactory.java:508)
 at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:154)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2021-10-29 09:28:12,358 [myid:] - INFO [SessionTracker:ZooKeeperServer@628] - Expiring session 0x10013a65a780164, timeout of 20000ms exceeded
2021-10-29 09:30:14,272 [myid:] - WARN [NIOWorkerThread-103:NIOServerCnxn@371] - Unexpected exception
EndOfStreamException: Unable to read additional data from client, it probably closed the socket: address = /10.*.*.*:53590, session = 0x10013a65a780166
 at org.apache.zookeeper.server.NIOServerCnxn.handleFailedRead(NIOServerCnxn.java:170)
 at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:333)
 at org.apache.zookeeper.server.NIOServerCnxnFactory$IOWorkRequest.doWork(NIOServerCnxnFactory.java:508)
 at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:154)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
2021-10-29 09:30:42,358 [myid:] - INFO [SessionTracker:ZooKeeperServer@628] - Expiring session 0x10013a65a780166, timeout of 20000ms exceeded

 

I search something related of this warn in internet and increased tickTime, syncLimit, but it's also no effect.

 

And the client error:

WARNING - Connection dropped: outstanding heartbeat ping not received\n'
WARNING - Transition to CONNECTING\n'
INFO - [stdout] : b'4: 2021-10-31 23:23:10,581 - INFO - Zookeeper connection lost\n'
INFO - [stdout] : b'4: Traceback (most recent call last):\n'
INFO - [stdout] : b'4: File ""/home/kazoo/cluster.py"", line 376, in <module>\n'
INFO - [stdout] : b'4: test_cluster_config = sys.argv[1]\n'
INFO - [stdout] : b'4: File ""/home/kazoo/cluster.py"", line 219, in update_cumulus\n'
INFO - [stdout] : b'4: if label_not_found:\n'
INFO - [stdout] : b'4: File ""/home/kazoo/zkstate.py"", line 25, in processing\n'
INFO - [stdout] : b'4: return self._zk.exists(self._path+""/""+self._name+""processing"")\n'
INFO - [stdout] : b'4: File ""/usr/local/lib/python3.6/site-packages/kazoo/client.py"", line 1123, in exists\n'
INFO - [stdout] : b'4: return self.exists_async(path, watch=watch).get()\n'
INFO - [stdout] : b'4: File ""/usr/local/lib/python3.6/site-packages/kazoo/handlers/utils.py"", line 75, in get\n'
INFO - [stdout] : b'4: raise self._exception\n'
INFO - [stdout] : b'4: kazoo.exceptions.ConnectionLoss\n'

 ",[],Bug,ZOOKEEPER-4410,Major,TangHq,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Unable to read additional data from client, it probably closed the socket",2021-11-02T03:05:31.000+0000,"[<JIRA Version: name='3.7', id='12349953'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-11-01T08:15:49.000+0000,Yong-Hao Zou,"{code:java}
java.lang.NullPointerException
	at org.apache.zookeeper.server.quorum.SendAckRequestProcessor.flush(SendAckRequestProcessor.java:67)
	at org.apache.zookeeper.server.SyncRequestProcessor.flush(SyncRequestProcessor.java:248)
	at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:169)
{code}
The learner sock can be null when try to close it.",[],Bug,ZOOKEEPER-4409,Major,Yong-Hao Zou,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,NullPointerException in SendAckRequestProcessor,2021-12-24T10:15:01.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",2.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2021-10-31T05:41:56.000+0000,Ling Mao,,"[<JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-4406,Minor,Ling Mao,Fixed,2021-10-31T06:08:52.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,fix the znode type for Barrier implementation in the zookeeperTutorial.md,2021-10-31T06:08:52.000+0000,[],2.0
,[],2021-10-28T11:36:17.000+0000,WCM RnD,"Netty library used in ZooKeeper has the below high security vulnerabilities reported.
h2. BDSA-2021-2832

*Affected Component(s):* Netty Project
*Vulnerability Published:* 2021-09-23 06:15 EDT
*Vulnerability Updated:* 2021-09-23 06:15 EDT
*CVSS Score:* 6.5 (overall), {color:#FF0000}7.5{color} (base)

*Summary*: Netty is vulnerable to excessive memory usage due to being unable to set size restrictions on decompressed data input. An attacker could exploit this by supplying crafted input in order to cause a denial-of-service (DoS).

*Solution*: Fixed in version netty-4.1.68.Final 

 
h2. BDSA-2021-2831

*Affected Component(s):* Netty Project
*Vulnerability Published:* 2021-09-22 07:35 EDT
*Vulnerability Updated:* 2021-09-22 07:35 EDT
*CVSS Score:* 6.5 (overall), {color:#FF0000}7.5{color} (base)

*Summary*: Netty is susceptible to excessive memory usage due to missing chunk length restrictions and the potential buffering of reserved skippable chunks until the complete chunk has been received. An attacker could exploit this by supplying crafted input in order to cause a denial-of-service (DoS).

*Solution*: Fixed in version netty-4.1.68.Final 

 

Request to update the library to netty-4.1.68.Final where the vulnerability is fixed.",[],Bug,ZOOKEEPER-4405,Critical,WCM RnD,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,High Security issues reported with Netty  library bundled in ZooKeeper 3.6.3 and 3.7,2022-01-13T05:19:33.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>]",3.0
,"[<JIRA Component: name='security', id='12329414'>]",2021-10-28T09:01:54.000+0000,Dominique Mongelli,"netty has reported a couple of CVEs regarding the usage of Bzip2Decoder and SnappyFrameDecoder. 

Reference :

[CVE-2021-37136 - https://github.com/netty/netty/security/advisories/GHSA-grg4-wf29-r9vv|https://github.com/netty/netty/security/advisories/GHSA-grg4-wf29-r9vv]

[CVE-2021-37137 - https://github.com/netty/netty/security/advisories/GHSA-9vjp-v76f-g363|https://github.com/netty/netty/security/advisories/GHSA-9vjp-v76f-g363]

 

Can we upgrade Netty to version 4.1.68.Final to fix this ?",[],Bug,ZOOKEEPER-4404,Minor,Dominique Mongelli,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Upgrade Netty to 4.1.68 for CVE fixes,2022-03-28T07:31:11.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",3.0
,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='security', id='12329414'>]",2021-10-25T11:33:06.000+0000,Marc Richter,"When using SSL certificates for public IPv6 DNS endpoints as received from some public Service like ""Let's Encrypt"" for Quorum Encryption, Zookeeper validates the SNAs of that certificate for the IP Address instead of the DNS name, as configured.

As a Result, these certificates can't be used, since no certificates for IPv6 IPs issued.

This has been observed with Zookeeper Version 3.5.9, which is the one bundled in the most recent release of Kafka (2.8.1).

In the affected environment, there is a 3-node-Zookeeper-Cluster, which is configured as this in {{zookeeper.properties}} (mind the DNS name!):
{code}
server.1=zookeeper1.ourdomain.cloud:2888:3888
server.2=zookeeper2.ourdomain.cloud:2888:3888
server.3=zookeeper3.ourdomain.cloud:2888:3888
{code}
All these records do have a public IPv6 entry only.

The SSL certificates from Let's Encrypt are requested and added to the Quorum-Keystores like this:

# Using https://github.com/acmesh-official/acme.sh
# Requesting the cert from Let's Encrypt using:
   {code}./acme.sh --issue --dns dns_nsupdate -d zookeeper1.ourdomain.cloud{code}
   for each system.
# Merge fullchain- and certificate-file to a single PKCS12 file using:
   {code}openssl pkcs12 -export -in <certfile> -inkey <keyfile> -out <pkcs12_file> -name zookeeper1.ourdomain.cloud \
    -CAfile <fullchainfile> -password <JKS_password>{code}
# Adding the resulting PKCS12 file to the Quorum Keystore:
   {code}keytool -importkeystore -deststorepass <JKS_password> -destkeypass <JKS_password> -deststoretype pkcs12 \
    -srckeystore <pkcs12_file> -srcstoretype PKCS12 -srcstorepass <JKS_password> -destkeystore <quorum_jks> \
    -alias zookeeper1.ourdomain.cloud{code}

When any of the systems tries to initiate the quorum-connect, their logs state that the remote's Certificates could not be verified, since the SNA-List does not contain the IPv6 address.
For example: This is the log from {{zookeeper2.ourdomain.cloud}} when connecting {{zookeeper3.ourdomain.cloud}}:

{code}
[2021-10-13 15:13:49,960] INFO Received connection request from /2a01:--CUT--:750:47566 (org.apache.zookeeper.server.quorum.QuorumCnxManager)
[2021-10-13 15:13:50,094] ERROR Failed to verify host address: 2a01:--CUT--:750 (org.apache.zookeeper.common.ZKTrustManager)
javax.net.ssl.SSLPeerUnverifiedException: Certificate for <2a01:--CUT--:750> doesn't match any of the subject alternative names: [zookeeper3.ourdomain.cloud]
{code}

I think the log lines cited clearly show:
# Zookeeper is picking up the correct certificate from the quorum Keystore, since it states that the request does not match  any SNA and lists {{zookeeper3.ourdomain.cloud}} only, which it can only know from the certificate itself.
# Zookeeper is validating the wrong thing here: Even though the config clearly states to use a DNS name, the  certificates SNAs alre validated against the IPv6 address that record belongs to instead of the DNS name configured ({{ERROR Failed to verify host address: 2a01:--CUT--:750}})",[],Bug,ZOOKEEPER-4403,Major,Marc Richter,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Quorum TLS certificate validation uses wrong name,2022-02-17T16:48:15.000+0000,"[<JIRA Version: name='3.5.9', id='12348201'>]",5.0
,[],2021-10-23T18:01:32.000+0000,Zhewei Hu,"In the ZooKeeperServer.java, we report the number of outstanding requests by below method:
{code:java}
/**     
* return the outstanding requests     
* in the queue, which haven't been     
* processed yet     
*/    
public long getOutstandingRequests() {        
    return getInProcess();    
}
{code}
However, based on my understanding, the getInProcess() method returns the number of requests in the process pipeline (requestsInProcess) since we increase the value of requestsInProcess after the request enters the PreRequestProcessor and decrease the value of requestsInProcess after the request in the FinalRequestProcess.

Since we have already maintain the outstandingCount in ServerCnxn.java, shouldn't we call the getOutstandingRequests() method to report the number of outstanding requests?",[],Bug,ZOOKEEPER-4401,Minor,Zhewei Hu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper may report the number of outstanding requests incorrectly,2021-10-23T18:01:59.000+0000,[],1.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-10-22T11:23:12.000+0000,Sagar Satyawan Parab,"We have three(3) node zookeeper cluster running as a pod on Kubernetes cluster,
 Zookeeper version is 3.6.2,we are implementing graceful termination in our zookeeper development through 
 *terminationGracePeriodSeconds:*

*refer link:  [https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/*]

When we delete pods then kuberentes sends SIGTERM signal and wait for graceful termination(*terminationGracePeriodSeconds=30)* time before forceful deletion, then service should catch this signal(sigterm) and should shutdown gracefully within 30 seconds. 

 But  zookeeper  doesn't seems to be handling this sigterm and not terminating gracefully terminated without graceful.

Trying deleting the pod

 
{code:java}
 kubectl delete pod/test-zk-0 -n test
{code}
 
{code:java}
kubectl log -f pod/test-zk-0 -n test

2021-10-22T07:54:57.655+0000 [myid:] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=[0:0:0:0:0:0:0:0]:2281):QuorumPeer@868] - Peer state changed: following - synchronization
2021-10-22T07:54:57.661+0000 [myid:] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=[0:0:0:0:0:0:0:0]:2281):CommitProcessor@476] - Configuring CommitProcessor with readBatchSize -1 commitBatchSize 1
2021-10-22T07:54:57.662+0000 [myid:] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=[0:0:0:0:0:0:0:0]:2281):CommitProcessor@438] - Configuring CommitProcessor with 2 worker threads.
2021-10-22T07:54:57.666+0000 [myid:] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=[0:0:0:0:0:0:0:0]:2281):RequestThrottler@74] - zookeeper.request_throttler.shutdownTimeout = 10000
2021-10-22T07:54:57.680+0000 [myid:] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=[0:0:0:0:0:0:0:0]:2281):QuorumPeer@863] - Peer state changed: following - broadcast{code}
zookeeper not terminating gracefully when we delete the pod not logs of zookeeper shutdown coming and immediately pod get deleted.

 

Expectation: Zookeeper must shutdown gracefully when we delete the pod and its evidence should be coming in logs 

 

 

 ",[],Bug,ZOOKEEPER-4400,Major,Sagar Satyawan Parab,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper not getting Graceful Termination,2021-10-27T10:07:16.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",2.0
,"[<JIRA Component: name='jute', id='12312385'>]",2021-10-15T06:17:33.000+0000,Ryan,"After running for a while, the entire cluster (3 zookeeper) crash suddenly 

ERROR－［main:QuorumPeer@1148］- Unable to load database on disk 
java.io.IOException: Unreasonable length = 3015236
          at org.apache.jute.BinaryInputArchive.checkLength(BinaryInputArchive.java:166)
          at.org.apache.jute.BinaryInputArchive.readBuffer(BinaryInputArchive.java:127)",[],Bug,ZOOKEEPER-4397,Major,Ryan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper crashes: Unable to load database on disk java.io.IOException: Unreasonable length,2021-10-21T07:21:39.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>]",3.0
,"[<JIRA Component: name='tests', id='12312427'>]",2021-10-12T17:06:59.000+0000,Andrey Emelyanenko,"{code:java}
org.apache.zookeeper.test.system.SimpleSysTest.testSimpleCase


KeeperErrorCode = ConnectionLoss for /sysTest/availableСтек 


org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /sysTest/available
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:102)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:54)
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:2366)
	at org.apache.zookeeper.test.system.InstanceManager.<init>(InstanceManager.java:101)
	at org.apache.zookeeper.test.system.BaseSysTest.setUp(BaseSysTest.java:60)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:142)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:117)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



(STDOUT)2021-10-12 15:35:45,291 [myid:] - INFO  [main:Environment@98] - Client environment:zookeeper.version=3.6.2-3.5.0.2-1-SNAPSHOT-d329520cef8878e30344bf627dc694dcd151c8f9-dirty, built on 2021-10-12 15:11 UTC
2021-10-12 15:35:45,295 [myid:] - INFO  [main:Environment@98] - Client environment:host.name=e3a73c76c1c7
2021-10-12 15:35:45,296 [myid:] - INFO  [main:Environment@98] - Client environment:java.version=1.8.0_262
2021-10-12 15:35:45,296 [myid:] - INFO  [main:Environment@98] - Client environment:java.vendor=Oracle Corporation
2021-10-12 15:35:45,296 [myid:] - INFO  [main:Environment@98] - Client environment:java.home=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.262.b10-0.el7_8.x86_64/jre
2021-10-12 15:35:45,296 [myid:] - INFO  [main:Environment@98] - Client environment:java.class.path=/var/lib/jenkins_slave/workspace/r_feature_SDP-3143_upgrade_to_37/zookeeper-it/target/classes:/var/lib/jenkins_slave/workspace/r_feature_SDP-3143_upgrade_to_37/zookeeper-it/target/classes:/var/lib/jenkins_slave/workspace/r_feature_SDP-3143_upgrade_to_37/zookeeper-server/target/classes:/var/lib/jenkins_slave/workspace/r_feature_SDP-3143_upgrade_to_37/zookeeper-jute/target/zookeeper-jute-3.6.2.3.5.0.2-1-SNAPSHOT.jar:/var/cache/.m2/repository/org/apache/yetus/audience-annotations/0.12.0/audience-annotations-0.12.0.jar:/var/cache/.m2/repository/io/netty/netty-handler/4.1.59.Final/netty-handler-4.1.59.Final.jar:/var/cache/.m2/repository/io/netty/netty-common/4.1.59.Final/netty-common-4.1.59.Final.jar:/var/cache/.m2/repository/io/netty/netty-resolver/4.1.59.Final/netty-resolver-4.1.59.Final.jar:/var/cache/.m2/repository/io/netty/netty-buffer/4.1.59.Final/netty-buffer-4.1.59.Final.jar:/var/cache/.m2/repository/io/netty/netty-transport/4.1.59.Final/netty-transport-4.1.59.Final.jar:/var/cache/.m2/repository/io/netty/netty-codec/4.1.59.Final/netty-codec-4.1.59.Final.jar:/var/cache/.m2/repository/io/netty/netty-transport-native-epoll/4.1.59.Final/netty-transport-native-epoll-4.1.59.Final.jar:/var/cache/.m2/repository/io/netty/netty-transport-native-unix-common/4.1.59.Final/netty-transport-native-unix-common-4.1.59.Final.jar:/var/cache/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/var/cache/.m2/repository/org/slf4j/slf4j-log4j12/1.7.30/slf4j-log4j12-1.7.30.jar:/var/cache/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/var/lib/jenkins_slave/workspace/r_feature_SDP-3143_upgrade_to_37/zookeeper-server/target/test-classes:/var/cache/.m2/repository/org/junit/vintage/junit-vintage-engine/5.6.2/junit-vintage-engine-5.6.2.jar:/var/cache/.m2/repository/org/apiguardian/apiguardian-api/1.1.0/apiguardian-api-1.1.0.jar:/var/cache/.m2/repository/org/junit/platform/junit-platform-engine/1.6.2/junit-platform-engine-1.6.2.jar:/var/cache/.m2/repository/org/opentest4j/opentest4j/1.2.0/opentest4j-1.2.0.jar:/var/cache/.m2/repository/org/junit/platform/junit-platform-commons/1.6.2/junit-platform-commons-1.6.2.jar:/var/cache/.m2/repository/junit/junit/4.13/junit-4.13.jar:/var/cache/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/var/cache/.m2/repository/org/openjdk/jmh/jmh-core/1.23/jmh-core-1.23.jar:/var/cache/.m2/repository/net/sf/jopt-simple/jopt-simple/4.6/jopt-simple-4.6.jar:/var/cache/.m2/repository/org/apache/commons/commons-math3/3.2/commons-math3-3.2.jar:/var/cache/.m2/repository/org/openjdk/jmh/jmh-generator-annprocess/1.23/jmh-generator-annprocess-1.23.jar:
2021-10-12 15:35:45,296 [myid:] - INFO  [main:Environment@98] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2021-10-12 15:35:45,296 [myid:] - INFO  [main:Environment@98] - Client environment:java.io.tmpdir=/tmp
2021-10-12 15:35:45,297 [myid:] - INFO  [main:Environment@98] - Client environment:java.compiler=<NA>
2021-10-12 15:35:45,297 [myid:] - INFO  [main:Environment@98] - Client environment:os.name=Linux
2021-10-12 15:35:45,297 [myid:] - INFO  [main:Environment@98] - Client environment:os.arch=amd64
2021-10-12 15:35:45,297 [myid:] - INFO  [main:Environment@98] - Client environment:os.version=3.10.0-1160.36.2.el7.x86_64
2021-10-12 15:35:45,297 [myid:] - INFO  [main:Environment@98] - Client environment:user.name=jenkins
2021-10-12 15:35:45,297 [myid:] - INFO  [main:Environment@98] - Client environment:user.home=/var/cache
2021-10-12 15:35:45,297 [myid:] - INFO  [main:Environment@98] - Client environment:user.dir=/var/lib/jenkins_slave/workspace/r_feature_SDP-3143_upgrade_to_37/zookeeper-it
2021-10-12 15:35:45,297 [myid:] - INFO  [main:Environment@98] - Client environment:os.memory.free=1829MB
2021-10-12 15:35:45,299 [myid:] - INFO  [main:Environment@98] - Client environment:os.memory.max=27305MB
2021-10-12 15:35:45,299 [myid:] - INFO  [main:Environment@98] - Client environment:os.memory.total=1930MB
2021-10-12 15:35:45,303 [myid:] - INFO  [main:ZooKeeper@637] - Initiating client connection, connectString=e3a73c76c1c7:2181 sessionTimeout=15000 watcher=org.apache.zookeeper.test.system.BaseSysTest$1@77ec78b9
2021-10-12 15:35:45,308 [myid:] - INFO  [main:X509Util@77] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2021-10-12 15:35:45,313 [myid:] - INFO  [main:ClientCnxnSocket@239] - jute.maxbuffer value is 1048575 Bytes
2021-10-12 15:35:45,321 [myid:] - INFO  [main:ClientCnxn@1726] - zookeeper.request.timeout value is 0. feature enabled=false
2021-10-12 15:35:45,330 [myid:e3a73c76c1c7:2181] - INFO  [main-SendThread(e3a73c76c1c7:2181):ClientCnxn$SendThread@1171] - Opening socket connection to server e3a73c76c1c7/172.17.0.5:2181.
2021-10-12 15:35:45,330 [myid:e3a73c76c1c7:2181] - INFO  [main-SendThread(e3a73c76c1c7:2181):ClientCnxn$SendThread@1173] - SASL config status: Will not attempt to authenticate using SASL (unknown error)
2021-10-12 15:35:45,336 [myid:e3a73c76c1c7:2181] - WARN  [main-SendThread(e3a73c76c1c7:2181):ClientCnxn$SendThread@1290] - Session 0x0 for sever e3a73c76c1c7/172.17.0.5:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:344)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1280)
2021-10-12 15:35:46,441 [myid:e3a73c76c1c7:2181] - INFO  [main-SendThread(e3a73c76c1c7:2181):ClientCnxn$SendThread@1171] - Opening socket connection to server e3a73c76c1c7/172.17.0.5:2181.
2021-10-12 15:35:46,442 [myid:e3a73c76c1c7:2181] - INFO  [main-SendThread(e3a73c76c1c7:2181):ClientCnxn$SendThread@1173] - SASL config status: Will not attempt to authenticate using SASL (unknown error)
2021-10-12 15:35:46,442 [myid:e3a73c76c1c7:2181] - WARN  [main-SendThread(e3a73c76c1c7:2181):ClientCnxn$SendThread@1290] - Session 0x0 for sever e3a73c76c1c7/172.17.0.5:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:344)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1280)
2021-10-12 15:35:47,543 [myid:e3a73c76c1c7:2181] - INFO  [main-SendThread(e3a73c76c1c7:2181):ClientCnxn$SendThread@1171] - Opening socket connection to server e3a73c76c1c7/172.17.0.5:2181.
2021-10-12 15:35:47,544 [myid:e3a73c76c1c7:2181] - INFO  [main-SendThread(e3a73c76c1c7:2181):ClientCnxn$SendThread@1173] - SASL config status: Will not attempt to authenticate using SASL (unknown error)
2021-10-12 15:35:47,544 [myid:e3a73c76c1c7:2181] - WARN  [main-SendThread(e3a73c76c1c7:2181):ClientCnxn$SendThread@1290] - Session 0x0 for sever e3a73c76c1c7/172.17.0.5:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:344)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1280)
2021-10-12 15:35:48,645 [myid:e3a73c76c1c7:2181] - INFO  [main-SendThread(e3a73c76c1c7:2181):ClientCnxn$SendThread@1171] - Opening socket connection to server e3a73c76c1c7/172.17.0.5:2181.
2021-10-12 15:35:48,645 [myid:e3a73c76c1c7:2181] - INFO  [main-SendThread(e3a73c76c1c7:2181):ClientCnxn$SendThread@1173] - SASL config status: Will not attempt to authenticate using SASL (unknown error)
2021-10-12 15:35:48,646 [myid:e3a73c76c1c7:2181] - WARN  [main-SendThread(e3a73c76c1c7:2181):ClientCnxn$SendThread@1290] - Session 0x0 for sever e3a73c76c1c7/172.17.0.5:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:344)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1280)
2021-10-12 15:35:49,747 [myid:e3a73c76c1c7:2181] - INFO  [main-SendThread(e3a73c76c1c7:2181):ClientCnxn$SendThread@1171] - Opening socket connection to server e3a73c76c1c7/172.17.0.5:2181.
2021-10-12 15:35:49,747 [myid:e3a73c76c1c7:2181] - INFO  [main-SendThread(e3a73c76c1c7:2181):ClientCnxn$SendThread@1173] - SASL config status: Will not attempt to authenticate using SASL (unknown error)
2021-10-12 15:35:49,748 [myid:e3a73c76c1c7:2181] - WARN  [main-SendThread(e3a73c76c1c7:2181):ClientCnxn$SendThread@1290] - Session 0x0 for sever e3a73c76c1c7/172.17.0.5:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:344)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1280)
2021-10-12 15:35:50,849 [myid:e3a73c76c1c7:2181] - INFO  [main-SendThread(e3a73c76c1c7:2181):ClientCnxn$SendThread@1171] - Opening socket connection to server e3a73c76c1c7/172.17.0.5:2181.
2021-10-12 15:35:50,849 [myid:e3a73c76c1c7:2181] - INFO  [main-SendThread(e3a73c76c1c7:2181):ClientCnxn$SendThread@1173] - SASL config status: Will not attempt to authenticate using SASL (unknown error)
2021-10-12 15:35:50,849 [myid:e3a73c76c1c7:2181] - WARN  [main-SendThread(e3a73c76c1c7:2181):ClientCnxn$SendThread@1290] - Session 0x0 for sever e3a73c76c1c7/172.17.0.5:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:344)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1280)
{code}",[],Bug,ZOOKEEPER-4395,Major,Andrey Emelyanenko,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,SimpleSysTest.testSimpleCase ConnectionLoss,2021-10-13T17:47:31.000+0000,[],1.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-10-09T08:47:52.000+0000,Liu Haifeng,"ZooKeeper follower node encountered NullPointerException during syncWithLeader.

Logs indicate that the follower has received NEWLEADER packet between a PROPOSAL packet and it's corresponding COMMIT packet. The NEWLEADER packet leads to packetsNotCommitted.clear(), yet the COMMIT packet still wants to do packetsNotCommitted.peekFirst() to get the former PROPOSAL packet, and the later if-statement raised NPE.
{code:java}
case Leader.COMMIT:
case Leader.COMMITANDACTIVATE:
    pif = packetsNotCommitted.peekFirst();
    if (pif.hdr.getZxid() == qp.getZxid() && qp.getType() == Leader.COMMITANDACTIVATE) {
        // ...
    }{code}
After look into the Leader side, I found:
 # LearnerHandler.syncFollower queues packets with zxid <= maxCommittedLog (PROPOSAL/COMMIT pairs);
 # Leader.startForwarding queues toBeApplied packets(PROPOSAL/COMMIT pairs);
 # Leader.startForwarding queues outstandingProposals packets(PROSOAL only);
 # LeanerHandler.run sends NEWLEADER message.

Seams if the outstandingProposals is not empty at the certain moment, the follower could then receive PROPOSAL/NEWLEADER/COMMIT packets in order.

The follower will retry from LOOKING again and is expected to be succeed at last, however, under heavy load it may be too many retries. Further more, I my case the follower has to sync data from leader's disk, and start over again after the NPE(prior sync not flushed?), which may harm the leader.

I don't know if it is designed so or not, but consider the performance, can we at least avoid wasting of network/disk IO?",[],Bug,ZOOKEEPER-4394,Major,Liu Haifeng,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Learner.syncWithLeader got NullPointerException,2021-10-09T08:50:38.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",1.0
,"[<JIRA Component: name='security', id='12329414'>]",2021-10-08T11:17:25.000+0000,Dipesh Kumar Dutta,"In my environment zookeeper is running in fips mode of 3 node cluster. My service is also running in fips mode with security provider org.bouncycastle.jcajce.provider.BouncyCastleFipsProvider

And from the my service when I am trying to connect to zookeeper I am getting the below error.
{code:java}
2021-10-06 17:14:52,645 [nioEventLoopGroup-5-1] WARN  io.netty.channel.ChannelInitializer - opc.request.id=none - Failed to initialize a channel. Closing: [id: 0xa129ece9] -
org.apache.zookeeper.common.X509Exception$SSLContextException: java.security.KeyManagementException: FIPS mode: only SunJSSE TrustManagers may be used
	at org.apache.zookeeper.common.X509Util.createSSLContextAndOptionsFromConfig(X509Util.java:386)
	at org.apache.zookeeper.common.X509Util.createSSLContextAndOptions(X509Util.java:328)
	at org.apache.zookeeper.common.X509Util.createSSLContext(X509Util.java:256)
{code}
The reason is the zookeeper has its own trust manager implementation which is 
{code:java}
public class ZKTrustManager extends X509ExtendedTrustManager
{code}
and jdk also provide a trust manager implementation as below.
{code:java}
X509TrustManagerImpl extends X509ExtendedTrustManager implements X509TrustManager
{code}
Because of this hierarchy in SSLContextImpl::chooseTrustManager() method the below instance check become false and hence it falls to the exception block.
{code:java}
if (SunJSSE.isFIPS() && !(var1[var2] instanceof X509TrustManagerImpl)) {
    throw new KeyManagementException(""FIPS mode: only SunJSSE TrustManagers may be used"");
}
{code}
 

 ",[],Bug,ZOOKEEPER-4393,Major,Dipesh Kumar Dutta,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Problem to connect to zookeeper in FIPS mode,2022-01-30T13:41:43.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>]",4.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-10-07T14:05:15.000+0000,Anoop Negi,"We are trying to add TLSv1.3 support in Zookeeper, currently by default TLSv1.2 is supported.

Following are the configuration

 
{code:java}
ssl.protocol=TLSv1.3
ssl.enabledProtocols=TLSv1.3,TLSv1.2
serverCnxnFactory=org.apache.zookeeper.server.NettyServerCnxnFactory
sslQuorumReloadCertFiles=true
quorumListenOnAllIPs=true
secureClientPort=2281
sslQuorum=false
portUnification=true
ssl.quorum.clientAuth=need
ssl.quorum.hostnameVerification=true
ssl.quorum.keyStore.location=/opt/zookeeper/cert/cert1.pem
ssl.quorum.trustStore.location=/opt/zookeeper/cert/cacert.pem
ssl.trustStore.location=/opt/zookeeper/cert/ca/clientcacert.pem
ssl.keyStore.location=/opt/zookeeper/cert/cert1.pem
ssl.clientAuth=need

{code}
by setting  ""*ssl.enabledProtocols=TLSv1.3,TLSv1.2*"", only TLSv1.2 communication is working but for TLSv1.3 following error coming

 
{code:java}
2021-10-07T12:24:44.121+0000 [myid:] - ERROR [nioEventLoopGroup-4-2:NettyServerCnxnFactory$CertificateVerifier@434] - Unsuccessful handshake with session 0                          x0
2021-10-07T12:24:44.123+0000 [myid:] - WARN  [nioEventLoopGroup-4-2:NettyServerCnxnFactory$CnxnChannelHandler@273] - Exception caught
io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: The client supported protocol versions [TLSv1.3] are not accepted by server p                          references [TLS12]
        at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:471) ~[netty-codec-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276) ~[netty-codec-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.50.Final.jar:4.1.5                          0.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.50.Final.jar:4.1.5                          0.Final]
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.50.Final.jar:4.1.50.                          Final]
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.50.Final.jar:4.1.50.Final                          ]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.50.Final.jar:4.1.5                          0.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.50.Final.jar:4.1.5                          0.Final]
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [netty-common-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-common-4.1.50.Final.jar:4.1.50.Final]
        at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: javax.net.ssl.SSLHandshakeException: The client supported protocol versions [TLSv1.3] are not accepted by server preferences [TLS12]
        at sun.security.ssl.Alert.createSSLException(Alert.java:131) ~[?:?]
        at sun.security.ssl.Alert.createSSLException(Alert.java:117) ~[?:?]
        at sun.security.ssl.TransportContext.fatal(TransportContext.java:336) ~[?:?]
        at sun.security.ssl.TransportContext.fatal(TransportContext.java:292) ~[?:?]
        at sun.security.ssl.TransportContext.fatal(TransportContext.java:283) ~[?:?]
        at sun.security.ssl.ClientHello$ClientHelloConsumer.negotiateProtocol(ClientHello.java:916) ~[?:?]
        at sun.security.ssl.ClientHello$ClientHelloConsumer.onClientHello(ClientHello.java:832) ~[?:?]
        at sun.security.ssl.ClientHello$ClientHelloConsumer.consume(ClientHello.java:813) ~[?:?]
        at sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:392) ~[?:?]
        at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:443) ~[?:?]
        at sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:1074) ~[?:?]
        at sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:1061) ~[?:?]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:?]
        at sun.security.ssl.SSLEngineImpl$DelegatedTask.run(SSLEngineImpl.java:1008) ~[?:?]
        at io.netty.handler.ssl.SslHandler.runAllDelegatedTasks(SslHandler.java:1542) ~[netty-handler-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.ssl.SslHandler.runDelegatedTasks(SslHandler.java:1556) ~[netty-handler-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1440) ~[netty-handler-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1267) ~[netty-handler-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1314) ~[netty-handler-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[netty-codec-4.1.50.Final.jar:4.1.50.                          Final]
        at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[netty-codec-4.1.50.Final.jar:4.1.50.Final]
        ... 17 more

{code}
error""The client supported protocol versions [TLSv1.3] are not accepted by server preferences""

 

 

Zookeeper using *netty 4.1.50  which support TLSv1.3*( netty 4.1.31 onwards support TLSv1.3  ref: [https://netty.io/news/2018/10/30/4-1-31-Final.html])

when trying to openssl with -tls1_3 to connect with zookeeper over TLS port it failed with following error coming
{code:java}
openssl s_client --connect zookeeper1:2281 --cert /run/secret/client/clicert.pem --key /run/secret/client/cliprivkey.pem --CAfile /run/secret/ca/cacert.pem -tls1_3
CONNECTED(00000003)
140629337047680:error:1409442E:SSL routines:ssl3_read_bytes:tlsv1 alert protocol version:ssl/record/rec_layer_s3.c:1544:SSL alert number 70
---
no peer certificate available
---
No client certificate CA names sent
---
SSL handshake has read 7 bytes and written 318 bytes
Verification: OK
---
New, (NONE), Cipher is (NONE)
Secure Renegotiation IS NOT supported
Compression: NONE
Expansion: NONE
No ALPN negotiated
Early data was not sent
Verify return code: 0 (ok)

{code}
 

and if *ssl.enabledProtocols=TLSv1.3*  (only TLSv1.3) then TLSv1.2 also not working and following error coming in logs
{code:java}
 at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [netty-common-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-common-4.1.50.Final.jar:4.1.50.Final]
        at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: javax.net.ssl.SSLHandshakeException: No appropriate protocol (protocol is disabled or cipher suites are inappropriate)
        at sun.security.ssl.HandshakeContext.<init>(HandshakeContext.java:170) ~[?:?]
        at sun.security.ssl.ServerHandshakeContext.<init>(ServerHandshakeContext.java:62) ~[?:?]
        at sun.security.ssl.TransportContext.kickstart(TransportContext.java:222) ~[?:?]
        at sun.security.ssl.SSLEngineImpl.readRecord(SSLEngineImpl.java:491) ~[?:?]
        at sun.security.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:454) ~[?:?]
        at sun.security.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:433) ~[?:?]
        at javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:637) ~[?:?]
        at io.netty.handler.ssl.SslHandler$SslEngineType$3.unwrap(SslHandler.java:282) ~[netty-handler-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1372) ~[netty-handler-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1267) ~[netty-handler-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1314) ~[netty-handler-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[netty-codec-4.1.50.Final.jar:4.1.50.Final]
        at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[netty-codec-4.1.50.Final.jar:4.1.50.Final]
        ... 17 more

{code}
error "" No appropriate protocol (protocol is disabled or cipher suites are inappropriate)""

I wonder if TLSv1.3 is really supported in zookeeper or not, if yes then from which version onwards?

so, would need help to enable TLSv1.3 support,

let us know if any further information required.",[],Bug,ZOOKEEPER-4392,Blocker,Anoop Negi,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper 3.6.2 : The client supported protocol versions [TLSv1.3] are not accepted by server preferences,2022-01-12T12:50:00.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",2.0
,"[<JIRA Component: name='security', id='12329414'>]",2021-09-28T05:15:30.000+0000,Ananya Singh,"Our security tool raised the following security flaws on zookeeper 3.5.9: 

CVE-2021-28163: [https://nvd.nist.gov/vuln/detail/CVE-2021-28163|https://nvd.nist.gov/vuln/detail/CVE-2021-21295]

CVE-2021-28169: [https://nvd.nist.gov/vuln/detail/CVE-2021-28169|https://nvd.nist.gov/vuln/detail/CVE-2021-21295]

CVE-2021-34428: [https://nvd.nist.gov/vuln/detail/CVE-2021-34428|https://nvd.nist.gov/vuln/detail/CVE-2021-21295]

 

It is a vulnerability related to jar jetty-http-9.4.35.v20201120.jar",[],Bug,ZOOKEEPER-4390,Major,Ananya Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Backport ZOOKEEPER-4337 for branch-3.5 and branch-3.6,2021-10-01T08:16:03.000+0000,"[<JIRA Version: name='3.5.9', id='12348201'>]",4.0
,"[<JIRA Component: name='scripts', id='12312384'>]",2021-09-27T04:57:04.000+0000,Ready,"I connect to ZooKeeper server via zkCli.

I want to create a Znode using the following command:
{code:java}
// ""-1"" is data 
set /seata/server.maxCommitRetryTimeout -1

{code}
However, it threw an exception as follows:
{code:java}
org.apache.commons.cli.UnrecognizedOptionException: Unrecognized option: -1
{code}
I tried to use character "" ( double-quote ) ,  ' ( single-quote ) or  \ ( backslash ) to escape, but it still did NOT work.

I want to know, how should this special character be escaped in zookeeper?",[],Bug,ZOOKEEPER-4389,Major,Ready,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Cannot create a data node whose value starts with ""-""",2021-09-27T04:57:04.000+0000,"[<JIRA Version: name='3.7', id='12349953'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-09-26T14:33:04.000+0000,shixiaoxiao,"The follower/observer enable read only. When the node returns to normal from the partitioned, the ephemerals nodes will be  inconsistent with the leader node. The reason is that the request to close the timeout sessions is processed by the ReadOnly follower or observer  when they are partitioned and the ephemerals nodes created by these sessions also are delete. When the leader node uses diff to synchronize data with the follower/observer node, the transaction that needs to be synchronized does not include the creation of temporary nodes which created by sessions closed by followers.So the follower/observer  ephemerals nodes is inconsistent with leader.","[<JIRA Version: name='3.7.1', id='12350030'>]",Bug,ZOOKEEPER-4388,Major,shixiaoxiao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Recover from network partition, follower/observer ephemerals nodes is inconsistent with leader",2022-03-29T17:22:35.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.6.2', id='12347809'>]",2.0
Brahma Reddy Battula,[],2021-09-23T08:10:52.000+0000,Brahma Reddy Battula,Backport ZOOKEEPER-4278 to branch-3.5 to address CVE-2021-21409,"[<JIRA Version: name='3.5.10', id='12349434'>]",Bug,ZOOKEEPER-4385,Major,Brahma Reddy Battula,Fixed,2021-09-24T14:57:32.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Backport ZOOKEEPER-4278 to branch-3.5 to Address CVE-2021-21409,2021-09-24T16:50:16.000+0000,[],1.0
,[],2021-09-14T13:12:56.000+0000,linmaolin,"In the receiveMessage method the count is increased right after processPacket without judges anything as below.
{quote}if (initialized) {
 // TODO: if zks.processPacket() is changed to take a ByteBuffer[],
 // we could implement zero-copy queueing.
 zks.processPacket(this, bb);

if (zks.shouldThrottle(outstandingCount.incrementAndGet())) {
 disableRecvNoWait();
 }
 }
{quote}
But after the request is handled, the decrease operation is taken only when xid is larger than 0.
{quote}@Override
 public void sendResponse(ReplyHeader h, Record r, String tag)
 throws IOException
Unknown macro: \{ if (closingChannel || !channel.isOpen()) Unknown macro}
super.sendResponse(h, r, tag);
 if (h.getXid() > 0)
Unknown macro: \{ // zks cannot be null otherwise we would not have gotten here! if (!zkServer.shouldThrottle(outstandingCount.decrementAndGet())) Unknown macro}
}
 }
{quote}
 

So the bultin xids like ""PING"", ""AUTH"", will make outstandingCount larger and larger, until it hits the limit; All the request on that connection will be refused.

 

I see the problem is solved in 3.6.0 version, should there be a patch for 3.5.9? Looking forward for your reply, Thantks!","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-4378,Major,linmaolin,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"org.apache.zookeeper.server.NettyServerCnxn does not increase outstandingCount properly, cause outstandingCount to overflow.",2021-09-15T06:52:49.000+0000,"[<JIRA Version: name='3.5.9', id='12348201'>]",2.0
Ling Mao,"[<JIRA Component: name='server', id='12312382'>]",2021-09-14T12:40:02.000+0000,Ling Mao,"{code:java}
blishment complete on server localhost/127.0.0.1:2180, sessionid = 0x1000278adba0129, negotiated timeout = 30000blishment complete on server localhost/127.0.0.1:2180, sessionid = 0x1000278adba0129, negotiated timeout = 30000java.lang.NullPointerException at org.apache.zookeeper.KeeperException.create(KeeperException.java:94) at org.apache.zookeeper.KeeperException.create(KeeperException.java:54) at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1538) at site.ycsb.db.zookeeper.ZKClient.insert(ZKClient.java:131) at site.ycsb.DBWrapper.insert(DBWrapper.java:227) at site.ycsb.workloads.CoreWorkload.doInsert(CoreWorkload.java:621) at site.ycsb.ClientThread.run(ClientThread.java:135) at java.lang.Thread.run(Thread.java:748)java.lang.NullPointerExceptionjava.lang.NullPointerException
 at org.apache.zookeeper.KeeperException.create(KeeperException.java:94) at org.apache.zookeeper.KeeperException.create(KeeperException.java:54) at org.apache.zookeeper.KeeperException.create(KeeperException.java:94) at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1538)
{code}","[<JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>]",Bug,ZOOKEEPER-4377,Minor,Ling Mao,Fixed,2021-10-17T07:10:43.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,KeeperException.create has NullPointerException when low version client requests the high version server,2022-01-28T11:23:35.000+0000,"[<JIRA Version: name='3.8.0', id='12349587'>]",1.0
,"[<JIRA Component: name='java client', id='12312381'>]",2021-09-13T20:56:03.000+0000,Gaspard Petit,"In commit [https://github.com/apache/zookeeper/commit/11c07921c15e2fb7692375327b53f26a583b77ca] the message printed when closing a connection to Zookeeper was changed from being logged as a debug message to a warning message.  The comment next to the message confirms that this is ""expected"" and therefore, it seems unjustified to use a warning level. 

Can we tune this down back to debug (or info) level? I would not mind submitting a pull request if there is consensus.

The full error we are seeing is the following:

 

{{An exception was thrown while closing send thread for session 0x1067b8748f00775.}}

 

with stack strace:

{{EndOfStreamException: Unable to read additional data from server sessionid 0x1067b8748f00775, likely server has closed socket at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:77) at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350) at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1280)}}

 ",[],Bug,ZOOKEEPER-4376,Minor,Gaspard Petit,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Zookeeper should not warn about ""An exception was thrown while closing send thread""",2022-02-01T21:53:17.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",4.0
Ling Mao,[],2021-09-12T09:47:30.000+0000,Ling Mao,"{code:java}
[zk: 127.0.0.1:2180(CONNECTED) 0] create /benchmark-09-14 ""123""
Created /benchmark-09-14
[zk: 127.0.0.1:2180(CONNECTED) 1] setquota -N 100 /benchmark-09-14
[zk: 127.0.0.1:2180(CONNECTED) 2]
[zk: 127.0.0.1:2180(CONNECTED) 2]
[zk: 127.0.0.1:2180(CONNECTED) 2] listquota /benchmark-09-14
absolute path is /zookeeper/quota/benchmark-09-14/zookeeper_limits
Output quota for /benchmark-09-14 count=-1,bytes=-1=;byteHardLimit=-1;countHardLimit=100
Output stat for /benchmark-09-14 count=115,bytes=135999
[zk: 127.0.0.1:2180(CONNECTED) 3] getAllChildrenNumber /benchmark-09-14
114

# we have 50 multiply concurrent client to create zonods under the parent node:/benchmark-09-14

➜  YCSB git:(master) ✗ ./bin/ycsb load zookeeper -s -threads 50 -P workloads/workloadb -p zookeeper.connectString=127.0.0.1:2180/benchmark-09-14

{code}",[],Bug,ZOOKEEPER-4375,Major,Ling Mao,,,This issue is being actively worked on at the moment by the assignee.,In Progress,0.0,Quota cannot limit the specify value when multiply clients create/set znodes,2021-09-14T12:44:22.000+0000,"[<JIRA Version: name='3.8.0', id='12349587'>]",1.0
,"[<JIRA Component: name='tests', id='12312427'>]",2021-09-11T06:00:50.000+0000,Ling Mao,"URL: [https://github.com/apache/zookeeper/pull/1742/checks?check_run_id=3450664638]
{code:java}
[INFO] Running org.apache.zookeeper.test.AsyncHammerTest
839[ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 49.215 s <<< FAILURE! - in org.apache.zookeeper.test.AsyncHammerTest
840[ERROR] testHammer  Time elapsed: 24.611 s  <<< ERROR!
841java.lang.NullPointerException
842	at org.apache.zookeeper.test.QuorumBase.startServers(QuorumBase.java:221)
843	at org.apache.zookeeper.test.QuorumBase.startServers(QuorumBase.java:182)
844	at org.apache.zookeeper.test.AsyncHammerTest.restart(AsyncHammerTest.java:59)
845	at org.apache.zookeeper.test.AsyncHammerTest.testHammer(AsyncHammerTest.java:194)
{code}",[],Bug,ZOOKEEPER-4374,Minor,Ling Mao,Invalid,2021-09-11T06:03:54.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Flaky test: org.apache.zookeeper.test.AsyncHammerTest#testHammer,2021-09-11T06:03:54.000+0000,[],1.0
,"[<JIRA Component: name='recipes', id='12313246'>]",2021-09-09T21:50:35.000+0000,Colin McIntosh,"According to the Zookeeper docs ZNode sequence numbers will rollover to -2147483648 after reaching 2147483647 (I've also confirmed this is the case in practice):

[https://zookeeper.apache.org/doc/r3.7.0/zookeeperProgrammers.html#Sequence+Nodes+–+Unique+Naming|https://zookeeper.apache.org/doc/r3.7.0/zookeeperProgrammers.html#Sequence+Nodes+%E2%80%93+Unique+Naming]

 

When this rollover happens the negative symbol is incorrectly stripped off the sequence number in ZNodeName due to this code searching for the last index of ""-"", thus the sequence number parsed is always the absolute value:
[https://github.com/apache/zookeeper/blob/1e74a3395a9c3c4d67093f9a073bffb5a176916c/zookeeper-recipes/zookeeper-recipes-lock/src/main/java/org/apache/zookeeper/recipes/lock/ZNodeName.java#L54]

 

This results in any subsequently created sequence numbers being considered the ""lowest"" due to the absolute value decrementing until reaching 0. It also looks like there is no test to check parsing of negative sequence numbers.

 

For context, this same issue arose in the unofficial Golang client recipe and there is a proposed fix there: [https://github.com/go-zookeeper/zk/pull/65]","[<JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-4372,Major,Colin McIntosh,Fixed,2021-10-02T06:00:09.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper lock recipe doesn't parse negative sequence numbers correctly,2021-10-02T06:00:09.000+0000,[],1.0
,[],2021-09-07T15:18:48.000+0000,Tifenn LE GOFF,"Some ZK cannot join cluster after moment
{code:java}
echo stat|nc $HOSTNAME 2181
This ZooKeeper instance is not currently serving requests
{code}
We have 3 ZK, zk1 with id1, zk2 with id2 and zk3 with id3.

ZK2 and ZK3 are already running. When ZK1 connect to ZK, we have
{code:java}
2021-09-07 13:33:09,585 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):FastLeaderElection@979] - Notification time out: 60000
2021-09-07 13:33:09,586 [myid:1] - INFO  [QuorumConnectionThread-[myid=1]-55:QuorumCnxManager@513] - Have smaller server identifier, so dropping the connection: (myId:1 --> sid:2)
2021-09-07 13:33:09,586 [myid:1] - INFO  [QuorumConnectionThread-[myid=1]-56:QuorumCnxManager@513] - Have smaller server identifier, so dropping the connection: (myId:1 --> sid:3)
2021-09-07 13:33:30,269 [myid:1] - WARN  [NIOWorkerThread-1:NIOServerCnxn@373] - Close of session 0x0
java.io.IOException: ZooKeeperServer not running
	at org.apache.zookeeper.server.NIOServerCnxn.readLength(NIOServerCnxn.java:544)
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:332)
	at org.apache.zookeeper.server.NIOServerCnxnFactory$IOWorkRequest.doWork(NIOServerCnxnFactory.java:522)
	at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:154)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
2021-09-07 13:33:30,941 [myid:1] - WARN  [NIOWorkerThread-2:NIOServerCnxn@373] - Close of session 0x0
java.io.IOException: ZooKeeperServer not running
	at org.apache.zookeeper.server.NIOServerCnxn.readLength(NIOServerCnxn.java:544)
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:332)
	at org.apache.zookeeper.server.NIOServerCnxnFactory$IOWorkRequest.doWork(NIOServerCnxnFactory.java:522)
	at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:154)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$W{code}
and on zk2 (actual leader), we have
{code:java}
2021-09-07 13:33:09,587 [myid:2] - INFO  [ListenerHandler-fqdn-zk2/172.17.0.2:3888:QuorumCnxManager$Listener$ListenerHandler@1070] - Received connection request from /ip-zk1:53102
2021-09-07 13:33:09,588 [myid:2] - WARN  [SendWorker:1:QuorumCnxManager$SendWorker@1281] - Interrupted while waiting for message on queue
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(Unknown Source)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
	at org.apache.zookeeper.util.CircularBlockingQueue.poll(CircularBlockingQueue.java:105)
	at org.apache.zookeeper.server.quorum.QuorumCnxManager.pollSendQueue(QuorumCnxManager.java:1446)
	at org.apache.zookeeper.server.quorum.QuorumCnxManager.access$900(QuorumCnxManager.java:98)
	at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:1270)
2021-09-07 13:33:09,588 [myid:2] - WARN  [SendWorker:1:QuorumCnxManager$SendWorker@1293] - Send worker leaving thread id 1 my id = 2
2021-09-07 13:33:09,588 [myid:2] - WARN  [RecvWorker:1:QuorumCnxManager$RecvWorker@1395] - Connection broken for id 1, my id = 2
java.net.SocketException: Socket closed
	at java.base/java.net.SocketInputStream.socketRead0(Native Method)
	at java.base/java.net.SocketInputStream.socketRead(Unknown Source)
	at java.base/java.net.SocketInputStream.read(Unknown Source)
	at java.base/java.net.SocketInputStream.read(Unknown Source)
	at java.base/java.io.BufferedInputStream.fill(Unknown Source)
	at java.base/java.io.BufferedInputStream.read(Unknown Source)
	at java.base/java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:1383)
2021-09-07 13:33:09,589 [myid:2] - WARN  [RecvWorker:1:QuorumCnxManager$RecvWorker@1401] - Interrupting SendWorker thread from RecvWorker. sid: 1. myId: 2
2021-09-07 13:33:09,589 [myid:2] - INFO  [ListenerHandler-fqdn-zk2/172.17.0.2:3888:QuorumCnxManager$Listener$ListenerHandler@1070] - Received connection request from /172.17.0.2:35380
2021-09-07 13:33:09,590 [myid:2] - WARN  [ListenerHandler-fqdn-zk2/172.17.0.2:3888:QuorumCnxManager@662] - We got a connection request from a server with our own ID. This should be either a configuration error, or a bug.
{code}
If we restart leader, it works. This issue happen very often since we have migrate our ZK services on docker instances.",[],Bug,ZOOKEEPER-4371,Major,Tifenn LE GOFF,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,False ID conflict when ZK try to connect to cluster,2021-12-16T12:02:05.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",2.0
,[],2021-09-03T06:25:37.000+0000,startjava,"please self test !

 

:(",[],Bug,ZOOKEEPER-4369,Major,startjava,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"great setQuota -n , logs file no WARN !",2021-09-03T06:25:37.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",2.0
,[],2021-09-03T06:11:18.000+0000,startjava,"[zk: localhost:2181(CONNECTED) 2] ls /
[a, b, c, zookeeper]
[zk: localhost:2181(CONNECTED) 3] create /d
Created /d
[zk: localhost:2181(CONNECTED) 4] setquota -B 3 /d
[zk: localhost:2181(CONNECTED) 5] listquota /d
absolute path is /zookeeper/quota/d/zookeeper_limits
Output quota for /d count=-1,bytes=-1=;byteHardLimit=3;countHardLimit=-1
Output stat for /d count=1,bytes=0
[zk: localhost:2181(CONNECTED) 6] set /d 1234567890
[zk: localhost:2181(CONNECTED) 7] get /d
1234567890
[zk: localhost:2181(CONNECTED) 8]",[],Bug,ZOOKEEPER-4368,Major,startjava,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,great quota no exception!,2021-09-07T03:40:38.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",1.0
,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='kerberos', id='12329415'>]",2021-09-02T20:08:40.000+0000,Rushabh Shah,"We are seeing 1000's of Zookeeper#Login threads leak in our production clusters.
[ZooKeeperSaslClient#createSaslClient|https://github.com/apache/zookeeper/blob/branch-3.4.13/src/java/main/org/apache/zookeeper/client/ZooKeeperSaslClient.java#L205] creates Login thread.
[ZooKeeperSaslClient#createSaslToken |https://github.com/apache/zookeeper/blob/branch-3.4.13/src/java/main/org/apache/zookeeper/client/ZooKeeperSaslClient.java#L310] throws SaslException which propagates all the way back to [ClientCnxn#SendThread#run|https://github.com/apache/zookeeper/blob/branch-3.4.13/src/java/main/org/apache/zookeeper/ClientCnxn.java#L1074] method.

[ClientCnxn#SendThread#run|https://github.com/apache/zookeeper/blob/branch-3.4.13/src/java/main/org/apache/zookeeper/ClientCnxn.java#L1075-L1078] handles SaslException by changing setting state to AUTH_FAILED, queueing the eventOfDeath for EventThread and exiting/cleaning up the SendThread but we DON'T close the zookeeperSaslClient which in turns shutDown the Login thread.

Logs are added below for one failed connection.
{noformat}
`20210831053800.393 jute.maxbuffer value is 4194304 Bytes
`20210831053800.393 Initiating client connection, connectString=<zookeeper-ensemble string> sessionTimeout=4000 watcher=org.apache.curator.ConnectionState@7b974f93

`20210831053800.401 zookeeper.request.timeout value is 10000. feature enabled=
`20210831053800.404 Client successfully logged in.
`20210831053800.405 Client will use GSSAPI as SASL mechanism.
`20210831053800.405 TGT refresh sleeping until: Wed Sep 01 00:59:06 GMT 2021
`20210831053800.405 TGT refresh thread started.
`20210831053800.405 TGT valid starting at:        Tue Aug 31 05:38:00 GMT 2021
`20210831053800.405 TGT expires:                  Wed Sep 01 05:38:00 GMT 2021

`20210831053800.407 Opening socket connection to server <zookeeper-server-1>. Will attempt to SASL-authenticate using Login Context section 'Client'

`20210831053800.419 Socket connection established to <zookeeper-server-1>, initiating session

`20210831053800.435 Session establishment complete on server <zookeeper-server-1>, sessionid = 0x1000004066cc52b, negotiated timeout = 6000

`20210831053800.438 An error: (java.security.PrivilegedActionException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - UNKNOWN_SERVER)]) occurred when evaluating Zookeeper Quorum Member's  received SASL token. This may be caused by Java's being unable to resolve the Zookeeper Quorum Member's hostname correctly. You may want to try to adding '-Dsun.net.spi.nameservice.provider.1=dns,sun' to your client's JVMFLAGS environment. Zookeeper Client will go to AUTH_FAILED state.

`20210831053800.438 EventThread shut down for session: 0x1000004066cc52b

`20210831053800.438 SASL authentication with Zookeeper Quorum member failed: javax.security.sasl.SaslException: An error: (java.security.PrivilegedActionException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - UNKNOWN_SERVER)]) occurred when evaluating Zookeeper Quorum Member's  received SASL token. This may be caused by Java's being unable to resolve the Zookeeper Quorum Member's hostname correctly. You may want to try to adding '-Dsun.net.spi.nameservice.provider.1=dns,sun' to your client's JVMFLAGS environment. Zookeeper Client will go to AUTH_FAILED state.
{noformat}


What is the correct way to shutdown Login thread in case of SaslException ?
We use Curator framework to connect to Zookeeper.

We fixed similar bug here where we were leaking EventThreads.  ZOOKEEPER-3059
This is similar except for Login threads. Please help.

","[<JIRA Version: name='3.5.10', id='12349434'>, <JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>]",Bug,ZOOKEEPER-4367,Critical,Rushabh Shah,Fixed,2021-09-27T15:49:07.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper#Login thread leak in case of Sasl AuthFailed.,2021-10-20T07:36:31.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",3.0
,[],2021-09-01T10:31:37.000+0000,Heureux do,dfdfdfdfdf,[],Bug,ZOOKEEPER-4365,Major,Heureux do,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,df,2021-09-01T10:31:37.000+0000,[],1.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-09-01T00:19:57.000+0000,LiAoNan,"ZKDatabase.txnCount logged non transactional requests


{code:java}
    public boolean append(Request si) throws IOException {
        txnCount.incrementAndGet();
        return this.snapLog.append(si);
    }
{code}


snaplog.append may return false, but txnCount increased

maybe it would be better

{code:java}
    public boolean append(Request si) throws IOException {
        if (this.snapLog.append(si)) {
            txnCount.incrementAndGet();
            return true;
        }
        return false;
    }
{code}

","[<JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>, <JIRA Version: name='3.6.4', id='12350076'>]",Bug,ZOOKEEPER-4362,Major,LiAoNan,Fixed,2021-11-13T07:21:10.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZKDatabase.txnCount logged non transactional requests,2022-01-20T01:47:10.000+0000,[],1.0
,"[<JIRA Component: name='metric system', id='12334405'>]",2021-08-30T09:49:00.000+0000,Nicoló Boschi,"On a follower node, we had this error

{code}
ago 20, 2021 1:46:28 PM org.apache.catalina.core.StandardWrapperValve invoke
GRAVE: Servlet.service() for servlet [metrics] in context with path [/metrics] threw exception
java.lang.NullPointerException: Cannot invoke ""org.apache.zookeeper.server.quorum.Leader.getProposalStats()"" because the return value of ""org.apache.zookeeper.server.quorum.LeaderZooKeeperServer.getLeader()"" is null
        at org.apache.zookeeper.server.quorum.LeaderZooKeeperServer.lambda$registerMetrics$5(LeaderZooKeeperServer.java:122)
        at magnews.zookeeper.ZooKeeperMetricsProviderAdapter$MetricsContextImpl.lambda$registerGauge$0(ZooKeeperMetricsProviderAdapter.java:91)
{code}

Unfortunately, I'm not able to reproduce this error deterministically
 
","[<JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>, <JIRA Version: name='3.6.4', id='12350076'>]",Bug,ZOOKEEPER-4360,Major,Nicoló Boschi,Fixed,2021-09-02T16:51:39.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Avoid NPE during metrics execution if the leader is not set on a FOLLOWER node ,2021-09-02T16:53:09.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",1.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2021-08-29T05:31:05.000+0000,smoort,"Typo in [https://zookeeper.apache.org/doc/r3.4.13/zookeeperOver.html] page, section ""Nodes and ephemeral nodes"".

 

The paragraph begins with ""Unlike {color:#de350b}*is*{color} standard file systems, each node in a ZooKeeper namespace...""

 

Believe it was meant to be ""Unlike {color:#de350b}*in*{color} standard file systems, each node in a ZooKeeper namespace""",[],Bug,ZOOKEEPER-4359,Trivial,smoort,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Typo in documentation,2022-03-22T21:06:41.000+0000,[],2.0
,"[<JIRA Component: name='metric system', id='12334405'>]",2021-08-27T15:43:26.000+0000,Mathieu Gaudin,"Hi,

I'm trying to understand why the values of min/avg/max latency are showing surprising results. The graph below shows the max latency value of a particular node for last 7 days. The value increases gradually over time and it only ever decreases when the node gets restarted as if the metric value gets reset.

[https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/server/ServerStats.java#L226]

!image-2021-08-27-16-10-28-783.png|width=984,height=204!
 * 3 nodes
 * Keberos enabled
 * TGT ticket cashe enabled.

I believes the values of min/avg/max latency should show more realistic variations. It's very unlikely that the max latency value is expected to always increase while the node is running.

[https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/server/ServerStats.java#L142]

 _public void updateLatency(Request request, long currentTime) {_
 _long latency = currentTime - request.createTime;_
 _if (latency < 0) {_
 _return;_
 _}_
 _*{color:#FF0000}requestLatency.addDataPoint(latency);{color}*_
 _if (request.getHdr() != null) {_
 _// Only quorum request should have header_
 _ServerMetrics.getMetrics().UPDATE_LATENCY.add(latency);_
 _} else {_
 _// All read request should goes here_
 _ServerMetrics.getMetrics().READ_LATENCY.add(latency);_
 _}_

The method called let me think that the max latency metric gets set if the current values happens to be lower. __ 

[https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/server/metric/AvgMinMaxCounter.java#L51]

 _private void setMax(long value) {_
 *{color:#FF0000}_long current;_{color}*
 *{color:#FF0000}_while (value > (current = max.get()) && !max.compareAndSet(current, value)) {_{color}*
 _// no op_
 _}_
 _}_

I put below a graph of a particular from a totally different cluster for last 2 days. The node has not been restarted and all the data is from the same process. We can see a more realistic variations of the max latency metric as it would normally. 

!image-2021-08-27-16-37-50-112.png|width=1084,height=222!

Thanks for you time in advance,

Math",[],Bug,ZOOKEEPER-4358,Minor,Mathieu Gaudin,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Latency metrics showing surprising results for a keberos-enabled cluster,2021-08-27T15:43:55.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",1.0
,[],2021-08-27T12:57:33.000+0000,Serhii Hryb,"Hi.
I have 3 nodes in ZK cluster. On one of the nodes, I notice the following:


{code:java}
2021-08-27 15:11:27,060 [myid:1] - WARN [CommitProcessor:1:RateLogger@56] - [69918 times] Message: Digests are not matching. Value is Zxid. Last value:94489403000{code}
 
Please tell me how to fix this without reconnecting the node in the cluster?",[],Bug,ZOOKEEPER-4357,Minor,Serhii Hryb,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Digests are not matching,2021-08-27T14:51:42.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>]",1.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2021-08-27T12:41:10.000+0000,Nihal Jain,"The code snippets defined inside a {{fenced code blocks}} renders correctly in Markdown; but the same code snippet does not render properly in HTML sites. This makes the docs site very ugly and difficult to comprehend. (One has to figure out where the code block starts and where it ends.)

The issue exists throughout the docs site.

For example see one of the sample pages:
 https://github.com/apache/zookeeper/blob/master/zookeeper-docs/src/main/resources/markdown/zookeeperCLI.md 
 !markdown_rendering.png! 

vs 

http://zookeeper.apache.org/doc/current/zookeeperCLI.html  !html_rendering_before_fix.png! ","[<JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>]",Bug,ZOOKEEPER-4356,Major,Nihal Jain,Fixed,2021-09-01T08:48:34.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Code blocks do not render correctly in ZK docs site,2021-09-01T09:38:03.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",1.0
,[],2021-08-27T07:22:15.000+0000,Gaurav Bisht,"We are using  Zookeeper version: 3.4.8 of 3 server's cluster. Our cluster goes down / restart automatically with below error.

2021-08-26 21:50:21,323 [myid:1] - WARN [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@213] - Ignoring unexpected runtime exception
java.nio.channels.CancelledKeyException
 at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:73)
 at sun.nio.ch.SelectionKeyImpl.readyOps(SelectionKeyImpl.java:87)
 at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:182)
 at java.lang.Thread.run(Thread.java:748)

 

Please provide the resolution of this problem. 

 

Thanks

 ",[],Bug,ZOOKEEPER-4354,Major,Gaurav Bisht,,,The issue is open and ready for the assignee to start work on it.,Open,0.0, Ignoring unexpected runtime exception java.nio.channels.CancelledKeyException,2021-08-27T07:22:15.000+0000,[],1.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2021-08-26T09:25:28.000+0000,hong,"leader is zookeeper-0, it add zk-1and zk-2. when adding zk-3, zk-2 become leader, but it minCommittedLog=0. After adding zk-3, zk-3  just receive the DIFF message from zk-2, so resulting in inconsistent data.

Is there any error?
{code:java}
else if ((maxCommittedLog >= peerLastZxid) && (minCommittedLog <= peerLastZxid)) { 
  // Follower is within commitLog range                
  LOG.info(""Using committedLog for peer sid: {}"", getSid());
  Iterator<Proposal> itr = db.getCommittedLog().iterator();
  currentZxid = queueCommittedProposals(itr, peerLastZxid, null, maxCommittedLog);
  needSnap = false;            
} 
{code}
there are logs:

###ZK-2

021-08-26 02:02:07,698 [myid:3] - INFO [NIOWorkerThread-1:NIOServerCnxn@507] - Processing ruok command from /127.0.0.1:56458
 2021-08-26 02:02:08,046 [myid:3] - INFO [ListenerHandler-zookeeper-2.zookeeper-headless.default.svc.cluster.local/10.233.64.34:3888:QuorumCnxManager$Listener$ListenerHandler@1070] - Received connection request from /10.233.64.36:50788
 2021-08-26 02:02:08,072 [myid:3] - INFO [LearnerHandler-/10.233.64.36:60310:LearnerHandler@504] - Follower sid: 4 : info : zookeeper-3.zookeeper-headless.default.svc.cluster.local:2888:3888:observer;0.0.0.0:2181
 2021-08-26 02:02:08,073 [myid:3] - INFO [LearnerHandler-/10.233.64.36:60310:ZKDatabase@345] - On disk txn sync enabled with snapshotSizeFactor 0.33
 2021-08-26 02:02:08,074 [myid:3] - INFO [LearnerHandler-/10.233.64.36:60310:LearnerHandler@807] - *{color:#ff0000}Synchronizing with Learner sid: 4 maxCommittedLog=0x200000003 minCommittedLog=0x0 lastProcessedZxid=0x200000003 peerLastZxid=0x0{color}*
 *{color:#ff0000}2021-08-26 02:02:08,074 [myid:3] - INFO [LearnerHandler-/10.233.64.36:60310:LearnerHandler@871] - Using committedLog for peer sid: 4{color}*
 *{color:#ff0000}2021-08-26 02:02:08,074 [myid:3] - INFO [LearnerHandler-/10.233.64.36:60310:LearnerHandler@979] - Sending DIFF zxid=0x200000003 for peer sid: 4{color}*
 2021-08-26 02:02:09,250 [myid:3] - INFO [WorkerReceiver[myid=3]:FastLeaderElection$Messenger$WorkerReceiver@471] - WorkerReceiver is down
 2021-08-26 02:02:09,250 [myid:3] - INFO [WorkerSender[myid=3]:FastLeaderElection$Messenger$WorkerSender@505] - WorkerSender is down
 2021-08-26 02:02:12,898 [myid:3] - INFO [NIOWorkerThread-2:NIOServerCnxn@507] - Processing ruok command from /127.0.0.1:56516

 

###ZK-3

2021-08-26 02:02:08,064 [myid:4] - INFO [QuorumPeer[myid=4](plain=0.0.0.0:2181)(secure=disabled):ZooKeeperServer@1512] - The large request threshold is set to -1
 2021-08-26 02:02:08,064 [myid:4] - INFO [QuorumPeer[myid=4](plain=0.0.0.0:2181)(secure=disabled):ZooKeeperServer@339] - Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 clientPortListenBacklog -1 datadir /data/version-2 snapdir /data/version-2
 2021-08-26 02:02:08,064 [myid:4] - INFO [QuorumPeer[myid=4](plain=0.0.0.0:2181)(secure=disabled):ObserverZooKeeperServer@55] - syncEnabled =true
 2021-08-26 02:02:08,066 [myid:4] - INFO [QuorumPeer[myid=4](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@864] - Peer state changed: observing - discovery
 2021-08-26 02:02:08,067 [myid:4] - INFO [QuorumPeer[myid=4](plain=0.0.0.0:2181)(secure=disabled):Observer@163] - Observing new leader sid=3 addr=zookeeper-2.zookeeper-headless.default.svc.cluster.local/10.233.64.34:2888
 2021-08-26 02:02:08,068 [myid:4] - INFO [LeaderConnector-zookeeper-2.zookeeper-headless.default.svc.cluster.local/10.233.64.34:2888:Learner$LeaderConnector@370] - Successfully connected to leader, using address: zookeeper-2.zookeeper-headless.default.svc.cluster.local/10.233.64.34:2888
 2021-08-26 02:02:08,072 [myid:4] - INFO [QuorumPeer[myid=4](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@864] - Peer state changed: observing - synchronization
 2021-08-26 02:02:08,073 [myid:4] - INFO [QuorumPeer[myid=4](plain=0.0.0.0:2181)(secure=disabled):Learner@551] - *{color:#ff0000}Getting a diff from the leader 0x200000003{color}*
 2021-08-26 02:02:08,073 [myid:4] - INFO [QuorumPeer[myid=4](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@869] - Peer state changed: observing - synchronization - diff
 2021-08-26 02:02:08,077 [myid:4] - *{color:#ff0000}WARN [QuorumPeer[myid=4](plain=0.0.0.0:2181)(secure=disabled):Learner@617] - Got zxid 0x100000023 expected 0x1{color}*
 *{color:#ff0000}2021-08-26 02:02:08,078 [myid:4] - ERROR [QuorumPeer[myid=4](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@1836] - setLastSeenQuorumVerifier called with stale config 4294967332. Current version: 8589934595{color}*
 *{color:#ff0000}2021-08-26 02:02:08,080 [myid:4] - WARN [QuorumPeer[myid=4](plain=0.0.0.0:2181)(secure=disabled):Learner@617] - Got zxid 0x200000001 expected 0x10000002e{color}*
 2021-08-26 02:02:08,082 [myid:4] - INFO [QuorumPeer[myid=4](plain=0.0.0.0:2181)(secure=disabled):Learner@717] - Learner received NEWLEADER message
 2021-08-26 02:02:08,083 [myid:4] - INFO [QuorumPeer[myid=4](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@869] - Peer state changed: observing - synchronization
 2021-08-26 02:02:08,086 [myid:4] - INFO [QuorumPeer[myid=4](plain=0.0.0.0:2181)(secure=disabled):CommitProcessor@476] - Configuring CommitProcessor with readBatchSize -1 commitBatchSize 1
 2021-08-26 02:02:08,086 [myid:4] - INFO [QuorumPeer[myid=4](plain=0.0.0.0:2181)(secure=disabled):CommitProcessor@438] - Configuring CommitProcessor with 1 worker threads.
 2021-08-26 02:02:08,088 [myid:4] - INFO [QuorumPeer[myid=4](plain=0.0.0.0:2181)(secure=disabled):RequestThrottler@74] - zookeeper.request_throttler.shutdownTimeout = 10000
 2021-08-26 02:02:08,098 [myid:4] - INFO [QuorumPeer[myid=4](plain=0.0.0.0:2181)(secure=disabled):Learner@701] - Learner received UPTODATE message
 2021-08-26 02:02:08,099 [myid:4] - INFO [SyncThread:4:FileTxnLog@284] - Creating new log file: log.100000023
 2021-08-26 02:02:08,099 [myid:4] - INFO [CommitProcessor:4:LearnerSessionTracker@116] - Committing global session 0x100001170e3000d
 2021-08-26 02:02:08,099 [myid:4] - INFO [QuorumPeer[myid=4](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@864] - Peer state changed: observing - broadcast
 2021-08-26 02:02:08,100 [myid:4] - WARN [CommitProcessor:4:RateLogger@86] - Message:Digests are not matching. Value is Zxid. Value:4294967331
 2021-08-26 02:02:08,101 [myid:4] - {color:#ff0000}ERROR [CommitProcessor:4:DataTree@1832] - First digest mismatch on txn: 72057668946296845,0,4294967331,1629943304746,-10{color}
 {color:#ff0000} , 4000{color}
 {color:#ff0000} , expected digest is 2,10124071835{color}
 {color:#ff0000} , actual digest is 1371985504,{color} 
 2021-08-26 02:02:08,102 [myid:4] - INFO [CommitProcessor:4:LearnerSessionTracker@116] - Committing global session 0x200000b302e0000

 ","[<JIRA Version: name='3.6.3', id='12348703'>]",Bug,ZOOKEEPER-4352,Critical,hong,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,"inconsistent data, when minCommittedLog=0, only diff data  send but not snapshots ",2021-12-15T02:11:29.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>]",1.0
,"[<JIRA Component: name='tests', id='12312427'>]",2021-08-22T05:21:13.000+0000,Ling Mao,"URL: https://github.com/apache/zookeeper/pull/1721/checks?check_run_id=3387790695
{code:java}
[ERROR]  RaceConditionTest.testRaceConditionBetweenLeaderAndAckRequestProcessor:85 Leader failed to transition to new state. Current state is leading ==> expected: <true> but was: <false>{code}",[],Bug,ZOOKEEPER-4348,Major,Ling Mao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Flaky test: RaceConditionTest.testRaceConditionBetweenLeaderAndAckRequestProcessor,2021-08-22T05:21:13.000+0000,"[<JIRA Version: name='3.8.0', id='12349587'>]",1.0
,"[<JIRA Component: name='tests', id='12312427'>]",2021-08-22T05:19:00.000+0000,Ling Mao," 
URL: https://github.com/apache/zookeeper/pull/1721/checks?check_run_id=3387790695
{code:java}
 [ERROR] Failures:     959[ERROR]  EagerACLFilterTest.testSetDataFail:222->assertTransactionState:115 Server State: FOLLOWING Check Enabled: true Transaction state on Leader after failed setData ==> expected: <4294967298> but was: <4294967299> {code}
 ",[],Bug,ZOOKEEPER-4347,Major,Ling Mao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Flaky test: EagerACLFilterTest.testSetDataFail,2022-02-03T08:50:26.000+0000,[],1.0
,[],2021-08-07T09:32:01.000+0000,Bo Cui,"in OS Flink, flink relocate zk to org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.*

[https://github.com/apache/flink-shaded/blob/82f8bb3324864491dc62c4d3e27f1c1ccc49ac84/flink-shaded-zookeeper-parent/pom.xml#L68]

the maven-shade-plugin changes all 'org.apache.zookeeper' to 'org.apache.flink.shaded.zookeeper3.org.apache.zookeeper'

if JVM has -Dzookeeper.clientCnxnSocket=org.apache.zookeeper.*, and in shaded zk jar, will get NoSunchMethodException

  !image-2021-08-07-18-52-00-633.png!

!image-2021-08-07-17-30-42-883.png!

code: [https://github.com/apache/zookeeper/blob/9a5da5f9a023e53bf339748b5b7b17278ae36475/zookeeper-server/src/main/java/org/apache/zookeeper/ZooKeeper.java#L3029]","[<JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>, <JIRA Version: name='3.6.4', id='12350076'>]",Bug,ZOOKEEPER-4345,Major,Bo Cui,Fixed,2021-08-11T15:14:47.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Avoid NoSunchMethodException caused by shaded zookeeper jar,2021-08-11T15:23:35.000+0000,[],2.0
,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='kerberos', id='12329415'>, <JIRA Component: name='security', id='12329414'>]",2021-08-06T10:32:12.000+0000,HanXu,"I am running a ZKAuthentication test. 

 

In the server part, I run zkServer.sh to start ZKserver.

 

In the client part, I create a C application and active ZK's SASL function. Two authentication methods were chosen. The one is Kerberos， the other is DIGEST-MD5. And i receive two report:

1) using Kerberos

!image-2021-08-06-18-30-12-608.png!

2) using DIGEST-MD5

!image-2021-08-06-18-30-41-408.png!

 

{color:#FF0000}*Do those results mean that the Zookeeper's C API doesn't support Kerberos authentication?*{color}",[],Bug,ZOOKEEPER-4344,Blocker,HanXu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Does Zookeeper's C API support Kerberos authentication?,2021-08-06T10:34:04.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",1.0
Damien Diederen,"[<JIRA Component: name='server', id='12312382'>]",2021-08-05T08:46:54.000+0000,Damien Diederen,"{noformat}
[ERROR] One or more dependencies were identified with vulnerabilities that have a CVSS score greater than or equal to '0,0': 
[ERROR] 
[ERROR] commons-io-2.6.jar: CVE-2021-29425
[ERROR] 
[ERROR] See the dependency-check report for more details.
{noformat}

The issue is fixed in release 2.7:
    
- https://nvd.nist.gov/vuln/detail/CVE-2021-29425
- https://issues.apache.org/jira/browse/IO-556
- https://issues.apache.org/jira/browse/IO-559
- https://commons.apache.org/proper/commons-io/changes-report.html#a2.7","[<JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-4343,Major,Damien Diederen,Fixed,2021-09-01T16:56:59.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"OWASP Dependency-Check fails with CVE-2021-29425, commons-io-2.6",2021-09-01T16:57:03.000+0000,"[<JIRA Version: name='3.8.0', id='12349587'>]",1.0
Damien Diederen,"[<JIRA Component: name='c client', id='12312380'>]",2021-08-04T07:47:07.000+0000,Damien Diederen,"1. The current client is ignoring the error field of the response header, and only considering SASL-level errors when processing a SASL response.

2. Such errors cause a double-free of the input buffer, which crashes the application.
","[<JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>]",Bug,ZOOKEEPER-4342,Blocker,Damien Diederen,Fixed,2021-08-25T12:15:33.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Robustify C client against errors during SASL negotiation,2021-08-25T12:21:52.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.8.0', id='12349587'>]",1.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2021-07-29T07:20:36.000+0000,priya Vijay,"Running ZK cluster in kubernetes environment. ZK ensemble has 3 nodes

ZK ensemble is in a state where it looks like quorum is lost and see bunch of I/O exceptions . ZK cluster is not recovering from this state.

Log files have exceptions ""Zookeeper is not running"" but on exec to pod and running command it seems ok
echo ruok | nc 127.0.0.1 2181
imok

 

attaching the logs ",[],Bug,ZOOKEEPER-4339,Major,priya Vijay,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZK ensemble lost quorum and lots of exceptions seen in all ZK pods,2021-07-29T07:34:11.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>]",1.0
,"[<JIRA Component: name='metric system', id='12334405'>]",2021-07-28T19:19:59.000+0000,Li Wang,"Backport the fix for ZOOKEEPER-4289 to 3.7, so it can be included. in 3.7.1 release.",[],Bug,ZOOKEEPER-4338,Major,Li Wang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Backport ZOOKEEPER-4289 to Branch 3.7 ,2021-07-28T21:25:13.000+0000,"[<JIRA Version: name='3.7', id='12349953'>]",1.0
Damien Diederen,"[<JIRA Component: name='security', id='12329414'>]",2021-07-28T13:50:49.000+0000,Dominique Mongelli,"Hi, our security tool detects the following CVE on zookeeper 3.7.0 :

[https://nvd.nist.gov/vuln/detail/CVE-2021-34429]

 

 
{noformat}
For Eclipse Jetty versions 9.4.37-9.4.42, 10.0.1-10.0.5 & 11.0.1-11.0.5, URIs can be crafted using some encoded characters to access the content of the WEB-INF directory and/or bypass some security constraints. This is a variation of the vulnerability reported in CVE-2021-28164/GHSA-v7ff-8wcx-gmc5.{noformat}
 

It is a vulnerability related to jetty jar in version {{9.4.38.v20210224.jar}}.

Here is the security advisory from jetty: https://github.com/eclipse/jetty.project/security/advisories/GHSA-vjv5-gp2w-65vm

The CVE has been fixed in 9.4.43, 10.0.6, 11.0.6. An upgrade to 9.4.43 should be done.

 

 ","[<JIRA Version: name='3.5.10', id='12349434'>, <JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>, <JIRA Version: name='3.6.4', id='12350076'>]",Bug,ZOOKEEPER-4337,Major,Dominique Mongelli,Fixed,2021-09-01T16:51:35.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,CVE-2021-34429 in jetty 9.4.38.v20210224 in zookeeper 3.7.0,2022-02-27T17:48:50.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.8.0', id='12349587'>]",2.0
,"[<JIRA Component: name='security', id='12329414'>]",2021-07-21T08:52:40.000+0000,prashanth reddy,"Hi Team,

I have configured DigestAuthenticationProvider as zookeeper auth provider in my environment to let only users configured in zookeeper-jass.conf to authenticate with zookeeper, but when I try to authenticate with a user not in zookeeper-jass.conf , it is able to authenticate successfully.

python3-kazoo-2.8.0 client is being used to connect to zookeeper service.

Please find the attachments for configuration files used.

Could someone please suggest?",[],Bug,ZOOKEEPER-4336,Major,prashanth reddy,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Able to authentication to zookeeper with user not configured in zookeeper-jaas.conf,2021-07-21T09:05:34.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-07-16T09:24:09.000+0000,Madison ,"we have created an authentication provider plugin that can authenticate clients based on the cert that client is presenting. our zookeeper instance has been configured (and started) to authenticate and allow only certain appid's. this works as intended when clients (ours are c-clients) send an auth message via yca_add_auth containing the cert *and* the authentication provider is configured to allow it.

however, if the clients do *not* present one (i.e. do not send an auth packet), and if the authentication provider allows only certain appid's, this connection still goes through - i.e. clients are able to connect, create/watch nodes etc.! this is unexpected and does *not* allow us to prevent certain clients from connecting to a zookeeper quorum (as they can still connect without present any credentials). 

it looks like zookeeper will only invoke the auth providers if it receives an auth packet from the client.

none of this block - https://github.com/sriramch/zookeeper/blob/master/src/java/main/org/apache/zookeeper/server/ZooKeeperServer.java#L1060

ever gets executed, and it directly jumps to this 

https://github.com/sriramch/zookeeper/blob/master/src/java/main/org/apache/zookeeper/server/ZooKeeperServer.java#L1108

we have a usecase where we only want clients that can present valid credentials to connect to zookeeper (zk). 

i was hoping to expose an interface where different auth providers (when they are loaded)  would let zk know if they need to authenticate a client before processing other data packets. the default ones (kerberos/ip/digest etc.) would say no to maintain compatibility. our auth provider can be configured to say yes/no (default no) depending on use-case. zk before processing a data packet can look at the auth info in the server connection to see the schemes that requires authentication and have successfully authenticated. connection will succeed if all schemes that require authentication have successfully authenticated; else, we disable receive.

can someone please look into this issue and evaluate the proposal? i can work on creating a pr for this.","[<JIRA Version: name='3.4.6', id='12323310'>]",Bug,ZOOKEEPER-4335,Major,Madison ,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,CLONE - zookeeper issues with handling authentication...,2021-07-16T09:24:10.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",1.0
,[],2021-07-12T19:31:12.000+0000,Emil Kleszcz,"I faced an issue while trying to use alternative aliases with Zookeeper quorum when SASL is enabled. The errors I get in zookeeper log are the following:
 ```
 2021-07-12 21:04:46,437 [myid:3] - WARN [NIOWorkerThread-3:ZooKeeperServer@1661] - Client /<IP addr>:37368 failed to SASL authenticate: {}
 javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: Failure unspecified at GSS-API level (Mechanism level: Checksum failed)]
 at com.sun.security.sasl.gsskerb.GssKrb5Server.evaluateResponse(GssKrb5Server.java:199)
 at org.apache.zookeeper.server.ZooKeeperSaslServer.evaluateResponse(ZooKeeperSaslServer.java:49)
 at org.apache.zookeeper.server.ZooKeeperServer.processSasl(ZooKeeperServer.java:1650)
 at org.apache.zookeeper.server.ZooKeeperServer.processPacket(ZooKeeperServer.java:1599)
 at org.apache.zookeeper.server.NIOServerCnxn.readRequest(NIOServerCnxn.java:379)
 at org.apache.zookeeper.server.NIOServerCnxn.readPayload(NIOServerCnxn.java:182)
 at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:339)
 at org.apache.zookeeper.server.NIOServerCnxnFactory$IOWorkRequest.doWork(NIOServerCnxnFactory.java:522)
 at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:154)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
 Caused by: GSSException: Failure unspecified at GSS-API level (Mechanism level: Checksum failed)
 at sun.security.jgss.krb5.Krb5Context.acceptSecContext(Krb5Context.java:856)
 at sun.security.jgss.GSSContextImpl.acceptSecContext(GSSContextImpl.java:342)
 at sun.security.jgss.GSSContextImpl.acceptSecContext(GSSContextImpl.java:285)
 at com.sun.security.sasl.gsskerb.GssKrb5Server.evaluateResponse(GssKrb5Server.java:167)
 ... 11 more
 Caused by: KrbException: Checksum failed
 at sun.security.krb5.internal.crypto.Aes256CtsHmacSha1EType.decrypt(Aes256CtsHmacSha1EType.java:102)
 at sun.security.krb5.internal.crypto.Aes256CtsHmacSha1EType.decrypt(Aes256CtsHmacSha1EType.java:94)
 at sun.security.krb5.EncryptedData.decrypt(EncryptedData.java:175)
 at sun.security.krb5.KrbApReq.authenticate(KrbApReq.java:281)
 at sun.security.krb5.KrbApReq.<init>(KrbApReq.java:149)
 at sun.security.jgss.krb5.InitSecContextToken.<init>(InitSecContextToken.java:108)
 at sun.security.jgss.krb5.Krb5Context.acceptSecContext(Krb5Context.java:829)
 ... 14 more
 Caused by: java.security.GeneralSecurityException: Checksum failed
 at sun.security.krb5.internal.crypto.dk.AesDkCrypto.decryptCTS(AesDkCrypto.java:451)
 at sun.security.krb5.internal.crypto.dk.AesDkCrypto.decrypt(AesDkCrypto.java:272)
 at sun.security.krb5.internal.crypto.Aes256.decrypt(Aes256.java:76)
 at sun.security.krb5.internal.crypto.Aes256CtsHmacSha1EType.decrypt(Aes256CtsHmacSha1EType.java:100)
 ... 20 more
 ```

What did I do?
 1) created host aliases for each quorum node (a,b,c): zk1, zk2, zk3
 2) Changed in zoo.cfg:
 changed from
 server.1=a
 server.2=b
 server.3=c

to:
 server.1=zk1
 server.2=zk2
 server.3=zk3

(at this stage after restarting the ensemble all works as expected.
 3) Generate new keytab with alias-based principals and host-based principals in zookeeper.keytab
 4) Change jaas.conf (server) definition from:
 Server

{ com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=""/etc/zookeeper/conf/zookeeper.keytab"" storeKey=true useTicketCache=false principal=""zookeeper/a.com@COM""; }

;

to
 Server

{ com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=""/etc/zookeeper/conf/zookeeper.keytab"" storeKey=true useTicketCache=false principal=""zookeeper/zk1.com@COM""; }

;

From that moment, after restarting quorum members, I get the above error.

Now, why do I do this?
 To allow other services such as zkfc,hbase,hdfs,yarn to connect to the quorum using aliases. Interestingly, without changing the zookeeper principal, hbase works perfectly, but the other 3 services fail with:
 ```
 <2021-07-12T20:45:19.491+0200> <INFO> <org.apache.zookeeper.ZooKeeper>: <Initiating client connection, connectString=zk01.com:2181,zk02.com:2181,zk03.com:2181 sessionTimeout=10000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@3246fb96>
 <2021-07-12T20:45:19.519+0200> <INFO> <org.apache.zookeeper.Login>: <Client successfully logged in.>
 <2021-07-12T20:45:19.521+0200> <INFO> <org.apache.zookeeper.Login>: <TGT refresh thread started.>
 <2021-07-12T20:45:19.524+0200> <INFO> <org.apache.zookeeper.Login>: <TGT valid starting at: Mon Jul 12 20:45:19 CEST 2021>
 <2021-07-12T20:45:19.524+0200> <INFO> <org.apache.zookeeper.Login>: <TGT expires: Tue Jul 13 21:45:19 CEST 2021>
 <2021-07-12T20:45:19.524+0200> <INFO> <org.apache.zookeeper.Login>: <TGT refresh sleeping until: Tue Jul 13 17:05:16 CEST 2021>
 <2021-07-12T20:45:19.524+0200> <INFO> <org.apache.zookeeper.client.ZooKeeperSaslClient>: <Client will use GSSAPI as SASL mechanism.>
 <2021-07-12T20:45:19.530+0200> <INFO> <org.apache.zookeeper.ClientCnxn>: <Opening socket connection to server zk02.com/<ip addr>:2181. Will attempt to SASL-authenticate using Login Context section 'Client'>
 <2021-07-12T20:45:19.535+0200> <INFO> <org.apache.zookeeper.ClientCnxn>: <Socket connection established to zk02.com/<ip addr>:2181, initiating session>
 <2021-07-12T20:45:19.543+0200> <INFO> <org.apache.zookeeper.ClientCnxn>: <Session establishment complete on server zk02.com/<ip addr>:2181, sessionid = 0x200247870fb0007, negotiated timeout = 10000>
 <2021-07-12T20:45:19.561+0200> <ERROR> <org.apache.zookeeper.client.ZooKeeperSaslClient>: <SASL authentication failed using login context 'Client' with exception: {}>
 javax.security.sasl.SaslException: Error in authenticating with a Zookeeper Quorum member: the quorum member's saslToken is null.
 at org.apache.zookeeper.client.ZooKeeperSaslClient.createSaslToken(ZooKeeperSaslClient.java:279)
 at org.apache.zookeeper.client.ZooKeeperSaslClient.respondToServer(ZooKeeperSaslClient.java:242)
 at org.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:805)
 at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:94)
 at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:366)
 at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1145)
 <2021-07-12T20:45:19.564+0200> <INFO> <org.apache.zookeeper.ClientCnxn>: <Unable to read additional data from server sessionid 0x200247870fb0007, likely server has closed socket, closing socket connection and attempting reconnect>
 <2021-07-12T20:45:19.671+0200> <INFO> <org.apache.hadoop.ha.ActiveStandbyElector>: <Session connected.>
 <2021-07-12T20:45:19.672+0200> <ERROR> <org.apache.hadoop.hdfs.tools.DFSZKFailoverController>: <DFSZKFailOverController exiting due to earlier exception java.io.IOException: Couldn't determine existence of znode
 ```
 When I change the principle of zookeeper hbase starts failing with this error and other services except for the zookeeper itself is somehow working fine. After that, I cannot connect manually to the zk quorum using zkCli and zookeeper-client with all possible combinations of principals.

I wonder if that may have something to do with the ""Server environment:host.name="" pointing to the canonical name (and not the alias) during the startup. The same happens after specifying the alias with clientPortAddress=.",[],Bug,ZOOKEEPER-4334,Critical,Emil Kleszcz,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,SASL authentication fails when using host aliases,2021-07-29T09:58:36.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>]",4.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-06-30T14:59:24.000+0000,Emil Kleszcz,"We experience problems with performing any operation (deleteall, get etc.) on a znode that has too many child nodes. In our case, it's above 200k. At the same time jute.max.buffer is 4194304. Increasing it by a few factors doesn't help. This should be either solved by limiting the number of direct znodes allowed by a parameter or by adding a hard limit by default.

I am attaching some screenshots of the commands and their results. What's interesting the numbers from getAllChildrenNumber and stat (numChildren) commands don't match.",[],Bug,ZOOKEEPER-4332,Critical,Emil Kleszcz,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Cannot access children of znode that owns too many znodes,2021-07-27T02:20:23.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>]",2.0
,"[<JIRA Component: name='build', id='12312383'>]",2021-06-29T08:15:33.000+0000,Marvin Wu,"Headers like {{Bundle-SymbolicName}}, {{Import-Package}} and {{Export-Package}} are required by OSGi runtime, but they are no longer available in manifest since 3.5.5.

The migration from ant to maven looks suspicious, as these headers are originally planted by ant. Either way, they are required for the artifact serving as a bundle directly.","[<JIRA Version: name='3.5.10', id='12349434'>, <JIRA Version: name='3.7.1', id='12350030'>, <JIRA Version: name='3.6.4', id='12350076'>]",Bug,ZOOKEEPER-4331,Major,Marvin Wu,Fixed,2021-08-03T09:56:30.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zookeeper artifact is not compatible with OSGi runtime,2021-08-03T12:32:53.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.5.5', id='12343268'>]",2.0
,[],2021-06-28T12:20:00.000+0000,Christian Beikov,"I guess this affects all versions, but I just checked 3.6.2 and this is a problem. SolrJ depends on the zookeeper and zookeeper-jute artifacts which both export the packages:

 * {{org.apache.zookeeper.server.persistence}}
 * {{org.apache.zookeeper.server.quorum}}

It is not possible to run in module mode with this split package problem. The zookeeper-jute artifact should use different packages e.g. 

 * {{org.apache.zookeeper.jute.persistence}}
 * {{org.apache.zookeeper.jute.quorum}}",[],Bug,ZOOKEEPER-4330,Critical,Christian Beikov,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Unable to run solrj in module path because zookeeper and zookeeper-jute export same packages,2021-06-28T14:49:43.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",3.0
,[],2021-06-28T09:14:13.000+0000,Ling Mao,"URL：https://github.com/apache/zookeeper/pull/1528/checks?check_run_id=2927891675
{code:java}
[ERROR] Failures: 
949[ERROR]   Zab1_0Test.testNormalFollowerRunWithDiff:706->testFollowerConversation:445 expected: <4294967298> but was: <0>
950[INFO] 
951[ERROR] Tests run: 2913, Failures: 1, Errors: 0, Skipped: 4
{code}",[],Bug,ZOOKEEPER-4329,Major,Ling Mao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Flaky test: Zab1_0Test.testNormalFollowerRunWithDiff,2021-06-28T09:14:13.000+0000,[],2.0
,[],2021-06-28T09:10:19.000+0000,Ling Mao,"URL: https://github.com/apache/zookeeper/runs/2924308474
{code:java}
[ERROR]   RaceConditionTest.testRaceConditionBetweenLeaderAndAckRequestProcessor:85 Leader failed to transition to new state. Current state is leading ==> expected: <true> but was: <false>
{code}",[],Bug,ZOOKEEPER-4328,Major,Ling Mao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Flaky test: RaceConditionTest.testRaceConditionBetweenLeaderAndAckRequestProcessor,2021-06-28T09:10:19.000+0000,[],2.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-06-23T07:48:40.000+0000,Reno Shen,"[ZkUtil::listSubTreeBFS|https://github.com/apache/zookeeper/blob/branch-3.6/zookeeper-server/src/main/java/org/apache/zookeeper/ZKUtil.java]

Assume zk has these nodes: /a_test/a, /b_test/b.

When use this method to list ""/"", according to BFS logic, the BFS queue would be added node like: ""//a_test"", then getChildren by this path, the next exception occurs:
{code:java}
java.lang.IllegalArgumentException: Invalid path string ""//a_test"" caused by empty node name specified @1
{code}
 

 Solutions:
{code:java}
public static List<String> listSubTreeBFS(
        ZooKeeper zk,
        final String pathRoot) throws KeeperException, InterruptedException {
    Queue<String> queue = new ArrayDeque<>();
    List<String> tree = new ArrayList<>();
    queue.add(pathRoot);
    tree.add(pathRoot);
    while (!queue.isEmpty()) {
        String node = queue.poll();
        List<String> children = zk.getChildren(node, false);
        for (final String child : children) {
            // Fix ""//some_path"" bugs when list ""/""                
            final String childPath = (node.equals(""/"") ? """" : node) + ""/"" + child;
            queue.add(childPath);
            tree.add(childPath);
        }
    }
    return tree;
}
{code}","[<JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-4325,Minor,Reno Shen,Fixed,2021-07-18T07:19:35.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"IllegalArgumentException when use ZkUtil::listSubTreeBFS to list ""/""",2021-07-18T07:19:35.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",2.0
,"[<JIRA Component: name='metric system', id='12334405'>]",2021-06-22T22:06:34.000+0000,Ron Dagostino,"The metrics subsystem has a hard dependency on io.dropwizard.metrics:metrics-core that cannot be avoided because of this code:

public final class ServerMetrics {
    /**
     * Dummy instance useful for tests.
     */
    public static final ServerMetrics DEFAULT_METRICS_FOR_TESTS = new ServerMetrics(new DefaultMetricsProvider());

Even if the config metricsProvider.className=org.apache.zookeeper.metrics.impl.NullMetricsProvider is set the above code will still execute and create the dependency.

It would be best to make the dependency optional by removing the above code.

Another option is to change the dependency scope from the current value of ""provided"" to be ""compile"" instead.
",[],Bug,ZOOKEEPER-4324,Major,Ron Dagostino,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Hard dependency on io.dropwizard.metrics:metrics-core cannot be avoided,2021-06-24T23:47:19.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>]",3.0
,"[<JIRA Component: name='c client', id='12312380'>]",2021-06-17T13:53:59.000+0000,Mukti Krishnan,"When a znode is created in c client, null value is set by default and get command gives value len 3 instead of 0",[],Bug,ZOOKEEPER-4322,Minor,Mukti Krishnan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,create command in c client sets value to null and gives value_len 3 ,2021-06-22T18:18:53.000+0000,[],1.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2021-06-11T10:49:00.000+0000,Arun Subramanian R,"I have a 3 node zookeeper cluster deployed as a stack using docker swarm. 
Deploying this stack causes zookeeper to fail with a SocketTimeoutException during leader election with the following log



 
{noformat}
2021-06-11 03:59:34,607 [myid:2] - WARN  [QuorumPeer[myid=2]/0.0.0.0:2181:QuorumCnxManager@584] - Cannot open channel to 3 at election address zoo3/10.0.11.5:3888
java.net.SocketTimeoutException: connect timed out
        at java.net.PlainSocketImpl.socketConnect(Native Method)
       at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:589)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:558)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:610)
        at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:838)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:957){noformat}
The docker overlay network itself appears to be sound. A netstat on one of the nodes outputs
{noformat}
bash-4.4# netstat -tuln
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 0.0.0.0:2181            0.0.0.0:*               LISTEN
tcp        0      0 0.0.0.0:3888            0.0.0.0:*               LISTEN
tcp        0      0 0.0.0.0:42941           0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.11:35453        0.0.0.0:*               LISTEN
udp        0      0 127.0.0.11:55009        0.0.0.0:*{noformat}
showing the 3888 port is open. but a tcpdump only shows send and re-transmissions and there are no responses in port 3888.
Suspecting the issue maybe due to a short timeout or small number of retries, I have tried increasing the cnxTimeout to 300000 and electionPortBindRetry to 0 (infinite), but even after 13 hrs of continuous running and retrying election the same error persists

I have attached the stack.yml, the custom docker-entrypoint.sh that we override on top of the official container to enable running from a root host user, and the zoo.cfg file from inside the container.

Any help in identifying the underlying issue or mis-configuration, or any configuration parameter that may help solve the issue is deeply appreciated.

 ",[],Bug,ZOOKEEPER-4316,Major,Arun Subramanian R,Fixed,2021-07-01T10:39:27.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Leader election fails due to SocketTimeoutException in QuorumCnxManager,2021-07-01T10:39:27.000+0000,"[<JIRA Version: name='3.4.12', id='12342040'>, <JIRA Version: name='3.5.7', id='12346098'>]",2.0
,"[<JIRA Component: name='java client', id='12312381'>]",2021-06-08T04:44:28.000+0000,Li Jian,"When use zkClient or curator listener one zookeeper node more than 4M will cause endless loop，because they catch error code for KeeperException.Code.ConnectionLoss then will retry getChildren method after a monent。
The root reason for zookeeper invoke readLength method occor IOException for “Package len is out of range” but when deal with the following process zookeeper changed the real exception to ConnectionLoss。",[],Bug,ZOOKEEPER-4313,Major,Li Jian,Won't Fix,2021-06-09T07:19:54.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Can not get real exception when getChildren more than 4M,2021-06-09T07:19:54.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",3.0
,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='server', id='12312382'>]",2021-06-02T11:57:37.000+0000,Dmitrii Kovalkov,"Class AtomicFileOutputStream has a non-trivial logic in its 'close' method. ([code|https://github.com/apache/zookeeper/blob/5c102298f8a160ea996be7b6d6f95189d4ff2f41/zookeeper-server/src/main/java/org/apache/zookeeper/common/AtomicFileOutputStream.java#L76-L106]).
 It ensures that data is persistently stored on the disk via 'flush' and 'fsync' to .tmp file, then tries to rename the file. In case of any errors, .tmp file is deleted and exception is thrown.

AtomicFileWritingIdiom, which is based on AtomicFileOutputStream, only calls 'flush' explicitly. 'close' method is called via IOUtils.closeStream ([code|https://github.com/apache/zookeeper/blob/5c102298f8a160ea996be7b6d6f95189d4ff2f41/zookeeper-server/src/main/java/org/apache/zookeeper/common/AtomicFileWritingIdiom.java#L87]).
But docs says that IOUtils.closeStream ignores IOException, which can happen during fsync. ([docs|https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/io/IOUtils.html#closeStream(java.io.Closeable)]). As a result, in case of fsync errors, .tmp file is deleted, main file is not updated, but zookeeper ignores an exception and assumes that everything is ok.

AtomicFileWritingIdiom is used in leader election to store 'currentEpoch' and 'acceptedEpoch' files. This bug theoreticly can lead to electing two leaders in one epoch in case of disk failures.

 ","[<JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-4311,Major,Dmitrii Kovalkov,Fixed,2021-06-21T11:16:20.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Fsync errors are ignored in AtomicFileWritingIdiom,2021-06-28T11:36:29.000+0000,[],3.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-05-31T13:13:04.000+0000,Francesco Nigro,QuorumCnxManager::Listener::run is creating a Executors.newFixedThreadPool(addresses.size()) without shutting it down after ListenerHandler task has been completed causing it to leak.,"[<JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>]",Bug,ZOOKEEPER-4309,Major,Francesco Nigro,Fixed,2021-06-14T09:46:28.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,QuorumCnxManager's ListenerHandler thread leak,2021-06-14T09:47:19.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>]",3.0
Kezhu Wang,"[<JIRA Component: name='tests', id='12312427'>]",2021-05-30T09:30:59.000+0000,Ling Mao,"URL:[https://github.com/apache/zookeeper/pull/1425/checks?check_run_id=2702959273]
{code:java}
[INFO] 
947[ERROR] Failures: 
948[ERROR]   EagerACLFilterTest.testSetDataFail:222->assertTransactionState:115 Server State: OBSERVING Check Enabled: true Transaction state on Leader after failed setData ==> expected: <4294967298> but was: <4294967299>
949[INFO] 
950[ERROR] Tests run: 2913, Failures: 1, Errors: 0, Skipped: 4
951[INFO] {code}",[],Bug,ZOOKEEPER-4308,Major,Ling Mao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Flaky test: EagerACLFilterTest.testSetDataFail,2022-04-05T08:56:25.000+0000,"[<JIRA Version: name='3.8.0', id='12349587'>]",3.0
,[],2021-05-28T03:09:50.000+0000,Lin Changrui,"We took a test about how many ephemal nodes can client create under one parent node with defalut configuration. The test caused cluster crash at last, exception stack trace like this.

follower:

!f.jpg!

leader:

!l1.png!

!l2.jpg!

It seems that leader sent a too large txn packet to followers. When follower try to deserialize the txn, it found the txn length out of its buffer size(default 1MB+1MB, jute.maxbuffer + jute.maxbuffer.extrasize). That causes followers crashed, and then, leader found there was no sufficient followers synced, so leader shutdown later. When leader shutdown, it called zkDb.fastForwardDataBase() , and leader found the txn read from txnlog out of its buffer size, so it crashed too.

After the servers crashed, they try to restart the quorum. But they would not success because the last txn is too large. We lose the log at that moment, but the stack trace is same as this one.

!r.jpg|width=1468,height=598!

 

*Root Cause*

We use org.apache.zookeeper.server.LogFormatter(-Djute.maxbuffer=74827780) visualize this log and found this. !cs.jpg|width=1400,height=581! So closeSessionTxn contains all ephemal nodes with absolute path. We know we will get a large getChildren respose if we create too many children nodes under one parent node, that is limited by jute.maxbuffer of client. If we create plenty of ephemal nodes under different parent nodes with one session, it may not cause out of buffer of client, but when the session close without delete these node first, it probably cause cluster crash.

Is it a bug or just a unspecified feature？If it just so, how should we judge the upper limit of creating nodes? 

 ",[],Bug,ZOOKEEPER-4306,Critical,Lin Changrui,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,CloseSessionTxn contains too many ephemal nodes cause cluster crash,2021-11-20T14:56:26.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",5.0
,"[<JIRA Component: name='security', id='12329414'>]",2021-05-28T02:50:40.000+0000,zhaozhengbin,Included in Log4j 1.2 is a SocketServer class that is vulnerable to deserialization of untrusted data which can be exploited to remotely execute arbitrary code when combined with a deserialization gadget when listening to untrusted network traffic for log data.this affects Log4j versions up to 1.2 up to 1.2.17.,[],Bug,ZOOKEEPER-4305,Critical,zhaozhengbin,Duplicate,2021-05-28T15:27:23.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,log4j CVE problem,2021-05-28T15:32:21.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>, <JIRA Version: name='3.7', id='12349953'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-05-26T12:36:59.000+0000,Anoop Negi,"We have three(3) node zookeeper cluster running as a pod on Kubernetes cluster,
Zookeeper version is 3.6.2, While upgrading zookeeper from Plain-text+Secure mode to only secure mode we are facing issue( i.e. disabling Plain-Text channel)

1. To disable plain-text we are removing <clientport> from the dynamic configuration file to enable only secure communication but after upgrade zookeeper ensemble failed to form. leader election continuous failing and getting notification timeout
{code:java}
#server configuration
server.1=server1zookeeper.svc.cluster.local:2888:3888:participant
server.2=server2zookeeper.svc.cluster.local:2888:3888:participant
server.3=server3zookeeper.svc.cluster.local:2888:3888:participant

#secure port enabled
secureClientPort=2281
{code}
{code:java}
 
2021-05-19T08:00:06.900+0000 [myid:] - WARN [QuorumConnectionThread-[myid=3]-3:QuorumCnxManager@400] - Cannot open channel to 1 at election address server1zookeeper/192.168.57.156:3888 java.net.SocketTimeoutException: connect timed out at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?] at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399) ~[?:?] at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242) ~[?:?] at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224) ~[?:?] at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:?] at java.net.Socket.connect(Socket.java:609) ~[?:?] at org.apache.zookeeper.server.quorum.QuorumCnxManager.initiateConnection(QuorumCnxManager.java:383) [zookeeper-3.6.2.jar:3.6.2] at org.apache.zookeeper.server.quorum.QuorumCnxManager$QuorumConnectionReqThread.run(QuorumCnxManager.java:457) [zookeeper-3.6.2.jar:3.6.2] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?] at java.lang.Thread.run(Thread.java:834) [?:?]
{code}
{code:java}
2021-05-19T07:47:56.894+0000 [myid:] - INFO  [QuorumPeer[myid=1](plain=disabled)(secure=0.0.0.0:2281):FastLeaderElection@979] - Notification time out: 60000

{code}
2. We also tried to perform reconfiguration from CLI using zkCli.sh but this also not working, we tried to use ""reconfig -member"" and provided servers details but zookeeper ensemble not updating and getting error. created DigestAuthenticationProvider user to allow reconfig
{code:java}
[zk: zookeeper:2281(CONNECTED) 0]
[zk: zookeeper:2281(CONNECTED) 0]
[zk: zookeeper:2281(CONNECTED) 0] config
server.1=server1zookeeper.svc.cluster.local:2888:3888:participant
server.2=server2zookeeper.svc.cluster.local:2888:3888:participant
server.3=server3zookeeper.svc.cluster.local:2888:3888:participant
version=1700000000
[zk: zookeeper:2281(CONNECTED) 1]
[zk: zookeeper:2281(CONNECTED) 1]
[zk: zookeeper:2281(CONNECTED) 1] addauth digest zookeeper:admin
[zk: zookeeper:2281(CONNECTED) 2]
[zk: zookeeper:2281(CONNECTED) 2]
[zk: zookeeper:2281(CONNECTED) 2] reconfig -members server.1=server1zookeeper.svc.cluster.local:2888:3888:participant;0.0.0.0:2181,server.2=server2zookeeper.svc.cluster.local:2888:3888:participant;0.0.0.0:2181,server.3=server3zookeeper.svc.cluster.local:2888:3888:participant;0.0.0.0:2181
2021-05-19T08:16:43.376+0000 [myid:zookeeper:2281] - WARN  [main-SendThread(zookeeper:2281):ClientCnxn$SendThread@1242] - Client session timed out, have not heard from server in 20000ms for session id 0x30169d99fdf0000
2021-05-19T08:16:43.377+0000 [myid:zookeeper:2281] - WARN  [main-SendThread(zookeeper:2281):ClientCnxn$SendThread@1285] - Session 0x30169d99fdf0000 for sever zookeeper/10.107.240.229:2281, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
org.apache.zookeeper.ClientCnxn$SessionTimeoutException: Client session timed out, have not heard from server in 20000ms for session id 0x30169d99fdf0000
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1243) [zookeeper-3.6.2.jar:3.6.2]WATCHER::WatchedEvent state:Disconnected type:None path:null
2021-05-19T08:16:43.390+0000 [myid:] - INFO  [nioEventLoopGroup-2-1:ClientCnxnSocketNetty$ZKClientHandler@469] - channel is disconnected: [id: 0xa97b55e0, L:/192.168.220.12:47114 ! R:zookeeper/10.107.240.229:2281]
2021-05-19T08:16:43.392+0000 [myid:] - INFO  [nioEventLoopGroup-2-1:ClientCnxnSocketNetty@249] - channel is told closing
KeeperErrorCode = ConnectionLoss
{code}
Kindly suggest the way to perform upgrade with desire changes and should also work with rollback.",[],Bug,ZOOKEEPER-4304,Blocker,Anoop Negi,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper Upgrade failed when disabling Plain-text communication and ensemble failed to form,2022-02-04T12:07:00.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",3.0
,[],2021-05-21T12:27:06.000+0000,Cheng Pan,,[],Bug,ZOOKEEPER-4302,Major,Cheng Pan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper 3.6.3 Docker Image missing in DockerHub,2021-05-21T14:48:32.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>]",3.0
,"[<JIRA Component: name='c client', id='12312380'>]",2021-05-20T03:27:00.000+0000,hong,"int zoo_amulti(zhandle_t *zh, int count, const zoo_op_t *ops,
 zoo_op_result_t *results, void_completion_t completion, const void *data)

{ struct RequestHeader h = \\{get_xid(), ZOO_MULTI_OP}

;
 struct MultiHeader mh = \{-1, 1, -1};
 struct oarchive *oa = create_buffer_oarchive();
 {color:#ff0000}completion_head_t clist = \{ 0 }; // not initialize for cond or lock{color}

int rc = serialize_RequestHeader(oa, ""header"", &h);

....

 

{color:#ff0000}queue_completion(&clist, entry, 0); //queue it will segment errors{color}

 

 

{color:#FF0000}// do lock or unlock which have not been initialized!!{color}
 static void queue_completion(completion_head_t *list, completion_list_t *c,
         int add_to_front)

{     lock_completion_list(list);     queue_completion_nolock(list, c, add_to_front);     unlock_completion_list(list); }

{color:#FF0000}// oh my god!!{color}
 int unlock_completion_list(completion_head_t *l)

{     p_thread_cond_broadcast(&l->cond);     return p_thread_mutex_unlock(&l->lock); }",[],Bug,ZOOKEEPER-4299,Blocker,hong,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,List head do not initialize before using it in zoo_amulti that will cause segment errors.,2021-05-20T03:28:16.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-05-19T08:25:23.000+0000,Anoop Negi,"We have three(3) node zookeeper cluster running as a pod on Kubernetes cluster,
Zookeeper version is 3.6.2, While upgrading zookeeper from Plain-text+Secure mode to only secure mode we are facing issue( i.e. disabling Plain-Text channel)

1. To disable plain-text we are removing <clientport> from the dynamic configuration file to enable only secure communication but after upgrade zookeeper ensemble failed to form. leader election continuous failing and getting notification timeout
{code:java}
#server configuration
server.1=server1zookeeper.svc.cluster.local:2888:3888:participant
server.2=server2zookeeper.svc.cluster.local:2888:3888:participant
server.3=server3zookeeper.svc.cluster.local:2888:3888:participant

#secure port enabled
secureClientPort=2281
{code}
{code:java}
 
2021-05-19T08:00:06.900+0000 [myid:] - WARN [QuorumConnectionThread-[myid=3]-3:QuorumCnxManager@400] - Cannot open channel to 1 at election address server1zookeeper/192.168.57.156:3888 java.net.SocketTimeoutException: connect timed out at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?] at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399) ~[?:?] at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242) ~[?:?] at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224) ~[?:?] at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:?] at java.net.Socket.connect(Socket.java:609) ~[?:?] at org.apache.zookeeper.server.quorum.QuorumCnxManager.initiateConnection(QuorumCnxManager.java:383) [zookeeper-3.6.2.jar:3.6.2] at org.apache.zookeeper.server.quorum.QuorumCnxManager$QuorumConnectionReqThread.run(QuorumCnxManager.java:457) [zookeeper-3.6.2.jar:3.6.2] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?] at java.lang.Thread.run(Thread.java:834) [?:?]
{code}
{code:java}
2021-05-19T07:47:56.894+0000 [myid:] - INFO  [QuorumPeer[myid=1](plain=disabled)(secure=0.0.0.0:2281):FastLeaderElection@979] - Notification time out: 60000

{code}
2. We also tried to perform reconfiguration from CLI using zkCli.sh but this also not working, we tried to use ""reconfig -member"" and provided servers details but zookeeper ensemble not updating and getting error. created DigestAuthenticationProvider user to allow reconfig
{code:java}
[zk: zookeeper:2281(CONNECTED) 0]
[zk: zookeeper:2281(CONNECTED) 0]
[zk: zookeeper:2281(CONNECTED) 0] config
server.1=server1zookeeper.svc.cluster.local:2888:3888:participant
server.2=server2zookeeper.svc.cluster.local:2888:3888:participant
server.3=server3zookeeper.svc.cluster.local:2888:3888:participant
version=1700000000
[zk: zookeeper:2281(CONNECTED) 1]
[zk: zookeeper:2281(CONNECTED) 1]
[zk: zookeeper:2281(CONNECTED) 1] addauth digest zookeeper:admin
[zk: zookeeper:2281(CONNECTED) 2]
[zk: zookeeper:2281(CONNECTED) 2]
[zk: zookeeper:2281(CONNECTED) 2] reconfig -members server.1=server1zookeeper.svc.cluster.local:2888:3888:participant;0.0.0.0:2181,server.2=server2zookeeper.svc.cluster.local:2888:3888:participant;0.0.0.0:2181,server.3=server3zookeeper.svc.cluster.local:2888:3888:participant;0.0.0.0:2181
2021-05-19T08:16:43.376+0000 [myid:zookeeper:2281] - WARN  [main-SendThread(zookeeper:2281):ClientCnxn$SendThread@1242] - Client session timed out, have not heard from server in 20000ms for session id 0x30169d99fdf0000
2021-05-19T08:16:43.377+0000 [myid:zookeeper:2281] - WARN  [main-SendThread(zookeeper:2281):ClientCnxn$SendThread@1285] - Session 0x30169d99fdf0000 for sever zookeeper/10.107.240.229:2281, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
org.apache.zookeeper.ClientCnxn$SessionTimeoutException: Client session timed out, have not heard from server in 20000ms for session id 0x30169d99fdf0000
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1243) [zookeeper-3.6.2.jar:3.6.2]WATCHER::WatchedEvent state:Disconnected type:None path:null
2021-05-19T08:16:43.390+0000 [myid:] - INFO  [nioEventLoopGroup-2-1:ClientCnxnSocketNetty$ZKClientHandler@469] - channel is disconnected: [id: 0xa97b55e0, L:/192.168.220.12:47114 ! R:zookeeper/10.107.240.229:2281]
2021-05-19T08:16:43.392+0000 [myid:] - INFO  [nioEventLoopGroup-2-1:ClientCnxnSocketNetty@249] - channel is told closing
KeeperErrorCode = ConnectionLoss
{code}
Kindly suggest the way to perform upgrade with desire changes and should also work with rollback.",[],Bug,ZOOKEEPER-4297,Major,Anoop Negi,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper Upgrade failed when disabling Plain-text communication and ensemble failed to form,2022-02-04T12:07:17.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",3.0
,[],2021-05-17T23:11:58.000+0000,Colvin Cowie,"I believe this bug was originally reported as ZOOKEEPER-2966 but that was closed as not reproducible in February 2019. I left a comment with these details on that issue in December. I can create a PR with a fix at some point this week.

 

In ZooKeeper 3.6.2, in the context of the SolrJ client, we hit the NPE reported on ZOOKEEPER-2966 when a DNS error causes an exception after the SolrZkClient trys to connect to ZooKeeper, but then immediately calls close on the {{ClientCnxn}} [https://github.com/apache/solr/blob/releases/lucene-solr%2F8.7.0/solr/solrj/src/java/org/apache/solr/common/cloud/SolrZkClient.java#L158-L204].
{noformat}
java.lang.NullPointerException: null
        at org.apache.zookeeper.ClientCnxnSocketNetty.onClosing(ClientCnxnSocketNetty.java:247) ~[zookeeper-3.6.2.jar:3.6.2]
        at org.apache.zookeeper.ClientCnxn$SendThread.close(ClientCnxn.java:1445) ~[zookeeper-3.6.2.jar:3.6.2]
        at org.apache.zookeeper.ClientCnxn.disconnect(ClientCnxn.java:1488) ~[zookeeper-3.6.2.jar:3.6.2]
        at org.apache.zookeeper.ClientCnxn.close(ClientCnxn.java:1517) ~[zookeeper-3.6.2.jar:3.6.2]
        at org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:1614) ~[zookeeper-3.6.2.jar:3.6.2]
        at org.apache.solr.common.cloud.SolrZooKeeper.close(SolrZooKeeper.java:97) ~[solr-solrj-8.7.0.jar:8.7.0 2dc63e901c60cda27ef3b744bc554f1481b3b067 - atrisharma - 2020-10-29 19:39:18]
        at org.apache.solr.common.cloud.SolrZkClient.<init>(SolrZkClient.java:198) ~[solr-solrj-8.7.0.jar:8.7.0 2dc63e901c60cda27ef3b744bc554f1481b3b067 - atrisharma - 2020-10-29 19:39:18]
        at org.apache.solr.common.cloud.SolrZkClient.<init>(SolrZkClient.java:127) ~[solr-solrj-8.7.0.jar:8.7.0 2dc63e901c60cda27ef3b744bc554f1481b3b067 - atrisharma - 2020-10-29 19:39:18]
        at org.apache.solr.common.cloud.SolrZkClient.<init>(SolrZkClient.java:122) ~[solr-solrj-8.7.0.jar:8.7.0 2dc63e901c60cda27ef3b744bc554f1481b3b067 - atrisharma - 2020-10-29 19:39:18]
        at org.apache.solr.common.cloud.SolrZkClient.<init>(SolrZkClient.java:109) ~[solr-solrj-8.7.0.jar:8.7.0 2dc63e901c60cda27ef3b744bc554f1481b3b067 - atrisharma - 2020-10-29 19:39:18]
{noformat}
This happens if the {{ClientCnxnSocketNetty}}'s {{onClosing()}} is called before {{connect(...)}} (or if connect isn't called at all) because the {{firstConnect}} {{CountDownLatch}} is only initialized in {{connect(...)}}.
 [https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/ClientCnxnSocketNetty.java#L129]
 [https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/ClientCnxnSocketNetty.java#L247]
 A null check in {{onClosing()}} will fix it, but I don't know if there's any greater change required, e.g. some synchronization around connect and onClosing.

The code in [3.5.3|https://github.com/apache/zookeeper/blame/1507f67a06175155003722297daeb60bc912af1d/zookeeper-server/src/main/java/org/apache/zookeeper/ClientCnxnSocketNetty.java#L206] looks very similar, it looks like it's been present since the initial commit of {{ClientCnxnSocketNetty}}.",[],Bug,ZOOKEEPER-4296,Minor,Colvin Cowie,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,NullPointerException when ClientCnxnSocketNetty is closed without being opened,2021-06-10T14:16:04.000+0000,"[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.6.2', id='12347809'>]",3.0
,"[<JIRA Component: name='security', id='12329414'>]",2021-05-17T11:21:47.000+0000,Piyush Gupta,"Netty versions before 4.1.61.Final is affected by CVE-2021-21409.

-> [https://nvd.nist.gov/vuln/detail/CVE-2021-21409]",[],Bug,ZOOKEEPER-4295,Major,Piyush Gupta,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Upgrade Netty library to > 4.1.60 due to security vulnerability CVE-2021-21409 in branch-3.5,2022-02-01T23:26:44.000+0000,"[<JIRA Version: name='3.5.10', id='12349434'>]",2.0
,"[<JIRA Component: name='java client', id='12312381'>]",2021-05-11T18:13:11.000+0000,Michael Edgar,"We have encountered a scenario where a `ClientCnxn#SendThread` has become blocked in `ClientCnxnSocketNetty#cleanup` in the call to `channel.close().syncUninterruptibly()`. As the `syncUninterruptibly` method blocks, processing in the `epollEventLoopGroup` thread for the same client is unable to obtain the lock in the listener associated with the client (line 151 in v3.5.8) and application threads using the `ZooKeeperAdmin` containing the `ClientCnxn` themselves become blocked on the same lock when attempting to close the client.

The thread dump for the situation is attached, with the three interesting threads/stacks inlined below. Each occurrence lists three threads as it occurred to three clients simultaneously. 
h2. SendThread
* vert.x-eventloop-thread-0-SendThread(foo-0xtq07v5viyjxv8-zookeeper-0.foo-0xtq07v5viyjxv8-zookeeper-nodes.foo-0xtq07v5viyjxv8.svc:2181) awaiting notification , holding [ 0x00000000f6b5fdd0 ]
* vert.x-eventloop-thread-0-SendThread(foo-lgr9apuzr7ngznmd-zookeeper-2.foo-lgr9apuzr7ngznmd-zookeeper-nodes.foo-lgr9apuzr7ngznmd.svc:2181) awaiting notification , holding [ 0x00000000f6b8e5c0 ]
* vert.x-eventloop-thread-0-SendThread(foo-tukt40lcdodsux6n-zookeeper-1.foo-tukt40lcdodsux6n-zookeeper-nodes.foo-tukt40lcdodsux6n.svc:2181) awaiting notification , holding [ 0x00000000f6b9d5a0 ] {code}at java.lang.Object.wait(java.base@11.0.11/Native Method)
at java.lang.Object.wait(java.base@11.0.11/Object.java:328)
at io.netty.util.concurrent.DefaultPromise.awaitUninterruptibly(DefaultPromise.java:275)
at io.netty.channel.DefaultChannelPromise.awaitUninterruptibly(DefaultChannelPromise.java:137)
at io.netty.channel.DefaultChannelPromise.awaitUninterruptibly(DefaultChannelPromise.java:30)
at io.netty.util.concurrent.DefaultPromise.syncUninterruptibly(DefaultPromise.java:411)
at io.netty.channel.DefaultChannelPromise.syncUninterruptibly(DefaultChannelPromise.java:125)
at io.netty.channel.DefaultChannelPromise.syncUninterruptibly(DefaultChannelPromise.java:30)
at org.apache.zookeeper.ClientCnxnSocketNetty.cleanup(ClientCnxnSocketNetty.java:212)
at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:1338)
at org.apache.zookeeper.ClientCnxn$SendThread.cleanAndNotifyState(ClientCnxn.java:1276)
at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1254){code}

h2. epollEventLoopGroup
* epollEventLoopGroup-11-1 awaiting notification on [ 0x00000000f6b8e5c0 ]
* epollEventLoopGroup-19-1 awaiting notification on [ 0x00000000f6b9d5a0 ]
* epollEventLoopGroup-2-1 awaiting notification on [ 0x00000000f6b5fdd0 ] {code}at jdk.internal.misc.Unsafe.park(java.base@11.0.11/Native Method)
at java.util.concurrent.locks.LockSupport.park(java.base@11.0.11/LockSupport.java:194)
at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(java.base@11.0.11/AbstractQueuedSynchronizer.java:885)
at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(java.base@11.0.11/AbstractQueuedSynchronizer.java:917)
at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(java.base@11.0.11/AbstractQueuedSynchronizer.java:1240)
at java.util.concurrent.locks.ReentrantLock.lock(java.base@11.0.11/ReentrantLock.java:267)
at org.apache.zookeeper.ClientCnxnSocketNetty$1.operationComplete(ClientCnxnSocketNetty.java:151)
at org.apache.zookeeper.ClientCnxnSocketNetty$1.operationComplete(ClientCnxnSocketNetty.java:146)
at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)
at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:571)
at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:550)
at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)
at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)
at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:605)
at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)
at io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
at io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.fulfillConnectPromise(AbstractEpollChannel.java:653)
at io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.finishConnect(AbstractEpollChannel.java:691)
at io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.epollOutReady(AbstractEpollChannel.java:567)
at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:470)
at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378)
at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.lang.Thread.run(java.base@11.0.11/Thread.java:829){code}

h2. vert.x-worker-thread (application thread)
* vert.x-worker-thread-16 awaiting notification on [ 0x00000000f6b8e5c0 ] , holding [ 0x00000000f6b8d148 0x00000000f6b76870 ]
* vert.x-worker-thread-19 awaiting notification on [ 0x00000000f6b9d5a0 ] , holding [    0x00000000f6b9b4c8 0x00000000f6b76df8 ]
* vert.x-worker-thread-4 awaiting notification on [ 0x00000000f6b5fdd0 ] , holding [    0x00000000f6b5ab08 0x00000000f6b43a98 ] {code}at jdk.internal.misc.Unsafe.park(java.base@11.0.11/Native Method)
at java.util.concurrent.locks.LockSupport.park(java.base@11.0.11/LockSupport.java:194)
at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(java.base@11.0.11/AbstractQueuedSynchronizer.java:885)
at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(java.base@11.0.11/AbstractQueuedSynchronizer.java:917)
at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(java.base@11.0.11/AbstractQueuedSynchronizer.java:1240)
at java.util.concurrent.locks.ReentrantLock.lock(java.base@11.0.11/ReentrantLock.java:267)
at org.apache.zookeeper.ClientCnxnSocketNetty.cleanup(ClientCnxnSocketNetty.java:205)
at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:1338)
at org.apache.zookeeper.ClientCnxn$SendThread.cleanAndNotifyState(ClientCnxn.java:1276)
at org.apache.zookeeper.ClientCnxn$SendThread.access$2800(ClientCnxn.java:805)
at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1534)
at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1512)
at org.apache.zookeeper.ClientCnxn.close(ClientCnxn.java:1481)
at org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:1415)
at org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:1437)
at io.strimzi.operator.cluster.operator.resource.ZookeeperScaler.lambda$closeConnection$10(ZookeeperScaler.java:244)
at io.strimzi.operator.cluster.operator.resource.ZookeeperScaler$$Lambda$501/0x0000000840629840.handle(Unknown Source)
at io.vertx.core.impl.ContextImpl.lambda$null$0(ContextImpl.java:179)
at io.vertx.core.impl.ContextImpl$$Lambda$208/0x00000008402ea440.handle(Unknown Source)
at io.vertx.core.impl.AbstractContext.dispatch(AbstractContext.java:96)
at io.vertx.core.impl.ContextImpl.lambda$executeBlocking$1(ContextImpl.java:177)
at io.vertx.core.impl.ContextImpl$$Lambda$205/0x00000008402e9040.run(Unknown Source)
at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@11.0.11/ThreadPoolExecutor.java:1128)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@11.0.11/ThreadPoolExecutor.java:628)
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.lang.Thread.run(java.base@11.0.11/Thread.java:829){code}",[],Bug,ZOOKEEPER-4293,Critical,Michael Edgar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Lock Contention in ClientCnxnSocketNetty (possible deadlock),2021-07-23T13:31:35.000+0000,"[<JIRA Version: name='3.5.8', id='12346950'>]",6.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-04-29T03:42:45.000+0000,karl,"I'm running Zookeeper from the 3.5.5 docker image in k8s. 3 pod are located on three nodes，which communicates with each other with k8s cluset service. I did the following operates: shut down node 2 for 3 hours and then joined the cluster. At this time, I found that zookeeper could not respond to*{color:#ff0000} any requests(include zkCli.sh){color}*. I used the runok and mntr commands to test the server, and only found one exception: outstanding_requests is abnormally high and keeps growing (I have a Kafka cluster using it)

 

my cluster：

 
{code:java}
//my zookeeper pods
[root@node1 ~]# kubectl get po -nsa -o wide | grep zookeeper-dp
itoa-zookeeper-dp1-5bc799c97f-t7wpz                        1/1     Running            0          17h   177.177.166.244   node1   <none>           <none>
itoa-zookeeper-dp2-6d89756876-k2ds2                        1/1     Running            0          13h   177.177.104.35    node2   <none>           <none>
itoa-zookeeper-dp3-b55ddd6c7-8h5p7                         1/1     Running            0          16h   177.177.135.54    node3   <none>           <none>

//my zookeeper cluster svc
[root@node1 ~]# kubectl get svc -nsa -o wide | grep zookeeper-dp                                                                                      44h   node=itoa-zookeeper-dp3
itoa-zookeeper-service1                          ClusterIP   10.96.235.75    <none>        2181/TCP,2888/TCP,3888/TCP                                                                          44h   node=itoa-zookeeper-dp1
itoa-zookeeper-service2                          ClusterIP   10.96.161.122   <none>        2181/TCP,2888/TCP,3888/TCP                                                                          44h   node=itoa-zookeeper-dp2
itoa-zookeeper-service3                          ClusterIP   10.96.67.1      <none>        2181/TCP,2888/TCP,3888/TCP                                                                          44h   node=itoa-zookeeper-dp3{code}
 

 

my cluster conf:

 
{code:java}
[root@node1 ~]# echo conf | nc 177.177.166.244 2181
clientPort=2181
secureClientPort=-1
dataDir=/opt/zookeeper/data/version-2
dataDirSize=135540411
dataLogDir=/opt/zookeeper/data/version-2
dataLogSize=135540411
tickTime=2000
maxClientCnxns=60
minSessionTimeout=4000
maxSessionTimeout=40000
serverId=1
initLimit=10
syncLimit=5
electionAlg=3
electionPort=3888
quorumPort=2888
peerType=0
membership:
server.1=0.0.0.0:2888:3888:participant
server.2=itoa-zookeeper-service2:2888:3888:participant
server.3=itoa-zookeeper-service3:2888:3888:participant
version=0
{code}
 

runok & mntr response

 
{code:java}
//177.177.166.244

[root@node1 ~]# echo ruok | nc 177.177.166.244 2181 && echo mntr | nc 177.177.166.244 2181
imokzk_version  3.5.5-390fe37ea45dee01bf87dc1c042b5e3dcce88653, built on 05/03/2019 12:07 GMT
zk_avg_latency  2
zk_max_latency  2
zk_min_latency  2
zk_packets_received     740
zk_packets_sent 105
zk_num_alive_connections        1
zk_outstanding_requests 634
zk_server_state follower
zk_znode_count  8092
zk_watch_count  0
zk_ephemerals_count     5
zk_approximate_data_size        708815
zk_open_file_descriptor_count   100
zk_max_file_descriptor_count    1048576

//177.177.135.54
[root@node1 ~]# echo ruok | nc 177.177.135.54 2181 && echo mntr | nc 177.177.135.54 2181
imokzk_version  3.5.5-390fe37ea45dee01bf87dc1c042b5e3dcce88653, built on 05/03/2019 12:07 GMT
zk_avg_latency  0
zk_max_latency  0
zk_min_latency  0
zk_packets_received     727
zk_packets_sent 107
zk_num_alive_connections        1
zk_outstanding_requests 619
zk_server_state leader
zk_znode_count  8092
zk_watch_count  0
zk_ephemerals_count     5
zk_approximate_data_size        708815
zk_open_file_descriptor_count   101
zk_max_file_descriptor_count    1048576
zk_followers    1
zk_synced_followers     1
zk_pending_syncs        0
zk_last_proposal_size   32
zk_max_proposal_size    162
zk_min_proposal_size    32

{code}
 

zkCli..sh responese 'client session timed out'

 
{code:java}
[zk: itoa-zookeeper-service1:2181,itoa-zookeeper-service2:2181,itoa-zookeeper-service3:2181(CONNECTING) 0] ls /
2021-04-29 11:08:22,459 [myid:itoa-zookeeper-service3:2181] - WARN  [main-SendThread(itoa-zookeeper-service3:2181):ClientCnxn$SendThread@1190] - Client session timed out, have not heard from server in 10007ms for sessionid 0x0
2021-04-29 11:08:22,459 [myid:itoa-zookeeper-service3:2181] - INFO  [main-SendThread(itoa-zookeeper-service3:2181):ClientCnxn$SendThread@1238] - Client session timed out, have not heard from server in 10007ms for sessionid 0x0, closing socket connection and attempting reconnect
KeeperErrorCode = ConnectionLoss for /
[zk: itoa-zookeeper-service1:2181,itoa-zookeeper-service2:2181,itoa-zookeeper-service3:2181(CONNECTING) 1] 2021-04-29 11:08:22,743 [myid:itoa-zookeeper-service2:2181] - INFO  [main-SendThread(itoa-zookeeper-service2:2181):ClientCnxn$SendThread@1112] - Opening socket connection to server itoa-zookeeper-service2/10.96.161.122:2181. Will not attempt to authenticate using SASL (unknown error)
2021-04-29 11:08:22,744 [myid:itoa-zookeeper-service2:2181] - INFO  [main-SendThread(itoa-zookeeper-service2:2181):ClientCnxn$SendThread@959] - Socket connection established, initiating session, client: /177.177.166.244:45456, server: itoa-zookeeper-service2/10.96.161.122:2181
2021-04-29 11:08:22,747 [myid:itoa-zookeeper-service2:2181] - INFO  [main-SendThread(itoa-zookeeper-service2:2181):ClientCnxn$SendThread@1240] - Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect
2021-04-29 11:08:23,761 [myid:itoa-zookeeper-service1:2181] - INFO  [main-SendThread(itoa-zookeeper-service1:2181):ClientCnxn$SendThread@1112] - Opening socket connection to server itoa-zookeeper-service1/10.96.235.75:2181. Will not attempt to authenticate using SASL (unknown error)
2021-04-29 11:08:23,762 [myid:itoa-zookeeper-service1:2181] - INFO  [main-SendThread(itoa-zookeeper-service1:2181):ClientCnxn$SendThread@959] - Socket connection established, initiating session, client: /177.177.166.244:42428, server: itoa-zookeeper-service1/10.96.235.75:2181
{code}
 

I shutted down node2 at 2021-04-28 17:12,uped at  20:40.  my pod logs see in the attachment(Due to the limitation of jira, I split the log of pod2)

These other information I collected:
 * The tcp link information of one of the pods(no abnormalities in 3 pods):

 
{code:java}
[root@node1 ~]# nsenter -n -t $(docker inspect -f '{{.State.Pid}}' $(docker ps | grep zookeeper-dp1 | grep -v pause | awk '{print $1}')) netstat -plan
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name
tcp 0 0 0.0.0.0:35883 0.0.0.0:* LISTEN 109187/java
tcp 0 0 0.0.0.0:3888 0.0.0.0:* LISTEN 109187/java
tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN 109187/java
tcp 0 0 0.0.0.0:2181 0.0.0.0:* LISTEN 109187/java
tcp 0 0 177.177.166.244:3888 177.177.104.35:57886 ESTABLISHED 109187/java
tcp 0 0 177.177.166.244:42204 10.96.67.1:2888 ESTABLISHED 109187/java
tcp 0 0 177.177.166.244:3888 177.177.135.54:44462 ESTABLISHED 109187/java
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags Type State I-Node PID/Program name Path
unix 2 [ ACC ] STREAM LISTENING 2342381971 109187/java /tmp/.java_pid1.tmp
unix 2 [ ] STREAM CONNECTED 2243039212 109187/java
unix 2 [ ] STREAM CONNECTED 2243034965 109187/java
{code}
 
 * Jstack information of one of the pods (see attachment，The other two pods have similar information), I suspect that a large number of waiting on condition threads are messing up

 

 

 

 

 

 

 

 ",[],Bug,ZOOKEEPER-4288,Blocker,karl,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zkServer runok responese 'imok' but any request(include zkCli.sh)  responese 'client session timed out',2021-04-29T03:58:54.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",2.0
,"[<JIRA Component: name='java client', id='12312381'>]",2021-04-24T05:16:01.000+0000,vajexal,"Example 


{code:java}
// manually create /foo path using zkcli for example
ZooKeeper zoo = new ZooKeeper(""127.0.0.1/foo"", 1000, event -> {});
zoo.create(""/bar"", ""baz"".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);
List<String> ephemerals = zoo.getEphemerals();
System.out.println(ephemerals.toString());
// expected [/bar], actual [/foo/bar]{code}",[],Bug,ZOOKEEPER-4286,Minor,vajexal,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Respect chroot in getEphemerals API,2021-04-24T05:16:01.000+0000,"[<JIRA Version: name='3.7', id='12349953'>]",2.0
Damien Diederen,[],2021-04-23T07:02:22.000+0000,priya Vijay,"On running clair scanner for Zookeeper 3.6.1, the following high priority vulnerability is reported: 

CVE-2019-25013  [https://nvd.nist.gov/vuln/detail/CVE-2019-25013]
 details: The iconv feature in the GNU C Library (aka glibc or libc6) through 2.32, when processing invalid multi-byte input sequences in the EUC-KR encoding, may have a buffer over-read",[],Bug,ZOOKEEPER-4285,Major,priya Vijay,Invalid,2021-05-06T09:19:11.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,High CVE-2019-25013 reported by Clair scanner for Zookeeper 3.6.1,2021-05-06T09:19:11.000+0000,[],2.0
,[],2021-04-15T08:20:44.000+0000,May,"I am using the latest stable release 3.6.3. Here is the minimal workload to reproduce the bug:
 # There is a zookeeper cluster with three nodes: node1, node2 and node3;
 # Start the cluster from scratch;
 # node2 becomes the leader;
 # Stop node2;
 # node3 becomes the new leader;
 # Client requests to create a znode;
 # Restart node2;
 # node2 receives SNAP from node3;
 # node2 receives the NEWLEADER message from node3;
 # node2 takes current snapshot;
 # node2 crashes before writing to the current epoch file;
 # Restart node2 but it fails since ""Unable to load database on disk"".

 ",[],Bug,ZOOKEEPER-4283,Major,May,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Fix of ZOOKEEPER-1653 was not merged in master and 3.6 branch,2021-12-09T06:28:50.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>]",2.0
Ayush Mantri,[],2021-04-07T18:48:22.000+0000,Mohammad Arshad,,"[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>]",Bug,ZOOKEEPER-4278,Blocker,Mohammad Arshad,Fixed,2021-04-08T14:37:36.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,dependency-check:check failing - netty-transport-4.1.60.Final CVE-2021-21409 ,2022-02-23T16:44:49.000+0000,[],6.0
Norbert Kalmár,[],2021-04-07T12:21:10.000+0000,Norbert Kalmár,"Upgrade needed to https://mvnrepository.com/artifact/org.eclipse.jetty/jetty-server/9.4.39.v20210325

Update license files as well!","[<JIRA Version: name='3.5.10', id='12349434'>, <JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>]",Bug,ZOOKEEPER-4277,Blocker,Norbert Kalmár,Fixed,2021-04-07T17:44:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,dependency-check:check failing - jetty-server-9.4.38 CVE-2021-28165,2021-04-07T17:44:56.000+0000,"[<JIRA Version: name='3.5.10', id='12349434'>, <JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-04-07T05:58:41.000+0000,Kei Kori,"clientPort in zoo.cfg is forcefully complemented from client address by QuorumPeerConfig#setupClientPort even though secureClientPort is set and matches with client address' port.
Because of this behavior, in case rolling update with replacing clientPort to secureClientPort in the same port number following [Upgrading existing non-TLS cluster with no downtime|https://zookeeper.apache.org/doc/r3.7.0/zookeeperAdmin.html#Upgrading+existing+nonTLS+cluster] conflicts and gets errors below.

{code}
2021-03-29 23:21:58,638 - INFO  [main:NettyServerCnxnFactory@590] - binding to port /0.0.0.0:2281
2021-03-29 23:21:58,748 - INFO  [main:NettyServerCnxnFactory@595] - bound to port 2281
2021-03-29 23:21:58,749 - INFO  [main:NettyServerCnxnFactory@590] - binding to port 0.0.0.0/0.0.0.0:2281
2021-03-29 23:21:58,753 - ERROR [main:QuorumPeerMain@101] - Unexpected exception, exiting abnormally
java.net.BindException: Address already in use
{code}

QuorumPeerConfig#setupClientPort should complement only when both clientPort and secureClientPort are empty, and allow serving zookeeper server only with secure client port.",[],Bug,ZOOKEEPER-4276,Major,Kei Kori,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Serving only with secureClientPort fails,2021-11-04T01:49:16.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.5.8', id='12346950'>, <JIRA Version: name='3.6.2', id='12347809'>]",2.0
Ravi Kishore Valeti,"[<JIRA Component: name='java client', id='12312381'>]",2021-04-06T11:54:08.000+0000,Ravi Kishore Valeti,"Zookeeper client does sasl auth (login and subject.doAs())as a preset before attempting a connection to server.
 If there is a delay in sasl auth (possibly due to slow Kerberos communication), ZK client falsely assumes that the zk server did not respond and runs in to unnecessary multiple retries.

Client configuration:
 ""zookeeper.session.timeout"" = ""3000""
 ""zookeeper.recovery.retry"" = ""1""
 ""zookeeper.recovery.retry.intervalmill"" = ""500""

This configuration translates to  

connect timeout as 1000ms
 Read Timeout as 2000ms

Example: There was a 3 second delay in logging in the user as seen from the logs below. The connection attempt was made later. However, zk client did not wait for server response but logged a timeout (3 seconds > 1 sec connect timeout), closed the connection and went to retries. Since there was a consistent delay at Kerberos master, we had seen this retries go as long as 10 mins causing requests to timeout/fail.

Logs:

3/23/21 4:15:*32.389* AM jute.maxbuffer value is xxxxx Bytes

3/23/21 4:15:*35.395* AM Client successfully logged in.

3/23/21 4:15:35.396 AM TGT refresh sleeping until: Wed Mar 24 00:34:31 GMT 2021

3/23/21 4:15:35.396 AM TGT refresh thread started.

3/23/21 4:15:35.396 AM Client will use GSSAPI as SASL mechanism.

3/23/21 4:15:35.396 AM TGT expires:                  xxx Mar xx 04:15:35 GMT 2021

3/23/21 4:15:35.396 AM TGT valid starting at:        xxx Mar xx 04:15:35 GMT 2021

3/23/21 4:15:*35.397* AM *Opening socket connection* to server xxxxx:2181. Will attempt to SASL-authenticate using Login Context section 'Client'

3/23/21 4:15:*35.397* AM *Client session timed out, have not heard from server in* *3008ms* for sessionid 0x0

3/23/21 4:15:35.397 AM Client session timed out, have not heard from server in 3008ms for sessionid 0x0, closing socket connection and attempting reconnect

3/23/21 4:15:35.498 AM TGT renewal thread has been interrupted and will exit.

3/23/21 4:15:38.503 AM Client successfully logged in.

3/23/21 4:15:38.503 AM TGT expires:                  xxx Mar xx 04:15:38 GMT 2021

3/23/21 4:15:38.503 AM Client will use GSSAPI as SASL mechanism.

3/23/21 4:15:38.503 AM TGT valid starting at:        xxx Mar xx 04:15:38 GMT 2021

3/23/21 4:15:38.503 AM TGT refresh thread started.

3/23/21 4:15:38.503 AM TGT refresh sleeping until: Wed Mar 24 00:10:10 GMT 2021

3/23/21 4:15:38.506 AM Opening socket connection to server xxxxx:2181. Will attempt to SASL-authenticate using Login Context section 'Client'

3/23/21 4:15:38.506 AM Client session timed out, have not heard from server in 3009ms for sessionid 0x0, closing socket connection and attempting reconnect

3/23/21 4:15:38.506 AM Client session timed out, have not heard from server in 3009ms for sessionid 0x0

3/23/21 4:15:38.606 AM TGT renewal thread has been interrupted and will exit.

3/23/21 4:15:41.610 AM Client successfully logged in.

3/23/21 4:15:41.611 AM TGT refresh sleeping until: xxx Mar xx 23:42:03 GMT 2021

3/23/21 4:15:41.611 AM Client will use GSSAPI as SASL mechanism.

3/23/21 4:15:41.611 AM TGT valid starting at:        xxx Mar xx 04:15:41 GMT 2021

3/23/21 4:15:41.611 AM TGT expires:                  xxx Mar xx 04:15:41 GMT 2021

3/23/21 4:15:41.611 AM TGT refresh thread started.

3/23/21 4:15:41.612 AM Opening socket connection to server xxxxx:2181. Will attempt to SASL-authenticate using Login Context section 'Client'

3/23/21 4:15:41.613 AM Client session timed out, have not heard from server in 3006ms for sessionid 0x0

3/23/21 4:15:41.613 AM Client session timed out, have not heard from server in 3006ms for sessionid 0x0, closing socket connection and attempting reconnect","[<JIRA Version: name='3.5.10', id='12349434'>, <JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>, <JIRA Version: name='3.6.4', id='12350076'>]",Bug,ZOOKEEPER-4275,Minor,Ravi Kishore Valeti,Fixed,2021-04-19T06:55:14.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"Slowness in sasl login or subject.doAs() causes zk client to falsely assume that the server did not respond, closes connection and goes to unnecessary retries",2021-06-09T12:32:06.000+0000,"[<JIRA Version: name='3.5.9', id='12348201'>]",4.0
Damien Diederen,"[<JIRA Component: name='server', id='12312382'>]",2021-04-05T14:32:57.000+0000,Amichai Rothman,"This test occasionally fails. e.g. in [https://github.com/apache/zookeeper/pull/1672/checks?check_run_id=2265118964]. A bit hard to recreate, but it pops up again eventually.",[],Bug,ZOOKEEPER-4274,Minor,Amichai Rothman,Duplicate,2021-08-25T15:38:37.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Flaky test - RequestThrottlerTest.testLargeRequestThrottling,2021-08-25T15:38:37.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",1.0
,[],2021-04-04T05:12:19.000+0000,Mohammad Arshad,"When you run zkServer.sh version the result includes a few spam lines and the version reports a trailing dash 
{noformat}
bin/zkServer.sh version
ZooKeeper JMX enabled by default
Using config: /xxxxxxxxxxx/bin/../conf/zoo.cfg
Apache ZooKeeper, version 3.6.2- 09/04/2020 12:44 GMT

{noformat}",[],Bug,ZOOKEEPER-4273,Minor,Mohammad Arshad,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Forward port ZOOKEEPER-3931: ""zkServer.sh version"" returns a trailing dash",2021-04-04T05:12:19.000+0000,"[<JIRA Version: name='3.7.1', id='12350030'>]",1.0
Ayush Mantri,"[<JIRA Component: name='security', id='12329414'>]",2021-03-30T14:53:35.000+0000,Dominique Mongelli,"Our security tool raised the following security flaw on zookeeper 3.6.2: [https://nvd.nist.gov/vuln/detail/CVE-2021-21295]

It is a vulnerability related to jar *netty-codec-4.1.50.Final.jar*.

Based on netty issue tracker, the vulnerability is fixed in 4.1.60.Final: [https://github.com/netty/netty/security/advisories/GHSA-wm47-8v5p-wjpj]","[<JIRA Version: name='3.5.10', id='12349434'>, <JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>]",Bug,ZOOKEEPER-4272,Major,Dominique Mongelli,Fixed,2021-03-31T15:48:41.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Upgrade Netty library to > 4.1.60 due to security vulnerability CVE-2021-21295,2021-12-10T14:52:23.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",4.0
,"[<JIRA Component: name='tests', id='12312427'>]",2021-03-29T16:19:23.000+0000,Amichai Rothman,The test fails sometimes. If I run this test class (with -Dtest=ReadOnlyModeTest) in a loop it always hits the failure eventually after a few runs.,[],Bug,ZOOKEEPER-4271,Minor,Amichai Rothman,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Flaky test - ReadOnlyModeTest.testConnectionEvents,2021-07-25T12:04:56.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",1.0
Mohammad Arshad,[],2021-03-29T10:23:34.000+0000,Mohammad Arshad,"accepted epoch is first written to temporary file acceptedEpoch.tmp then this file is renamed to acceptedEpoch.


Failure, either because of exception or power-off, in renaming the acceptedEpoch.tmp file will cause server startup error with message ""The current epoch, x, is older than the last zxid y""

To handle this scenario we should read accepted epoch from this temp file as well.

For more context, refer https://github.com/apache/zookeeper/pull/1109

 ","[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>]",Bug,ZOOKEEPER-4269,Major,Mohammad Arshad,Fixed,2021-03-31T21:23:09.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,acceptedEpoch.tmp rename failure will cause server startup error,2021-03-31T21:25:22.000+0000,[],2.0
,[],2021-03-29T08:20:27.000+0000,guoaomen,"Fix https://issues.apache.org/jira/browse/ZOOKEEPER-2184, when the client connects to the server via ip, it will also perform dns reverse lookup. I think this shouldn’t be the case, dns reverse lookup should only work Domain name is valid",[],Bug,ZOOKEEPER-4268,Major,guoaomen,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Avoid Reverse DNS lookup if the hostname in connection string is literal IP address.,2021-03-29T08:20:27.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>, <JIRA Version: name='3.4.14', id='12343587'>]",2.0
Mohammad Arshad,[],2021-03-29T07:42:36.000+0000,Mohammad Arshad,"Currently there check-style issues reported in following files which need to be fixed 
{noformat}
org.apache.zookeeper.common.CertificatesToPlayWith
org.apache.zookeeper.server.quorum.QuorumPeerMainTest
{noformat}
Because of these issues checkstyle:check check is failing in all the branches

 ","[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-4267,Minor,Mohammad Arshad,Fixed,2021-03-30T11:39:20.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Fix check-style issues,2021-12-05T02:58:28.000+0000,[],1.0
Mohammad Arshad,"[<JIRA Component: name='documentation', id='12312422'>]",2021-03-28T16:54:06.000+0000,Mohammad Arshad,"Both Master and branch-3.7 documentation header have ZooKeeper version as 3.6.
 These should be changed to 3.8 and 3.7 for master and branch-3.7 respectively

Master documentation currently:
 !image-2021-03-28-22-25-39-949.png!","[<JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>]",Bug,ZOOKEEPER-4266,Minor,Mohammad Arshad,Fixed,2021-04-03T11:44:31.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Correct ZooKeeper version in documentation header,2021-04-03T18:26:31.000+0000,[],1.0
Damien Diederen,[],2021-03-28T15:34:49.000+0000,Sebb,"The download page [1] has broken links for the following release versions:

3.6.1
3.5.9

Please remove them from the page.
If necessary, they can be linked from the archive server, in which case the page should make it clear that they historic releases.

[1] https://zookeeper.apache.org/releases.html","[<JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>]",Bug,ZOOKEEPER-4265,Major,Sebb,Fixed,2021-04-10T17:57:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Download page broken links,2021-04-13T07:09:46.000+0000,[],1.0
Mukti Krishnan,"[<JIRA Component: name='security', id='12329414'>]",2021-03-26T15:41:42.000+0000,Mukti Krishnan,To use SSL communication keystore location and password need to be set at server side. At present password of keystore to be set at server side is a plaintext. There should be an option which will say whether password is encrypted or not.  User-defined decryption algorithm then can be used to decrypt the password.,[],Bug,ZOOKEEPER-4263,Major,Mukti Krishnan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,In SSL communication provide option to use encrypted password,2022-02-03T08:50:19.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-03-26T11:35:17.000+0000,fanyang,"Backporting ZOOKEEPER-3911 to branch-3.5

 ","[<JIRA Version: name='3.5.10', id='12349434'>]",Bug,ZOOKEEPER-4262,Critical,fanyang,Fixed,2021-04-21T17:20:49.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Backport ZOOKEEPER-3911 to branch-3.5,2021-04-21T17:22:20.000+0000,[],1.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-03-24T22:39:40.000+0000,Haoze Wu,ZOOKEEPER-3575 is not available in branch-3.6. The fix of ZOOKEEPER-4074 requires it.,"[<JIRA Version: name='3.6.3', id='12348703'>]",Bug,ZOOKEEPER-4260,Major,Haoze Wu,Fixed,2021-03-29T16:18:49.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Backport ZOOKEEPER-3575 to branch-3.6,2021-03-29T16:20:51.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",1.0
,[],2021-03-17T03:49:28.000+0000,Damon Liu,"App AA connects to ZooKeeper and creates a temporary path such as /brokers/ids/10. Another APP BB also connects to ZooKeeper. At some point AA BB and ZooKeeper are restarted, and BB allocates AA's old session

before restart:
AA connect to zookeeper ,and log in zk:
[2021-03-07 08:47:43,468] INFO [SyncThread:0] Established session {color:#FF0000}0x10000053ef90000{color} with negotiated timeout 8000 for client /{color:#FF0000}22.20.0.45{color}:59966 (org.apache.zookeeper.server.ZooKeeperServer)

 

after restart:

BB connect to zookeeper ,and log in zk:

[2021-03-07 08:58:51,351] INFO [SyncThread:0] Established session {color:#FF0000}0x10000053ef90000{color} with negotiated timeout 30000 for client /{color:#FF0000}22.20.0.47{color}:39360 (org.apache.zookeeper.server.ZooKeeperServer)

 

I understand that after a restart, ZooKeeper loads old transaction logs and snapshot files, and old sessions and temporary paths are loaded. If BB obtains the old session 0x10000053ef90000 at this point, the session will not expire and the temporary path will not be deleted, causing AA to be unable to re-register the temporary path

This is my question. I look forward to your help. Thank you very much!",[],Bug,ZOOKEEPER-4256,Major,Damon Liu,Duplicate,2021-03-17T05:28:43.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,The same session is obtained at different times,2021-03-17T05:28:43.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",1.0
,"[<JIRA Component: name='java client', id='12312381'>]",2021-03-17T03:48:58.000+0000,Damon Liu,,[],Bug,ZOOKEEPER-4255,Major,Damon Liu,Duplicate,2021-03-17T05:28:23.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,The same session is obtained at different times,2021-03-17T05:28:23.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",1.0
,[],2021-03-17T03:46:27.000+0000,Damon Liu,"App AA connects to ZooKeeper and creates a temporary path such as /brokers/ids/10. Another APP BB also connects to ZooKeeper. At some point AA BB and ZooKeeper are restarted, and BB allocates AA's old session

before restart:
 AA connect to zookeeper ,and log in zk:
 [2021-03-07 08:47:43,468] INFO [SyncThread:0] Established session {color:#FF0000}0x10000053ef90000{color} with negotiated timeout 8000 for client /{color:#FF0000}22.20.0.45{color}:59966 (org.apache.zookeeper.server.ZooKeeperServer)

 

after restart:

BB connect to zookeeper ,and log in zk:

[2021-03-07 08:58:51,351] INFO [SyncThread:0] Established session {color:#FF0000}0x10000053ef90000{color} with negotiated timeout 30000 for client /{color:#FF0000}22.20.0.47{color}:39360 (org.apache.zookeeper.server.ZooKeeperServer)

 

I understand that after a restart, ZooKeeper loads old transaction logs and snapshot files, and old sessions and temporary paths are loaded. If BB obtains the old session 0x10000053ef90000 at this point, the session will not expire and the temporary path will not be deleted, causing AA to be unable to re-register the temporary path

This is my question. I look forward to your help. Thank you very much!",[],Bug,ZOOKEEPER-4254,Major,Damon Liu,Duplicate,2021-03-17T05:27:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Connecting to ZooKeeper at different times allocates the same session,2021-03-17T05:27:56.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",1.0
,[],2021-03-17T03:34:23.000+0000,Damon Liu,"App AA connects to ZooKeeper and creates a temporary path such as /brokers/ids/10. Another APP BB also connects to ZooKeeper. At some point AA BB and ZooKeeper are restarted, and BB allocates AA's old session

before restart:
 AA connect to zookeeper ,and log in zk:
 [2021-03-07 08:47:43,468] INFO [SyncThread:0] Established session {color:#ff0000}0x10000053ef90000{color} with negotiated timeout 8000 for client /{color:#ff0000}22.20.0.45{color}:59966 (org.apache.zookeeper.server.ZooKeeperServer)

 

after restart:

BB connect to zookeeper ,and log in zk:

[2021-03-07 08:58:51,351] INFO [SyncThread:0] Established session {color:#ff0000}0x10000053ef90000{color} with negotiated timeout 30000 for client /{color:#ff0000}22.20.0.47{color}:39360 (org.apache.zookeeper.server.ZooKeeperServer)

 

I understand that after a restart, ZooKeeper loads old transaction logs and snapshot files, and old sessions and temporary paths are loaded. If BB obtains the old session 0x10000053ef90000 at this point, the session will not expire and the temporary path will not be deleted, causing AA to be unable to re-register the temporary path

This is my question. I look forward to your help. Thank you very much!

 

2021/03/17 14:44

I have checked the source code of zookeeper, and the generation of session is as follows:
{code:java}
public static long initializeNextSession(long id) {
 long nextSid = 0;
 nextSid = (Time.currentElapsedTime() << 24) >>> 8;
 nextSid = nextSid | (id <<56);
 return nextSid;
}{code}
Id is a fixed value, and the only variable that affects session id is the current time. Obviously zookeeper it takes different time to create session twice, but why get the same session id?",[],Bug,ZOOKEEPER-4253,Major,Damon Liu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,The client connects to zookeeper twice and get the same session id,2021-05-10T01:24:09.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",2.0
Mate Szalay-Beko,"[<JIRA Component: name='server', id='12312382'>]",2021-03-12T08:49:46.000+0000,Devarshi Shah,"*Problem:*

While upgrading K8S cluster, container running Zookeeper (during serving it's client) will rollover one by one.
 During this rollover, +Null Pointer Exception+ was observed as below.
 After updating to the latest Zookeeper 3.6.2 we still see the problem.
 This is happening on a fresh install (and has all the time).

 

*Stack-trace**:*

<from zk-pod-0-log>
{code:java}
2021-02-08T12:42:08.229+0000 [myid:] - ERROR [nioEventLoopGroup-4-1:NettyServerCnxnFactory$CnxnChannelHandler@329] - Unexpected exception in receive
 java.lang.NullPointerException: null
         at org.apache.zookeeper.server.NettyServerCnxn.receiveMessage(NettyServerCnxn.java:518) ~[zookeeper-3.6.2.jar:3.6.2]
         at org.apache.zookeeper.server.NettyServerCnxn.processMessage(NettyServerCnxn.java:368) ~[zookeeper-3.6.2.jar:3.6.2]
         at org.apache.zookeeper.server.NettyServerCnxnFactory$CnxnChannelHandler.channelRead(NettyServerCnxnFactory.java:326) [zookeeper-3.6.2.jar:3.6.2]
         at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
         at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
         at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
         at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
         at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
         at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
         at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
         at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
         at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
         at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
         at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
         at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.50.Final.jar:4.1.50.Final]
         at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [netty-common-4.1.50.Final.jar:4.1.50.Final]
         at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.50.Final.jar:4.1.50.Final]
         at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-common-4.1.50.Final.jar:4.1.50.Final]
         at java.lang.Thread.run(Thread.java:834) [?:?]
{code}
 

 

*Expectation:*

This scenario should be handled and Zookeeper should not print Null Pointer Exception in logs when peer member goes down as a part of the upgrade procedure. 

We are kindly requesting Apache Zookeeper team to fix this issue.","[<JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>, <JIRA Version: name='3.6.4', id='12350076'>]",Bug,ZOOKEEPER-4247,Major,Devarshi Shah,Fixed,2021-04-14T16:56:11.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,NPE while processing message from restarted quorum member,2021-04-19T08:00:38.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-03-11T22:45:38.000+0000,Martin Kellogg," There are three (related) possible resource leaks in the `getInputStream` and `getOutputStream` methods in `SnapStream.java`. I noticed the first because of the use of the error-prone `GZIPOutputStream`, and the other two after looking at the surrounding code.

Here is the offending code (copied from [here|https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/server/persistence/SnapStream.java#L102]):
{noformat}
    /**
     * Return the CheckedInputStream based on the extension of the fileName.
     *
     * @param file the file the InputStream read from
     * @return the specific InputStream
     * @throws IOException
     */
    public static CheckedInputStream getInputStream(File file) throws IOException {
        FileInputStream fis = new FileInputStream(file);
        InputStream is;
        switch (getStreamMode(file.getName())) {
        case GZIP:
            is = new GZIPInputStream(fis);
            break;
        case SNAPPY:
            is = new SnappyInputStream(fis);
            break;
        case CHECKED:
        default:
            is = new BufferedInputStream(fis);
        }
        return new CheckedInputStream(is, new Adler32());
    }

    /**
     * Return the OutputStream based on predefined stream mode.
     *
     * @param file the file the OutputStream writes to
     * @param fsync sync the file immediately after write
     * @return the specific OutputStream
     * @throws IOException
     */
    public static CheckedOutputStream getOutputStream(File file, boolean fsync) throws IOException {
        OutputStream fos = fsync ? new AtomicFileOutputStream(file) : new FileOutputStream(file);
        OutputStream os;
        switch (streamMode) {
        case GZIP:
            os = new GZIPOutputStream(fos);
            break;
        case SNAPPY:
            os = new SnappyOutputStream(fos);
            break;
        case CHECKED:
        default:
            os = new BufferedOutputStream(fos);
        }
        return new CheckedOutputStream(os, new Adler32());
    }
{noformat}

All three possible resource leaks are caused by the constructors of the intermediate streams (i.e. `is` and `os`), some of which might throw `IOException`s:
 * in `getOutputStream`, the call to `new GZIPOutputStream` can throw an exception, because `GZIPOutputStream` writes out the header in the constructor. If it does throw, then `fos` is never closed. That it does so makes it hard to use correctly; someone raised this as an issue with the JDK folks [here|https://bugs.openjdk.java.net/browse/JDK-8180899], but they closed it as ""won't fix"" because the constructor is documented to throw (hence the need to catch the exception here).
 * in `getInputStream`, the call to `new GZIPInputStream` can throw an `IOException` for a similar reason, causing the file handle held by `fis` to leak.
 * similarly, the call to `new SnappyInputStream` can throw an `IOException`, because it tries to read the file header during construction, which also causes `fis` to leak. `SnappyOutputStream` cannot throw; I checked [here|https://github.com/xerial/snappy-java/blob/master/src/main/java/org/xerial/snappy/SnappyOutputStream.java].

I'll submit a PR with a (simple) fix shortly after this bug report goes up and gets assigned an issue number, and add a link to this issue.",[],Bug,ZOOKEEPER-4246,Major,Martin Kellogg,Fixed,2021-05-15T15:52:27.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Resource leaks in org.apache.zookeeper.server.persistence.SnapStream#getInputStream and #getOutputStream,2021-05-15T15:52:27.000+0000,[],1.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-03-11T22:43:03.000+0000,Martin Kellogg," There are three (related) possible resource leaks in the `getInputStream` and `getOutputStream` methods in `SnapStream.java`. I noticed the first because of the use of the error-prone `GZIPOutputStream`, and the other two after looking at the surrounding code.

Here is the offending code (copied from [here|https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/server/persistence/SnapStream.java#L102]):
{noformat}
    /**
     * Return the CheckedInputStream based on the extension of the fileName.
     *
     * @param file the file the InputStream read from
     * @return the specific InputStream
     * @throws IOException
     */
    public static CheckedInputStream getInputStream(File file) throws IOException {
        FileInputStream fis = new FileInputStream(file);
        InputStream is;
        switch (getStreamMode(file.getName())) {
        case GZIP:
            is = new GZIPInputStream(fis);
            break;
        case SNAPPY:
            is = new SnappyInputStream(fis);
            break;
        case CHECKED:
        default:
            is = new BufferedInputStream(fis);
        }
        return new CheckedInputStream(is, new Adler32());
    }

    /**
     * Return the OutputStream based on predefined stream mode.
     *
     * @param file the file the OutputStream writes to
     * @param fsync sync the file immediately after write
     * @return the specific OutputStream
     * @throws IOException
     */
    public static CheckedOutputStream getOutputStream(File file, boolean fsync) throws IOException {
        OutputStream fos = fsync ? new AtomicFileOutputStream(file) : new FileOutputStream(file);
        OutputStream os;
        switch (streamMode) {
        case GZIP:
            os = new GZIPOutputStream(fos);
            break;
        case SNAPPY:
            os = new SnappyOutputStream(fos);
            break;
        case CHECKED:
        default:
            os = new BufferedOutputStream(fos);
        }
        return new CheckedOutputStream(os, new Adler32());
    }
{noformat}

All three possible resource leaks are caused by the constructors of the intermediate streams (i.e. `is` and `os`), some of which might throw `IOException`s:
 * in `getOutputStream`, the call to `new GZIPOutputStream` can throw an exception, because `GZIPOutputStream` writes out the header in the constructor. If it does throw, then `fos` is never closed. That it does so makes it hard to use correctly; someone raised this as an issue with the JDK folks [here|https://bugs.openjdk.java.net/browse/JDK-8180899], but they closed it as ""won't fix"" because the constructor is documented to throw (hence the need to catch the exception here).
 * in `getInputStream`, the call to `new GZIPInputStream` can throw an `IOException` for a similar reason, causing the file handle held by `fis` to leak.
 * similarly, the call to `new SnappyInputStream` can throw an `IOException`, because it tries to read the file header during construction, which also causes `fis` to leak. `SnappyOutputStream` cannot throw; I checked [here|https://github.com/xerial/snappy-java/blob/master/src/main/java/org/xerial/snappy/SnappyOutputStream.java].

I'll submit a PR with a (simple) fix shortly after this bug report goes up and gets assigned an issue number, and add a link to this issue.",[],Bug,ZOOKEEPER-4245,Major,Martin Kellogg,Duplicate,2021-03-25T10:45:19.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Resource leaks in org.apache.zookeeper.server.persistence.SnapStream#getInputStream and #getOutputStream,2021-03-25T17:53:05.000+0000,[],2.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-03-11T22:40:58.000+0000,Martin Kellogg," There are three (related) possible resource leaks in the `getInputStream` and `getOutputStream` methods in `SnapStream.java`. I noticed the first because of the use of the error-prone `GZIPOutputStream`, and the other two after looking at the surrounding code.

Here is the offending code (copied from [here|https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/server/persistence/SnapStream.java#L102]):
{noformat}
    /**
     * Return the CheckedInputStream based on the extension of the fileName.
     *
     * @param file the file the InputStream read from
     * @return the specific InputStream
     * @throws IOException
     */
    public static CheckedInputStream getInputStream(File file) throws IOException {
        FileInputStream fis = new FileInputStream(file);
        InputStream is;
        switch (getStreamMode(file.getName())) {
        case GZIP:
            is = new GZIPInputStream(fis);
            break;
        case SNAPPY:
            is = new SnappyInputStream(fis);
            break;
        case CHECKED:
        default:
            is = new BufferedInputStream(fis);
        }
        return new CheckedInputStream(is, new Adler32());
    }

    /**
     * Return the OutputStream based on predefined stream mode.
     *
     * @param file the file the OutputStream writes to
     * @param fsync sync the file immediately after write
     * @return the specific OutputStream
     * @throws IOException
     */
    public static CheckedOutputStream getOutputStream(File file, boolean fsync) throws IOException {
        OutputStream fos = fsync ? new AtomicFileOutputStream(file) : new FileOutputStream(file);
        OutputStream os;
        switch (streamMode) {
        case GZIP:
            os = new GZIPOutputStream(fos);
            break;
        case SNAPPY:
            os = new SnappyOutputStream(fos);
            break;
        case CHECKED:
        default:
            os = new BufferedOutputStream(fos);
        }
        return new CheckedOutputStream(os, new Adler32());
    }
{noformat}
All three possible resource leaks are caused by the constructors of the intermediate streams (i.e. `is` and `os`), some of which might throw `IOException`s:
 * in `getOutputStream`, the call to `new GZIPOutputStream` can throw an exception, because `GZIPOutputStream` writes out the header in the constructor. If it does throw, then `fos` is never closed. That it does so makes it hard to use correctly; someone raised this as an issue with the JDK folks [here|https://bugs.openjdk.java.net/browse/JDK-8180899], but they closed it as ""won't fix"" because the constructor is documented to throw (hence the need to catch the exception here).
 * in `getInputStream`, the call to `new GZIPInputStream` can throw an `IOException` for a similar reason, causing the file handle held by `fis` to leak.
 * similarly, the call to `new SnappyInputStream` can throw an `IOException`, because it tries to read the file header during construction, which also causes `fis` to leak. `SnappyOutputStream` cannot throw; I checked [here|https://github.com/xerial/snappy-java/blob/master/src/main/java/org/xerial/snappy/SnappyOutputStream.java].

I'll submit a PR with a (simple) fix shortly after this bug report goes up and gets assigned an issue number, and add a link to this issue.",[],Bug,ZOOKEEPER-4244,Major,Martin Kellogg,Duplicate,2021-03-25T10:44:07.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Resource leaks in org.apache.zookeeper.server.persistence.SnapStream#getInputStream and #getOutputStream,2021-03-25T10:44:07.000+0000,[],1.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-03-11T22:31:23.000+0000,Martin Kellogg,"There are three (related) possible resource leaks in the getInputStream and getOutputStream methods in SnapStream.java. I noticed the first because of the use of the error-prone GZIPOutputStream, and the other two after looking at the surrounding code.

Here is the offending code (copied from [https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/server/persistence/SnapStream.java#L102):]

 
{code:java}
/**     
  * Return the CheckedInputStream based on the extension of the fileName.
  *     
  * @param file the file the InputStream read from
  * @return the specific InputStream     
  * @throws IOException    
  */    
public static CheckedInputStream getInputStream(File file) throws IOException {          
    FileInputStream fis = new FileInputStream(file);
    InputStream is; 
    switch (getStreamMode(file.getName())) { 
    case GZIP:            
        is = new GZIPInputStream(fis);
        break;        
    case SNAPPY:    
        is = new SnappyInputStream(fis);  
        break;        
    case CHECKED:       
    default:            
        is = new BufferedInputStream(fis);
    }        
    return new CheckedInputStream(is, new Adler32());
}
    
/**     
  * Return the OutputStream based on predefined stream mode.     
  *     
  * @param file the file the OutputStream writes to     
  * @param fsync sync the file immediately after write     
  * @return the specific OutputStream     
  * @throws IOException     
  */    
public static CheckedOutputStream getOutputStream(File file, boolean fsync) throws IOException {        
    OutputStream fos = fsync ? new AtomicFileOutputStream(file) : new FileOutputStream(file);        
    OutputStream os;        
    switch (streamMode) {        
        case GZIP:            
        os = new GZIPOutputStream(fos);  
        break;    
    case SNAPPY:        
        os = new SnappyOutputStream(fos);  
        break;        
    case CHECKED:       
    default:             
        os = new BufferedOutputStream(fos); 
    }       
    return new CheckedOutputStream(os, new Adler32());  
}{code}
All three possible resource leaks are caused by the constructors of the intermediate streams (i.e. is and os), some of which might throw IOExceptions:
 * in getOutputStream, the call to ""new GZIPOutputStream"" can throw an exception, because GZIPOutputStream writes out the header in the constructor. If it does throw, then fos is never closed. That it does so makes it hard to use correctly; someone raised this as an issue with the JDK folks [here|[https://bugs.openjdk.java.net/browse/JDK-8180899]], but they closed it as ""won't fix"" because the constructor is documented to throw (hence why we need to catch the exception here).
 * in getInputStream, the call to ""new GZIPInputStream"" can throw an IOException for a similar reason, causing the file handle held by fis to leak.
 * similarly, the call to ""new SnappyInputStream"" can throw an IOException, because it tries to read the file header during construction, which also causes fis to leak. SnappyOutputStream cannot throw; I checked [here|[https://github.com/xerial/snappy-java/blob/master/src/main/java/org/xerial/snappy/SnappyOutputStream.java]].

I will submit a PR with a fix on Github shortly and update this description with a link.",[],Bug,ZOOKEEPER-4243,Major,Martin Kellogg,Duplicate,2021-03-25T10:43:42.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Resource leaks in org.apache.zookeeper.server.persistence.SnapStream#getInputStream and #getOutputStream,2021-03-25T10:43:42.000+0000,[],1.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-03-10T08:56:53.000+0000,KARTHIK,"The IPAuthenticationProvider class doesn't handle the IPV6 addresses properly. Because of this, the authorization is failing when using the ACL with the IP scheme.

*private byte[] addr2Bytes(String addr)* - validates only the IPV4 and not the IPV6 address. Below is the code snippet of this method.
{code:java}
// This is a bit weird but we need to return the address and the number of
// bytes (to distinguish between IPv4 and IPv6
private byte[] addr2Bytes(String addr) {
 byte[] b = v4addr2Bytes(addr);
 // TODO Write the v6addr2Bytes
 return b;
}{code}
 

Is there any other workaround to work with ZooKeeper ACL's IP scheme with IPV6 address?

 ",[],Bug,ZOOKEEPER-4240,Major,KARTHIK,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,IPV6 support in ZooKeeper ACL,2021-03-10T09:12:22.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",1.0
,"[<JIRA Component: name='license', id='12336162'>]",2021-03-09T08:38:20.000+0000,Johan Forssell,"I'm still getting a NPE in ContainerManager, as descripbed in ZOOKEEPER-2464.

{noformat}
...
COMPANY-zookeeper | 2021-03-09 07:57:21,320 [myid:1] - INFO  [main:Server@399] - Started @445ms
COMPANY-zookeeper | 2021-03-09 07:57:21,320 [myid:1] - INFO  [main:JettyAdminServer@182] - Started AdminServer on address 0.0.0.0, port 8080 and command URL /commands
COMPANY-zookeeper | 2021-03-09 07:57:21,321 [myid:1] - INFO  [main:ContainerManager@83] - Using checkIntervalMs=60000 maxPerMinute=10000 maxNeverUsedIntervalMs=0
COMPANY-zookeeper | 2021-03-09 07:57:21,322 [myid:1] - INFO  [main:ZKAuditProvider@42] - ZooKeeper audit is disabled.
COMPANY-zookeeper | 2021-03-09 07:58:21,331 [myid:1] - ERROR [ContainerManagerTask:ContainerManager$1@102] - Error checking containers
COMPANY-zookeeper | java.lang.NullPointerException
COMPANY-zookeeper | 	at org.apache.zookeeper.server.ContainerManager.getCandidates(ContainerManager.java:161)
COMPANY-zookeeper | 	at org.apache.zookeeper.server.ContainerManager.checkContainers(ContainerManager.java:128)
COMPANY-zookeeper | 	at org.apache.zookeeper.server.ContainerManager$1.run(ContainerManager.java:96)
COMPANY-zookeeper | 	at java.base/java.util.TimerThread.mainLoop(Unknown Source)
COMPANY-zookeeper | 	at java.base/java.util.TimerThread.run(Unknown Source)
COMPANY-zookeeper | 2021-03-09 07:59:21,326 [myid:1] - ERROR [ContainerManagerTask:ContainerManager$1@102] - Error checking containers
COMPANY-zookeeper | java.lang.NullPointerException
COMPANY-zookeeper | 	at org.apache.zookeeper.server.ContainerManager.getCandidates(ContainerManager.java:161)
COMPANY-zookeeper | 	at org.apache.zookeeper.server.ContainerManager.checkContainers(ContainerManager.java:128)
COMPANY-zookeeper | 	at org.apache.zookeeper.server.ContainerManager$1.run(ContainerManager.java:96)
COMPANY-zookeeper | 	at java.base/java.util.TimerThread.mainLoop(Unknown Source)
COMPANY-zookeeper | 	at java.base/java.util.TimerThread.run(Unknown Source)
COMPANY-zookeeper | 2021-03-09 08:00:21,325 [myid:1] - ERROR [ContainerManagerTask:ContainerManager$1@102] - Error checking containers
COMPANY-zookeeper | java.lang.NullPointerException
COMPANY-zookeeper | 	at org.apache.zookeeper.server.ContainerManager.getCandidates(ContainerManager.java:161)
COMPANY-zookeeper | 	at org.apache.zookeeper.server.ContainerManager.checkContainers(ContainerManager.java:128)
COMPANY-zookeeper | 	at org.apache.zookeeper.server.ContainerManager$1.run(ContainerManager.java:96)
COMPANY-zookeeper | 	at java.base/java.util.TimerThread.mainLoop(Unknown Source)
COMPANY-zookeeper | 	at java.base/java.util.TimerThread.run(Unknown Source)
COMPANY-zookeeper | 2021-03-09 08:01:21,322 [myid:1] - ERROR [ContainerManagerTask:ContainerManager$1@102] - Error checking containers
...
{noformat}",[],Bug,ZOOKEEPER-4239,Blocker,Johan Forssell,Duplicate,2021-03-09T08:39:03.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,NullPointerException in ContainerManager is still a thing,2021-03-09T08:39:03.000+0000,"[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.6.2', id='12347809'>]",2.0
,[],2021-03-09T08:31:22.000+0000,Johan Forssell,"I'm running Zookeeper from the latest docker image. Also tried 3.5.9.

After 1 minute I´m getting the same error as in ZOOKEEPER-2464: NPE in {{ContainerManager.getCandidates(ContainerManager.java:161)}}

{noformat}
...
COMPANY-zookeeper | 2021-03-09 07:57:21,320 [myid:1] - INFO  [main:Server@399] - Started @445ms
COMPANY-zookeeper | 2021-03-09 07:57:21,320 [myid:1] - INFO  [main:JettyAdminServer@182] - Started AdminServer on address 0.0.0.0, port 8080 and command URL /commands
COMPANY-zookeeper | 2021-03-09 07:57:21,321 [myid:1] - INFO  [main:ContainerManager@83] - Using checkIntervalMs=60000 maxPerMinute=10000 maxNeverUsedIntervalMs=0
COMPANY-zookeeper | 2021-03-09 07:57:21,322 [myid:1] - INFO  [main:ZKAuditProvider@42] - ZooKeeper audit is disabled.
COMPANY-zookeeper | 2021-03-09 07:58:21,331 [myid:1] - ERROR [ContainerManagerTask:ContainerManager$1@102] - Error checking containers
COMPANY-zookeeper | java.lang.NullPointerException
COMPANY-zookeeper | 	at org.apache.zookeeper.server.ContainerManager.getCandidates(ContainerManager.java:161)
COMPANY-zookeeper | 	at org.apache.zookeeper.server.ContainerManager.checkContainers(ContainerManager.java:128)
COMPANY-zookeeper | 	at org.apache.zookeeper.server.ContainerManager$1.run(ContainerManager.java:96)
COMPANY-zookeeper | 	at java.base/java.util.TimerThread.mainLoop(Unknown Source)
COMPANY-zookeeper | 	at java.base/java.util.TimerThread.run(Unknown Source)
COMPANY-zookeeper | 2021-03-09 07:59:21,326 [myid:1] - ERROR [ContainerManagerTask:ContainerManager$1@102] - Error checking containers
COMPANY-zookeeper | java.lang.NullPointerException
COMPANY-zookeeper | 	at org.apache.zookeeper.server.ContainerManager.getCandidates(ContainerManager.java:161)
COMPANY-zookeeper | 	at org.apache.zookeeper.server.ContainerManager.checkContainers(ContainerManager.java:128)
COMPANY-zookeeper | 	at org.apache.zookeeper.server.ContainerManager$1.run(ContainerManager.java:96)
COMPANY-zookeeper | 	at java.base/java.util.TimerThread.mainLoop(Unknown Source)
COMPANY-zookeeper | 	at java.base/java.util.TimerThread.run(Unknown Source)
COMPANY-zookeeper | 2021-03-09 08:00:21,325 [myid:1] - ERROR [ContainerManagerTask:ContainerManager$1@102] - Error checking containers
COMPANY-zookeeper | java.lang.NullPointerException
COMPANY-zookeeper | 	at org.apache.zookeeper.server.ContainerManager.getCandidates(ContainerManager.java:161)
COMPANY-zookeeper | 	at org.apache.zookeeper.server.ContainerManager.checkContainers(ContainerManager.java:128)
COMPANY-zookeeper | 	at org.apache.zookeeper.server.ContainerManager$1.run(ContainerManager.java:96)
COMPANY-zookeeper | 	at java.base/java.util.TimerThread.mainLoop(Unknown Source)
COMPANY-zookeeper | 	at java.base/java.util.TimerThread.run(Unknown Source)
COMPANY-zookeeper | 2021-03-09 08:01:21,322 [myid:1] - ERROR [ContainerManagerTask:ContainerManager$1@102] - Error checking containers
...
{noformat}


 

 ",[],Bug,ZOOKEEPER-4238,Blocker,Johan Forssell,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,NullPointerException in ContainerManager is still a thing,2021-11-04T03:13:12.000+0000,"[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.6.2', id='12347809'>]",4.0
,[],2021-03-09T08:18:52.000+0000,Johan Forssell,ZOOKEEPER-2464,[],Bug,ZOOKEEPER-4237,Major,Johan Forssell,Duplicate,2021-03-09T10:38:19.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Starting,2021-03-09T10:38:19.000+0000,"[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.6.2', id='12347809'>]",1.0
,[],2021-03-08T21:30:17.000+0000,Daniel Wong,"Hi I am an Apache Phoenix committer and I help manage many many zookeeper clusters at my employment primarily using ZK for HBase use cases.  We recently had a production incident where some of our ACLs were not setup preventing connectivity from the client to the ZK nodes and the failure path exposed 2 issues to fix. This Jira and ZooKeeper-4235.  This Jira is the less important of the 2 and handles numerous objects.  We had hundreds of threads per JVM with the following stack trace.  
{code:java}
java.lang.Thread.State: RUNNABLE at java.net.PlainSocketImpl.socketConnect(java.base@11.0.4.0.101/Native Method) at java.net.AbstractPlainSocketImpl.doConnect(java.base@11.0.4.0.101/AbstractPlainSocketImpl.java:399) - locked <0x00000015004fde20> (a java.net.SocksSocketImpl) at java.net.AbstractPlainSocketImpl.connectToAddress(java.base@11.0.4.0.101/AbstractPlainSocketImpl.java:242) at java.net.AbstractPlainSocketImpl.connect(java.base@11.0.4.0.101/AbstractPlainSocketImpl.java:224) at java.net.SocksSocketImpl.connect(java.base@11.0.4.0.101/SocksSocketImpl.java:403) at java.net.Socket.connect(java.base@11.0.4.0.101/Socket.java:609) at sun.security.krb5.internal.TCPClient.<init>(java.security.jgss@11.0.4.0.101/NetClient.java:62) at sun.security.krb5.internal.NetClient.getInstance(java.security.jgss@11.0.4.0.101/NetClient.java:42) at sun.security.krb5.KdcComm$KdcCommunication.run(java.security.jgss@11.0.4.0.101/KdcComm.java:401) at sun.security.krb5.KdcComm$KdcCommunication.run(java.security.jgss@11.0.4.0.101/KdcComm.java:364) at java.security.AccessController.doPrivileged(java.base@11.0.4.0.101/Native Method) at sun.security.krb5.KdcComm.send(java.security.jgss@11.0.4.0.101/KdcComm.java:348) at sun.security.krb5.KdcComm.sendIfPossible(java.security.jgss@11.0.4.0.101/KdcComm.java:253) at sun.security.krb5.KdcComm.send(java.security.jgss@11.0.4.0.101/KdcComm.java:234) at sun.security.krb5.KdcComm.send(java.security.jgss@11.0.4.0.101/KdcComm.java:200) at sun.security.krb5.KrbAsReqBuilder.send(java.security.jgss@11.0.4.0.101/KrbAsReqBuilder.java:326) at sun.security.krb5.KrbAsReqBuilder.action(java.security.jgss@11.0.4.0.101/KrbAsReqBuilder.java:371) at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(jdk.security.auth@11.0.4.0.101/Krb5LoginModule.java:754) at com.sun.security.auth.module.Krb5LoginModule.login(jdk.security.auth@11.0.4.0.101/Krb5LoginModule.java:592) at javax.security.auth.login.LoginContext.invoke(java.base@11.0.4.0.101/LoginContext.java:726) at javax.security.auth.login.LoginContext$4.run(java.base@11.0.4.0.101/LoginContext.java:665) at javax.security.auth.login.LoginContext$4.run(java.base@11.0.4.0.101/LoginContext.java:663) at java.security.AccessController.doPrivileged(java.base@11.0.4.0.101/Native Method) at javax.security.auth.login.LoginContext.invokePriv(java.base@11.0.4.0.101/LoginContext.java:663) at javax.security.auth.login.LoginContext.login(java.base@11.0.4.0.101/LoginContext.java:574) at org.apache.zookeeper.Login.login(Login.java:304) - locked <0x000000151c477148> (a org.apache.zookeeper.Login) at org.apache.zookeeper.Login.<init>(Login.java:106) at org.apache.zookeeper.client.ZooKeeperSaslClient.createSaslClient(ZooKeeperSaslClient.java:249) - locked <0x000000151c476f68> (a org.apache.zookeeper.client.ZooKeeperSaslClient) at org.apache.zookeeper.client.ZooKeeperSaslClient.<init>(ZooKeeperSaslClient.java:141) at org.apache.zookeeper.ClientCnxn$SendThread.startConnect(ClientCnxn.java:972) at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1031)
{code}
Note that these were logging in to our 10 ZK nodes but we had 100s of Logins.  In theory we  should only need at most 10 Logins.  

This Jira is intended to improve the behavior in limiting the number of Login objects/clients to the needed number.  Note that a combination of JIRAs https://issues.apache.org/jira/browse/ZOOKEEPER-2375 and https://issues.apache.org/jira/browse/ZOOKEEPER-2139  removed the singleton at the Login level but left in unnecessary synchronization code.  This could be again improved via either a singleton perhaps at the SaslClient layer or some sort of connection -> login cache so that new connections would reuse/wait for the same objects in failure paths.",[],Bug,ZOOKEEPER-4236,Minor,Daniel Wong,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Java Client SendThread create many unnecessary Login objects,2021-05-06T15:52:59.000+0000,[],3.0
Ravi Kishore Valeti,"[<JIRA Component: name='java client', id='12312381'>]",2021-03-08T21:14:31.000+0000,Daniel Wong,"Hi I am an Apache Phoenix committer and I help manage many many zookeeper clusters at my employment primarily using ZK for HBase use cases.  We recently had a production incident where some of our ACLs were not setup preventing connectivity from the client to the ZK nodes and the failure path exposed 2 issues to fix. This Jira and https://issues.apache.org/jira/browse/ZOOKEEPER-4236 .  This Jira is the more important of the 2 and handles the failure observed in that we had a FD/thread leak from the ZK java client send thread.  We had hundreds of threads per JVM with the following stack trace.
{code:java}
java.lang.Thread.State: RUNNABLE at java.net.PlainSocketImpl.socketConnect(java.base@11.0.4.0.101/Native Method) at java.net.AbstractPlainSocketImpl.doConnect(java.base@11.0.4.0.101/AbstractPlainSocketImpl.java:399) - locked <0x00000015004fde20> (a java.net.SocksSocketImpl) at java.net.AbstractPlainSocketImpl.connectToAddress(java.base@11.0.4.0.101/AbstractPlainSocketImpl.java:242) at java.net.AbstractPlainSocketImpl.connect(java.base@11.0.4.0.101/AbstractPlainSocketImpl.java:224) at java.net.SocksSocketImpl.connect(java.base@11.0.4.0.101/SocksSocketImpl.java:403) at java.net.Socket.connect(java.base@11.0.4.0.101/Socket.java:609) at sun.security.krb5.internal.TCPClient.<init>(java.security.jgss@11.0.4.0.101/NetClient.java:62) at sun.security.krb5.internal.NetClient.getInstance(java.security.jgss@11.0.4.0.101/NetClient.java:42) at sun.security.krb5.KdcComm$KdcCommunication.run(java.security.jgss@11.0.4.0.101/KdcComm.java:401) at sun.security.krb5.KdcComm$KdcCommunication.run(java.security.jgss@11.0.4.0.101/KdcComm.java:364) at java.security.AccessController.doPrivileged(java.base@11.0.4.0.101/Native Method) at sun.security.krb5.KdcComm.send(java.security.jgss@11.0.4.0.101/KdcComm.java:348) at sun.security.krb5.KdcComm.sendIfPossible(java.security.jgss@11.0.4.0.101/KdcComm.java:253) at sun.security.krb5.KdcComm.send(java.security.jgss@11.0.4.0.101/KdcComm.java:234) at sun.security.krb5.KdcComm.send(java.security.jgss@11.0.4.0.101/KdcComm.java:200) at sun.security.krb5.KrbAsReqBuilder.send(java.security.jgss@11.0.4.0.101/KrbAsReqBuilder.java:326) at sun.security.krb5.KrbAsReqBuilder.action(java.security.jgss@11.0.4.0.101/KrbAsReqBuilder.java:371) at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(jdk.security.auth@11.0.4.0.101/Krb5LoginModule.java:754) at com.sun.security.auth.module.Krb5LoginModule.login(jdk.security.auth@11.0.4.0.101/Krb5LoginModule.java:592) at javax.security.auth.login.LoginContext.invoke(java.base@11.0.4.0.101/LoginContext.java:726) at javax.security.auth.login.LoginContext$4.run(java.base@11.0.4.0.101/LoginContext.java:665) at javax.security.auth.login.LoginContext$4.run(java.base@11.0.4.0.101/LoginContext.java:663) at java.security.AccessController.doPrivileged(java.base@11.0.4.0.101/Native Method) at javax.security.auth.login.LoginContext.invokePriv(java.base@11.0.4.0.101/LoginContext.java:663) at javax.security.auth.login.LoginContext.login(java.base@11.0.4.0.101/LoginContext.java:574) at org.apache.zookeeper.Login.login(Login.java:304) - locked <0x000000151c477148> (a org.apache.zookeeper.Login) at org.apache.zookeeper.Login.<init>(Login.java:106) at org.apache.zookeeper.client.ZooKeeperSaslClient.createSaslClient(ZooKeeperSaslClient.java:249) - locked <0x000000151c476f68> (a org.apache.zookeeper.client.ZooKeeperSaslClient) at org.apache.zookeeper.client.ZooKeeperSaslClient.<init>(ZooKeeperSaslClient.java:141) at org.apache.zookeeper.ClientCnxn$SendThread.startConnect(ClientCnxn.java:972) at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1031)
{code}
Note that today ZooKeeperSaslClient as well as Login both allocate resources in their constructors and thus cannot be cleaned up or interrupted via close/shutdown/disconnect of their parents due to still being a null object during initialization.  This leaves the thread/sockets at the mercy of the configured kdc retry/timeout configuration.

This Jira is intended to break the constructor and the initialization path into separate methods and properly clean up the resulting objects.

  ",[],Bug,ZOOKEEPER-4235,Major,Daniel Wong,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Java Client SendThread does not clean up created objects during constructor of SaslClient and Login,2021-09-22T16:13:16.000+0000,[],5.0
Mukti Krishnan,"[<JIRA Component: name='server', id='12312382'>]",2021-03-08T14:54:20.000+0000,Mukti Krishnan,"In unsecured zk cluster, adding new ZooKeeper server through reconfig fails",[],Bug,ZOOKEEPER-4234,Major,Mukti Krishnan,,,"This issue was once resolved, but the resolution was deemed incorrect. From here issues are either marked assigned or resolved.",Reopened,0.0,"In unsecured zk cluster, adding new ZooKeeper server through reconfig fails",2021-03-10T15:07:20.000+0000,[],2.0
Damien Diederen,[],2021-03-06T18:08:01.000+0000,Damien Diederen,"{{InvalidSnapshotTest.testSnapshot}} starts an instance of {{ZooKeeperServer}} on the version-controlled {{invalidsnap}} directory, which, as a side-effect, ""fixes"" the following snapshot—which is broken on purpose (see ZOOKEEPER-367):

{{zookeeper-server/src/test/resources/data/invalidsnap/version-2/snapshot.83f}}

This status quo creates a number of problems:
 # It makes the test ineffective after the first run;
 # The file shows as modified in version control tools, which can be annoying;
 # The ""fixed"" snapshot can end up being committed by mistake, invalidating the test.

(#3 is not theoretical; that ""fixed"" snapshot frequently shows up in pull requests, and was recently merged into master.).","[<JIRA Version: name='3.5.10', id='12349434'>, <JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-4232,Major,Damien Diederen,Fixed,2021-03-09T17:18:49.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,InvalidSnapshotTest corrupts its own test data,2021-03-28T08:53:56.000+0000,[],1.0
Mukti Krishnan,[],2021-03-03T16:40:42.000+0000,Mukti Krishnan,Currenlty in RestMain static temp folder is used. Even though we are not using this ZK rest component but to comply with security tools it is better to use dynamically created temp folder,"[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-4230,Minor,Mukti Krishnan,Fixed,2021-03-11T05:07:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Use dynamic temp folder instead of static temp folder in RestMain,2021-03-28T08:54:17.000+0000,[],1.0
Mohammad Arshad,"[<JIRA Component: name='tests', id='12312427'>]",2021-03-01T11:03:45.000+0000,Mohammad Arshad,"X509AuthFailureTest is failing consistently.
","[<JIRA Version: name='3.6.3', id='12348703'>]",Bug,ZOOKEEPER-4227,Minor,Mohammad Arshad,Fixed,2021-03-10T16:13:07.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,X509AuthFailureTest is failing consistently,2021-03-17T07:45:05.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>, <JIRA Version: name='3.8.0', id='12349587'>]",1.0
Damien Diederen,"[<JIRA Component: name='tests', id='12312427'>]",2021-02-27T10:41:00.000+0000,Ling Mao,"{code:java}
ERROR] Failures: 
943[ERROR]   RequestThrottlerTest.testLargeRequestThrottling:297 expected: <2> but was: <0>
944[INFO] 
945[ERROR] Tests run: 2901, Failures: 1, Errors: 0, Skipped: 4
{code}
URL: https://github.com/apache/zookeeper/pull/1608/checks?check_run_id=1953408348",[],Bug,ZOOKEEPER-4226,Minor,Ling Mao,Duplicate,2021-08-25T15:37:50.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Flaky test: RequestThrottlerTest.testLargeRequestThrottling,2021-08-25T15:37:50.000+0000,[],1.0
Mukti Krishnan,"[<JIRA Component: name='server', id='12312382'>]",2021-02-26T08:35:09.000+0000,Mukti Krishnan,Backporting ZOOKEEPER-3642 to branch-3.6,"[<JIRA Version: name='3.6.3', id='12348703'>]",Bug,ZOOKEEPER-4225,Major,Mukti Krishnan,Fixed,2021-03-09T16:16:25.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Backport ZOOKEEPER-3642 to branch-3.6,2021-03-09T16:16:25.000+0000,[],2.0
Mukti Krishnan,[],2021-02-26T08:26:24.000+0000,Mukti Krishnan,Backporting ZOOKEEPER-3891 to branch-3.6,"[<JIRA Version: name='3.6.3', id='12348703'>]",Bug,ZOOKEEPER-4224,Major,Mukti Krishnan,Fixed,2021-03-09T11:19:09.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Backport ZOOKEEPER-3891 to branch-3.6,2021-03-09T11:19:09.000+0000,[],1.0
Mukti Krishnan,"[<JIRA Component: name='java client', id='12312381'>]",2021-02-26T07:12:46.000+0000,Mukti Krishnan,Issue ZOOKEEPER-3706 is not there in branch-3.6.,"[<JIRA Version: name='3.6.3', id='12348703'>]",Bug,ZOOKEEPER-4223,Major,Mukti Krishnan,Fixed,2021-03-09T07:16:16.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Backport ZOOKEEPER-3706 to branch-3.6,2021-03-09T08:10:25.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",1.0
Mate Szalay-Beko,"[<JIRA Component: name='server', id='12312382'>]",2021-02-24T10:15:46.000+0000,Mukti Krishnan,Issue ZOOKEEPER-2307 is not present in branch-3.6,"[<JIRA Version: name='3.6.3', id='12348703'>]",Bug,ZOOKEEPER-4222,Major,Mukti Krishnan,Fixed,2021-02-25T12:07:16.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Backport ZOOKEEPER-2307 to branch-3.6,2021-02-26T07:20:51.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",2.0
Mate Szalay-Beko,"[<JIRA Component: name='server', id='12312382'>]",2021-02-23T18:26:32.000+0000,Alex Mirgorodskiy,"We've seen a few failures or long delays in electing a new leader when the previous one has a hard host reset (as opposed to just the service process down, since connections don't need to wait for timeout there). Symptoms are similar to https://issues.apache.org/jira/browse/ZOOKEEPER-2164. Reducing cnxTimeout from 5 to 1.5 seconds makes the problem much less frequent, but doesn't fix it completely. We are still using an old ZooKeeper version (3.5.5), and the new async connect feature will presumably avoid it.

But we noticed a pattern of twice the expected number of connection attempts to the same downed instance in the log, and it appears to be due to a code glitch in QuorumCnxManager.java:

 
{code:java}
synchronized void connectOne(long sid) {
    ...
    if (lastCommittedView.containsKey(sid)) {
        knownId = true;
        if (connectOne(sid, lastCommittedView.get(sid).electionAddr))
            return;
    }
    if (lastSeenQV != null && lastProposedView.containsKey(sid)
            && (!knownId || (lastProposedView.get(sid).electionAddr !=   <----
            lastCommittedView.get(sid).electionAddr))) {
        knownId = true;
        if (connectOne(sid, lastProposedView.get(sid).electionAddr))
            return;
    }
{code}
Comparing electionAddrs should be done with !equals presumably, otherwise connectOne will be invoked an extra time even in the common case when the addresses do match.

The code around it has changed recently, but the check itself still exists at the top of master. It might not matter as much with the async connects, but perhaps it helps even then.","[<JIRA Version: name='3.5.10', id='12349434'>, <JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-4220,Major,Alex Mirgorodskiy,Fixed,2021-03-06T20:54:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Potential redundant connection attempts during leader election,2021-03-28T08:54:13.000+0000,"[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.6.2', id='12347809'>]",2.0
Damien Diederen,[],2021-02-23T09:04:07.000+0000,Damien Diederen,"On a server with {{enforceQuota=true}}, {{multi()}} transactions containing {{setData}} operations can throw {{NullPointerException}} during the quota check.

On the server side, this looks like:

{code:java}
java.lang.NullPointerException
    at org.apache.zookeeper.server.ZooKeeperServer.checkQuota(ZooKeeperServer.java:2048)
    at org.apache.zookeeper.server.PrepRequestProcessor.pRequest2Txn(PrepRequestProcessor.java:397)
        [...]
{code}

On the client side, this is reflected as a {{MarshallingError}}:

{code:java}
org.apache.zookeeper.KeeperException$MarshallingErrorException: KeeperErrorCode = MarshallingError
    at org.apache.zookeeper.KeeperException.create(KeeperException.java:104)
    at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:1758)
        [...]
{code}

This is visibly due to the quota check trying to access node data by path from the {{PrepRequestProcessor}} without considering the collection of {{outstandingChanges}}.  I have a patch in the works.
","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-4219,Blocker,Damien Diederen,Fixed,2021-03-06T19:54:27.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Quota checks break setData in multi transactions,2021-03-28T08:54:11.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.8.0', id='12349587'>]",1.0
,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2021-02-20T09:52:58.000+0000,pengWei Dou,"zk client initialize a saslClient occurred an IOException:

!image-2021-02-20-16-30-09-316.png!

!image-2021-02-20-17-52-43-968.png!

and then will send a event which state is null

!image-2021-02-20-16-31-32-251.png!

 

 ",[],Bug,ZOOKEEPER-4218,Minor,pengWei Dou,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zooKeeperSaslClient will product a null state event and then caused NPE while used switch(state) to deal with event,2021-03-26T03:32:50.000+0000,"[<JIRA Version: name='3.5.6', id='12345243'>]",3.0
,"[<JIRA Component: name='tests', id='12312427'>]",2021-02-19T02:49:12.000+0000,Ling Mao,"{code:java}
[INFO] Running org.apache.zookeeper.server.watch.WatchManagerTest
[INFO] Running org.apache.zookeeper.server.watch.WatchManagerTest[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.836 s - in org.apache.zookeeper.server.PrepRequestProcessorMetricsTest
[INFO] Running org.apache.zookeeper.server.ZooKeeperServerCreationTest

[ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 5.26 s <<< FAILURE! - in org.apache.zookeeper.server.watch.WatcherCleanerTest[ERROR] testDeadWatcherMetrics  Time elapsed: 0.142 s  <<< FAILURE!org.opentest4j.AssertionFailedError: expected: <20.0> but was: <27.3333> at org.apache.zookeeper.server.watch.WatcherCleanerTest.testDeadWatcherMetrics(WatcherCleanerTest.java:166)

[INFO] Running org.apache.zookeeper.server.InvalidSnapshotTest[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.196 s - in org.apache.zookeeper.server.ZooKeeperServerCreationTest[INFO] Tests run: 8, Failures: 0,
{code}",[],Bug,ZOOKEEPER-4216,Minor,Ling Mao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Flaky test: WatcherCleanerTest.testDeadWatcherMetrics,2022-02-03T08:50:25.000+0000,[],2.0
,[],2021-02-18T16:43:48.000+0000,Dmitry Sherstobitov,"rmr command will be deprecated soon, so all my code has been updated
After that I started to get *Node not exists* issue while trying to use deleteall from follower server

So solution is to use deprecated rmr or detect leader server and run deleteall from there.",[],Bug,ZOOKEEPER-4215,Major,Dmitry Sherstobitov,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,deleteall may not remove non-empty nodes if evaluated in follower server,2021-02-20T12:43:41.000+0000,[],2.0
,[],2021-02-17T19:07:47.000+0000,Damien Diederen,"The padding of transaction log files does not seem to result in the intended effect, at least on Linux (JDKs 8 & 14) and macOS Catalina (JDK 8).

The files have an apparent size of ~64 MiB, as instructed, but only two blocks are allocated:

{noformat}
$ ls -lh log.*
-rw-r--r-- 1 dash users 65M Feb 17 19:39 log.1
$ du -h --apparent-size log.*
65M    log.1
$ du -h log.*
8.0K   log.1
{noformat}

If the goal of log padding is to avoid filesystem allocations under load (as it was when the ZooKeeper book was written), then sparse files are not desirable.

(This may be ""minor"" as it seems that nobody noticed—presumably because nobody is running their log on spinning rust.)",[],Bug,ZOOKEEPER-4213,Major,Damien Diederen,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Ineffective transaction log padding on current OSes/JDKs,2021-02-17T19:07:47.000+0000,[],1.0
,"[<JIRA Component: name='c client', id='12312380'>]",2021-02-15T14:50:13.000+0000,Sam Mikes,"When an async operation is performed in the C client, the client will attempt to send the command to the server if this would not block.

When this send reports an error (eg: EPIPE) this is reported up to the async message, but the return code is not stored and checked, making it impossible for the user of the c client library to identify this case.

This can eventually trigger an assertion in some IO libraries, eg libev, because the file descriptor libev is watching is not associated with a valid stream. ","[<JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-4210,Minor,Sam Mikes,Fixed,2021-03-09T20:30:28.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper c client: detect connection loss during opportunistic async send,2021-03-09T20:31:43.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.4.14', id='12343587'>, <JIRA Version: name='3.6.2', id='12347809'>]",1.0
Andor Molnar,"[<JIRA Component: name='build-infrastructure', id='12333105'>]",2021-02-09T11:49:40.000+0000,Andor Molnar,"For some reason the new ASF CI build checks out the master branch even in branch builds:
{noformat}
[2021-02-08T14:24:12.673Z]  > git checkout -b master 4faf507771889c7f6280d41ac4a615467680553f # timeout=10
[2021-02-08T14:24:12.673Z]  > git checkout -b master 4faf507771889c7f6280d41ac4a615467680553f # timeout=10{noformat}
I suspect that we don't need the ""git"" step under ""Steps"", because at that point the repo is already checked out.","[<JIRA Version: name='3.5.10', id='12349434'>, <JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-4207,Major,Andor Molnar,Fixed,2021-02-09T15:32:09.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,New CI pipeline checks out master in branch builds too,2021-03-28T08:53:49.000+0000,"[<JIRA Version: name='3.8.0', id='12349587'>]",2.0
,[],2021-02-09T07:39:02.000+0000,Mate Szalay-Beko,"I was submitting a GitHub PR on branch-3.5 and tried to debug why a test failed on the CI. Then I realized, that the test in question is not even present on branch-3.5. Also the maven verify output of the CI job is showing to build ""Apache ZooKeeper 3.8.0-SNAPSHOT"" instead of the expected 3.5.10-SNAPSHOT.

PR: https://github.com/apache/zookeeper/pull/1594
CI job: https://ci-hadoop.apache.org/blue/organizations/jenkins/zookeeper-precommit-github-pr/detail/PR-1594/1/pipeline
",[],Bug,ZOOKEEPER-4206,Major,Mate Szalay-Beko,Duplicate,2021-02-09T12:00:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,CI executes tests on wrong branch for 3.5 PRs,2021-02-09T12:00:56.000+0000,[],1.0
Amichai Rothman,"[<JIRA Component: name='tests', id='12312427'>]",2021-02-08T14:34:46.000+0000,Amichai Rothman,"Port 8080 is used by default on various web servers and applications. On any system running such a server the ZooKeeper build fails because the port is in use. It should use a dynamically assigned free port instead.

 

java.lang.NullPointerException
 at org.apache.zookeeper.server.X509AuthFailureTest.testSecureStandaloneServerAuthNFailure(X509AuthFailureTest.java:81)","[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-4205,Major,Amichai Rothman,Fixed,2021-03-17T08:33:01.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Test fails when port 8080 is in use,2021-03-31T15:53:49.000+0000,"[<JIRA Version: name='3.8.0', id='12349587'>]",1.0
,"[<JIRA Component: name='tests', id='12312427'>]",2021-02-08T13:09:58.000+0000,Amichai Rothman,"This test sometimes fails on a laptop. Timed performance tests in unit tests can be problematic in general due to the variety of hardware it might run on, but I have a little fix that reduces the test overhead and tightens the timing, so it's a good first step (and works for me).

 

org.opentest4j.AssertionFailedError: expected: <true> but was: <false>
 at org.apache.zookeeper.server.util.RequestPathMetricsCollectorTest.testMultiThreadPerf(RequestPathMetricsCollectorTest.java:448)","[<JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>]",Bug,ZOOKEEPER-4204,Major,Amichai Rothman,Fixed,2021-07-28T13:31:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Flaky test - RequestPathMetricsCollectorTest.testMultiThreadPerf,2021-07-28T13:34:03.000+0000,"[<JIRA Version: name='3.8.0', id='12349587'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-02-06T18:55:18.000+0000,Haoze Wu,"We found a bug similar to ZOOKEEPER-2029. Leader.LearnerCnxAcceptor is a ZooKeeperCriticalThread and its exception is handled by ZooKeeperCriticalThread#handleException, which is supposed to shutdown the process or rejoin the quorum. However, in some concurrency condition, this correct handling does not occur, and thus this leader without LearnerCnxAcceptor will not accept any follower, which is the same symptom as ZOOKEEPER-2029. Ater reproduction, we confirmed that this bug exists in both the 3.6.2 release version and the master branch.

*Reproduction*

This concurrency condition can be constructed as follows: start a ZooKeeper cluster of 3 nodes, and the ServerSocket#accept invocation throws an exception in the LearnerCnxAcceptorHandler for the second follower that tries to join the quorum. If these 3 nodes get started almost simultaneously, e.g., within 2 seconds, then The aforementioned symptom may occur. In the log, we can observe that one of the followers keeps trying to join the quorum and keeps failing, and the other 2 servers always get the request from this follower but never let it join the quorum.

We prepared the reproduction scripts in a gist link ([https://gist.github.com/functioner/d5aad57669f4e4b92b4a8757fa2d0720]), and the throwing exception of ServerSocket#accept is done by the Byteman injection script `serverSocketAccept-exception.btm`.

*Diagnosis*

The root cause of this bug is that when the Leader.LearnerCnxAcceptor thread handles an exception in ZooKeeperCriticalThread#handleException, the exception is not really ""handled"", meaning that it basically does nothing but set the ZooKeeperServerState as ERROR in the following stack trace: ZooKeeperCriticalThread#handleException -> ZooKeeperServerListenerImpl#notifyStopping -> QuorumZooKeeperServer#setState. The QuorumZooKeeperServer#setState method basically setting the state as ZooKeeperServer.State.ERROR in this scenario.

Under normal Circumstances, this ERROR state will be detected by another thread in the following stack trace: QuorumPeer#run -> Leader#lead -> Leader#isRunning -> ZooKeeperServer#isRunning. In the code, it is within the infinite while loop at the end of Leader#lead ([https://github.com/apache/zookeeper/blob/release-3.6.2/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Leader.java#L758]).
{code:java}
        void lead() throws IOException, InterruptedException {
                    // ...
                    startZkServer();                       // line 685
                    // ...
                    String shutdownMessage = null;
                    while (true) {
                        // ...
                        synchronized (this) {
                            // ...
                            if (!this.isRunning()) {       // line 758
                                shutdownMessage = ""Unexpected internal error"";
                                break;
                            }
                            // ...
                        }
                        // ...
                    }
                    if (shutdownMessage != null) {
                        shutdown(shutdownMessage);         // line 779
                    }
                    // ...
        }
{code}
If the ERROR state from Leader.LearnerCnxAcceptor or any other ZooKeeperCriticalThread could be detected in this way, then the leader would be aware of the ERROR state and turn to [https://github.com/apache/zookeeper/blob/release-3.6.2/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Leader.java#L779], which is correct.

However, in some concurrency condition, the ERROR state can't be detected in this way, because in Leader#lead, [https://github.com/apache/zookeeper/blob/release-3.6.2/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Leader.java#L685] can cover this ERROR state with a RUNNING state in the following stack trace: Leader#lead -> Leader#startZkServer -> LeaderZooKeeperServer#startup -> ZooKeeperServer#startup -> QuorumZooKeeperServer#setState. Therefore, if the ERROR state occurs before the invocation of Leader#startZkServer in Leader#lead, then this ERROR state will be covered, because QuorumZooKeeperServer#setState does not record or handle the old state.

The reproduction script we provided can construct this concurrency condition, because, usually, after the server startup, it takes some time for the QuorumPeer thread to reach [https://github.com/apache/zookeeper/blob/release-3.6.2/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Leader.java#L685]. Thus the exception LearnerCnxAcceptorHandler, if any, usually occurs earlier than that. Then the aforementioned symptom happens.

*Fix*

In terms of the fix, we basically add a sanity check to prevent the RUNNING/INITIAL state from covering the possible ERROR state.

Notice that this fix also influences the follower, which also uses the QuorumZooKeeperServer#setState method. Given that the follower may be affected by the similar symptom (RUNNING state covers ERROR state in the server startup), this fix solves this potential issue, too.

The fix can be confirmed by using same reproduction script and then observe the behavior of the servers. The leader should be able to recover from the fault and all followers are able to join the quorum.",[],Bug,ZOOKEEPER-4203,Critical,Haoze Wu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Leader swallows the ZooKeeperServer.State.ERROR from Leader.LearnerCnxAcceptor in some concurrency condition,2021-12-02T02:04:49.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",2.0
Damien Diederen,"[<JIRA Component: name='c client', id='12312380'>]",2021-02-04T19:25:05.000+0000,Damien Diederen,"Originally reported by [~eolivelli]:

{quote}
{noformat}
   [exec] .../apache-zookeeper-3.7.0/zookeeper-client/zookeeper-client-c/src/cli.c:960:14: error: 'sasl_client_init' is deprecated: first deprecated in macOS 10.11 [-Werror,-Wdeprecated-declarations]
   [exec]     sr = sasl_client_init(NULL);
   [exec]       ^
   [exec]
   [...]
{noformat}
{quote}

The above is in {{cli.c}}, but similar issues also arise when compiling {{zk_sasl.c}}, which is part of the client library.

Enrico also wrote:

{quote}
And if I remove ""-Werror"" I get

{noformat}
[exec] .../apache-zookeeper-3.7.0/zookeeper-client/zookeeper-client-c/tests/LibCSymTable.h:85:36: error: unknown type name 'clockid_t'; did you mean 'clock_t'?
   [exec]   DECLARE_SYM(int,clock_gettime,(clockid_t clk_id, struct timespec*));
   [exec]                  ^~~~~~~~~
   [exec]                  clock_t
{noformat}
{quote}

but this happens in the test suite, which does not currently work on macOS due to the {{--wrap}} option, which is not supported by the native linker.  Fixing the test suite is considered out of scope for this ticket.","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-4201,Major,Damien Diederen,Fixed,2021-02-17T19:26:23.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,C client: SASL-related compilation issues on macOS Catalina,2021-03-28T08:54:05.000+0000,[],1.0
Damien Diederen,[],2021-02-04T11:34:08.000+0000,Damien Diederen,"{{WatcherCleanerTest}} performs latency checks which fail when outside of a 20+5ms window, whereas 30+ is frequently seen on an i5 Mac Mini running macOS Catalina.","[<JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-4200,Major,Damien Diederen,Fixed,2021-02-17T19:19:35.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,WatcherCleanerTest often fails on macOS Catalina,2021-03-28T08:53:48.000+0000,[],1.0
Damien Diederen,[],2021-02-04T10:56:42.000+0000,Damien Diederen,"A single run of {{QuorumRequestPipelineTest}} requires 4000+ threads on Linux, and miserably fails on Catalina where the number seems to be capped to 2048.  This is due to a leak of quorum servers, itself caused by multiple invocations of {{QuorumBase.setUp}}.","[<JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-4199,Major,Damien Diederen,Fixed,2021-02-18T08:42:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Avoid thread leak in QuorumRequestPipelineTest,2021-03-28T08:54:15.000+0000,[],2.0
,"[<JIRA Component: name='tests', id='12312427'>]",2021-02-02T15:26:05.000+0000,Amichai Rothman,"Various tests have ""localhost"" hardcoded and/or do a reverse lookup on 127.0.0.1 and expect it to be ""localhost"". This fails on systems, such as several Linux distributions, where that resolves to ""localhost.localdomain"".

Tests should be fixed so the build can succeed on all such systems, even if localhost is named localhost.localdomain.",[],Bug,ZOOKEEPER-4197,Major,Amichai Rothman,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Various tests fail on localhost.localdomain,2021-03-14T18:19:37.000+0000,"[<JIRA Version: name='3.6.3', id='12348703'>]",2.0
,"[<JIRA Component: name='tests', id='12312427'>]",2021-02-02T05:41:11.000+0000,Ling Mao,"URL: [https://ci-hadoop.apache.org/blue/organizations/jenkins/zookeeper-precommit-github-pr/detail/PR-1444/5/pipeline/]
{code:java}
[2021-02-01T19:38:54.034Z] [INFO]  T E S T S
[2021-02-01T19:38:54.034Z] [INFO] -------------------------------------------------------
[2021-02-01T19:38:54.559Z] [INFO] Running org.apache.zookeeper.recipes.lock.ZNodeNameTest
[2021-02-01T19:38:54.559Z] [INFO] Running org.apache.zookeeper.recipes.lock.WriteLockTest
[2021-02-01T19:38:54.559Z] [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.091 s - in org.apache.zookeeper.recipes.lock.ZNodeNameTest
[2021-02-01T19:39:10.327Z] [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 13.917 s <<< FAILURE! - in org.apache.zookeeper.recipes.lock.WriteLockTest
[2021-02-01T19:39:10.327Z] [ERROR] testRun  Time elapsed: 13.854 s  <<< ERROR!
[2021-02-01T19:39:10.327Z] java.net.BindException: Address already in use
[2021-02-01T19:39:10.327Z] 	at sun.nio.ch.Net.bind0(Native Method)
[2021-02-01T19:39:10.327Z] 	at sun.nio.ch.Net.bind(Net.java:444)
[2021-02-01T19:39:10.327Z] 	at sun.nio.ch.Net.bind(Net.java:436)
[2021-02-01T19:39:10.327Z] 	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:225)
[2021-02-01T19:39:10.327Z] 	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
[2021-02-01T19:39:10.327Z] 	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:67)
[2021-02-01T19:39:10.327Z] 	at org.apache.zookeeper.server.NIOServerCnxnFactory.configure(NIOServerCnxnFactory.java:662)
[2021-02-01T19:39:10.327Z] 	at org.apache.zookeeper.server.ServerCnxnFactory.configure(ServerCnxnFactory.java:109)
[2021-02-01T19:39:10.328Z] 	at org.apache.zookeeper.server.ServerCnxnFactory.createFactory(ServerCnxnFactory.java:191)
[2021-02-01T19:39:10.328Z] 	at org.apache.zookeeper.server.ServerCnxnFactory.createFactory(ServerCnxnFactory.java:178)
[2021-02-01T19:39:10.328Z] 	at org.apache.zookeeper.test.ClientBase.createNewServerInstance(ClientBase.java:425)
[2021-02-01T19:39:10.328Z] 	at org.apache.zookeeper.test.ClientBase.startServer(ClientBase.java:515)
[2021-02-01T19:39:10.328Z] 	at org.apache.zookeeper.test.ClientBase.startServer(ClientBase.java:505)
[2021-02-01T19:39:10.328Z] 	at org.apache.zookeeper.recipes.lock.WriteLockTest.runTest(WriteLockTest.java:114)
[2021-02-01T19:39:10.328Z] 	at org.apache.zookeeper.recipes.lock.WriteLockTest.testRun(WriteLockTest.java:45)
[2021-02-01T19:39:10.328Z] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2021-02-01T19:39:10.328Z] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2021-02-01T19:39:10.328Z] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2021-02-01T19:39:10.328Z] 	at java.lang.reflect.Method.invoke(Method.java:498)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:686)
[2021-02-01T19:39:10.328Z] 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
[2021-02-01T19:39:10.328Z] 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
[2021-02-01T19:39:10.328Z] 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
[2021-02-01T19:39:10.328Z] 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
[2021-02-01T19:39:10.328Z] 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
[2021-02-01T19:39:10.328Z] 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
[2021-02-01T19:39:10.328Z] 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
[2021-02-01T19:39:10.328Z] 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
[2021-02-01T19:39:10.328Z] 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
[2021-02-01T19:39:10.328Z] 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
[2021-02-01T19:39:10.328Z] 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
[2021-02-01T19:39:10.328Z] 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
[2021-02-01T19:39:10.328Z] 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
[2021-02-01T19:39:10.328Z] 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:212)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
[2021-02-01T19:39:10.328Z] 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:208)
[2021-02-01T19:39:10.328Z] 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:137)
[2021-02-01T19:39:10.328Z] 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:71)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:135)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:125)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:135)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:123)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:122)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:80)
[2021-02-01T19:39:10.328Z] 	at java.util.ArrayList.forEach(ArrayList.java:1259)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:125)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:135)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:123)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:122)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:80)
[2021-02-01T19:39:10.328Z] 	at java.util.ArrayList.forEach(ArrayList.java:1259)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:125)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:135)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:123)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:122)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:80)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
[2021-02-01T19:39:10.328Z] 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
[2021-02-01T19:39:10.328Z] 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:142)
[2021-02-01T19:39:10.328Z] 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:113)
[2021-02-01T19:39:10.328Z] 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
[2021-02-01T19:39:10.328Z] 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
[2021-02-01T19:39:10.328Z] 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
[2021-02-01T19:39:10.328Z] 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
[2021-02-01T19:39:10.328Z] 
[2021-02-01T19:39:10.328Z] [INFO] 
[2021-02-01T19:39:10.328Z] [INFO] Results:
[2021-02-01T19:39:10.328Z] [INFO] 
[2021-02-01T19:39:10.328Z] [ERROR] Errors: 
[2021-02-01T19:39:10.328Z] [ERROR]   WriteLockTest.testRun:45->runTest:114->ClientBase.startServer:505->ClientBase.startServer:515->ClientBase.createNewServerInstance:425 » Bind
[2021-02-01T19:39:10.328Z] [INFO] 
{code}",[],Bug,ZOOKEEPER-4196,Major,Ling Mao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Flaky test: org.apache.zookeeper.recipes.lock.WriteLockTest caused by Address already in use,2022-02-03T08:50:23.000+0000,[],2.0
,"[<JIRA Component: name='contrib', id='12312700'>]",2021-01-27T05:19:48.000+0000,Brent,"In certain scenarios when investigating empty nodes, ZooInspector encounters NullPointerExceptions such as (from testing an Apache Helix cluster):
{code:java}
ERROR [SwingWorker-pool-1-thread-3] (ZooInspectorManagerImpl.java:255) - Error occurred getting data for node: /BrentTest/EXTERNALVIEWERROR [SwingWorker-pool-1-thread-3] (ZooInspectorManagerImpl.java:255) - Error occurred getting data for node: /BrentTest/EXTERNALVIEWjava.lang.NullPointerException at java.lang.String.<init>(String.java:566) at org.apache.zookeeper.inspector.encryption.BasicDataEncryptionManager.decryptData(BasicDataEncryptionManager.java:33) at org.apache.zookeeper.inspector.manager.ZooInspectorManagerImpl.getData(ZooInspectorManagerImpl.java:251) at org.apache.zookeeper.inspector.gui.nodeviewer.NodeViewerData$2.doInBackground(NodeViewerData.java:105) at org.apache.zookeeper.inspector.gui.nodeviewer.NodeViewerData$2.doInBackground(NodeViewerData.java:100) at javax.swing.SwingWorker$1.call(SwingWorker.java:295) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at javax.swing.SwingWorker.run(SwingWorker.java:334) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)
{code}
By default, ZooInspector uses org.apache.zookeeper.inspector.encryption.BasicDataEncryptionManager to decrypt node data using this code:
{code:java}
public String decryptData(byte[] encrypted) throws Exception { 
    return new String(encrypted);
}
{code}
But ""encrypted"" can get passed as ""null"" which causes the issue.  It seems like this is probably a pretty easy fix (this code seems to do it):
{code:java}
public String decryptData(byte[] encrypted) throws Exception { 
    return encrypted != null ? new String(encrypted) : """";
}
{code}
 I can put this on my list of PRs to submit if that seems reasonable.","[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>]",Bug,ZOOKEEPER-4194,Major,Brent,Fixed,2021-03-27T18:01:15.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZooInspector throws NullPointerExceptions to console when node data is null,2021-03-27T18:01:15.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",1.0
Damien Diederen,[],2021-01-24T09:36:07.000+0000,Damien Diederen,"[~symat] noticed that the source tarball for 3.7.0rc0 [is missing executable bits|https://mail-archives.apache.org/mod_mbox/zookeeper-dev/202101.mbox/%3cCAAMoRKLMf7tLosgqyiwYfFxXq-Zmiz=0oTGDijX5M=MHDF_JCg@mail.gmail.com%3e].

[~ztzg] noticed that this [can be worked around|https://mail-archives.apache.org/mod_mbox/zookeeper-dev/202101.mbox/%3C875z3n9w75.fsf%40crosstwine.com%3E] by reinstating the ""old"" version of the {{maven-assembly-plugin}}, which had been aligned in ZOOKEEPER-3833.

This ticket will concentrate on the temporary workaround, so that 3.7.0 can be released.

","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-4191,Major,Damien Diederen,Fixed,2021-01-24T18:13:03.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Missing executable bits in source release tarball,2021-03-28T08:54:26.000+0000,[],1.0
,"[<JIRA Component: name='contrib', id='12312700'>]",2021-01-23T07:06:39.000+0000,Rajesh Bathala,"Hi Team,

i am getting below error while connecting to zookeeper from zooinspector.

 

Exception in thread ""AWT-EventQueue-0"" java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory
 at org.apache.zookeeper.inspector.logger.LoggerFactory.<clinit>(LoggerFactory.java:26)
 at org.apache.zookeeper.inspector.gui.ZooInspectorPanel$8.done(ZooInspectorPanel.java:269)
 at javax.swing.SwingWorker$5.run(SwingWorker.java:737)
 at javax.swing.SwingWorker$DoSubmitAccumulativeRunnable.run(SwingWorker.java:832)
 at sun.swing.AccumulativeRunnable.run(AccumulativeRunnable.java:112)
 at javax.swing.SwingWorker$DoSubmitAccumulativeRunnable.actionPerformed(SwingWorker.java:842)
 at javax.swing.Timer.fireActionPerformed(Timer.java:313)
 at javax.swing.Timer$DoPostEvent.run(Timer.java:245)
 at java.awt.event.InvocationEvent.dispatch(InvocationEvent.java:311)
 at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:744)
 at java.awt.EventQueue.access$400(EventQueue.java:97)
 at java.awt.EventQueue$3.run(EventQueue.java:697)
 at java.awt.EventQueue$3.run(EventQueue.java:691)
 at java.security.AccessController.doPrivileged(Native Method)
 at java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:75)
 at java.awt.EventQueue.dispatchEvent(EventQueue.java:714)
 at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:201)
 at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:116)
 at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:105)
 at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:101)
 at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:93)
 at java.awt.EventDispatchThread.run(EventDispatchThread.java:82)
Caused by: java.lang.ClassNotFoundException: org.slf4j.LoggerFactory
 at java.net.URLClassLoader$1.run(URLClassLoader.java:372)
 at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
 at java.security.AccessController.doPrivileged(Native Method)
 at java.net.URLClassLoader.findClass(URLClassLoader.java:360)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
 at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
 ... 22 more",[],Bug,ZOOKEEPER-4189,Critical,Rajesh Bathala,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zooinspector Error while connecting to zookeeper,2021-01-26T05:27:12.000+0000,"[<JIRA Version: name='3.4.12', id='12342040'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-01-19T13:02:07.000+0000,Harald Musum,"We have a working setup with ZooKeeper 3.5.6 with 3 servers and server config similar to:

server.1=1.foo.com:2182:2183:participant
server.2=2.foo.com:2182:2183:participant
server.3=3.foo.com:2182:2183:participan


ZooKeeper servers are running in Docker containers and have IP addresses in the 10.x.x.x range, but also IP addresses from the default Docker network (172.17.x.x addresses) that are only usable inside the Docker
container, /etc/hosts has e.g.:

172.17.2.192 1.foo.com

When upgrading to 3.5.7 leader election failed with the following error in zookeeper log:


.org.apache.zookeeper.server.quorum.QuorumCnxManager Received connection request 10.2.2.192:37028
.org.apache.zookeeper.server.quorum.UnifiedServerSocket Accepted TLS connection from /10.2.2.192:37028 - TLSv1.2 - TLS_ECDHE_RSA_WITH_AES_128_GCM_SH
A256
.org.apache.zookeeper.server.quorum.QuorumCnxManager
 Cannot open channel to 0 at election address /172.17.2.192:2183
 exception=
 java.net.NoRouteToHostException: No route to host (Host unreachable)
 at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
 at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)
 at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)
 at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)
 at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:403)
 at java.base/java.net.Socket.connect(Socket.java:609)
 at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:285)


We tried using version 3.6.2, but had the same issue there.

After reading the code and looking at changes between 3.5.6 and 3.5.7 I
found https://issues.apache.org/jira/browse/ZOOKEEPER-3057 and the corresponding PR https://github.com/apache/zookeeper/pull/548
It seems like this PR changed the way election addresses are sent in the message when doing leader election, from using
hostnames (when hostnames are specified in server config) to always using IP addresses.

Patching 3.6.2 with the following change makes this work again for us:
{code:java}
diff --git a/zookeeper-server/zookeeper-server-3.6.2/src/main/java/org/apache/zookeeper/common/NetUtils.java b/zookeeper-server/zookeeper-server-3.6.2/src/main/java/org/apache/zookeeper/common/NetUtils.java
index be8cb9a638..f32f1da7c8 100644
--- a/zookeeper-server/zookeeper-server-3.6.2/src/main/java/org/apache/zookeeper/common/NetUtils.java
+++ b/zookeeper-server/zookeeper-server-3.6.2/src/main/java/org/apache/zookeeper/common/NetUtils.java
@@ -27,13 +27,18 @@ import java.net.InetSocketAddress;
  */
 public class NetUtils {+    // Note: Changed from original to use hostname from InetSocketAddress if there exists one
     public static String formatInetAddr(InetSocketAddress addr) {
+        String hostName = addr.getHostName();
+        if (hostName != null) {
+            return String.format(""%s:%s"", hostName, addr.getPort());
+        }
+
         InetAddress ia = addr.getAddress();         if (ia == null) {
             return String.format(""%s:%s"", addr.getHostString(), addr.getPort());
         }         if (ia instanceof Inet6Address) {
             return String.format(""[%s]:%s"", ia.getHostAddress(), addr.getPort());
         } else {{code}
 ",[],Bug,ZOOKEEPER-4183,Major,Harald Musum,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Leader election not working when using hostname in server config and hostname resolves to an internal IP addresses,2021-01-19T13:02:07.000+0000,[],2.0
,"[<JIRA Component: name='server', id='12312382'>]",2021-01-17T01:01:57.000+0000,Haoze Wu,"We were doing some systematic fault injection testing on the latest ZooKeeper stable release 3.6.2 and found an untimely network issue can cause ZooKeeper followers to hang: clients connected to this follower get stuck in their requests, while the follower would not rejoin the quorum for a long time.

Our overall experience through the fault injection testing is that ZooKeeper is robust to tolerate network issues (delay or packet loss) in various places. If a thread is doing a socket write and hangs in this operation due to the network fault, this thread will get stuck. In general, ZooKeeper can handle the issue correctly even though this thread hangs. For example, in a leader, if the `LearnerHandler` thread hangs in this way, the `QuorumPeer` (which is running the Leader#lead method) is able to confirm the stale PING state and abandon the problematic `QuorumPeer`. 
h2. Symptom

However, in the latest ZooKeeper stable release 3.6.2, we found that if a network issue happens to occur while the `writePacket` method of the `Learner` class is executing, the entire follower can get stuck. In particular, the whole `QuorumPeer` thread would be blocked because it is calling the `writePacket` method. Unlike the other situations in which the fault could be tolerated by ZooKeeper, QuorumPeer itself and other threads did not detect or handle this issue. Therefore, this follower hangs in the sense that it is not able to communicate with the leader, and the leader will abandon this follower once it does not reply `PING` packets in time, although this follower still believes it's a follower. 

Steps to reproduce are as follows:
 # Start a cluster with 1 leader and 2 followers.
 # Manually create some datanodes, and do some reads and writes.
 # Inject network fault, either using a tool like `tcconfig` or the attached Byteman scripts. 
 # Once stuck, you may observe new requests to this follower would also get stuck.

The scripts for reproduction are provided in [https://gist.github.com/functioner/ad44b5e457c8cb22eac5fc861f56d0d4].

We confirmed the issue also occurs in the latest master branch version.
h2. Root Cause

The `writePacket` method can be invoked by 3 threads: FollowerRequestProcessor, SyncRequestProcessor, and QuorumPeer. In particular, we have these possible stack traces in the attached `stacktrace.md` in [https://gist.github.com/functioner/ad44b5e457c8cb22eac5fc861f56d0d4].

There are two key issues. First, the network I/O is performed inside a synchronization block. Second, unlike the socket connection or read operations that are protected by timeouts in ZooKeeper, the socket (and OutputArchive with the socket OutputStream) write would not throw the timeout exception when the write is stuck (until the network issue is resolved). In this case, while the `QuorumPeer#readPacket` method contains a socket timeout, the reason that the follower (QuorumPeer) did not initiate the rejoin is because the QuorumPeer is blocked in writePacket and would not proceed to the receiving stage (`readPacket`), so no timeout exception is thrown to trigger the error handling.
{code:java}
    void writePacket(QuorumPacket pp, boolean flush) throws IOException {
        synchronized (leaderOs) {
            if (pp != null) {
                messageTracker.trackSent(pp.getType());
                leaderOs.writeRecord(pp, ""packet"");
            }
            if (flush) {
                bufferedOutput.flush();
            }
        }
    }

{code}
h2. Fix 

When we are preparing to submit this bug report, we find that the ZOOKEEPER-3575 has already proposed a fix to move the packet sending in learner to a separate thread. But the reason that we are still able to expose the symptom in the `master` branch is because the fix is somehow disabled by default with a configuration parameter `learner.asyncSending` that is not documented. We tried the fault injection testing on the `master` branch version with this parameter set to be true (`-Dlearner.asyncSending=true`) and found the symptom would be gone. Specifically, even though with the network issue, the packet writes to the leader would still be stuck, now the `QuorumPeer` thread would not be blocked and it can detect the issue during the phase of receiving packet from the leader thanks to the timeout in the socket read. As a result, the follower would be able to quickly go back to `LOOKING` and then `FOLLOWING` state again, while the problematic `LearnerSender` would be abandoned and recreated.
h2. Proposed Improvements

It seems that the fix in ZOOKEEPER-3575 was not enabled by default is perhaps because it was not clear whether the issue could really occur. 

We would like to first confirm this is an issue and hope the attached reproducing scripts can be helpful. Also, our testing shows this issue occurs not only in the shutdown phase as pointed out in ZOOKEEPER-3575 but also in regular requests handling, which can be serious.   

In addition, we would like to propose making the parameter `learner.asyncSending` default to be true so the fix can be enabled by default. A related improvement is to add a description of this parameter in the documentation. Otherwise, administrators would have to read the source code and to be lucky enough to stumble upon the beginning section of Learner.java to realize there is a parameter to fix the behavior.

Lastly, the async packet sending only appears in the `master` branch (and 3.7.x). There is no such fix or parameter in the latest stable release (3.6.2). We are wondering if this fix should be backported to the 3.6.x branch?

P.S. We may need to add a fault injection tool (as ZOOKEEPER-3601 suggests) so that we can provide a test for this issue.",[],Bug,ZOOKEEPER-4074,Critical,Haoze Wu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Network issue while Learner is executing writePacket can cause the follower to hang,2021-06-30T19:36:01.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",3.0
,"[<JIRA Component: name='build', id='12312383'>]",2021-01-10T02:35:31.000+0000,okumin,"`dev/docker/Dockerfile` is based on `maven:3.6.3-jdk-8` and the maven image is now based on Debian buster. The libcppunit-dev 1.14.0 shipped with Debian buster doesn't have `AM_PATH_CPPUNIT`.

 

And also `PKG_CHECK_MODULES`, the fallback macro, is not linked in the docker image.

https://github.com/apache/zookeeper/blob/4e82a8be889d91dde46a11b38c8a20e82f3220f2/zookeeper-client/zookeeper-client-c/acinclude.m4#L314-L327


 As a result, the current Docker image is not capable of building `zookeeper-client-c`.
{code:java}
$ docker build -t zookeeper/dev -f dev/docker/Dockerfile dev/docker
$ docker run --rm -it -w /root/zk -v ""$PWD:/root/zk"" zookeeper/dev mvn clean install -Pfull-build -DskipTests
{code}
{code:java}
[INFO] --- exec-maven-plugin:1.6.0:exec (autoreconf) @ zookeeper-client-c ---
acinclude.m4:315: warning: macro 'AM_PATH_CPPUNIT' not found in library
configure.ac:38: error: Missing AM_PATH_CPPUNIT or PKG_CHECK_MODULES m4 macro.
acinclude.m4:317: CHECK_CPPUNIT is expanded from...
configure.ac:38: the top level
autom4te: /usr/bin/m4 failed with exit status: 1
aclocal: error: echo failed with exit status: 1
autoreconf: aclocal failed with exit status: 1
[ERROR] Command execution failed.
org.apache.commons.exec.ExecuteException: Process exited with an error: 1 (Exit value: 1)
    at org.apache.commons.exec.DefaultExecutor.executeInternal (DefaultExecutor.java:404)
{code}","[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-4055,Major,okumin,Fixed,2021-01-10T18:54:21.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Dockerfile can't build Zookeeper C client library,2021-03-28T08:53:51.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",1.0
Huizhi Lu,"[<JIRA Component: name='java client', id='12312381'>]",2021-01-08T09:56:25.000+0000,Huizhi Lu,"h2. Description

A client would fail to read a large znode that is successfully written. The reason is the lengths of the extra fields for CreateRequest and GetDataResponse are different: GetDataResponse's extra fields have slightly more length because of the stat field. The stat field occupies more bytes in a packet.
h2. How to Reproduce

On my MacBook, the length of incoming buffer adds 84 more bytes on the original bytes data. So I try to write (1024 * 1024 - 1 - 84) bytes to ZK which is successful. And when I read it, it fails:
{code:java}
Closing socket connection. Attempting reconnect except it is a SessionExpiredException.Closing socket connection. Attempting reconnect except it is a SessionExpiredException.java.io.IOException: Packet len 1048606 is out of range! at org.apache.zookeeper.ClientCnxnSocket.readLength(ClientCnxnSocket.java:121) at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:84) at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350) at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1275)2021-01-08 01:53:36,931 [main-EventThread] INFO  org.apache.helix.zookeeper.zkclient.ZkClient - zkclient 0, zookeeper state changed ( Disconnected )
{code}
h2. Solution

We can add extra buffer size on both the client and the server for the sanity check based on the jute.maxbuffer, just like what has been done in BinaryInputArchive.java
{code:java}
// Since this is a rough sanity check, add some padding to maxBuffer to
// make up for extra fields, etc. (otherwise e.g. clients may be able to
// write buffers larger than we can read from disk!)
private void checkLength(int len) throws IOException {
    if (len < 0 || len > maxBufferSize + extraMaxBufferSize) {
        throw new IOException(UNREASONBLE_LENGTH + len);
    }
}
{code}
 ",[],Bug,ZOOKEEPER-4052,Major,Huizhi Lu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Failed to read large znode that is written successfully,2021-01-08T23:19:08.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.6.2', id='12347809'>]",1.0
,[],2021-01-08T06:51:11.000+0000,Badai Aqrandista,"This Zookeeper ensemble has 5 nodes: node 1, 2, 3, 4 and 5. At the time, leader was node 4. The quorum consisted of node 3, 4 and 5. Node 1 and 2 kept disconnecting from node 4, so they never joined the quorum.

 

At 2020-12-08 14:10:23, lost its quorum. But this only occurred after node 3 disconnected from node 4 multiple times. The disconnection message from node 3 had occurred more for more than 12 hours prior to this (but no logs prior to that). But the quorum was not lost. Following is form node 4. It shows that when the quorum was lost, there were only 2 nodes left in the quorum: node 4 and 5.

 

Note that all IP addresses are replaced with 0.0.0.0 to allow it to be included in this bug report.

 
{noformat}
[2020-12-08 14:10:20,702] INFO Notification: 2 (message format version), 2 (n.leader), 0x503300000000 (n.zxid), 0x3c (n.round), LOOKING (n.state), 2 (n.sid), 0x5033 (n.peerEPoch), LEADING (my state)0 (n.config version) (org.apache.zookeeper.server.quorum.FastLeaderElection)
[2020-12-08 14:10:20,918] INFO Notification: 2 (message format version), 2 (n.leader), 0x503300000000 (n.zxid), 0x3c (n.round), LOOKING (n.state), 2 (n.sid), 0x5033 (n.peerEPoch), LEADING (my state)0 (n.config version) (org.apache.zookeeper.server.quorum.FastLeaderElection)
[2020-12-08 14:10:21,045] INFO Received connection request 0.0.0.0:41966 (org.apache.zookeeper.server.quorum.QuorumCnxManager)
[2020-12-08 14:10:21,056] INFO Notification: 2 (message format version), 2 (n.leader), 0x503300000000 (n.zxid), 0x3d (n.round), LOOKING (n.state), 2 (n.sid), 0x5033 (n.peerEPoch), LEADING (my state)0 (n.config version) (org.apache.zookeeper.server.quorum.FastLeaderElection)
[2020-12-08 14:10:21,193] INFO Notification: 2 (message format version), 1 (n.leader), 0x420400000004 (n.zxid), 0x3d (n.round), LOOKING (n.state), 1 (n.sid), 0x5033 (n.peerEPoch), LEADING (my state)0 (n.config version) (org.apache.zookeeper.server.quorum.FastLeaderElection)
[2020-12-08 14:10:21,193] INFO Notification: 2 (message format version), 1 (n.leader), 0x420400000004 (n.zxid), 0x3d (n.round), LOOKING (n.state), 1 (n.sid), 0x5033 (n.peerEPoch), LEADING (my state)0 (n.config version) (org.apache.zookeeper.server.quorum.FastLeaderElection)
[2020-12-08 14:10:21,204] INFO Notification: 2 (message format version), 2 (n.leader), 0x503300000000 (n.zxid), 0x3d (n.round), LOOKING (n.state), 1 (n.sid), 0x5033 (n.peerEPoch), LEADING (my state)0 (n.config version) (org.apache.zookeeper.server.quorum.FastLeaderElection)
[2020-12-08 14:10:22,180] INFO Received connection request 0.0.0.0:41970 (org.apache.zookeeper.server.quorum.QuorumCnxManager)
[2020-12-08 14:10:22,181] WARN Connection broken for id 3, my id = 4, error =  (org.apache.zookeeper.server.quorum.QuorumCnxManager)
java.net.SocketException: Socket closed
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
        at java.net.SocketInputStream.read(SocketInputStream.java:171)
        at java.net.SocketInputStream.read(SocketInputStream.java:141)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
        at java.io.DataInputStream.readInt(DataInputStream.java:387)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:1212)
[2020-12-08 14:10:22,181] WARN Interrupting SendWorker (org.apache.zookeeper.server.quorum.QuorumCnxManager)
[2020-12-08 14:10:22,182] ERROR Failed to send last message. Shutting down thread. (org.apache.zookeeper.server.quorum.QuorumCnxManager)
java.net.SocketException: Socket closed
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:118)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:134)
        at java.io.DataOutputStream.writeInt(DataOutputStream.java:197)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.send(QuorumCnxManager.java:1088)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:1115)
[2020-12-08 14:10:22,182] WARN Send worker leaving thread  id 3 my id = 4 (org.apache.zookeeper.server.quorum.QuorumCnxManager)
[2020-12-08 14:10:22,402] INFO Received connection request 0.0.0.0:41974 (org.apache.zookeeper.server.quorum.QuorumCnxManager)
[2020-12-08 14:10:22,403] ERROR Failed to send last message. Shutting down thread. (org.apache.zookeeper.server.quorum.QuorumCnxManager)
java.net.SocketException: Socket closed
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:118)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:134)
        at java.io.DataOutputStream.writeInt(DataOutputStream.java:197)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.send(QuorumCnxManager.java:1088)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:1115)
[2020-12-08 14:10:22,403] WARN Send worker leaving thread  id 3 my id = 4 (org.apache.zookeeper.server.quorum.QuorumCnxManager)
[2020-12-08 14:10:22,404] WARN Interrupting SendWorker (org.apache.zookeeper.server.quorum.QuorumCnxManager)
[2020-12-08 14:10:22,829] INFO Received connection request 0.0.0.0:51666 (org.apache.zookeeper.server.quorum.QuorumCnxManager)
[2020-12-08 14:10:22,830] WARN Send worker leaving thread  id 3 my id = 4 (org.apache.zookeeper.server.quorum.QuorumCnxManager)
[2020-12-08 14:10:22,830] INFO Notification: 2 (message format version), 2 (n.leader), 0x503300000000 (n.zxid), 0x3d (n.round), LOOKING (n.state), 3 (n.sid), 0x5033 (n.peerEPoch), LEADING (my state)0 (n.config version) (org.apache.zookeeper.server.quorum.FastLeaderElection)
[2020-12-08 14:10:22,975] INFO Notification: 2 (message format version), 2 (n.leader), 0x503300000000 (n.zxid), 0x3d (n.round), LOOKING (n.state), 3 (n.sid), 0x5033 (n.peerEPoch), LEADING (my state)0 (n.config version) (org.apache.zookeeper.server.quorum.FastLeaderElection)
[2020-12-08 14:10:23,443] INFO Shutting down (org.apache.zookeeper.server.quorum.Leader)
[2020-12-08 14:10:23,443] INFO Shutdown called (org.apache.zookeeper.server.quorum.Leader)
 
java.lang.Exception: shutdown Leader! reason: Not sufficient followers synced, only synced with sids: [ [4, 5],[4, 5] ]
 
        at org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:682)
 
        at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:662)
 
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1266)
 
[2020-12-08 14:10:23,444] INFO exception while shutting down acceptor: java.net.SocketException: Socket closed (org.apache.zookeeper.server.quorum.Leader)
[2020-12-08 14:10:23,448] INFO shutting down (org.apache.zookeeper.server.ZooKeeperServer)
[2020-12-08 14:10:23,448] INFO Shutting down (org.apache.zookeeper.server.SessionTrackerImpl)
[2020-12-08 14:10:23,448] INFO Shutting down (org.apache.zookeeper.server.quorum.LeaderRequestProcessor)
[2020-12-08 14:10:23,449] INFO Shutting down (org.apache.zookeeper.server.PrepRequestProcessor)
[2020-12-08 14:10:23,449] INFO Shutting down (org.apache.zookeeper.server.quorum.ProposalRequestProcessor)
[2020-12-08 14:10:23,449] INFO Shutting down (org.apache.zookeeper.server.quorum.CommitProcessor)
[2020-12-08 14:10:23,449] INFO CommitProcessor exited loop! (org.apache.zookeeper.server.quorum.CommitProcessor)
[2020-12-08 14:10:23,449] INFO PrepRequestProcessor exited loop! (org.apache.zookeeper.server.PrepRequestProcessor)
[2020-12-08 14:10:23,455] INFO Shutting down (org.apache.zookeeper.server.quorum.Leader)
[2020-12-08 14:10:23,455] INFO shutdown of request processor complete (org.apache.zookeeper.server.FinalRequestProcessor)
[2020-12-08 14:10:23,455] INFO Shutting down (org.apache.zookeeper.server.SyncRequestProcessor)
[2020-12-08 14:10:23,455] INFO SyncRequestProcessor exited! (org.apache.zookeeper.server.SyncRequestProcessor)
[2020-12-08 14:10:23,530] WARN PeerState set to LOOKING (org.apache.zookeeper.server.quorum.QuorumPeer)
[2020-12-08 14:10:23,530] INFO LOOKING (org.apache.zookeeper.server.quorum.QuorumPeer)
[2020-12-08 14:10:23,531] WARN ******* GOODBYE /0.0.0.0:46138 ******** (org.apache.zookeeper.server.quorum.LearnerHandler)


{noformat}
 ",[],Bug,ZOOKEEPER-4051,Major,Badai Aqrandista,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Leader did not lose the quorum when a node left the quorum of 3 out of 5 nodes.,2021-01-08T06:56:56.000+0000,"[<JIRA Version: name='3.5.7', id='12346098'>]",1.0
,"[<JIRA Component: name='contrib', id='12312700'>]",2021-01-07T18:37:48.000+0000,Brent,"This is a follow-up to the issue ZOOKEEPER-3943 and the discussion on [https://github.com/apache/zookeeper/pull/1551].  PR 1551 fixes the location issue for all of the various Icons used by the UI, but does not address a similar issue with the defaultNodeViewers.cfg and defaultConnectionSettings.cfg files (as pointed out in the PR comments).  

As a result, if one builds a ""Fat JAR"" of the Zookeeper Inspector application and then moves it to a different directory, this error appears when inspecting a node at runtime:

WARN [main] (ZooInspectorManagerImpl.java:851) - List of default node viewers is empty""

And the node viewer window cannot populate correctly.","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-4050,Major,Brent,Fixed,2021-02-04T08:17:39.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Zookeeper Inspector reports ""List of default node viewers is empty"" when not specifically run from the zookeeper-contrib/zookeeper-contrib-zooinspector directory",2021-03-28T08:54:38.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",2.0
,[],2021-01-07T13:30:36.000+0000,Norbert Kalmár,"A c test is constantly failing on docker environment:


{code}
[exec]
/Downloads/zk359/apache-zookeeper-3.5.9/zookeeper-client/zookeeper-client-c/tests/TestClient.cc:789:
Assertion: equality assertion failed [Expected: 0, Actual  : -4]
{code}
",[],Bug,ZOOKEEPER-4049,Major,Norbert Kalmár,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,C client test failure on docker,2021-01-14T16:04:49.000+0000,"[<JIRA Version: name='3.5.8', id='12346950'>]",3.0
,[],2021-01-06T01:31:45.000+0000,Parag,"I have been using a zookeeper C-client libraries to communicate with the Zookeeper Cluster(Ensemble).

The communication is set to be established using mTLS.

 

While running some tests I had an incorrect certificate installed on the client side. I was expecting that the library would return a error indication AUTH failure or an callback session even indicating a failure.

But it seems the no error or callback is returned in this case. I see the loglevel to DEBUG in the client and I don’t see any logs coming out either.

 

I intend to write some re-try code and do some alarming based on the events returned by library .",[],Bug,ZOOKEEPER-4047,Major,Parag,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,C-Client does not return a error notification or callback,2021-01-06T01:31:45.000+0000,[],2.0
,[],2021-01-05T13:48:25.000+0000,Edwin Hobor,"Jackson reported a vulnerability under *CVE-2020-25649*. Upgrading to *2.10.5.1* will resolve the problem. See [https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.10#micro-patches] for more details.
  ","[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-4045,Major,Edwin Hobor,Fixed,2021-01-06T18:55:14.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,CVE-2020-25649 - Upgrade jackson databind to 2.10.5.1,2021-03-28T08:53:53.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>, <JIRA Version: name='3.6.2', id='12347809'>]",3.0
,"[<JIRA Component: name='java client', id='12312381'>]",2020-12-29T09:57:35.000+0000,Mykhailo Stefantsiv,"Curator version: 4.2.0

When usin CuratorFramework to get path data 

 
{code:java}
client.getData().forPath(path){code}
 

Sometimes, a response is an IP address of a machine where Zookeeper is running.

For example, I am expecting a simple Long value, like 5, but I am receiving 10.99.97.35

 ",[],Bug,ZOOKEEPER-4041,Major,Mykhailo Stefantsiv,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper returns an IP address when requesting a path data.,2020-12-30T03:54:18.000+0000,"[<JIRA Version: name='3.5.6', id='12345243'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2020-12-28T10:26:42.000+0000,pengfei,"h4. Overview (mechanically translated from ZOOKEEPER-4039):

The acceptedEpoch is too large and the corresponding node cannot join the cluster

After the leader receives the acceptedEpoch of more than half of the nodes, it will set its acceptedEpoch to the maximum value of these nodes plus 1, but at this time, the leader’s downtime will cause the leader node’s acceptedEpoch to be 1 larger than other nodes, and then this node will restart again Be elected as the leader, go down again, and then the remaining nodes re-elect a leader. The epoch of this leader will be smaller than the acceptedEpoch of the original leader, which causes the original node to always look and switch the follower state

Steps to reproduce:

3 nodes, server1, server2, server3

Start server1, server2, and then stop server1 and server2 at the red dot below. At this time, the corresponding acceptedEpoch=1 of server2

Restart server1, server2, and then stop server1 and server2 at the red dot below. At this time, the corresponding acceptedEpoch=2 of server2

Restart server1, server3, wait for server1 and server3 to elect the corresponding leader as server3, and then start server2, the following exception will be repeated

h4. errorlog:

java.io.IOException: Leaders epoch, 1 is less than accepted epoch, 2java.io.IOException: Leaders epoch, 1 is less than accepted epoch, 2 at org.apache.zookeeper.server.quorum.Learner.registerWithLeader(Learner.java:353) at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:78) at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1271)2020-12-28 18:09:25,176 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2182)(secure=disabled):Follower@201] - shutdown calledjava.lang.Exception: shutdown Follower at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:201) at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1275)

 
h4. sample:

cluster all servers server1,server2,server3
 * start server1 and server2 ,then shutdown them when they arrive below, now the accpetedEpoch of server2 is 1 , server1 is 0, server3 is 0  !image-2020-12-28-18-23-14-073.png!
 * then repeat step 1 , now the accpetedEpoch of server1 is 0,server2 is 2,server3 is 0  !image-2020-12-28-18-25-31-960.png!
 * then start server1 and server3 , wait unti the leader of the cluster is server3 , start server2 ,now generate the error below  !image-2020-12-28-18-28-07-015.png!",[],Bug,ZOOKEEPER-4040,Major,pengfei,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"java.io.IOException: Leaders epoch, 1 is less than accepted epoch, 2",2021-02-22T03:26:34.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.5.8', id='12346950'>, <JIRA Version: name='3.6.2', id='12347809'>]",4.0
Damien Diederen,"[<JIRA Component: name='server', id='12312382'>]",2020-12-28T10:02:37.000+0000,pengfei,"!image-2020-12-28-17-54-09-661.png!

leader会在收到过半的节点的accpetedEpoch后会将本身的accpetedEpoch设置为这些节点的最大值加1，但是此时leader宕机会导致leader节点的accpetedEpoch比其他节点大1，然后此节点再重启，再次被选为leader，再次宕机，然后剩下的节点再重新选举一个leader，这个leader的epoch会比原来的leader的accpetedEpoch要小，从而导致原来的节点一直在looking和follower状态切换

 
h4. 复现步骤:

3个节点，server1,server2,server3
 * 启动server1,server2,然后在下面红点位置停止server1和server2此时server2的对应的accpetedEpoch=1 !image-2020-12-28-18-01-46-005.png!
 * 再启动server1,server2,然后再在下面红点位置停止server1和server2此时server2的对应的accpetedEpoch=2 !image-2020-12-28-18-02-21-563.png!
 * 再启动server1，server3，等server1和server3选举出对应的leader为server3,然后再启动server2，就会一直重复下面的异常 !image-2020-12-28-18-03-58-557.png!

 

 ",[],Bug,ZOOKEEPER-4039,Major,pengfei,Duplicate,2020-12-30T20:06:12.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,accpetedEpoch过大导致对应的节点无法加入集群,2020-12-30T20:06:12.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",2.0
,[],2020-12-14T12:44:11.000+0000,LYD,"I test like this(configure: tickTime=1000):
1.connect to the zookeeper server with timeout 2000ms;
2.kill client, the client log shows be killed at 20:09:43
3.check zookeeper log, almost at the same time, zookeeper touch the socket issue:
2020-12-14 20:09:43,488 [myid:] - WARN [NIOWorkerThread-2:NIOServerCnxn@366] - Unable to read additional data from client sessionid 0x10000989ff10001, likely client has closed socket
4.I expected after 2000ms(2s) zookeeper should set the session timetout, but zookeeper set the session timeout state after 17s has pased! I don't know why. anyone has suggestion?",[],Bug,ZOOKEEPER-4028,Trivial,LYD,Invalid,2021-01-15T10:58:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zookeeper client timeout,2021-01-15T10:58:56.000+0000,[],2.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2020-12-10T06:01:51.000+0000,Tan Yee Fan,"At [https://zookeeper.apache.org/doc/r3.6.2/zookeeperReconfig.html#ch_reconfig_format], an updated configuration format is introduced, in which *client port* is now specified as part of the *server keyword specification*.

There are two issues with the documentation:
 * Elsewhere in the documentation, such as [https://zookeeper.apache.org/doc/r3.6.2/zookeeperStarted.html] and [https://zookeeper.apache.org/doc/r3.6.2/zookeeperAdmin.html], the documentation still uses the old format. System administrators who did not read the *Dynamic Reconfiguration* page will miss this change.
 * The *Backwards Compatibility* subsection is very easily missed when glancing through the [https://zookeeper.apache.org/doc/r3.6.2/zookeeperReconfig.html#ch_reconfig_format]. I think this might be the cause for CURATOR-526.",[],Bug,ZOOKEEPER-4027,Major,Tan Yee Fan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Documentation: Configuration Format,2020-12-15T02:44:47.000+0000,"[<JIRA Version: name='3.5.8', id='12346950'>, <JIRA Version: name='3.6.2', id='12347809'>]",2.0
Damien Diederen,"[<JIRA Component: name='server', id='12312382'>]",2020-12-08T17:11:35.000+0000,Charles-Henri de Boysson,"When making a MULTI request with a CREATE2 payload, the reply from the server only contains a regular CREATE response (the path but without the stat data).

 

See attachment for a capture and decode of the request/reply.

 

How to reproduce:
 * Connect to the ensemble
 * Make a MULTI (OpCode 14) request with a CREATE2 operation (OpCode 15)
 * Reply from server is success, znode is create, but the MULTI reply contains a CREATE (OpCode 1)

 ",[],Bug,ZOOKEEPER-4026,Major,Charles-Henri de Boysson,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,CREATE2 requests embeded in a MULTI request only get a regular CREATE response,2021-07-29T10:07:23.000+0000,"[<JIRA Version: name='3.5.8', id='12346950'>, <JIRA Version: name='3.6.2', id='12347809'>]",3.0
,"[<JIRA Component: name='java client', id='12312381'>]",2020-12-05T00:14:49.000+0000,Kai Sun,"Here I want to share some deficiency of ZooKeeper client connection deficiency we debugged and met in large scale operation.
 * Dead IP. Let us say one Zookeeper server is dead. The connection string just has one DNS name that can be resolved to N IPs. For >= 3.4.13 ZooKeeper client, HostProvider would size() would be 1 and next() go resolve the single DNS name which contains one bad IP of N IPs. There is 1/N chance to use this dead host and can't establish TCP connection. Next try, you still have 1/N chance to hit the same IP. So on and so forth till application level timeout. For a large number of clients, there are bound to be some application level session establishment failure. Here we probably need make sure second round of try we will exclude the previously tried IP address.
 * TCP connection timeout. If the observer size is very large say M. The TCP connection timeout is set as initial session timeout divided by HostProvider.size(). If you have a hundred observers, this can cause cross data center TCP connection not being able to established. This is especially problem for ZooKeeper version < =3.4.11. As the ZooKeeper (client) would call DNS resolving first and one connection string (DNS name) can be mapped to 100 IP address. 
 * IP address of ZooKeeper server (observers) configuration can't be picked up by client timely: This issue is mostly affecting older version of Zookeeper. As they ZooKeeper (client) object would only resolve DNS name once upon construction. Say after running for a month, IT gradually adding more servers to the meet traffic growth. The newly added ip to the DNS name won't be seen. If IT retired some servers, the client would still try to connect to them and may cause session timeout etc. ",[],Bug,ZOOKEEPER-4022,Major,Kai Sun,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper client session establishment deficiency,2020-12-26T03:19:35.000+0000,"[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.4.13', id='12342973'>, <JIRA Version: name='3.4.14', id='12343587'>]",3.0
,"[<JIRA Component: name='c client', id='12312380'>]",2020-12-03T06:51:47.000+0000,lyg,  Zookeeper c client request server by dns ， when the zookeeper server and dns server down at the same time ， c client poll will  set  revent to POLLNVAL .  At this time The parameter timeout of poll  is not valid. it cause very frequent requests and high cpu use.,[],Bug,ZOOKEEPER-4021,Critical,lyg,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper C Client ERROR about poll,2021-07-09T01:48:36.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",2.0
,"[<JIRA Component: name='c client', id='12312380'>]",2020-12-03T00:02:31.000+0000,Roger Luo,"There is memory leak in the SSL support of Zookeeper C Client, see below code (line 1457 of zookeeper.c):
{code:java}
    zcert.ca = strtok(strdup(cert), "",""); 
{code}
In the above code, memory is allocated by calling strdup, but it's never been released.",[],Bug,ZOOKEEPER-4020,Minor,Roger Luo,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Memory leak in  Zookeeper C Client,2020-12-03T01:55:26.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",2.0
,[],2020-12-02T15:00:57.000+0000,Yassin Mohii,"Using zookeeper 3.6.1 on k8s cluster, the deployment keep throwing this exception every 3~5 minutes. 
{code:java}
ERROR [FileChangeWatcher:FileChangeWatcher$WatcherThread@223] - Error from callbackERROR [FileChangeWatcher:FileChangeWatcher$WatcherThread@223] - Error from callbackjava.lang.RuntimeException: org.apache.zookeeper.common.X509Exception$SSLContextException: Failed to create KeyManager at org.apache.zookeeper.common.X509Util.handleWatchEvent(X509Util.java:638) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.lambda$newFileChangeWatcher$0(X509Util.java:562) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.FileChangeWatcher$WatcherThread.runLoop(FileChangeWatcher.java:221) [zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.FileChangeWatcher$WatcherThread.run(FileChangeWatcher.java:194) [zookeeper-3.6.1.jar:3.6.1]Caused by: org.apache.zookeeper.common.X509Exception$SSLContextException: Failed to create KeyManager at org.apache.zookeeper.common.X509Util.createSSLContextAndOptionsFromConfig(X509Util.java:350) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.createSSLContextAndOptions(X509Util.java:328) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.createSSLContextAndOptions(X509Util.java:282) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.resetDefaultSSLContextAndOptions(X509Util.java:272) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.handleWatchEvent(X509Util.java:636) ~[zookeeper-3.6.1.jar:3.6.1] ... 3 moreCaused by: org.apache.zookeeper.common.X509Exception$KeyManagerException: java.security.KeyStoreException: did not find a private key at org.apache.zookeeper.common.X509Util.createKeyManager(X509Util.java:447) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.createSSLContextAndOptionsFromConfig(X509Util.java:348) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.createSSLContextAndOptions(X509Util.java:328) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.createSSLContextAndOptions(X509Util.java:282) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.resetDefaultSSLContextAndOptions(X509Util.java:272) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.handleWatchEvent(X509Util.java:636) ~[zookeeper-3.6.1.jar:3.6.1] ... 3 moreCaused by: java.security.KeyStoreException: did not find a private key at org.apache.zookeeper.util.PemReader.loadPrivateKey(PemReader.java:148) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.util.PemReader.loadPrivateKey(PemReader.java:142) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.util.PemReader.loadKeyStore(PemReader.java:103) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.PEMFileLoader.loadKeyStore(PEMFileLoader.java:50) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.loadKeyStore(X509Util.java:400) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.createKeyManager(X509Util.java:436) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.createSSLContextAndOptionsFromConfig(X509Util.java:348) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.createSSLContextAndOptions(X509Util.java:328) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.createSSLContextAndOptions(X509Util.java:282) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.resetDefaultSSLContextAndOptions(X509Util.java:272) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.handleWatchEvent(X509Util.java:636) ~[zookeeper-3.6.1.jar:3.6.1] ... 3 more
{code}

When enabling the debug logs it appears that it tries to reload the key certificates and it fails on the first try then succeeds afterwards,
Worth to mention here that the certificates are not renewed this frequently anyway. 
Logs when enabling Debug
{code:java}
2020-12-02T12:32:33.846+0000 [myid:] - DEBUG [FileChangeWatcher:FileChangeWatcher$WatcherThread@219] - Got file changed event: ENTRY_MODIFY with context: ..2020_12_02_12_23_22.5578198682020-12-02T12:32:33.846+0000 [myid:] - DEBUG [FileChangeWatcher:FileChangeWatcher$WatcherThread@219] - Got file changed event: ENTRY_MODIFY with context: ..2020_12_02_12_23_22.5578198682020-12-02T12:32:33.846+0000 [myid:] - DEBUG [FileChangeWatcher:X509Util@641] - Ignoring watch event and keeping previous default SSL context. Event kind: ENTRY_MODIFY with context: ..2020_12_02_12_23_22.5578198682020-12-02T12:32:33.866+0000 [myid:] - DEBUG [FileChangeWatcher:FileChangeWatcher$WatcherThread@219] - Got file changed event: ENTRY_MODIFY with context: certWithPrivateKey.pem2020-12-02T12:32:33.866+0000 [myid:] - DEBUG [FileChangeWatcher:X509Util@631] - Attempting to reset default SSL context after receiving watch event: ENTRY_MODIFY with context: certWithPrivateKey.pem2020-12-02T12:32:33.867+0000 [myid:] - ERROR [FileChangeWatcher:FileChangeWatcher$WatcherThread@223] - Error from callbackjava.lang.RuntimeException: org.apache.zookeeper.common.X509Exception$SSLContextException: Failed to create KeyManager at org.apache.zookeeper.common.X509Util.handleWatchEvent(X509Util.java:638) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.lambda$newFileChangeWatcher$0(X509Util.java:562) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.FileChangeWatcher$WatcherThread.runLoop(FileChangeWatcher.java:221) [zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.FileChangeWatcher$WatcherThread.run(FileChangeWatcher.java:194) [zookeeper-3.6.1.jar:3.6.1]Caused by: org.apache.zookeeper.common.X509Exception$SSLContextException: Failed to create KeyManager at org.apache.zookeeper.common.X509Util.createSSLContextAndOptionsFromConfig(X509Util.java:350) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.createSSLContextAndOptions(X509Util.java:328) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.createSSLContextAndOptions(X509Util.java:282) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.resetDefaultSSLContextAndOptions(X509Util.java:272) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.handleWatchEvent(X509Util.java:636) ~[zookeeper-3.6.1.jar:3.6.1] ... 3 moreCaused by: org.apache.zookeeper.common.X509Exception$KeyManagerException: java.security.KeyStoreException: did not find a private key at org.apache.zookeeper.common.X509Util.createKeyManager(X509Util.java:447) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.createSSLContextAndOptionsFromConfig(X509Util.java:348) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.createSSLContextAndOptions(X509Util.java:328) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.createSSLContextAndOptions(X509Util.java:282) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.resetDefaultSSLContextAndOptions(X509Util.java:272) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.handleWatchEvent(X509Util.java:636) ~[zookeeper-3.6.1.jar:3.6.1] ... 3 moreCaused by: java.security.KeyStoreException: did not find a private key at org.apache.zookeeper.util.PemReader.loadPrivateKey(PemReader.java:148) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.util.PemReader.loadPrivateKey(PemReader.java:142) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.util.PemReader.loadKeyStore(PemReader.java:103) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.PEMFileLoader.loadKeyStore(PEMFileLoader.java:50) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.loadKeyStore(X509Util.java:400) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.createKeyManager(X509Util.java:436) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.createSSLContextAndOptionsFromConfig(X509Util.java:348) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.createSSLContextAndOptions(X509Util.java:328) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.createSSLContextAndOptions(X509Util.java:282) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.resetDefaultSSLContextAndOptions(X509Util.java:272) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.common.X509Util.handleWatchEvent(X509Util.java:636) ~[zookeeper-3.6.1.jar:3.6.1] ... 3 more2020-12-02T12:32:33.868+0000 [myid:] - DEBUG [FileChangeWatcher:FileChangeWatcher$WatcherThread@219] - Got file changed event: ENTRY_MODIFY with context: certWithPrivateKey.pem2020-12-02T12:32:33.868+0000 [myid:] - DEBUG [FileChangeWatcher:X509Util@631] - Attempting to reset default SSL context after receiving watch event: ENTRY_MODIFY with context: certWithPrivateKey.pem2020-12-02T12:32:33.874+0000 [myid:] - DEBUG [FileChangeWatcher:X509Util@540] - Using Java9+ optimized cipher suites for Java version 112020-12-02T12:32:38.273+0000 [myid:] - DEBUG [qtp877363600-30-acceptor-0@1b32cd16-ServerConnector@718607eb{HTTP/1.1,[http/1.1]}{127.0.0.1:8080}:ManagedSelector@171] - Queued change org.eclipse.jetty.io.ManagedSelector$Accept@5aad0436 on ManagedSelector@40021799{STARTED} id=0 keys=0 selected=0 updates=02020-12-02T12:32:38.273+0000 [myid:] - DEBUG [qtp877363600-30-acceptor-0@1b32cd16-ServerConnector@718607eb{HTTP/1.1,[http/1.1]}{127.0.0.1:8080}:ManagedSelector@189] - Wakeup on submit ManagedSelector@40021799{STARTED} id=0 keys=0 selected=0 updates=1
{code}",[],Bug,ZOOKEEPER-4019,Major,Yassin Mohii,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,FileChangeWatcher keep throwing exceptions on k8s deployment,2021-06-06T19:59:48.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>]",5.0
,"[<JIRA Component: name='contrib', id='12312700'>]",2020-11-24T06:34:49.000+0000,putri ulala cantika,"Membangun Diri Menjadi Pemain Poker Tak Terkalahkan

Semua permain penjudi poker pasti punya keinginan membangun diri menjadi pemain poker tak terkalahkan. Karena jika didalam sebuah permainan judi poker para pemain memang sangat ingin menjadi permain tak terkalahkan. Agar para pemain penjudi poker bisa bermain dengan baik dan nyaman dalam sebuha permainan poker tersebut, Jangan lupa untuk memiliki akun poker online profesional, dengan cara mendaftarkan diri ke [lapak303|https://199.192.31.253] Dan juga jika mereka menjadi permain poker tak terkalahkan. Maka mereka bisa mendapatkan sebuah keuntungan besar dari sebuah permainan perjudian poker tersebut. Maka dari itu banyak pemain penjudi poker mencari cara agar membangun diri menjadi pemain poker tak terkalahkan. Dan dalam sebuah artikel saya dikali ini saya ingin membagikan sedikit tips untuk anda. Agar anda bisa membangun diri menjadi pemain poker tak terkalahkan dalam permainan poker anda. Dan dibawa ini saya akan bahas abis untuk anda agar anda bisa membangun diri menjadi pemain poker tak terkalahkan. Dan anda bisa menang banyak dalam permainan poker yang anda mainkan offline atau pun online.

Tips Jadi Pemain Poker Tak Terkalahkan

Menjadi keterampilan luar biasa dalam pemain poker online-Untuk menjadi pemain poker online profesional, Anda perlu mempelajari pemain poker online canggih. Pemain perlu tahu banyak tentang poker online dan cara belajar poker online. Lebih penting lagi, Anda seharusnya tidak merasakannya saat meminta pemain berpengalaman. Apakah pemain tertarik pada keterampilan atau gameplay yang banyak digunakan oleh pemain poker. Karena semakin banyak pemain mencoba untuk mengumpulkan pengetahuan melalui poker online, ini adalah modal awal yang kuat. Pemain yang mencari saran tentang jenis pekerjaan ini adalah modal utama untuk memenangkan judi poker. Tentu saja, akan sulit bagi pemain untuk mengalahkan lawan mereka saat bermain poker. [domino 88|https://199.192.27.76]

Kami juga akan membahas kiat tentang cara menjadi pemain poker online. Kami juga akan mencoba menjelaskan komentar ini dan memberikan tips untuk pemula poker online. Memahami beberapa teknik yang akan kita bahas dalam artikel ini akan menjadi modal bagi pemain poker online. Dengan cara ini, pemain mendapatkan beberapa pengetahuan dan bisa menjadi pemenang melalui poker online. Untuk poker, judi adalah jenis judi yang tidak biasa yang dimainkan atau dimainkan secara luas di setiap wilayah. Oleh karena itu, permainan poker selalu memiliki banyak tempat untuk menjadi jenis permainan yang paling populer. Ini juga merupakan kandidat untuk permainan judi online paling populer, yang ditemukan oleh pecinta judi Indonesia. Pemain yang ingin menikmati poker online dan menjadi ahli dapat mendaftar sekarang di [dewa togel|https://162.0.236.7]


Akibatnya, banyak situs memiliki judi online yang menawarkan pemain jalan. Dengan cara ini, pemain dapat dengan mudah memenangkan judi online dan mendapatkan keuntungan ini di Indonesia. Karena itu, mereka selalu memberikan banyak informasi kepada pemain judi online Indonesia. Karena itu, mereka juga memberikan saran dan sarana untuk pemain poker online. Dengan cara ini, pemain poker online dapat menang di situs poker online. Cara pertama adalah memegang modal sebelum menjadi anggota atau distributor. Tentu saja, mereka memberi saran pemain tentang cara bermain poker online dan menjadi pemain taruhan. Ini karena ketika seorang pemain menjadi penjudi, dia bisa mendapatkan lebih banyak kesuksesan dan kesuksesan. Bandingkan dengan pemain yang hanya bermain di situs game online ini. [dewapoker|https://199.188.206.214/]

Anda kemudian dapat dengan mudah menang dengan bermain di situs judi online menggunakan metode rahasia. Ini karena metode rahasia adalah salah satu prasyarat untuk semua perjudian online. Karena itu, Anda dapat menghasilkan banyak keuntungan di Indonesia. Cukup daftarkan diri anda di [poker ace 99|https://199.188.206.214/]

, Dengan demikian, pemain judi online bisa mendapatkan kemenangan besar dan kekayaan secara rahasia. Anda kemudian bisa menjadi pemenang game dengan mengendalikan emosi dan kesabaran Anda. Dengan begitu, jika Anda mendapatkan kartu yang tidak pandai bermain kartu, tidak mudah untuk dibodohi. Bermain game dengan santai adalah salah satu cara paling efektif untuk memenangkan banyak hal di situs poker online. Karenanya CS memperingatkan mereka yang ingin memainkan game yang menarik. Di masa depan, menjadi seorang pemenang tidaklah serumit dan sesulit yang Anda pikirkan. Anda hanya perlu mempelajari dan menggunakan metode khusus ini ketika bermain game poker online.","[<JIRA Version: name='3.5.7', id='12346098'>]",Bug,ZOOKEEPER-4018,Major,putri ulala cantika,Won't Fix,2020-11-24T09:58:26.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Playing Until Die,2020-11-24T09:58:46.000+0000,"[<JIRA Version: name='3.4.11', id='12339207'>]",1.0
,"[<JIRA Component: name='c client', id='12312380'>]",2020-11-23T12:47:51.000+0000,Ling Mao,"{code:java}
[2020-11-23T12:01:26.250Z]      [exec] libtool: compile:  gcc -DHAVE_CONFIG_H -I. -I/home/jenkins/jenkins-home/workspace/eper-precommit-github-pr_PR-1528/zookeeper-client/zookeeper-client-c -I/home/jenkins/jenkins-home/workspace/eper-precommit-github-pr_PR-1528/zookeeper-client/zookeeper-client-c/include -I/home/jenkins/jenkins-home/workspace/eper-precommit-github-pr_PR-1528/zookeeper-client/zookeeper-client-c/tests -I/home/jenkins/jenkins-home/workspace/eper-precommit-github-pr_PR-1528/zookeeper-client/zookeeper-client-c/generated -DHAVE_OPENSSL_H -DHAVE_CYRUS_SASL_H -Wall -Werror -Wdeclaration-after-statement -DTHREADED -g -O2 -D_GNU_SOURCE -MT libzkmt_la-zk_log.lo -MD -MP -MF .deps/libzkmt_la-zk_log.Tpo -c /home/jenkins/jenkins-home/workspace/eper-precommit-github-pr_PR-1528/zookeeper-client/zookeeper-client-c/src/zk_log.c  -fPIC -DPIC -o .libs/libzkmt_la-zk_log.o
[2020-11-23T12:01:26.250Z]      [exec] Makefile:1033: recipe for target 'libzkmt_la-zk_log.lo' failed
[2020-11-23T12:01:26.250Z]      [exec] In file included from /home/jenkins/jenkins-home/workspace/eper-precommit-github-pr_PR-1528/zookeeper-client/zookeeper-client-c/include/zookeeper_log.h:22:0,
[2020-11-23T12:01:26.250Z]      [exec]                  from /home/jenkins/jenkins-home/workspace/eper-precommit-github-pr_PR-1528/zookeeper-client/zookeeper-client-c/src/zk_log.c:23:
[2020-11-23T12:01:26.250Z]      [exec] /home/jenkins/jenkins-home/workspace/eper-precommit-github-pr_PR-1528/zookeeper-client/zookeeper-client-c/include/zookeeper.h:36:10: fatal error: openssl/ossl_typ.h: No such file or directory
[2020-11-23T12:01:26.250Z]      [exec]  #include <openssl/ossl_typ.h>
[2020-11-23T12:01:26.250Z]      [exec]           ^~~~~~~~~~~~~~~~~~~~
[2020-11-23T12:01:26.250Z]      [exec] compilation terminated.
[2020-11-23T12:01:26.250Z]      [exec] make: *** [libzkmt_la-zk_log.lo] Error 1
{code}
URL: https://ci-hadoop.apache.org/blue/organizations/jenkins/zookeeper-precommit-github-pr/detail/PR-1528/4/pipeline

 ",[],Bug,ZOOKEEPER-4016,Minor,Ling Mao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0, C client build issue causes the failure of CI build ,2020-11-26T08:47:00.000+0000,[],3.0
,[],2020-11-22T07:49:19.000+0000,Ling Mao,"{code:java}
[2020-11-22T07:40:12.646Z] [ERROR] Failures: 
[2020-11-22T07:40:12.646Z] [ERROR]   RequestPathMetricsCollectorTest.testMultiThreadPerf:448 expected: <true> but was: <false>
[2020-11-22T07:40:12.646Z] [INFO] 
[2020-11-22T07:40:12.646Z] [ERROR] Tests run: 2880, Failures: 1, Errors: 0, Skipped: 4
[2020-11-22T07:40:12.646Z] [INFO] 
[2020-11-22T07:40:14.238Z] [INFO] -----------------------------------------------------------------------

{code}
URL: https://ci-hadoop.apache.org/blue/organizations/jenkins/zookeeper-precommit-github-pr/detail/PR-1527/2/pipeline",[],Bug,ZOOKEEPER-4015,Minor,Ling Mao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Flaky test: RequestPathMetricsCollectorTest.testMultiThreadPerf,2022-02-03T08:50:19.000+0000,[],1.0
,[],2020-11-22T03:42:53.000+0000,B,"Admittedly, there's a high probability that the problem lies between the chair and the keyboard.

If I do this:

```
; cd zookeeper-client/zookeeper-client-c
; cmake .
; make
```

`libzookeeper.a` ends up not having linked to `libhashmap.a`:
```
; nm libzookeeper.a | grep create_hashtable
 U _create_hashtable
```

To cover some bases: `libzookeeper.a` has its own symbols, as expected:
```
; nm libzookeeper.a | grep zookeeper_init
0000000000000f70 T _zookeeper_init
00000000000016a0 T _zookeeper_init2
0000000000000fe0 t _zookeeper_init_internal
0000000000001710 T _zookeeper_init_sasl
```

And the sister `libhashtable.a` has symbols of its own:
```
; nm libhashtable.a | grep create_hashtable
0000000000000000 T _create_hashtable
```

And CMakeLists says, to my untrained eyes at least, that libzookeeper.a should be linking to libhashtable.a:
```
target_link_libraries(zookeeper PUBLIC
 hashtable
```

But I don't see the link actually happening with `make VERBOSE=1`.

The result is that the generated `libzookeeper.a` just doesn't work.

I was able to monkey-patch it by doing this:

```

diff --git a/zookeeper-client/zookeeper-client-c/CMakeLists.txt b/zookeeper-client/zookeeper-client-c/CMakeLists.txt
index e89549d7a..fdefd67e7 100644
--- a/zookeeper-client/zookeeper-client-c/CMakeLists.txt
+++ b/zookeeper-client/zookeeper-client-c/CMakeLists.txt
@@ -196,7 +196,7 @@ if(WIN32)
 list(APPEND zookeeper_sources src/winport.c)
 endif()

-add_library(zookeeper STATIC ${zookeeper_sources})
+add_library(zookeeper STATIC ${zookeeper_sources} ${hashtable_sources})
 target_include_directories(zookeeper PUBLIC include ${CMAKE_CURRENT_BINARY_DIR}/include generated)
 target_link_libraries(zookeeper PUBLIC
 hashtable
```

That is, by skipping libhashtables.a entirely and just bundling the hashtable.o's directly into libzookeeper.a.  This is a total cop-out, but I couldn't figure out the right cmake incantation to make it work properly.",[],Bug,ZOOKEEPER-4014,Minor,B,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,libzookeeper.a not properly linking to libhashtable.a,2020-11-23T11:19:01.000+0000,[],2.0
,"[<JIRA Component: name='c client', id='12312380'>]",2020-11-21T22:44:19.000+0000,B,"`CMakeLists.txt` in `zookeeper-client/zookeeper-client-c` has no `INSTALL()` directives, so running `make install` ends up not installing anything.",[],Bug,ZOOKEEPER-4012,Minor,B,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,No install target for C library,2021-04-13T07:17:17.000+0000,"[<JIRA Version: name='3.5.7', id='12346098'>, <JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>, <JIRA Version: name='3.6.2', id='12347809'>]",1.0
Huizhi Lu,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='tests', id='12312427'>]",2020-11-20T22:35:48.000+0000,Huizhi Lu,"h2. Problem

maven build fails on branch-3.6 because a commit that uses jUnit 5 was back ported to branch-3.6.

Affected version: branch-3.6

Fix Version: branch-3.6
h2. How to reproduce

checkout branch 3.6 and run maven build

Error message:
{code:java}
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR :
[INFO] -------------------------------------------------------------
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[22,36] package org.junit.jupiter.api does not exist
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[22,1] static import only from classes and interfaces
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[23,36] package org.junit.jupiter.api does not exist
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[23,1] static import only from classes and interfaces
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[24,36] package org.junit.jupiter.api does not exist
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[24,1] static import only from classes and interfaces
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[25,36] package org.junit.jupiter.api does not exist
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[25,1] static import only from classes and interfaces
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[42,29] package org.junit.jupiter.api does not exist
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[43,29] package org.junit.jupiter.api does not exist
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[44,29] package org.junit.jupiter.api does not exist
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[51,6] cannot find symbol
  symbol:   class Test
  location: class org.apache.zookeeper.server.quorum.DIFFSyncConsistencyTest
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[52,6] cannot find symbol
  symbol:   class Timeout
  location: class org.apache.zookeeper.server.quorum.DIFFSyncConsistencyTest
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[220,6] cannot find symbol
  symbol:   class AfterEach
  location: class org.apache.zookeeper.server.quorum.DIFFSyncConsistencyTest
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[78,13] cannot find symbol
  symbol:   method assertTrue(boolean,java.lang.String)
  location: class org.apache.zookeeper.server.quorum.DIFFSyncConsistencyTest
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[108,13] cannot find symbol
  symbol:   method fail(java.lang.String)
  location: class org.apache.zookeeper.server.quorum.DIFFSyncConsistencyTest
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[113,9] cannot find symbol
  symbol:   method assertTrue(boolean)
  location: class org.apache.zookeeper.server.quorum.DIFFSyncConsistencyTest
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[116,9] cannot find symbol
  symbol:   method assertNotNull(org.apache.zookeeper.server.quorum.Leader.Proposal,java.lang.String)
  location: class org.apache.zookeeper.server.quorum.DIFFSyncConsistencyTest
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[123,17] cannot find symbol
  symbol:   method fail(java.lang.String)
  location: class org.apache.zookeeper.server.quorum.DIFFSyncConsistencyTest
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[144,21] cannot find symbol
  symbol:   method fail(java.lang.String)
  location: class org.apache.zookeeper.server.quorum.DIFFSyncConsistencyTest
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[164,17] cannot find symbol
  symbol:   method assertNotNull(org.apache.zookeeper.data.Stat,java.lang.String)
  location: class org.apache.zookeeper.server.quorum.DIFFSyncConsistencyTest
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[190,21] cannot find symbol
  symbol:   method fail(java.lang.String)
  location: class org.apache.zookeeper.server.quorum.DIFFSyncConsistencyTest
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[200,9] cannot find symbol
  symbol:   method assertNotEquals(int,int,java.lang.String)
  location: class org.apache.zookeeper.server.quorum.DIFFSyncConsistencyTest
[ERROR] /Users/myname/Projects/zookeeper/zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[213,13] cannot find symbol
  symbol:   method assertNotNull(org.apache.zookeeper.data.Stat,java.lang.String)
  location: class org.apache.zookeeper.server.quorum.DIFFSyncConsistencyTest
[INFO] 24 errors
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Apache ZooKeeper 3.6.3-SNAPSHOT:
[INFO]
[INFO] Apache ZooKeeper ................................... SUCCESS [  2.449 s]
[INFO] Apache ZooKeeper - Documentation ................... SUCCESS [  1.110 s]
[INFO] Apache ZooKeeper - Jute ............................ SUCCESS [  5.511 s]
[INFO] Apache ZooKeeper - Server .......................... FAILURE [  4.667 s]
[INFO] Apache ZooKeeper - Metrics Providers ............... SKIPPED
[INFO] Apache ZooKeeper - Prometheus.io Metrics Provider .. SKIPPED
[INFO] Apache ZooKeeper - Client .......................... SKIPPED
[INFO] Apache ZooKeeper - Recipes ......................... SKIPPED
[INFO] Apache ZooKeeper - Recipes - Election .............. SKIPPED
[INFO] Apache ZooKeeper - Recipes - Lock .................. SKIPPED
[INFO] Apache ZooKeeper - Recipes - Queue ................. SKIPPED
[INFO] Apache ZooKeeper - Assembly ........................ SKIPPED
[INFO] Apache ZooKeeper - Compatibility Tests ............. SKIPPED
[INFO] Apache ZooKeeper - Compatibility Tests - Curator ... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  13.916 s
[INFO] Finished at: 2020-11-20T14:06:50-08:00
[INFO] ------------------------------------------------------------------------
{code}
h2. Solution

As discussed in [https://github.com/apache/zookeeper/pull/1445,] since cherry-picking jUnit 5 upgrades and there won't be too many back ports to 3.6 afterwards, we can just fix the issue by using jUnit 4 in the test.","[<JIRA Version: name='3.6.3', id='12348703'>]",Bug,ZOOKEEPER-4011,Major,Huizhi Lu,Fixed,2020-11-22T03:48:50.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Maven build fails on branch-3.6 because of jUnit 5 usage in DIFFSyncConsistencyTest,2020-11-23T08:25:36.000+0000,[],1.0
Damien Diederen,"[<JIRA Component: name='build', id='12312383'>]",2020-11-20T01:15:44.000+0000,Li Wang,"Got compilation error of ""package org.junit.jupiter.api does not exist"" after checking out the code from Branch 3.6. 

The error started to happen after the following PR: [https://github.com/apache/zookeeper/pull/1445]

 

Error:

zookeeper-server/src/test/java/org/apache/zookeeper/server/quorum/DIFFSyncConsistencyTest.java:[22,36] package org.junit.jupiter.api does not exist

 

Steps to produce
 # pull latest code from branch-3.6
 # run ""mvn test""

 ",[],Bug,ZOOKEEPER-4008,Major,Li Wang,Duplicate,2020-11-23T08:25:36.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"""package org.junit.jupiter.api does not exist"" compilation error in Branch 3.6",2020-11-23T08:25:36.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2020-11-19T09:51:43.000+0000,Ling Mao,"Here direcory is a typo
{code:java}
if (file.isDirectory()) {
    return ""'"" + file.getAbsolutePath() + ""' is a direcory. it must be a file."";
}
{code}","[<JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-4007,Trivial,Ling Mao,Fixed,2021-02-08T12:11:13.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,A typo in the ZKUtil#validateFileInput method,2021-05-10T11:27:29.000+0000,[],1.0
,"[<JIRA Component: name='server', id='12312382'>]",2020-11-18T09:18:50.000+0000,pengWei Dou,"The  version 3.5.1 of ZK does not detect whether the snapshot file exists or not. However, the snapshot file detection is added in the subsequent version. When I tested the upgrade from 3.5.1 to 3.5.6, I found that after the completion of ZK election, One of the ZK node has an error in the following phase due to some network reasons, and then it will receive a diff reply. At this time,  both snapshotNeeded and isPreZAB1_0 （   Learner#syncWithLeader(long newLeaderZxid)  ）are assigned to false.  So, none of snapshot file will be generated but a log file, which will cause the startup failure after the upgrade. 

 

 

 ",[],Bug,ZOOKEEPER-4005,Major,pengWei Dou,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Zookeeper will not sync snapshot while get DIFF and cause start failed.,2021-02-22T01:25:29.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.6', id='12345243'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2020-11-13T08:52:31.000+0000,xiaotong.wang,"*error log* 

WARN [New I/O worker #16:NettyServerCnxn@400] - Closing connection to /x.x.x.x:43766
java.io.IOException: ZK down
 at org.apache.zookeeper.server.NettyServerCnxn.receiveMessage(NettyServerCnxn.java:337)
 at org.apache.zookeeper.server.NettyServerCnxnFactory$CnxnChannelHandler.processMessage(NettyServerCnxnFactory.java:243)
 at org.apache.zookeeper.server.NettyServerCnxnFactory$CnxnChannelHandler.messageReceived(NettyServerCnxnFactory.java:165)
 at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
 at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
 at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
 at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
 at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
 at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
 at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
 at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
 at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
 at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
 at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
 at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 at java.lang.Thread.run(Thread.java:748)

 

and 

 

 


[myid:2] - WARN [New I/O worker #15:NettyServerCnxnFactory$CnxnChannelHandler@141] - Exception caught [id: 0x9ba504cb, /x.x.x.x:39780 :> /x.x.x.x:2181] EXCEPTION: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
 at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:270)
 at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:461)
 at org.jboss.netty.channel.socket.nio.SocketSendBufferPool$UnpooledSendBuffer.transferTo(SocketSendBufferPool.java:203)
 at org.jboss.netty.channel.socket.nio.AbstractNioWorker.write0(AbstractNioWorker.java:201)
 at org.jboss.netty.channel.socket.nio.AbstractNioWorker.writeFromTaskLoop(AbstractNioWorker.java:151)
 at org.jboss.netty.channel.socket.nio.AbstractNioChannel$WriteTask.run(AbstractNioChannel.java:292)
 at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:391)
 at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:315)
 at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
 at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
 at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
 at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 at java.lang.Thread.run(Thread.java:748)

 

 netstat -an|grep 2181|grep CLOSE_WAIT|wc -l
*28441*

 

sample：

!image-2020-11-13-16-51-11-960.png!

 ",[],Bug,ZOOKEEPER-4003,Critical,xiaotong.wang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper server breakdown Frequently,2021-02-04T02:38:34.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",4.0
,[],2020-11-13T06:09:20.000+0000,zengchao,"When other place call ZKAuditProvider.log(...) ,it will  throw a NullPointerException  if 'AUDIT_ENABLE' is false. if 'AUDIT_ENABLE' is false, the static block have not instance 'auditLogger',so when we call log(...),we need to check if 'AUDIT_ENABLE' is true

 
{code:java}
static {
    auditEnabled = Boolean.getBoolean(AUDIT_ENABLE);
    if (auditEnabled) {
        //initialise only when audit logging is enabled
        auditLogger = getAuditLogger();
        LOG.info(""ZooKeeper audit is enabled."");
    } else {
        LOG.info(""ZooKeeper audit is disabled."");
    }
}
public static void log(String user, String operation, String znode, String acl,
                       String createMode, String session, String ip, Result result) {
    auditLogger.logAuditEvent(createLogEvent(user, operation, znode, acl, createMode, session, ip, result));
}
{code}
Change to:
{code:java}
public static void log(String user, String operation, String znode, String acl,
                       String createMode, String session, String ip, Result result) {
    if (isAuditEnabled()) {
        auditLogger.logAuditEvent(createLogEvent(user, operation, znode, acl, createMode, session, ip, result));
    }
}
{code}
 ",[],Bug,ZOOKEEPER-4002,Major,zengchao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZKAuditProvider throw NullPointerException if 'AUDIT_ENABLE' is false,2020-11-18T06:02:23.000+0000,[],1.0
,[],2020-11-10T12:01:17.000+0000,priya Vijay,"seeing an intermittent issue where the quorum is not reached, pods either 2nd or 3rd enters crashloopbackoff state .

On inspecting the pod which keeps restarting, file with name as id, example ""2"" or """"3"" is not created. 

attaching the logs from all 3 nodes of ZK",[],Bug,ZOOKEEPER-3998,Major,priya Vijay,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,quorum not established - java.net.SocketException: Connection reset seen in leader log,2020-11-10T12:01:17.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>]",1.0
,[],2020-11-07T23:45:34.000+0000,Vikramark,"There is a behavior difference between zookeeper 3.4.14 and 3.5.8.  In 3.5.8 given a cluster of 3 nodes, when the leader   node is shutdown, the follower does not seem to be in sync with the new leader. The clients are not able to establish session and requests gets queued up as outstanding requests.  We don't see this issue in case of 3.4.14 version.

Below are the details of how to recreate the issue:

 

*With version 3.5.8:*

 

Initial fresh setup of 3 node cluster:
  
|Zoo1|Zoo2|Zoo3|
|Zookeeper version: 3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:07 GMT
 Latency min/avg/max: 0/0/0
 Received: 3
 Sent: 2
 Connections: 1
 *Outstanding: 0*
 *Zxid: 0x0*
 Mode: follower
 Node count: 5|Zookeeper version: 3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:07 GMT
 Latency min/avg/max: 0/0/0
 Received: 3
 Sent: 2
 Connections: 1
 Outstanding: 0
 *Zxid: 0x100000000*
 *Mode: leader*
 Node count: 5
 Proposal sizes last/min/max: -1/-1/-1|Zookeeper version: 3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:07 GMT
 Latency min/avg/max: 0/0/0
 Received: 2
 Sent: 1
 Connections: 1
 *Outstanding: 0*
 *Zxid: 0x100000000*
 *Mode: follower*
 Node count: 5|

 

After starting one session using zkCli.sh on Zoo1 node:
|Zoo1|Zoo2|Zoo3|
|Zookeeper version: 3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:07 GMT
 Latency min/avg/max: 1/9/23
 Received: 7
 Sent: 6
 Connections: 2
 Outstanding: 0
 *Zxid: 0x100000001*
 *Mode: follower*
 Node count: 5|Zookeeper version: 3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:07 GMT
 Latency min/avg/max: 0/0/0
 Received: 4
 Sent: 3
 Connections: 1
 Outstanding: 0
 *Zxid: 0x100000001*
 *Mode: leader*
 Node count: 5
 Proposal sizes last/min/max: 36/36/36|Zookeeper version: 3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:07 GMT
 Latency min/avg/max: 0/0/0
 Received: 3
 Sent: 2
 Connections: 1
 Outstanding: 0
 *Zxid: 0x100000001*
 *Mode: follower*
 Node count: 5|

 

+Note: We can see that Zxid is now consistent across all nodes.+

 

After shutting down leader node zoo2. We can see ZOO3 became the Leader. For some reason the ZXID is not same between zoo1 and zoo3.

 

Start a new zkCli.sh session on same node (zoo1).  *The session was not established, the cli client just keeps retrying and created many outstanding requests on zoo1.* 

*Expected behavior:* The zoo1 should have same zxid as zoo3. The client session should be allowed to be created. No outstanding requests should be added. 

 
|Zoo1|Zoo2|Zoo3|
|Zookeeper version: 3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:07 GMT
 Latency min/avg/max: 0/0/2
 Received: 50
 Sent: 43
 Connections: 2
 *{color:#de350b}Outstanding: 6{color}*
 *{color:#de350b}Zxid: 0x100000001{color}*
 Mode: follower
 Node count: 5|down|Zookeeper version: 3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:07 GMT
 Latency min/avg/max: 0/0/0
 Received: 1
 Sent: 0
 Connections: 1
 Outstanding: 0
 *Zxid: 0x200000000*
 *Mode: leader*
 Node count: 5
 Proposal sizes last/min/max: -1/-1/-1|

 

*With version 3.4.14*

First initial setup:

 
|Zoo1|Zoo2|Zoo3|
|Zookeeper version: 3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT
 Latency min/avg/max: 0/0/0
 Received: 1
 Sent: 0
 Connections: 1
 Outstanding: 0
 Zxid: 0x0
 Mode: follower
 Node count: 4|Zookeeper version: 3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT
 Latency min/avg/max: 0/0/0
 Received: 1
 Sent: 0
 Connections: 1
 Outstanding: 0
 Zxid: 0x100000000
 Mode: leader
 Node count: 4
 Proposal sizes last/min/max: -1/-1/-1|Zookeeper version: 3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT
 Latency min/avg/max: 0/0/0
 Received: 1
 Sent: 0
 Connections: 1
 Outstanding: 0
 Zxid: 0x100000000
 Mode: follower
 Node count: 4|

 

After connecting with zkCli on ZOO1.

 

 
|Zoo1|Zoo2|Zoo3|
|Zookeeper version: 3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT
 Latency min/avg/max: 0/14/33
 Received: 5
 Sent: 4
 Connections: 2
 Outstanding: 0
 Zxid: 0x100000001
 Mode: follower
 Node count: 4|Zookeeper version: 3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT
 Latency min/avg/max: 0/0/0
 Received: 2
 Sent: 1
 Connections: 1
 Outstanding: 0
 Zxid: 0x100000001
 Mode: leader
 Node count: 4
 Proposal sizes last/min/max: 36/36/36|Zookeeper version: 3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT
 Latency min/avg/max: 0/0/0
 Received: 2
 Sent: 1
 Connections: 1
 Outstanding: 0
 Zxid: 0x100000001
 Mode: follower
 Node count: 4|

 

+Note: The zkid is now same for all the nodes.+

 

 

After shutting down leader node zoo2. We can see ZOO3 became the Leader. For some reason the ZXID is not same between zoo1 and zoo3 initially. ZOO3 has new zkid as a new epoch was created but zoo1 still had old zkid.

 

Now closed the existing zxcli and started a new zkCli.sh session on same node (zoo1).  *This time session was established!.*

 
|Zoo1|Zoo2|Zoo3|
|Zookeeper version: 3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT
 Latency min/avg/max: 0/1/4
 Received: 8
 Sent: 7
 Connections: 2
 *{color:#00875a}Outstanding: 0{color}*
 *{color:#00875a}Zxid: 0x200000001{color}*
 Mode: follower
 Node count: 4|down
  |Zookeeper version: 3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT
 Latency min/avg/max: 0/0/0
 Received: 3
 Sent: 2
 Connections: 1
 *Outstanding: 0*
 *Zxid: 0x200000001*
 Mode: leader
 Node count: 4
 Proposal sizes last/min/max: 36/36/36|

 

 

 

 
  ","[<JIRA Version: name='3.5.8', id='12346950'>]",Bug,ZOOKEEPER-3997,Major,Vikramark,Not A Problem,2020-11-08T02:07:52.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Why Zookeeper 3.5.8 leader shutdown makes the follower not allow new sessions?,2020-11-08T02:07:52.000+0000,[],1.0
,"[<JIRA Component: name='tests', id='12312427'>]",2020-11-07T12:28:25.000+0000,Ling Mao,"We noticed that the unit case: ReadOnlyModeTest.testConnectionEvents has failed frequently when building the CI.

The link is: https://ci-hadoop.apache.org/blue/organizations/jenkins/zookeeper-precommit-github-pr/detail/PR-1527/1/pipeline/
{code:java}
[2020-11-06T13:21:34.527Z] [INFO] Running org.apache.zookeeper.RemoveWatchesTest
[2020-11-06T13:21:36.136Z] [INFO] Tests run: 352, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.475 s - in org.apache.zookeeper.common.X509UtilTest
[2020-11-06T13:22:06.176Z] [INFO] Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 414.867 s - in org.apache.zookeeper.server.quorum.QuorumSSLTest
[2020-11-06T13:22:41.949Z] [INFO] Tests run: 46, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 66.898 s - in org.apache.zookeeper.RemoveWatchesTest
[2020-11-06T13:22:41.949Z] [INFO] 
[2020-11-06T13:22:41.949Z] [INFO] Results:
[2020-11-06T13:22:41.949Z] [INFO] 
[2020-11-06T13:22:41.949Z] [ERROR] Errors: 
[2020-11-06T13:22:41.949Z] [ERROR]   ReadOnlyModeTest.testConnectionEvents:205 » Timeout Failed to connect in read-...
[2020-11-06T13:22:41.949Z] [INFO] 
[2020-11-06T13:22:41.949Z] [ERROR] Tests run: 2863, Failures: 0, Errors: 1, Skipped: 4
[2020-11-06T13:22:41.949Z] [INFO] 
[2020-11-06T13:22:43.552Z] [INFO] ------------------------------------------------------------------------
[2020-11-06T13:22:43.552Z] [INFO] Reactor Summary for Apache ZooKeeper 3.7.0-SNAPSHOT:{code}",[],Bug,ZOOKEEPER-3996,Minor,Ling Mao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Flaky test: ReadOnlyModeTest.testConnectionEvents,2020-11-10T02:48:39.000+0000,[],1.0
,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='other', id='12333125'>]",2020-11-06T11:53:02.000+0000,Sri Darshan Sreedharan,"On running the Aquasec Trivy Vulnerability scanner on the latest version on zookeeper docker (3.6.2) 124 vulnerabilities were identified with the following severities LOW: 88, MEDIUM: 13, HIGH: 23

These issues were across various libraries packaged along with zookeeper in the docker image.

 

Attaching the list of vulnerabilities.

 

Expected: The libraries packaged need to be up to date with no known vulnerabilities.

 ",[],Bug,ZOOKEEPER-3995,Minor,Sri Darshan Sreedharan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Vulnerabilities in components packaged in zookeeper docker,2022-03-03T18:48:15.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",3.0
huangwenbo,"[<JIRA Component: name='server', id='12312382'>]",2020-11-06T07:19:03.000+0000,huangwenbo,"when ZookeeperServer process connect request, the reason for disconnecting was wrong
 
 
 ","[<JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3994,Major,huangwenbo,Fixed,2020-11-16T15:39:17.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,disconnect reason wrong,2021-03-28T08:54:58.000+0000,[],1.0
Damien Diederen,"[<JIRA Component: name='java client', id='12312381'>]",2020-11-03T13:23:15.000+0000,Ling Mao,"{code:java}
public void addWatch(String basePath, Watcher watcher, AddWatchMode mode)
        throws KeeperException, InterruptedException {
    PathUtils.validatePath(basePath);
    String serverPath = prependChroot(basePath);

    RequestHeader h = new RequestHeader();
    h.setType(ZooDefs.OpCode.addWatch);
    AddWatchRequest request = new AddWatchRequest(serverPath, mode.getMode());
    ReplyHeader r = cnxn.submitRequest(h, request, new ErrorResponse(),
    
{code}
we need to _*validateWatcher(watcher)*_ to ** avoid the case:
{code:java}
zk.addWatch(""/a/b"", null, PERSISTENT_RECURSIVE);
{code}","[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-3992,Minor,Ling Mao,Fixed,2020-11-09T11:21:59.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,addWatch api should check the null watch,2021-04-04T05:24:05.000+0000,[],2.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2020-11-03T12:46:50.000+0000,Lander Visterin,"We run Zookeeper in a container environment where DNS is not stable. As recommended by the documentation, we set _electionPortBindRetry_ to 0 (keeps retrying forever).

On some instances, we get the following exception in an infinite loop, even though the address already became resolve-able:

 
{noformat}
zk-2_1  | 2020-11-03 10:57:08,407 [myid:3] - ERROR [ListenerHandler-zk-2.test:3888:QuorumCnxManager$Listener$ListenerHandler@1093] - Exception while listening
zk-2_1  | java.net.SocketException: Unresolved address
zk-2_1  | 	at java.base/java.net.ServerSocket.bind(Unknown Source)
zk-2_1  | 	at java.base/java.net.ServerSocket.bind(Unknown Source)
zk-2_1  | 	at org.apache.zookeeper.server.quorum.QuorumCnxManager$Listener$ListenerHandler.createNewServerSocket(QuorumCnxManager.java:1140)
zk-2_1  | 	at org.apache.zookeeper.server.quorum.QuorumCnxManager$Listener$ListenerHandler.acceptConnections(QuorumCnxManager.java:1064)
zk-2_1  | 	at org.apache.zookeeper.server.quorum.QuorumCnxManager$Listener$ListenerHandler.run(QuorumCnxManager.java:1033)
zk-2_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
zk-2_1  | 	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
zk-2_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
zk-2_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
zk-2_1  | 	at java.base/java.lang.Thread.run(Unknown Source){noformat}
Zookeeper does not actually retry the DNS resolution, it just keeps using the old failed result.

 

This happens because the InetSocketAddress is created once and the DNS lookup happens when it is created.

This issue has come up previously in https://issues.apache.org/jira/browse/ZOOKEEPER-1506 but it appears to still happen here.

I have attached a repro.tar.gz to help reproduce this issue. Steps:
 * Untar repro.tar.gz
 * docker-compose up
 * See the exception keeps happening for zk-2, not for the others
 * Open db.test and uncomment the zk-2 line, increment the serial and save
 * Wait a few seconds for the DNS to refresh
 * Verify that you can resolve zk-2.test now (dig @172.16.60.2 zk-2.test) but the error keeps appearing

I have also attached a patch that resolves this. The patch will retry DNS resolution if the address is still unresolved every time it tries to create the server socket.

 ","[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3991,Minor,Lander Visterin,Fixed,2020-11-16T07:53:07.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,QuorumCnxManager Listener port bind retry does not retry DNS lookup,2021-03-28T08:54:55.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",3.0
Damien Diederen,[],2020-11-03T10:41:03.000+0000,SAICHARAN REDDY KOTLA,"Hello everyone,

I work for a product which uses apache/zookeeper 3.6.1.  We scanned our product with a security scanner which reported CVE-2019-17571. 

After analysis we found that this vulnerability is coming from zookeeper 3.6.1 because of direct dependency on log4j 1.2.17. 

Statement regarding 1.x version of log4j from [official |http://logging.apache.org/log4j/1.2/] site:
{quote}A security vulnerability, CVE-2019-17571 has been identified against Log4j 1. Log4j includes a SocketServer that accepts serialized log events and deserializes them without verifying whether the objects are allowed or not. This can provide an attack vector that can be expoited. Since Log4j 1 is no longer maintained this issue will not be fixed. Users are urged to upgrade to Log4j 2.x
{quote}
Could you please share your rationale on not upgrading log4j to 2.x",[],Bug,ZOOKEEPER-3990,Major,SAICHARAN REDDY KOTLA,Duplicate,2020-11-03T19:29:54.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Log4j 1.2.17 used by zookeeper 3.6.1 is vulnerable to CVE-2019-17571,2020-11-03T19:29:54.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.6.2', id='12347809'>]",2.0
Enrico Olivelli,"[<JIRA Component: name='server', id='12312382'>]",2020-11-02T08:48:24.000+0000,Pratik Thacker,"While upgrading K8S cluster, container running zookeeper will rollover one by one.
During this rollover, Null Pointer Exception was observed as below.
{code:java}
INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):Follower@292] - shutdown FollowerINFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):Follower@292] - shutdown FollowerINFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):LearnerZooKeeperServer@160] - Shutting downINFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):ZooKeeperServer@784] - shutting downINFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):RequestThrottler@244] - Shutting downINFO  [RequestThrottler:RequestThrottler@205] - Draining request throttler queueINFO  [RequestThrottler:RequestThrottler@181] - RequestThrottler shutdown. Dropped 0 requestsINFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):FollowerRequestProcessor@148] - Shutting downINFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):CommitProcessor@617] - Shutting downINFO  [FollowerRequestProcessor:1:FollowerRequestProcessor@112] - FollowerRequestProcessor exited loop!INFO  [CommitProcessor:1:CommitProcessor@406] - CommitProcessor exited loop!INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):FinalRequestProcessor@662] - shutdown of request processor completeERROR [nioEventLoopGroup-4-22:NettyServerCnxnFactory$CnxnChannelHandler@329] - Unexpected exception in receivejava.lang.NullPointerException: null at org.apache.zookeeper.server.NettyServerCnxn.receiveMessage(NettyServerCnxn.java:515) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.server.NettyServerCnxn.processMessage(NettyServerCnxn.java:365) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.server.NettyServerCnxnFactory$CnxnChannelHandler.channelRead(NettyServerCnxnFactory.java:326) [zookeeper-3.6.1.jar:3.6.1] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [netty-common-4.1.48.Final.jar:4.1.48.Final] at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.48.Final.jar:4.1.48.Final] at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-common-4.1.48.Final.jar:4.1.48.Final] at java.lang.Thread.run(Thread.java:834) [?:?]WARN  [nioEventLoopGroup-4-22:NettyServerCnxnFactory$CnxnChannelHandler@273] - Exception caughtjava.lang.NullPointerException: null at org.apache.zookeeper.server.NettyServerCnxn.receiveMessage(NettyServerCnxn.java:515) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.server.NettyServerCnxn.processMessage(NettyServerCnxn.java:365) ~[zookeeper-3.6.1.jar:3.6.1] at org.apache.zookeeper.server.NettyServerCnxnFactory$CnxnChannelHandler.channelRead(NettyServerCnxnFactory.java:326) ~[zookeeper-3.6.1.jar:3.6.1] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.48.Final.jar:4.1.48.Final] at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [netty-common-4.1.48.Final.jar:4.1.48.Final] at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.48.Final.jar:4.1.48.Final] at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-common-4.1.48.Final.jar:4.1.48.Final] at java.lang.Thread.run(Thread.java:834) [?:?] {code}

Expectation: This scenario should be handled and application should not throw such Null Pointer exception.
","[<JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>, <JIRA Version: name='3.6.4', id='12350076'>]",Bug,ZOOKEEPER-3988,Major,Pratik Thacker,Fixed,2022-01-25T13:12:44.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,org.apache.zookeeper.server.NettyServerCnxn.receiveMessage throws NullPointerException,2022-01-26T08:47:19.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>]",6.0
Christopher Tubbs,"[<JIRA Component: name='tests', id='12312427'>]",2020-11-01T19:00:48.000+0000,Christopher Tubbs,"It seems that many of the surefire/JUnit tests reuse the same bind address. This causes many tests to fail due to a bind address being already in use.

A recent run (using {{-Dsurefire-forkcount=2}} shows several tests that failed for this reason. These same tests do not fail when forkcount is 1 (and they don't always fail when the forkcount is higher... just sometimes, depending on timing):
{code}
[ERROR] Errors: 
[ERROR]   SaslAuthTest.testZKOperationsAfterClientSaslAuthFailure:172 » Timeout Failed t...
[ERROR]   SnapshotDigestTest.testSnapshotDigest:86->reloadSnapshotAndCheckDigest:190 » Runtime
[ERROR]   CloseSessionTxnTest.testCloseSessionTxnCompatile:44->testCloseSessionWithDifferentConfig:87->QuorumPeerTestBase.waitForOne:536 » Runtime
[ERROR]   QuorumPeerMainTest.testFailedTxnAsPartOfQuorumLoss:784->QuorumPeerTestBase.waitForOne:536 » Runtime
[ERROR]   ChrootAsyncTest.setUp:38->AsyncOpsTest.setUp:47->ClientBase.setUp:478->ClientBase.setUpWithServerId:503->ClientBase.startServer:519->ClientBase.createNewServerInstance:429 » Bind
[ERROR]   GetChildren2Test.setUp:42->ClientBase.setUp:478->ClientBase.setUpWithServerId:503->ClientBase.startServer:519->ClientBase.createNewServerInstance:429 » Bind
[ERROR]   SaslKerberosAuthOverSSLTest.testAuth » Bind Address already in use
[ERROR]   UnsupportedAddWatcherTest.setUp:105->ClientBase.setUp:478->ClientBase.setUpWithServerId:503->ClientBase.startServer:519->ClientBase.createNewServerInstance:429 » Bind
[INFO] 
[ERROR] Tests run: 2846, Failures: 2, Errors: 8, Skipped: 4
{code}","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-3987,Major,Christopher Tubbs,Fixed,2021-02-17T17:15:59.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Build failures when running surefire tests concurrently due to bind address already in use,2021-03-28T08:54:22.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2020-10-26T20:28:22.000+0000,Jonathan Halterman,"We have some Zookeeper clusters under pretty heavy load that are occasionally throwing ConcurrentModificationException during ACL checking:
{noformat}
 
ERROR [CommitProcWorkThread-17:FinalRequestProcessor@439] - Failed to process sessionid:0xa006cd06b181510 type:getChildren2 cxid:0x4dae zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
java.util.ConcurrentModificationException
 at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909)
 at java.util.ArrayList$Itr.next(ArrayList.java:859)
 at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1044)
 at org.apache.zookeeper.server.PrepRequestProcessor.checkACL(PrepRequestProcessor.java:321)
 at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:383)
 at org.apache.zookeeper.server.quorum.CommitProcessor$CommitWorkRequest.doWork(CommitProcessor.java:297)
 at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:162)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
{noformat}
This appears to be caused by auths changing while we're processing a request, such as here:

[https://github.com/apache/zookeeper/blob/branch-3.5/zookeeper-server/src/main/java/org/apache/zookeeper/server/ServerCnxn.java#L101-L110]

The result of this appears to be that occasionally requests fail unexpectedly.

 ",[],Bug,ZOOKEEPER-3986,Major,Jonathan Halterman,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ConcurrentModificationException in PrepRequestProcessor.checkACL when iterating and add authInfo List concurrently,2020-11-22T08:38:09.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",3.0
Christopher Tubbs,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='tests', id='12312427'>]",2020-10-22T04:03:50.000+0000,Christopher Tubbs,"While testing a full build on the master branch (3.7.0-SNAPSHOT), I observed that the C client tests hang indefinitely when reaching the zktest-mt's Zookeeper_close test suite (nothing is printed from that suite, but Zookeeper_init finishes and then nothing is printed)

Attaching gdb as a debugger to the zktest-mt process, with the help of [~ztzg] and [~symat] I saw that it was stuck in the sss because of the test mocking sockets, and sss requiring a non-mocked socket to call getpwuid_r and getlogin functions in zookeeper-client/zookeeper-client-c/src/zookeeper.c

Disabling the lines in zookeeper.c that called those functions, which seemed to be used for logging information only, the tests were able to proceed and complete in under 9 minutes (full-build, without running any surefire/junit tests).","[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3983,Major,Christopher Tubbs,Fixed,2020-10-22T07:53:58.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,C client test suite hangs forever 'sss' is configured in /etc/nsswitch.conf,2021-03-28T08:54:47.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",1.0
Damien Diederen,"[<JIRA Component: name='server', id='12312382'>]",2020-10-19T19:40:55.000+0000,Damien Diederen,"As discussed in [this PR|https://github.com/apache/zookeeper/pull/1503#issuecomment-710549123], it is possible for a client (authenticated or not) to ""SPAM"" and corrupt the audit log.

As far as I can tell, the following works on any server, because the {{digest}} provider is always active and accepts (almost) arbitrary strings:

{noformat}
addauth digest veryimportant@EXAMPLE.COM:whatever
create /dangerousnode
{noformat}

Note that ""whatever"" in the example above doesn't have to be a known or valid password. Similarly, the string to the left of {{:}} is not validated in any way; it is just copied as-is into the {{Id}} associated with the connection.

This results in entries akin to the following in the audit log:

{noformat}
2020-10-15 09:40:43,173 INFO audit.Log4jAuditLogger: session=0x100eefe34a40000	user=zkcli@CROSSTWINE.COM,veryimportant@EXAMPLE.COM,0:0:0:0:0:0:0:1	ip=0:0:0:0:0:0:0:1	operation=create	znode=/dangerousnode	znode_type=persistent	result=success
{noformat}

Note how the scheme is not mentioned; all that is visible is the ""user name"" part of the {{Id}}.

This could lead an hypothetical audit application to conclude that it was okay for that connection to create {{/dangerousnode}} because it was ""seriously"" authenticated as {{veryimportant@EXAMPLE.COM}}.

It is possible to use that loophole to corrupt the audit log in various ways, including creating fake entries.  It is not even necessary to use a dedicated client; {{Ctrl+Q Ctrl+J}} can cause literal newlines to be inserted via {{zkCli.sh}}:

{noformat}
addauth digest ""fakeid^JTHIS REALLY SHOULDN'T BE THERE:whatever""
{noformat}

The result is a ""two-line entry"" in the audit log:

{noformat}
2020-10-16 21:42:06,546 INFO audit.Log4jAuditLogger: session=0x100f6b85af80001 user=""fakeid
THIS REALLY SHOULDN'T BE THERE,zkcli@CROSSTWINE.COM,0:0:0:0:0:0:0:1 ip=0:0:0:0:0:0:0:1	operation=create	znode=/yolo4	znode_type=persistent	result=success
{noformat}

I would suggest:

# Adding a setting which allows disabling the {{digest}} provider on production servers;
# Filtering (or quoting/escaping/censoring) the user names/principals which are written to the audit log (by scheme, or perhaps by dangerous characters).

","[<JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3979,Major,Damien Diederen,Fixed,2020-11-16T08:05:10.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Clients can corrupt the audit log,2021-03-28T08:54:44.000+0000,[],2.0
,"[<JIRA Component: name='jute', id='12312385'>]",2020-10-16T14:53:54.000+0000,Diego Lucas Jiménez,"After running for a while, the entire cluster (3 zookeeper) crash suddenly, all of them logging:

 
{code:java}
2020-10-16 10:37:00,459 [myid:2] - WARN [NIOWorkerThread-4:NIOServerCnxn@373] - Close of session 0x0 java.io.IOException: ZooKeeperServer not running at org.apache.zookeeper.server.NIOServerCnxn.readLength(NIOServerCnxn.java:544) at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:332) at org.apache.zookeeper.server.NIOServerCnxnFactory$IOWorkRequest.doWork(NIOServerCnxnFactory.java:522) at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:154) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at java.base/java.lang.Thread.run(Thread.java:834)
2020-10-16 10:37:00,475 [myid:2] - ERROR [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@1139] - Unable to load database on disk
java.io.IOException: Unreasonable length = 5089607
        at org.apache.jute.BinaryInputArchive.checkLength(BinaryInputArchive.java:166)
        at org.apache.jute.BinaryInputArchive.readBuffer(BinaryInputArchive.java:127)
        at org.apache.zookeeper.server.persistence.Util.readTxnBytes(Util.java:159)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:768)
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.fastForwardFromEdits(FileTxnSnapLog.java:352)
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.lambda$restore$0(FileTxnSnapLog.java:258)
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:303)
        at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:285)
        at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:1093)
        at org.apache.zookeeper.server.quorum.QuorumPeer.getLastLoggedZxid(QuorumPeer.java:1249)
        at org.apache.zookeeper.server.quorum.FastLeaderElection.getInitLastLoggedZxid(FastLeaderElection.java:868)
        at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:941)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1428){code}
Apparently the ""corrupted"" file appears in all the servers, so no solution such as ""removing version-2 on the faulty server and letting replicate from a healthy one"" :(.

The entire cluster goes down, we have downtime, every-single-day since we upgraded from 3.4.9. 

 ",[],Bug,ZOOKEEPER-3975,Critical,Diego Lucas Jiménez,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper crashes: Unable to load database on disk java.io.IOException: Unreasonable length,2021-10-15T06:06:20.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",3.0
,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='c client', id='12312380'>]",2020-10-16T14:03:40.000+0000,Christopher Tubbs,"Currently, the zookeeper-client/zookeeper-client-c generates a bunch of automake files directly in the source tree.

Placing intermediate build files directly in the source tree can cause problems, such as accidentally checking them in to the source tree using the project's SCM (git). These files are currently in .gitignore, so that issue is somewhat mitigated, but they can still be added accidentally, even if they are ignored.

The bigger problem is that Maven plugins assume that most everything that is not in a target/ directory is a source file. This can cause the apache-rat-plugin's check to fail except on a clean checkout, and can cause the maven-assembly-plugin to include those files in assemblies it creates, among other problems.

These files should be generated in the target/ directory during the build, and removed from .gitignore. If they are needed in an assembly, they should be explicitly added with the appropriate assembly descriptor during the build.

I would work on this, but I'm not adequately familiar with the C parts of the build. If somebody works on this, I'd be happy to help review or consult, as needed to fix this.",[],Bug,ZOOKEEPER-3974,Minor,Christopher Tubbs,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Generated build files from zookeeper-client-c should be in target/ directory,2020-10-21T07:52:28.000+0000,[],2.0
,"[<JIRA Component: name='server', id='12312382'>]",2020-10-15T15:24:16.000+0000,anaud,"It is possible that a leader may have incomplete commitlog because it resync'ed with the old leader via SNAPSHOT replication.

Then, a follower may try to resync with the leader, but because there may be some transactions the follower missed earlier and the leader does not have in its commitlog.

They decided to use txnlog + commitlog to resync. However, this will lead to convergence failure because the leader does not send the missing transactions that are not in its commitlog.

Here is the abstract step to reproduce the bug, and I attached the patch with the test case that can reproduce the bug.

Initially, node A,B,C are all sync'ed.
 1. Node A crashes; setData 0x11 on B and C
 2. Node B and C crash
 3. Node A and B restart
 4. Node A crashes; setData 0x21 on B
 5. Node B crashes
 6. Node B and C restart
 7. Node C crashes; setData 0x32 on B
 8. Node A and C restart
 9. Node B restarts

At step 6, C is a follower getting a snapshot from B, and C does not have the transaction 0x21 in its commitlog (only in the snapshot).

At step 8, C is the leader which does not have 0x21 in its commitlog, which A never gets.

In the end, 0x21 only exists on B and C, but not on A.

I think the solution would be made to LearnerHandler's  syncFollower method as follows:
 1. Check the last transaction it has in its txnlog + commitlog
 2. If it is more recent than what it has in its txnlog + commitlog, then it should use Snapshot
 3. Otherwise, continue with txnlog + commitlog replication

I attached a patch containing the proposed fix.",[],Bug,ZOOKEEPER-3972,Major,anaud,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Convergence fail when a follower tries to resync with a leader having incomplete commitlog,2020-10-19T03:34:00.000+0000,"[<JIRA Version: name='3.5.8', id='12346950'>]",2.0
,[],2020-10-13T07:27:59.000+0000,Norbert Kalmár,"This is an umbrella Jira for the PRs we got during the Bug Bash event. A lot of them are easy, few line fixes without any Jira to track.",[],Bug,ZOOKEEPER-3968,Minor,Norbert Kalmár,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ApacheCon Bug Bash fixes,2020-10-13T07:27:59.000+0000,[],1.0
,[],2020-10-06T08:43:08.000+0000,Joydeep Sen Gupta,"This solves the issue Shellcheck found by muse.dev (as part of the BugBash)

1. Added the shebang
 2. Added $(...) notation instead of legacy backticked
 3. Added double quote to prevent globbing and word splitting","[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3955,Major,Joydeep Sen Gupta,Fixed,2020-10-12T11:00:08.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,added a shebang or a 'shell' directive to lastRevision.sh,2021-03-28T08:55:05.000+0000,[],1.0
Damien Diederen,"[<JIRA Component: name='c client', id='12312380'>]",2020-10-04T23:39:10.000+0000,Michael Hudson-Doyle,"When compiled with {{-O3}} and {{gcc-10}} (which is the default for Ubuntu on ppc64el), compilation fails like this:
{code}
/bin/bash ./libtool -tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I./include -I./tests -I./generated -Wdate-time -D_FORTIFY_SOURCE=2 -Wall -Werror -g -O3 -fdebug-prefix-map=/<<PKGBUILDDIR>>=. -fstack-protector-strong -Wformat -Werror=format-security -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c -o zookeeper.lo `test -f 'src/zookeeper.c' || echo './'`src/zookeeper.c libtool: compile: gcc -DHAVE_CONFIG_H -I. -I./include -I./tests -I./generated -Wdate-time -D_FORTIFY_SOURCE=2 -Wall -Werror -g -O3 -fdebug-prefix-map=/<<PKGBUILDDIR>>=. -fstack-protector-strong -Wformat -Werror=format-security -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c src/zookeeper.c -fPIC -DPIC -o .libs/zookeeper.o
src/zookeeper.c: In function 'free_completions': 
src/zookeeper.c:284:9: error: 'a_list.next' may be used uninitialized in this function [-Werror=maybe-uninitialized] 
284 | tmp = a_list>next; 
    | ~~~^~~~~~~~~~~~~ 
cc1: all warnings being treated as errors
{code}
 What's happening here is that free_auth_completions is being inlined into free_completions, and this lets gcc see that members of a_list are being accessed without initialization. I don't know anything like enough about this code to see if this is a bug in code paths that are actually taken but at a glance it's certainly not obviously impossible: if the two if conditions at the top level of free_completions evaluate false, the function effectively looks like this:
 
{code:c}
void free_completions(zhandle_t *zh,int callCompletion,int reason)
{ 
 auth_completion_list_t a_list; 
 free_auth_completion(&a_list); 
} 
{code}

so it's pretty clear that a_list is backed by uninitialized stack memory. Explicitly initializing the variable with ""{{a_list = \{NULL, NULL, NULL}}}"" makes the warning go away.
 ","[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3954,Minor,Michael Hudson-Doyle,Fixed,2020-10-12T06:41:21.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,use of uninitialized data in zookeeper-client/zookeeper-client-c/src/zookeeper.c:free_auth_completion,2021-03-28T08:54:00.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",2.0
Damien Diederen,"[<JIRA Component: name='c client', id='12312380'>]",2020-10-03T13:40:10.000+0000,Parag,There seems to be a error in zookeeper.c at line 2849 where the field sasl_client is not protected by the conditional compile by having #ifdef HAVE_CYRUS_SASL_H. There is one more place where sasl_client field is not protected. There are many places in this file where the protection exists but not in two places. I was building with SASL disabled and I got these errors during build. ,"[<JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3951,Major,Parag,Fixed,2020-10-12T06:44:31.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Compile Error in Zookeeper.c without SASL,2021-03-28T08:54:01.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2020-09-25T03:16:02.000+0000,anaud,"It is possible that there is a follower, which is the old leader, with a transaction only it saw as the single entry in the log file from the last epoch. The follower tries to sync with the new leader. It gets TRUNC request, but in the truncate method of FileTxnLog.java, the FileTxnIterator will fast-forward to the log file containing the sole transaction. Also, the pos is pointing to the sole transaction, which is actually the target to truncate. Therefore, doing a setLength(pos) does not discard this transaction. The itr.goToNextLog() is not true in this case if the log file was the last one. Consequently, the transaction remains in the followers log and gets applied to the in-memory data tree of the follower. Therefore, convergence fails.",[],Bug,ZOOKEEPER-3947,Major,anaud,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,truncate in FileTxnLog.java is buggy and fails to correctly truncate a file containing a single transaction only the follower saw,2020-09-25T03:20:08.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",1.0
,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2020-09-25T03:09:44.000+0000,anaud,"If a follower which is the old leader that saw a transaction by itself tries to sync with the new leader, it is possible that the follower goes for the fifth cases in the syncFollower method of LearnerHandler.java. That is, it is the follower missed the committedLog. The leader may end up sending snapshot. However, the leader does not send TRUNC. Therefore, the transaction only the follower saw remains and gets applied to the in-memory data tree of the follower even after synchronization is complete. ",[],Bug,ZOOKEEPER-3946,Major,anaud,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Convergence fails when a follower missed the committedLog synching with the leader if it was an old leader and the leader falls back to send snapshot.,2020-09-25T03:13:18.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>]",1.0
,"[<JIRA Component: name='c client', id='12312380'>]",2020-09-24T11:43:30.000+0000,Xiaotian Qi,"I find this memory leak through sanitizer.

[Here|https://github.com/apache/zookeeper/blob/master/zookeeper-client/zookeeper-client-c/src/zookeeper.c#L664-L665] within zookeeper_close() it destroy the client and reset it to NULL. This client is created using calloc()

In zoo_sasl_client_destroy(), the client is not freed, neither within zookeeper_close(). 

I would suggest add a free() before setting it to NULL in zookeeper_close","[<JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3944,Major,Xiaotian Qi,Fixed,2020-09-28T07:27:38.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper c api sasl client memory leak,2021-03-28T08:55:13.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",3.0
,"[<JIRA Component: name='contrib', id='12312700'>]",2020-09-22T17:43:50.000+0000,Brent,"I tried a number of different ways to get Zookeeper Inspector running yesterday, but repeatedly ran into the same issue.  The Java Swing window will launch, but in an incomplete state (see attached images for broken and working views) and the corresponding console shows this stack trace repeatedly:
{code:java}
Exception in thread ""AWT-EventQueue-0"" java.lang.NullPointerExceptionException in thread ""AWT-EventQueue-0"" java.lang.NullPointerException at javax.swing.GrayFilter.createDisabledImage(GrayFilter.java:49) at javax.swing.LookAndFeel.getDisabledIcon(LookAndFeel.java:557) at javax.swing.AbstractButton.getDisabledIcon(AbstractButton.java:653) at com.apple.laf.AquaButtonUI.paintIcon(AquaButtonUI.java:363) at com.apple.laf.AquaButtonUI.paint(AquaButtonUI.java:304) at javax.swing.plaf.ComponentUI.update(ComponentUI.java:161) at javax.swing.JComponent.paintComponent(JComponent.java:780) at javax.swing.JComponent.paint(JComponent.java:1056) at javax.swing.JComponent.paintChildren(JComponent.java:889) at javax.swing.JComponent.paint(JComponent.java:1065) at javax.swing.JComponent.paintToOffscreen(JComponent.java:5210) at javax.swing.RepaintManager$PaintManager.paintDoubleBuffered(RepaintManager.java:1579) at javax.swing.RepaintManager$PaintManager.paint(RepaintManager.java:1502) at javax.swing.RepaintManager.paint(RepaintManager.java:1272) at javax.swing.JComponent._paintImmediately(JComponent.java:5158) at javax.swing.JComponent.paintImmediately(JComponent.java:4969) at javax.swing.RepaintManager$4.run(RepaintManager.java:831) at javax.swing.RepaintManager$4.run(RepaintManager.java:814) at java.security.AccessController.doPrivileged(Native Method) at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:74) at javax.swing.RepaintManager.paintDirtyRegions(RepaintManager.java:814) at javax.swing.RepaintManager.paintDirtyRegions(RepaintManager.java:789) at javax.swing.RepaintManager.prePaintDirtyRegions(RepaintManager.java:738) at javax.swing.RepaintManager.access$1200(RepaintManager.java:64) at javax.swing.RepaintManager$ProcessingRunnable.run(RepaintManager.java:1732) at java.awt.event.InvocationEvent.dispatch(InvocationEvent.java:311) at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:758) at java.awt.EventQueue.access$500(EventQueue.java:97) at java.awt.EventQueue$3.run(EventQueue.java:709) at java.awt.EventQueue$3.run(EventQueue.java:703) at java.security.AccessController.doPrivileged(Native Method) at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:74) at java.awt.EventQueue.dispatchEvent(EventQueue.java:728) at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:205) at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:116) at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:105) at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:101) at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:93) at java.awt.EventDispatchThread.run(EventDispatchThread.java:82){code}
{{I was able to narrow this down to the icons in the *zookeeper/zookeeper-contrib/zookeeper-contrib-zooinspector/src/main/resources/icons* folder not making it into the built JAR for the subproject and being unable to be found dynamically in most scenarios.  I tried running from a number of different directories and messing with my classpath, but it didn't seem consistent and the *zooInspector.sh* script would not work either.}}

{{I found that if I added this to }}{{*zookeeper/zookeeper-contrib/zookeeper-contrib-zooinspector/pom.xml*, the tool will display and run correctly (this forces the resources folder to get packed into the built JAR): }}{{}}
{code:java}
<build>
    <resources>
        <resource>
            <directory>src/main/resources</directory>
            <includes>
                <include>**/*</include>
            </includes>
        </resource>
    </resources>
</build>{code}
As far as I can tell, this issue still exists on the master branch and in release 3.6.2.

Is this something that I could help by contributing a PR for?

I might also suggest leveraging *maven-assembly-plugin* for this particular project to build one single ""fat jar"" that includes all the required dependencies so there's only a single jar necessary to make Zookeeper Inspector run if no one has a major issue with that.

Either way, happy to help, just let me know.","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-3943,Major,Brent,Fixed,2021-01-24T18:19:31.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Zookeeper Inspector throwing NullPointerExceptions and not displaying properly,2022-02-08T08:46:27.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",2.0
,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2020-09-21T14:04:53.000+0000,Stan Henderson,"We have configured a 3 node zookeeper cluster using the 3.6.2 version in a Docker version 1.12.1 containerized environment. This corresponds to Sep 16 20:03:01 in the attached docker-containers.log files.

NOTE: We use the Dockerfile from https://hub.docker.com/_/zookeeper for 3.6 branch

As a part of our testing, we have restarted each of the zookeeper nodes and have seen the following behaviour:

zoo1, zoo2, and zoo3 healthy (zoo1 is leader)

We started our testing at approximately Sep 17 13:01:05 in the attached docker-containers.log files.

1. (simulate patching zoo2)
- restart zoo2
- zk_synced_followers 1
- zoo1 leader
- zoo2 unhealthy (This ZooKeeper instance is not currently serving requests)
- zoo3 healthy
- waited 5 minutes with no change
- restart zoo3
- zoo1 leader
- zk_synced_followers 1
- zoo2 unhealthy (This ZooKeeper instance is not currently serving requests)
- zoo3 healthy
- restart zoo2
- no changes
- restart zoo3
- zoo1 leader
- zk_synced_followers 2
- zoo2 healthy
- zoo3 unhealthy (This ZooKeeper instance is not currently serving requests)
- waited 5 minutes and zoo3 returned to healthy

2. simulate patching zoo3
- zoo1 leader
- restart zoo3
- zk_synced_followers 2
- zoo1, zoo2, and zoo3 healthy

3. simulate patching zoo1
- zoo1 leader
- restart zoo1
- zoo1, zoo2, and zoo3 unhealthy (This ZooKeeper instance is not currently serving requests)
- waited 5 minutes to see if they resolve Sep 17 14:39 - Sep 17 14:44
- tried restarting in this order: zoo2, zoo3, zoo1 and no change; all still unhealthy (this step was not collected in the log files).

The third case in the above scenarios is the critical one since we are no longer able to start any of the zk nodes.

 

[~maoling] this issue may relate to https://issues.apache.org/jira/browse/ZOOKEEPER-3920 which corresponds to the first and second cases above that I am working with [~blb93] on.",[],Bug,ZOOKEEPER-3940,Critical,Stan Henderson,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper restart of leader causes all zk nodes to not serve requests,2021-08-27T17:28:57.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",10.0
,"[<JIRA Component: name='security', id='12329414'>]",2020-09-21T07:34:35.000+0000,Maks Savkin,"Zookeeper uses a key store password as a key private password for setting TLS. If we have another password for the private key we receive a strange mistake ""no cipher suite in common"" which is not clear.
Full logs:
{code}
2020-08-28 14:32:21,339 [myid:] - ERROR [nioEventLoopGroup-7-2:NettyServerCnxnFactory$CertificateVerifier@363] - Unsuccessful handshake with session 0x0
2020-08-28 14:32:21,342 [myid:] - DEBUG [nioEventLoopGroup-7-2:NettyServerCnxn@91] - close called for sessionid:0x0
2020-08-28 14:32:21,343 [myid:] - DEBUG [nioEventLoopGroup-7-2:NettyServerCnxn@103] - cnxns size:0
nioEventLoopGroup-7-2, called closeOutbound()
nioEventLoopGroup-7-2, closeOutboundInternal()
nioEventLoopGroup-7-2, called closeInbound()
nioEventLoopGroup-7-2, fatal: engine already closed. Rethrowing javax.net.ssl.SSLException: Inbound closed before receiving peer's close_notify: possible truncation attack?
2020-08-28 14:32:21,348 [myid:] - WARN [nioEventLoopGroup-7-2:NettyServerCnxnFactory$CnxnChannelHandler@220] - Exception caught
io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: no cipher suites in common
at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:468)
at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276)
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:377)
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363)
at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:355)
at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:377)
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363)
at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.lang.Thread.run(Unknown Source)
Caused by: javax.net.ssl.SSLHandshakeException: no cipher suites in common
at sun.security.ssl.Handshaker.checkThrown(Unknown Source)
at sun.security.ssl.SSLEngineImpl.checkTaskThrown(Unknown Source)
at sun.security.ssl.SSLEngineImpl.readNetRecord(Unknown Source)
at sun.security.ssl.SSLEngineImpl.unwrap(Unknown Source)
at javax.net.ssl.SSLEngine.unwrap(Unknown Source)
at io.netty.handler.ssl.SslHandler$SslEngineType$3.unwrap(SslHandler.java:281)
at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1324)
at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1219)
at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1266)
at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:498)
at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:437)
... 17 more
Caused by: javax.net.ssl.SSLHandshakeException: no cipher suites in common
at sun.security.ssl.Alerts.getSSLException(Unknown Source)
at sun.security.ssl.SSLEngineImpl.fatal(Unknown Source)
at sun.security.ssl.Handshaker.fatalSE(Unknown Source)
at sun.security.ssl.Handshaker.fatalSE(Unknown Source)
at sun.security.ssl.ServerHandshaker.chooseCipherSuite(Unknown Source)
at sun.security.ssl.ServerHandshaker.clientHello(Unknown Source)
at sun.security.ssl.ServerHandshaker.processMessage(Unknown Source)
at sun.security.ssl.Handshaker.processLoop(Unknown Source)
at sun.security.ssl.Handshaker$1.run(Unknown Source)
at sun.security.ssl.Handshaker$1.run(Unknown Source)
at java.security.AccessController.doPrivileged(Native Method)
at sun.security.ssl.Handshaker$DelegatedTask.run(Unknown Source)
at io.netty.handler.ssl.SslHandler.runAllDelegatedTasks(SslHandler.java:1494)
at io.netty.handler.ssl.SslHandler.runDelegatedTasks(SslHandler.java:1508)
at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1392)
... 21 more
{code}

It happens because of the code:
https://github.com/apache/zookeeper/blob/4a2d58219b7435c3b8cdf8f7ab04b158c1900223/zookeeper-server/src/main/java/org/apache/zookeeper/common/X509Util.java#L438

 ",[],Bug,ZOOKEEPER-3939,Major,Maks Savkin,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,There is not property for private key password. no cipher suites in common,2021-08-10T06:51:35.000+0000,"[<JIRA Version: name='3.5.7', id='12346098'>]",1.0
Damien Diederen,"[<JIRA Component: name='c client', id='12312380'>]",2020-09-16T10:59:32.000+0000,Damien Diederen,"This is about preventing SASL-enabled C clients from sending request packets for as long as SASL negotiation is not complete.

More details in the [pull request 1457|https://github.com/apache/zookeeper/pull/1457].","[<JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3937,Major,Damien Diederen,Fixed,2020-09-28T07:23:57.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,C client: avoid out-of-order packets during SASL negotiation,2021-03-28T08:54:25.000+0000,[],1.0
,[],2020-09-15T02:15:35.000+0000,Badai Aqrandista,"Adding 2 nodes to 3 nodes ensemble with the following steps:

1. Start the new nodes (node 4, 5) and let them join the ensemble 
2. Update existing nodes' configuration (node 1, 2, 3) to add the new nodes addresses and ports 
3. Rolling restart the existing nodes 
4. Stop the current leader (node 2)
5. New node (node 5) becomes leader

After this, zxid in the new leader (node 5) becomes the same as the old leader (node 2).

{noformat}
badai@192-168-1-129 3 % grep -i zxid logs-zk*/logs/before/*stat*
logs-zk1/logs/before/zk-1-stat-before-crash.log:Zxid: 0xe0000050c
logs-zk2/logs/before/zk-2-stat-before-crash.log:Zxid: 0xf00000000
logs-zk3/logs/before/zk-3-stat-before-crash.log:Zxid: 0xe0000050c
logs-zk4/logs/before/zk-4-stat-before-crash.log:Zxid: 0xe0000050c
logs-zk5/logs/before/zk-5-stat-before-crash.log:Zxid: 0xf00000000
badai@192-168-1-129 3 %
{noformat}

Please ignore the ""before"" wording on the logs name as they are irrelevant.
",[],Bug,ZOOKEEPER-3936,Minor,Badai Aqrandista,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zxid mismatch after new node becomes the leader,2020-09-20T09:51:22.000+0000,"[<JIRA Version: name='3.5.8', id='12346950'>]",2.0
,"[<JIRA Component: name='security', id='12329414'>]",2020-09-13T05:21:58.000+0000,Patrick D. Hunt,"dependency-check is failing with:

json-simple-1.1.1.jar: CVE-2020-10663, CVE-2020-7712",[],Bug,ZOOKEEPER-3933,Blocker,Patrick D. Hunt,Invalid,2020-09-13T06:12:09.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"owasp failing with json-simple-1.1.1.jar: CVE-2020-10663, CVE-2020-7712",2021-04-04T04:56:45.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.5.8', id='12346950'>, <JIRA Version: name='3.6.2', id='12347809'>]",3.0
Suraj Naik,[],2020-09-09T05:31:18.000+0000,Enrico Olivelli,"When you run zkServer.sh version the result includes a few spam lines and the version reports a trailing dash 
{noformat}
bin/zkServer.sh version
ZooKeeper JMX enabled by default
Using config: /xxxxxxxxxxx/bin/../conf/zoo.cfg
Apache ZooKeeper, version 3.6.2- 09/04/2020 12:44 GMT

{noformat}
 ","[<JIRA Version: name='3.6.3', id='12348703'>]",Bug,ZOOKEEPER-3931,Major,Enrico Olivelli,Fixed,2021-04-04T05:15:34.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"""zkServer.sh version"" returns a trailing dash",2021-04-04T05:18:30.000+0000,[],4.0
,"[<JIRA Component: name='metric system', id='12334405'>]",2020-09-08T10:52:48.000+0000,Eugene Klimov,"I run the following command on fresh installed Zookeeper:
{code:java}
wget -O- -q http://127.0.0.1:7000/metrics | grep -i readlatency{code}
and got the following results:
{code:java}
# HELP readlatency readlatency
# TYPE readlatency summary
readlatency{quantile=""0.5"",} NaN
readlatency{quantile=""0.9"",} NaN
readlatency{quantile=""0.99"",} NaN
readlatency_count 220.0
readlatency_sum 330.0
{code}
look like quantiles don't calculate properly",[],Bug,ZOOKEEPER-3929,Major,Eugene Klimov,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"prometheus metrics with type ""summary"" always show quantiles as NaN",2020-09-18T08:55:58.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.6.1', id='12346764'>]",2.0
,"[<JIRA Component: name='metric system', id='12334405'>]",2020-09-08T08:44:22.000+0000,Eugene Klimov,"on fresh installed Zookeeper 3.6.1 (without workload)

I run the following command:
{code:java}
wget -q -O- http://127.0.0.1:7000/metrics | grep -i packets_

{code}
{code:java}

# HELP packets_received packets_received
# TYPE packets_received gauge
packets_received 8004.0
# HELP packets_sent packets_sent
# TYPE packets_sent gauge
packets_sent 8009.0{code}
looks like `packets_sent` and `packets_recived` have wrong metric type
 cause 
{code:java}
sleep 5
wget -q -O- http://127.0.0.1:7000/metrics | grep -i packets_{code}
returns
{code:java}
# HELP packets_received packets_received
# TYPE packets_received gauge
packets_received 8114.0
# HELP packets_sent packets_sent
# TYPE packets_sent gauge
packets_sent 8119.0
{code}",[],Bug,ZOOKEEPER-3928,Major,Eugene Klimov,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,prometheus metrics packets_sent and packets_received wrong type,2020-09-18T10:13:49.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.6.1', id='12346764'>]",2.0
,[],2020-08-28T16:31:12.000+0000,Matteo Merli,"We have seen repeated occasion in which restarting the ZK leader node can lead to ~2min of quorum unavailability. This is easily reproducible on a cluster when leader election happens through a TCP proxying layer (such as Istio) in a Kubernetes environment.

This happens > ~80% of the times on a 3 nodes cluster. 

After investigation, this is sequence of events:

# zk-1 is the leader and gets restarted
#  zk-2 is voted as leader
# zk-0 tries to connect to zk-2:2888 to sync-up. The TCP connection is established, though it's remotely closed immediately after
# zk-2 starts listening to port 2888 but never receives any connection
# zk-0, after the read error on the connection goes back into LOOKING mode, ready for a new leader election
# zk-2 is still waiting for follower to sync-up, at waits until the timeout expires (eg: 30sec) after which it goes back into looking state.

This sequence might get repeated several times until finally one leader election round can get through.

h3. Logs excerpt:

ZK-2 becomes leader: 
{noformat}
2020-08-27 16:40:07.216000+00:00 [INFO ] [che.zookeeper.server.quorum.Leader]  LEADING - LEADER ELECTION TOOK - 214 MS
2020-08-27 16:40:07.218000+00:00 [INFO ] [zookeeper.server.quorum.QuorumPeer]  Peer state changed: leading - discovery
2020-08-27 16:40:07.218000+00:00 [INFO ] [.server.persistence.FileTxnSnapLog]  Snapshotting: 0xf00000650 to /streamlio/zookeeper/data/version-2/snapshot.f00000650
2020-08-27 16:40:07.249000+00:00 [INFO ] [e.zookeeper.server.ZooKeeperServer]  Snapshot taken in 30 ms
{noformat}
 

ZK-0 is following but immediately goes back into LOOKING state:
{noformat}
2020-08-27 16:40:07.207000+00:00 [INFO ] [he.zookeeper.server.quorum.Learner]  FOLLOWING - LEADER ELECTION TOOK - 211 MS
2020-08-27 16:40:07.208000+00:00 [INFO ] [zookeeper.server.quorum.QuorumPeer]  Peer state changed: following - discovery
2020-08-27 16:40:07.208000+00:00 [INFO ] [he.zookeeper.server.quorum.Learner]  Successfully connected to leader, using address: zookeeper-zk35-2.zookeeper-zk35.pulsar-developers.svc.cluster.local/100.101.166.47:2888
2020-08-27 16:40:07.214000+00:00 [WARN ] [he.zookeeper.server.quorum.Learner]  Exception when following the leader
	java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(SocketInputStream.java:186) ~[?:?]
	at java.net.SocketInputStream.read(SocketInputStream.java:140) ~[?:?]
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:252) ~[?:?]
	at java.io.BufferedInputStream.read(BufferedInputStream.java:271) ~[?:?]
	at java.io.DataInputStream.readInt(DataInputStream.java:392) ~[?:?]
	at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:96)
	at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:86)
	at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:134)
	at org.apache.zookeeper.server.quorum.Learner.readPacket(Learner.java:182)
	at org.apache.zookeeper.server.quorum.Learner.registerWithLeader(Learner.java:451)
	at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:89)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1458)

2020-08-27 16:40:07.215000+00:00 [INFO ] [he.zookeeper.server.quorum.Learner]  Disconnected from leader (with address: zookeeper-zk35-2.zookeeper-zk35.pulsar-developers.svc.cluster.local/100.101.166.47:2888). Was connected for 6ms. Sync state: false
2020-08-27 16:40:07.215000+00:00 [INFO ] [he.zookeeper.server.quorum.Learner]  shutdown Follower
2020-08-27 16:40:07.215000+00:00 [INFO ] [zookeeper.server.quorum.QuorumPeer]  Peer state changed: looking
2020-08-27 16:40:07.215000+00:00 [WARN ] [zookeeper.server.quorum.QuorumPeer]  PeerState set to LOOKING
2020-08-27 16:40:07.215000+00:00 [INFO ] [zookeeper.server.quorum.QuorumPeer]  LOOKING
2020-08-27 16:40:07.215000+00:00 [INFO ] [r.server.quorum.FastLeaderElection]  New election. My id = 1, proposed zxid=0xf00000650
{noformat}


After timeout, ZK-2 goes back into looking and a new leader election takes place:
{noformat}
2020-08-27 16:40:27.251000+00:00 [WARN ] [zookeeper.server.quorum.QuorumPeer]  Unexpected exception
	java.lang.InterruptedException: Timeout while waiting for epoch from quorum
	at org.apache.zookeeper.server.quorum.Leader.getEpochToPropose(Leader.java:1428)
	at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:599)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1471)

2020-08-27 16:40:27.251000+00:00 [INFO ] [che.zookeeper.server.quorum.Leader]  Shutting down
2020-08-27 16:40:27.251000+00:00 [INFO ] [che.zookeeper.server.quorum.Leader]  Shutdown called. For the reason Forcing shutdown
2020-08-27 16:40:27.251000+00:00 [INFO ] [zookeeper.server.quorum.QuorumPeer]  Peer state changed: looking
2020-08-27 16:40:27.252000+00:00 [WARN ] [zookeeper.server.quorum.QuorumPeer]  PeerState set to LOOKING
2020-08-27 16:40:27.252000+00:00 [INFO ] [zookeeper.server.quorum.QuorumPeer]  LOOKING
{noformat}

 

The main issue that triggers this is that there is an intrinsic race condition between the leader to start listening on port 2888 and the followers trying to connect to it. In normal deployment scenario, the followers will get a connection refused error, on which they will be retrying up to 5 times.

Instead, with a TCP proxy in between, the follower is able to establish the TCP connection (to the proxy) but this connection will be terminated immediately as we try to read or write on the socket, since when the proxy connects to the leader that will be failing.

One solution here, would be to apply the same retry logic, within the boundaries of the timeout also when we get a read error and not only on connection error. ",[],Bug,ZOOKEEPER-3923,Major,Matteo Merli,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Leader election issues with Istio,2020-08-28T17:27:10.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",3.0
,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2020-08-26T17:15:03.000+0000,Andre Price,"[Sorry I believe this is a dupe of https://issues.apache.org/jira/browse/ZOOKEEPER-3828 and potentially https://issues.apache.org/jira/browse/ZOOKEEPER-3466 

But i am not able to attach files there for some reason so creating a new issue which hopefully allows me]

We are encountering an issue where failing over from the leader results in zookeeper clients not being able to connect successfully. They timeout waiting for a response from the server. We are attempting to upgrade some existing zookeeper clusters from 3.4.14 to 3.6.1 (not sure if relevant but stating incase it helps with pinpointing issue) which is effectively blocked by this issue. We perform the rolling upgrade (followers first then leader last) and it seems to go successfully by all indicators. But we end up in the state described in this issue where if the leader changes (either due to restart or stopping) the cluster does not seem able to start new sessions.

I've gathered some TRACE logs from our servers and will attach in the hopes they can help figure this out. 

Attached zk_repro.zip which contains the following:
 * zoo.cfg used in one of the instances (they are all the same except for the local server's ip being 0.0.0.0 in each)
 * zoo.cfg.dynamic.next (don't think this is used anywhere but is written by zookeeper at some point - I think when the first 3.6.1 container becomes leader based on the value – the file is in all containers and is the same in all as well)
 * s\{1,2,3}_zk.log - logs from each of the 3 servers. Estimated time of repro start indicated by ""// REPRO START"" text and whitespace in logs
 * repro_steps.txt - rough steps executed that result in the server logs attached

 

I'll summarize the repro here also:
 # Initially it appears to be a healthy 3 node ensemble all running 3.6.1. Server ids are 1,2,3 and 3 is the leader. Dynamic config/reconfiguration is disabled.
 # invoke srvr on each node (to verify setup and also create bookmark in logs)
 # Do a zkCli get of /zookeeper/quota  which succeeds
 # Do a restart of the leader (to same image/config) (server 2 now becomes leader, 3 is back as follower)
 # Try to perform the same zkCli get which times out (this get is done within the container)
 # Try to perform the same zkCli get but from another machine, this also times out
 # Invoke srvr on each node again (to verify that 2 is now the leader/bookmark)
 # Do a restart of server 2 (3 becomes leader, 2 follower)
 # Do a zkCli get of /zookeeper/quota which succeeds
 # Invoke srvr on each node again (to verify that 3 is leader)

I tried to keep the other ZK traffic to a minimum but there are likely some periodic mntr requests mixed from our metrics scraper.","[<JIRA Version: name='3.6.2', id='12347809'>]",Bug,ZOOKEEPER-3920,Major,Andre Price,Fixed,2020-09-10T03:11:15.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Zookeeper clients timeout after leader change due to 0.0.0.0 address when in docker environment,2020-09-25T05:58:53.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>]",7.0
,"[<JIRA Component: name='server', id='12312382'>]",2020-08-21T18:59:49.000+0000,Peter Welch,A portion of the CRC32 hash in DigestCalculator comes from converting the path String to bytes.  The code does not specify a locale so the default is used.  We experienced a situation where we had participants on different environments (each with a different locale) and the digest mismatched incorrectly. ,[],Bug,ZOOKEEPER-3915,Minor,Peter Welch,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Digest false mismatch across systems with different default locale,2020-08-23T01:06:23.000+0000,[],2.0
,[],2020-08-20T07:49:32.000+0000,Ella Kurginyan,"my zoo.cfg: 
{code:java}
tickTime=2000
initLimit=5
syncLimit=2
dataDir=/var/lib/zookeeper
clientPort=2181
maxClientCnxns=0
autopurge.snapRetainCount=3
autopurge.purgeInterval=24
standaloneEnabled=false
authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider
zookeeper.allowSaslFailedClients=false
jaasLoginRenew=3600000
kerberos.removeHostFromPrincipal=true
kerberos.removeRealmFromPrincipal=true
server.1=myhost.internal:2888:3888
{code}
 

zookeeper-env.sh 
{code:java}
export JAVA=""$JAVA_HOME/bin/java""
export ZOO_LOG4J_PROP=""DEBUG, CONSOLE, ROLLINGFILE""
export ZOO_LOG_DIR=""/var/log/zookeeper""
export ZOOPIDFILE=""/var/run/zookeeper/zookeeper_server.pid""
export SERVER_JVMFLAGS=""-Djava.security.auth.login.config=/etc/zookeeper/conf/zookeeper_server_jaas.conf""

export CLIENT_JVMFLAGS=""$CLIENT_JVMFLAGS -Djava.security.auth.login.config=/etc/zookeeper/conf/zookeeper_client_jaas.conf""
{code}
 

zookeeper_server_jaas.conf:
{code:java}
Server {
 com.sun.security.auth.module.Krb5LoginModule required
 doNotPrompt=true
 useKeyTab=true
 storeKey=true
 useTicketCache=false
 keyTab=""/tmp/keytabs/zookeeper.keytab""
 principal=""zookeeper/myhost.internal@MYDOMAIN.COM"";
};{code}
zookeeper_client_jaas.conf:
{code:java}
Client {
 com.sun.security.auth.module.Krb5LoginModule required
 useKeyTab=false
 useTicketCache=true;
};{code}
For anonymous user its allowed to connect:
{code:java}
2020-08-17 13:58:18,603 - WARN [main-SendThread(localhost:2181):SaslClientCallbackHandler@60] - Could not login: the Client is being asked for a password, but the ZooKeeper Client code does not currently support obtaining a password from the user. Make sure that the Client is configured to use a ticket cache (using the JAAS configuration setting 'useTicketCache=true)' and restart the Client. If you still get this message after that, the TGT in the ticket cache has expired and must be manually refreshed. To do so, first determine if you are using a password or a keytab. If the former, run kinit in a Unix shell in the environment of the user who is running this Zookeeper Client using the command 'kinit <princ>' (where <princ> is the name of the Client's Kerberos principal). If the latter, do 'kinit -k -t <keytab> <princ>' (where <princ> is the name of the Kerberos principal, and <keytab> is the location of the keytab file). After manually refreshing your cache, restart this Client. If you continue to see this message after manually refreshing your cache, ensure that your KDC host's clock is in sync with this host's clock. 2020-08-17 13:58:18,604 - WARN [main-SendThread(localhost:2181):ClientCnxn$SendThread@1010] - SASL configuration failed: javax.security.auth.login.LoginException: No password provided Will continue connection to Zookeeper server without SASL authentication,if Zookeeper server allows it. 2020-08-17 13:58:18,606 - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@1028] - Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181 WATCHER:: WatchedEvent state:AuthFailed type:None path:null 2020-08-17 13:58:18,653 - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@878] - Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session 2020-08-17 13:58:18,662 - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@1302] - Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x1001d710bf9003e, negotiated timeout = 30000 WATCHER:: WatchedEvent state:SyncConnected type:None path:null [zk: localhost:2181(CONNECTED) 0] ls / {code}
For wrong user connection is dropped:
{code:java}
2020-08-18 16:09:41,628 [myid:localhost:2181] - ERROR [main-SendThread(localhost:2181):ZooKeeperSaslClient@341] - An error: (java.security.PrivilegedActionException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7))]) occurred when evaluating Zookeeper Quorum Member's received SASL token. Zookeeper Client will go to AUTH_FAILED state. 2020-08-18 16:09:41,629 [myid:localhost:2181] - ERROR [main-SendThread(localhost:2181):ClientCnxn$SendThread@1151] - SASL authentication with Zookeeper Quorum member failed: javax.security.sasl.SaslException: An error: (java.security.PrivilegedActionException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7))]) occurred when evaluating Zookeeper Quorum Member's received SASL token. Zookeeper Client will go to AUTH_FAILED state. WATCHER:: WatchedEvent state:AuthFailed type:None path:null 2020-08-18 16:09:41,629 [myid:] - INFO [main-EventThread:ClientCnxn$EventThread@524] - EventThread shut down for session: 0x1008b3112ae0004 [zk: localhost:2181(AUTH_FAILED) 0] [zk: localhost:2181(AUTH_FAILED) 0] {code}",[],Bug,ZOOKEEPER-3914,Critical,Ella Kurginyan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Kerberized Zookeeper doesn't drop connection for unauthenticated clients,2020-08-24T07:41:34.000+0000,"[<JIRA Version: name='3.5.6', id='12345243'>]",3.0
,"[<JIRA Component: name='metric system', id='12334405'>]",2020-08-17T10:16:13.000+0000,Philipp Trulson,"We use zookeeper 3.6.1 in combination with solr 8.6.0 on our kubernetes cluster. For monitoring purposes we activated the prometheus metrics for zookeeper, which had the unexpected side effect that the 4lw mntr command now also outputs floats instead of the usual integers. This broke the parser on the solr side (SOLR-14752), because it was expecting ints.

This ticket is to discuss if this is actually a bug or expected behavior because this change isn't mentioned anywhere in the release notes or documentation.",[],Bug,ZOOKEEPER-3912,Major,Philipp Trulson,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,4LW MNTR command outputs in float when prometheus activated,2020-08-25T06:25:19.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>]",6.0
Michael Han,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2020-08-15T08:22:51.000+0000,lixun,"Since version 3.4, the quorum of followers and the leader did not synchronize the files immediately when the synchronization was completed, and the data was not persisted to the files in an instant, and at this time the zk server can provide external access, such as webapp access, if it appears at this time Failure, phantom reading may occur

There is a example in the link.    [ here example|[https://drive.google.com/file/d/1jy3kkVQTDYGb4iV1RaPMBbEWLZZltTQG/view?usp=sharing]]

-----------------mail list-----------------

mail response from hanm@apache.org

Hi Xun,

I think this is a bug, your test case is sound to me. Do you mind
 creating a JIRA for this issue?

Followers should not ACK NEWLEADER without ACK every transaction from the
 DIFF sync. To ACK every transaction, a follower either persists the
 transaction in log, or takes a snapshot before sending the ACK of the
 NEWLEADER (which we did, before ZOOKEEPER-2678 where the snapshot
 optimization was introduced).

A potential fix I have in mind is to make sure to persist all DIFF sync
 proposals from LEADER (similar to what we are already doing for proposals
 coming between NEWLEADER and UPTODATE). By doing so, when the leader
 receives NEWLEADER ACK from a quorum, it's guaranteed that
 every transaction leader DIFF sync to follower is quorum committed. Thus
 there will not be inconsistent views moving forward. Alternatively we can
 take a snapshot before ACK NEWLEADER but that will be a big performance hit
 for big data trees.

I am also interested to hear what others think about this.

On Fri, Aug 28, 2020 at 12:20 AM li xun <274952496@qq.com> wrote:

 
{quote}There is a example in the link, would you understand what I mean？

[https://drive.google.com/file/d/1jy3kkVQTDYGb4iV1RaPMBbEWLZZltTQG/view?usp=sharing]

Since version 3.4, the quorum of followers and the leader did not
 synchronize the files immediately when the synchronization was completed,
 and the data was not persisted to the files in an instant, and at this time
 the zk server can provide external access, such as webapp access, if it
 appears at this time Failure, phantom reading may occur
{quote}
2020年8月28日 14:51，Justin Ling Mao <maoling199210191@sina.com> 写道：

 

@李珣The situation you describe may have conceptual deviations about how
{quote}the consensus protocol works:---> Since the data of the follower when the
 follower uses the DIFF method to synchronize with the leader is still in
 the memory, it has not had time to persist1. The write path is: write
 transaction log(WAL) firstly, after reaching a consensus, then apply to
 memory, other than the opposite.
{quote}
---> but at this time, the latest zxid_n of the leader has not been
{quote}supported by the quorum of the follower. At this time, if a client connects
 to the leader and sees zxid_n,2. If a write has not been supported by the
 quorum, it's not safe to apply to the state machine and the client is not
 able to see this write.
{quote}
I guess that your question may be: how the system handles the
{quote}uncommitted logs when leader changes?
{quote}
 

----- Original Message -----
 From: Ted Dunning <ted.dunning@gmail.com>
 To: dev@zookeeper.apache.org
 Subject: Re: May violate the ZAB agreement – version 3.6.1
 Date: 2020-08-28 01:25

How is it that participant A would have a later zxid than the leader?
 In particular, it seems to me that it should be impossible to have these
 two facts be true:
 1) a transaction has been committed with zxid = z_0. This implies that a
 quorum of the cluster has accepted this transaction and it has been
 committed.
 2) a new leader election nominates a leader with latest zxid < z_0.
 My reasoning is that any new leader election has to involve a quorum and
{quote}at
{quote}
least a sufficient number of that quorum must have accepted zxid >= z_0
{quote}and
{quote}
therefore would refuse to be part of the quorum (this is a
{quote}contradiction).
{quote}
Thus, no leader could be elected with zxid < z_0 if fact (1) is true.
 What you are describing seems to require both of these facts.
 Perhaps I am missing something about your suggested scenario. Could you
 describe what you are thinking in more detail?
 On Thu, Aug 27, 2020 at 2:08 AM 李珣 <274952496@qq.com> wrote:

 
{quote}version 3.6.1
 org.apache.zookeeper.server.quorum.Learner.java line:605
 Suppose there is a situation
 zxid_n is the largest zxid of Participant A (the leader has just resumed
 from downtime). Zxid_n has not been recognized by the quorum. Assuming
 Participant A is elected as the Leader, then if a follower appears to
{quote}
{quote}use
{quote}
{quote}DIFF to synchronize data with the Leader, Leader After sending the
 UPTODATE, the leader can already provide external access, but at this
{quote}
{quote}time,
{quote}
{quote}the latest zxid_n of the leader has not been supported by the quorum of
{quote}
{quote}the
{quote}
{quote}follower. At this time, if a client connects to the leader and sees
{quote}
{quote}zxid_n,
{quote}
{quote}then at this time both the leader and the follower are down. For some
 reason, the leader cannot be started, and the follower can start
{quote}
{quote}normally.
{quote}
{quote}At this time, a new leader can only be elected from the follower. Since
{quote}
{quote}the
{quote}
{quote}data of the follower when the follower uses the DIFF method to
{quote}
{quote}synchronize
{quote}
{quote}with the leader is still in the memory, it has not had time to persist,
 then this The newly elected leader does not have the data of zxid_n, but
 before zxid_n has been seen by the client on the old leader, there will
{quote}
{quote}be
{quote}
{quote}inconsistencies in the data view.
 Is the above situation possible?
{quote}
 
  ","[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3911,Critical,lixun,Fixed,2020-11-12T15:10:18.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Data inconsistency caused by DIFF sync uncommitted log,2022-04-05T03:49:08.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>, <JIRA Version: name='3.4.13', id='12342973'>, <JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.5.6', id='12345243'>, <JIRA Version: name='3.5.7', id='12346098'>, <JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",7.0
,[],2020-08-07T17:01:56.000+0000,Saswati,"When we restart a zookeeper, it doesn't successfully join the cluster and start serving clients. We see the zookeeper services starts successfully, but it stays ideal and throws the message: ""This ZooKeeper instance is not currently serving requests""

The Zookeeper cluster size is 5. Whenever we feel the need of restarting the zookeepers, we do one at a time. There are two ways we restart the zookeepers,
 # just stop the services and start it back up again.
 # stop the services, replace the host, and start it back up again.

And, in both the cases we see the same issue.

-----------

When investigated the zookeepers logs, we see the below errors/warnings,

""[QuorumPeer[myid=1](plain=x.x.x.x:0000)(secure=disabled)] WARN  org.apache.zookeeper.server.quorum.Learner - Exception when following the leader
 [java.io|http://java.io/].IOException: Leaders epoch, xx is less than accepted epoch, xy""

-------------------------

But, when we check the current epoch of the leader is always same as the accepted epoch, which is also matches of the zookeeper we are trying to bring back to the quorum.

------------------------

Also, when we get the Zxid of every quorum member, they have the same first byte; only the last two numbers change, so we can safely assume that they are in sync, I guess.

Somehow this zookeeper that we re restarting sees an advancing of the epoch and shuts down as a follower.

--------------

The current solution we have at the moment for this issue is,

stop the zookeeper services --> rename the current zookeeper data directory (version-2) --> start it backup again.

It immediately joins the cluster as a follower as it doesn't have any idea of the epoch and start serving clients. 

----------",[],Bug,ZOOKEEPER-3909,Critical,Saswati,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Zookeeper Unable to Join the Cluster after it is Restarted; Error: ""This ZooKeeper instance is not currently serving requests""",2020-08-07T17:12:08.000+0000,"[<JIRA Version: name='3.5.7', id='12346098'>]",3.0
,"[<JIRA Component: name='build', id='12312383'>]",2020-08-04T17:22:30.000+0000,Bogdan Baba,"There are currently multiple issues affecting zktreeutil 

 
 # it does not compile successfully from source on 3.4.X 3.5.X and 3.6.X
 # It compiles successfully (after finding patched) only on older compilers
 # It fails to connect to the ZK host to dump/import/export/diff",[],Bug,ZOOKEEPER-3908,Major,Bogdan Baba,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zktreeutil multiple issues,2020-11-23T04:51:22.000+0000,"[<JIRA Version: name='3.4.14', id='12343587'>, <JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",3.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2020-08-03T18:11:46.000+0000,Gangadhar,"*Issue*: Data Inconsistency Between Zookeeper Leader and zookeeper Followers. zookeeper followers and zookeeper leaders have other information. We try to delete the information from the follower's, but information not present in zookeeper leader, it's throwing error like *Node does not exist:*

*Expected behaviour:* Data consistency between zookeeper leader and Zookeeper followers should be same.

 

Steps followed as part of troubleshooting:

We have 5 zookeepers in clusters.

*Step1:*  verified all zookeepers are following the leader or not?. As per below information its following all 4 zookeepers to zookeeper leader

zk_version      3.5.7-f0fdd52973d373ffd9c86b81d99842dc2c7f660e, built on 02/11/2020 11:30 GMT
 zk_avg_latency  0
 zk_max_latency  823
 zk_min_latency  0
 zk_packets_received     30214264
 zk_packets_sent 32424272
 zk_num_alive_connections        7
 zk_outstanding_requests 0
 zk_server_state leader
 zk_znode_count  75190
 zk_watch_count  21394
 zk_ephemerals_count     793
 zk_approximate_data_size        24706628
 zk_open_file_descriptor_count   281
 zk_max_file_descriptor_count    4096
 zk_followers    4
 zk_synced_followers     4
 zk_pending_syncs        0
 zk_last_proposal_size   166
 zk_max_proposal_size    121947
 zk_min_proposal_size    32

*Step 2:* Verified znode in all the zookeepers , but we are not getting same information from zookeeper leader and followers.

*Step 3:* Try to delete the Zookeeper node and received below error. Also, we are suspecting that when trying to delete the info of znode, it's trying to reach zookeeper leader and throwing *Node does not exist* error. 

*Error:* 
 14:04:54.769 [main] INFO  org.apache.zookeeper.ClientCnxnSocket - jute.maxbuffer value is 10485760 Bytes
 14:04:54.775 [main] INFO  org.apache.zookeeper.ClientCnxn - zookeeper.request.timeout value is 0. feature enabled=
 14:04:54.824 [main-SendThread(11.111.226.146:2181)] INFO  org.apache.zookeeper.ClientCnxn - Opening socket connection to server 11-111-226-146.ebiz.verizon.com/11.111.226.146:2181. Will not attempt to authenticate using SASL (unknown error)
 14:04:54.831 [main-SendThread(11.111.226.146:2181)] INFO  org.apache.zookeeper.ClientCnxn - Socket connection established, initiating session, client: /11.111.225.75:38804, server: 11-111-226-146.ebiz.verizon.com/11.111.20.146:2181
 14:04:54.835 [main-SendThread(11.111.226.146:2181)] INFO  org.apache.zookeeper.ClientCnxn - Session establishment complete on server 11-111-226-146.ebiz.verizon.com/11.111.226.146:2181, sessionid = 0x500001bbbeb0651, negotiated timeout = 20000

WATCHER::

WatchedEvent state:SyncConnected type:None path:null
 *Node does not exist: /namespace/$tenant/$Namespace/$zk-path*",[],Bug,ZOOKEEPER-3906,Major,Gangadhar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Data Inconsistency Between Zookeeper Leader and zookeeper Followers,2020-08-05T02:33:28.000+0000,"[<JIRA Version: name='3.5.7', id='12346098'>]",5.0
Andor Molnar,[],2020-08-03T17:46:14.000+0000,Karan Mehta,"To reproduce, apply the diff and run ClientSSLTest#testSecureStandaloneServer() test. The logs would show that a valid session was created before connection was rejected and client had to retry

 
{code:java}
diff --git a/zookeeper-server/src/main/java/org/apache/zookeeper/common/ZKTrustManager.java b/zookeeper-server/src/main/java/org/apache/zookeeper/common/ZKTrustManager.java
 index aa02145b2..df1bdcc0f 100644
 — a/zookeeper-server/src/main/java/org/apache/zookeeper/common/ZKTrustManager.java
 +++ b/zookeeper-server/src/main/java/org/apache/zookeeper/common/ZKTrustManager.java
 @@ -111,6 +111,7 @@ public void checkServerTrusted(X509Certificate[] chain, String authType, SSLEngi
 @Override
 public void checkClientTrusted(X509Certificate[] chain, String authType) throws CertificateException
{ x509ExtendedTrustManager.checkClientTrusted(chain, authType); + throw new CertificateException(); }
@Override
{code}
  
 What should have happened:
 Server should instantly close the client's connection and NOT process any request.
  
 Potential threat:
 Malicious clients may be able to put unnecessary load/traffic on the leader by creating these sessions.
  
 Race Condition:
 In CertificateVerifier#operationComplete(), `addCnxn(cnxn)` method is only called after auth is completed. NettyServerCnxn#close() returns as a no-op [here|https://github.com/apache/zookeeper/blob/branch-3.5/zookeeper-server/src/main/java/org/apache/zookeeper/server/NettyServerCnxn.java#L105].
  
 I see this as an issue. Please assess the risk and let me know if this is a legit behavior or not.
  ","[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",Bug,ZOOKEEPER-3905,Major,Karan Mehta,Fixed,2020-08-08T09:53:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Race condition causes sessions to be created for clients even though their certificate authentication has failed,2020-09-10T10:43:34.000+0000,"[<JIRA Version: name='3.5.8', id='12346950'>, <JIRA Version: name='3.6.2', id='12347809'>]",5.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2020-07-28T19:01:01.000+0000,Rajan Dhabalia,"I saw this issue in one of the zookeeper cluster where LEADER host crashed due to h/w issue and all the follower hosts immediately closed client connection with leader host but zk-cluster did not trigger leader reelection until we manually restarted few zookeeper host after some time.

Snapshot size: 200MB
ZooKeeper version: 3.4.13


Below logs are from Leader and one of the follower host, when Leader host was crashed.

1. Leader host: zookeeper-4.us-west.com crashed at 16:21:09 and no application logs.
2. Follower host: zookeeper-2.us-west.com immediately closed client-connection with zookeeper-4.us-west.com at 16:21:09
3. None of the Follower hosts triggered LEADER reelection and don't see any such logs
4. All the zk-clients started getting zookeeper session timeouts
5. after 27 minutes at 16:48:30, Follower host found out the issue in quorum and logged
*""Have smaller server identifier, so dropping the connection""*
6. But it still did not trigger leader reelection until we manually restarted zk-process in few zk hosts after 5 mins
7. and finally after restarting process in few zk hosts manually: zk cluster did leader reelection and created the quorum

*Logs:*

*Leader zookeeper: zookeeper-4.us-west.com*
{noformat}
Leader zookeeper: zookeeper-4.us-west.com
Host crashed at : 16:21:10,095 and last log was `16:21:10,095`. and then it came back after an hour: `17:25:27` and joined as FOLLOWER.

16:21:09,965 - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@215] - Accepted socket connection from /0.0.0.1:45186
16:21:09,965 - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@908] - Processing mntr command from /0.0.0.1:45186
16:21:10,095 - INFO [Thread-194783:NIOServerCnxn@1056] - Closed socket connection for client /0.0.0.1:45186 (no session established for client)
17:25:27,262 - INFO [main:QuorumPeerConfig@136] - Reading configuration from: 
{noformat}

 

*Follower zookeeper: zookeeper-2.us-west.com*
{noformat}
16:21:09,743 - INFO [Thread-78288:NIOServerCnxn@1056] - Closed socket connection for client /4.4.4.4:52216 (zookeeper-4.us-west.com-IP) (no session established for client)
:
# ZK host didn't trigger the LEADER-REELECTION even despite zookeeper-4.us-west.com crashed and lost connection with that host at : `16:21:09,743`
# Client was keep connecting and keep disconnecting due to zk session timeout
16:48:29,961 - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@215] - Accepted socket connection from /0.0.0.2:43938
16:48:29,965 - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@949] - Client attempting to establish new session at /0.0.0.2:43938
16:48:29,977 - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@215] - Accepted socket connection from /0.0.0.3:54630
16:48:29,981 - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@949] - Client attempting to establish new session at /0.0.0.3:54630
:
# After 27 Minutes: Saw first log from QuorumCnxManager and zk host was figuring out if needs leaer-relection
16:48:30,475 - INFO [WorkerSender[myid=2]:QuorumCnxManager@347] - Have smaller server identifier, so dropping the connection: (3, 2)
16:48:30,475 - INFO [WorkerReceiver[myid=2]:FastLeaderElection@595] - Notification: 1 (message format version), 2 (n.leader), 0x6d00328f73 (n.zxid), 0x1 (n.round), LOOKING (n.state), 2 (n.sid), 0x6d (n.peerEpoch) LOOKING (my state)
16:48:30,476 - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@908] - Processing mntr command from /2.2.2.2:55136
16:48:30,476 - INFO [WorkerReceiver[myid=2]:FastLeaderElection@595] - Notification: 1 (message format version), 4 (n.leader), 0x6c00b36a5a (n.zxid), 0x35e (n.round), LOOKING (n.state), 1 (n.sid), 0x6c (n.peerEpoch) LOOKING (my state)
16:48:30,476 - INFO [prod1-zk2.messaging.gq1.yahoo.com/2.2.2.2:2183:QuorumCnxManager$Listener@743] - Received connection request /3.3.3.3:34092
16:48:30,476 - INFO [WorkerSender[myid=2]:QuorumCnxManager@347] - Have smaller server identifier, so dropping the connection: (4, 2)
:
# After manually restarting follower host, finally follower hosts 
16:50:53,986 - INFO [main:QuorumPeerConfig@398] - Defaulting to majority quorums
16:50:53,991 - INFO [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3
16:50:53,991 - INFO [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 1
16:50:53,992 - INFO [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
16:50:54,003 - INFO [main:QuorumPeerMain@130] - Starting quorum peer
16:50:54,026 - INFO [main:FileSnap@86] - Reading snapshot
:
16:55:04,377 - INFO [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:2181:Follower@65] - FOLLOWING - LEADER ELECTION TOOK - 233617
{noformat}",[],Bug,ZOOKEEPER-3902,Major,Rajan Dhabalia,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper cluster did not perform leader reelection when leader host crashed,2020-07-29T11:05:19.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",4.0
,"[<JIRA Component: name='server', id='12312382'>]",2020-07-24T06:32:08.000+0000,Jens M Kofoed,"Creating a minimum zoo.cfg file where the servers are specified with there hostnames/fqdn in stead of ip adress. The result is that the server only binds to 127.0.0.1 for the election on port 3888
 to make a workaround I had to add the quorumListenOnAllIPs=true to the config file

Enabling quorumListenOnAllIPs introduce another error. The leader server gets connections request from localhost and writes errors in the log.",[],Bug,ZOOKEEPER-3897,Major,Jens M Kofoed,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,When using hostname zookeeper only binds to 127.0.0.1 for quorum,2020-07-27T11:13:26.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>]",2.0
Enrico Olivelli,"[<JIRA Component: name='java client', id='12312381'>]",2020-07-23T06:45:31.000+0000,Enrico Olivelli,"I saw this error in an application that uses Apache BookKeeper DistributedLog library.

This is a regression on 3.6.x release series

The bug is in ZooKeeper Java Client.
{code:java}
Caused by: java.lang.NullPointerExceptionCaused by: java.lang.NullPointerException at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:2105) at org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:2050) at org.apache.bookkeeper.zookeeper.ZooKeeperClient.access$1101(ZooKeeperClient.java:70) at org.apache.bookkeeper.zookeeper.ZooKeeperClient$3.zkRun(ZooKeeperClient.java:498) at org.apache.bookkeeper.zookeeper.ZooKeeperClient$ZkRetryRunnable.run(ZooKeeperClient.java:389) at org.apache.bookkeeper.zookeeper.ZooKeeperClient.multi(ZooKeeperClient.java:510) at org.apache.distributedlog.zk.ZKTransaction.execute(ZKTransaction.java:67) at org.apache.distributedlog.BKLogWriteHandler.setLogSegmentTruncationStatus(BKLogWriteHandler.java:1223) at org.apache.distributedlog.BKLogWriteHandler.setLogSegmentsOlderThanDLSNTruncated(BKLogWriteHandler.java:1117) at org.apache.distributedlog.BKLogWriteHandler.lambda$setLogSegmentsOlderThanDLSNTruncated$0(BKLogWriteHandler.java:1083) at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1183) at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2299) at org.apache.distributedlog.BKLogWriteHandler.setLogSegmentsOlderThanDLSNTruncated(BKLogWriteHandler.java:1082) at org.apache.distributedlog.BKAsyncLogWriter.truncate(BKAsyncLogWriter.java:449){code}","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",Bug,ZOOKEEPER-3895,Blocker,Enrico Olivelli,Fixed,2020-07-29T17:13:37.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Client side NullPointerException in case of empty Multi operation,2020-09-10T10:43:52.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2020-07-21T16:20:32.000+0000,Jake McArthur,"A bug in NIOServerCnxn can result in a client failing with an error about out of order xids. What actually happens, as I understand it, is:
 # Client attempts to renew its session on slow server S1.
 # The attempt times out.
 # Client attempts to renew its session on server S2.
 # The attempt succeeds. S2 now owns the session.
 # The client sends one or more requests. The responses are large enough that they fill the socket's buffer in S2.
 # The original attempt finally succeeds. S1 now owns the session, but the client is still connected to S2.
 # The client sends an asynchronous request A to S2. Because the session has moved, S2 instructs the NIOServerCnxn to close. This is implemented as an empty sentinel value added to the queue of outgoing buffers.
 # The client sends some read request B to S2, and the response is enqueued behind the sentinel.
 # The doIO method of NIOServerCnxn writes its enqueued buffers to the socket, and then it closes the socket because one of the buffers was the sentinel.
 # Before the client observes that the socket it closed, it receives the response for B, and fails with an error because it expected the response for A.

I think the fix is simply to avoid writing messages that were enqueued after the sentinel.",[],Bug,ZOOKEEPER-3894,Major,Jake McArthur,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Out-of-order response after session moved,2020-07-21T16:20:32.000+0000,[],1.0
Ling Mao,"[<JIRA Component: name='server', id='12312382'>]",2020-07-17T09:51:44.000+0000,Ling Mao,"We have three servers(s1[follower], s2[Leader], s3[follower]), after doing a large scala benchmark test, the client cannot connect to server anymore.

No any resource panic(CPU, memory)

 
{code:java}
# connect to s2[Leader]

2020-07-17 17:44:56,767 [myid:127.0.0.1:2182] - WARN  [main-SendThread(127.0.0.1:2182):ClientCnxn$SendThread@1278] - Session 0x0 for sever localhost/127.0.0.1:2182, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.2020-07-17 17:44:56,767 [myid:127.0.0.1:2182] - WARN  [main-SendThread(127.0.0.1:2182):ClientCnxn$SendThread@1278] - Session 0x0 for sever localhost/127.0.0.1:2182, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.org.apache.zookeeper.ClientCnxn$SessionTimeoutException: Client session timed out, have not heard from server in 30000ms for session id 0x0 at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1236)2020-07-17 17:44:58,519 [myid:127.0.0.1:2182] - INFO  [main-SendThread(127.0.0.1:2182):ClientCnxn$SendThread@1159] - Opening socket connection to server localhost/127.0.0.1:2182.2020-07-17 17:44:58,519 [myid:127.0.0.1:2182] - INFO  [main-SendThread(127.0.0.1:2182):ClientCnxn$SendThread@1161] - SASL config status: Will not attempt to authenticate using SASL (unknown error)2020-07-17 17:44:58,519 [myid:127.0.0.1:2182] - INFO  [main-SendThread(127.0.0.1:2182):ClientCnxn$SendThread@993] - Socket connection established, initiating session, client: /127.0.0.1:64219, server: localhost/127.0.0.1:21822020-07-17 17:45:28,522 [myid:127.0.0.1:2182] - WARN  [main-SendThread(127.0.0.1:2182):ClientCnxn$SendThread@1235] - Client session timed out, have not heard from server in 30003ms for session id 0x02020-07-17 17:45:28,522 [myid:127.0.0.1:2182] - WARN  [main-SendThread(127.0.0.1:2182):ClientCnxn$SendThread@1278] - Session 0x0 for sever localhost/127.0.0.1:2182, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.org.apache.zookeeper.ClientCnxn$SessionTimeoutException: Client session timed out, have not heard from server in 30003ms for session id 0x0 at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1236)2020-07-17 17:45:30,115 [myid:127.0.0.1:2182] - INFO  [main-SendThread(127.0.0.1:2182):ClientCnxn$SendThread@1159] - Opening socket connection to server localhost/127.0.0.1:2182.2020-07-17 17:45:30,115 [myid:127.0.0.1:2182] - INFO  [main-SendThread(127.0.0.1:2182):ClientCnxn$SendThread@1161] - SASL config status: Will not attempt to authenticate using SASL (unknown error)2020-07-17 17:45:30,116 [myid:127.0.0.1:2182] - INFO  [main-SendThread(127.0.0.1:2182):ClientCnxn$SendThread@993] - Socket connection established, initiating session, client: /127.0.0.1:64223, server: localhost/127.0.0.1:2182
[zk: 127.0.0.1:2182(CONNECTING) 0]

./zkServer.sh status
JMX enabled by default
Using config: /data/software/zookeeper/zookeeper-two/bin/../conf/zoo.cfg
Mode: leader
{code}
However, we cannot connect to s2,s3, but we can connect to s1 and create znodes
{code:java}
[zk: 127.0.0.1:2181(CONNECTED) 0] getAllChildrenNumber /
1645338
[zk: 127.0.0.1:2181(CONNECTED) 1] create /test-666
Created /test-666
[zk: 127.0.0.1:2181(CONNECTED) 2] create /test-777 ""777""
Created /test-777
[zk: 127.0.0.1:2181(CONNECTED) 3] get /test-777
777
[zk: 127.0.0.1:2181(CONNECTED) 4] create /test-888 ""88""
Created /test-888
[zk: 127.0.0.1:2181(CONNECTED) 5] get /test-888
88
{code}
Attach the logs for debugging

 

 

 ",[],Bug,ZOOKEEPER-3892,Major,Ling Mao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,client cannot connect to server when we do a large scala benchmark test,2020-07-18T09:33:08.000+0000,[],1.0
Enrico Olivelli,[],2020-07-15T17:18:47.000+0000,Mohammad Arshad,"ZKCli commands give error message ""Authentication is not valid"" for insufficient permissions .(when KeeperException.NoAuthException is thrown). This is misleading message. 

Steps: to get the error
{code:java}
[zk: vm1:2181(CONNECTED) 0] create /b
Created /b
[zk: vm1:2181(CONNECTED) 1] getAcl /b
'world,'anyone
: cdrwa
[zk: vm1:2181(CONNECTED) 2] setAcl /b world:anyone:ra
[zk: vm1:2181(CONNECTED) 3] getAcl /b
'world,'anyone
: ra
[zk: vm1:2181(CONNECTED) 4] create /b/b1
Authentication is not valid : /b/b1
[zk: vm1:2181(CONNECTED) 5]
{code}
 

I think we should change  message ""Authentication is not valid"" to ""Insufficient permission""
 ","[<JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3891,Major,Mohammad Arshad,Fixed,2020-07-29T10:01:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"ZKCli commands give wrong error message ""Authentication is not valid"" for insufficient permissions",2022-01-06T07:00:05.000+0000,[],1.0
,[],2020-07-14T12:22:05.000+0000,Lea Morschel,"When a ZooKeeper client session disappears, the associated ephemeral node that is used for leader election is occasionally not deleted and persists (indefinitely, it seems).
 A leader election process may select such a stale node to be the leader. In a scenario where there is a redundant service that takes action when acquiring leadership by means of a ZooKeeper election process, this leads to none of the services being active when the stale ephemeral node is elected.

One of the scenarios where such a stale ephemeral node is created can be triggered by force-killing the  ZooKeeper server ({{kill -9 <pid}}>) as well as the client, which leads to the session being recreated after restarting the server on its side, even though the actual client session is gone. This node even persists after regular restarts from now on. No pings from its owner-session are received, compared to an active one, yet the session never expires. This scenario involves a single ZooKeeper server, but the problem has also been observed in a cluster of three.

When the ephemeral node is first persisted after restarting (and every restart thereafter), the following is observable in the ZooKeeper server logs. The scenario involves a local ZooKeeper server (version 3.5.7) and a single leader election participant.
{code:java}
Opening datadir:/my/path snapDir:/my/path
zookeeper.snapshot.trust.empty : true
tickTime set to 2000
minSessionTimeout set to 4000
maxSessionTimeout set to 40000
zookeeper.snapshotSizeFactor = 0.33
Reading snapshot /my/path/version-2/snapshot.71
Created new input stream /my/path/version-2/log.4b
Created new input archive /my/path/version-2/log.4b
EOF exception java.io.EOFException: Failed to read /my/path/version-2/log.4b
Created new input stream /my/path/version-2/log.72
Created new input archive /my/path/version-2/log.72
Ignoring processTxn failure hdr: -1 : error: -110
Ignoring processTxn failure hdr: -1, error: -110, path: null
Ignoring processTxn failure hdr: -1 : error: -110
Ignoring processTxn failure hdr: -1, error: -110, path: null
Ignoring processTxn failure hdr: -1 : error: -110
Ignoring processTxn failure hdr: -1, error: -110, path: null
Ignoring processTxn failure hdr: -1 : error: -110
Ignoring processTxn failure hdr: -1, error: -110, path: null
Ignoring processTxn failure hdr: -1 : error: -110
Ignoring processTxn failure hdr: -1, error: -110, path: null
Ignoring processTxn failure hdr: -1 : error: -110
Ignoring processTxn failure hdr: -1, error: -110, path: null
EOF exception java.io.EOFException: Failed to read /my/path/version-2/log.72
Snapshotting: 0x8b to /my/path/version-2/snapshot.8b
ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes
autopurge.snapRetainCount set to 3
autopurge.purgeInterval set to 3{code}
Could this problem be solved by ZooKeeper checking the sessions for each participating node before starting a leader election?
So far only manual intervention (removing the stale ephemeral node) seems to ""fix"" the issue temporarily.",[],Bug,ZOOKEEPER-3890,Major,Lea Morschel,Not A Bug,2020-10-21T15:22:31.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"Ephemeral node not deleted after session is gone, then elected as leader",2020-10-21T15:22:31.000+0000,"[<JIRA Version: name='3.4.14', id='12343587'>, <JIRA Version: name='3.5.7', id='12346098'>]",6.0
,[],2020-07-14T10:48:04.000+0000,Guo Jiwei,"Variable finished should add volatile in ReadOnlyRequestProcessor:
```
private volatile boolean finished = false;
```",[],Bug,ZOOKEEPER-3889,Minor,Guo Jiwei,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Add volatile for variable finished in ReadOnlyRequestProcessor,2021-04-26T07:11:39.000+0000,[],1.0
,[],2020-07-14T09:01:55.000+0000,Guo Jiwei,"If not specify the source and target version, compile from the master will fail:
zookeeper/zookeeper-jute/src/main/java/org/apache/jute/compiler/CppGenerator.java:[65,13] try-with-resources is not supported in -source 1.6",[],Bug,ZOOKEEPER-3888,Minor,Guo Jiwei,Invalid,2020-07-23T15:49:27.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Add source and target version for compile plugin,2020-08-13T00:04:59.000+0000,[],3.0
Mohammad Arshad,"[<JIRA Component: name='scripts', id='12312384'>]",2020-07-13T22:23:04.000+0000,Mohammad Arshad,"When only SSL client port is enabled, zkServer.sh status command should use secureClientPortAddress value instead of clientPortAddress. 

As clientPortAddress is not configured, zkServer.sh status command tries to connect to localhost and fails.

ZOOKEEPER-3818 has addressed the port issue, same way we should address the host issue.","[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>]",Bug,ZOOKEEPER-3887,Major,Mohammad Arshad,Fixed,2021-03-27T17:10:49.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,In SSL-only server zkServer.sh status command should use secureClientPortAddress instead of clientPortAddress,2022-01-05T10:32:14.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",2.0
,"[<JIRA Component: name='c client', id='12312380'>]",2020-07-13T17:29:43.000+0000,Tudor Bosman,"We're encountering the following segfault (stack abridged).

This happens because the watcher hashtable has no locking, and is accessed concurrently from multiple threads:
 - the thread doing zoo_aremove_watches, and
 - the IO thread adding / firing watchers

We encountered this with zookeeper 3.5.8, but by code inspection the code appears the same in 3.6.

 *** Aborted at 1594473472 (Unix time, try 'date -d @1594473472') ***
*** Signal 11 (SIGSEGV) (0xae000000aa) received by PID 199 (pthread TID 0x7f1d64667700) (linux TID 1273) (code: address not mapped to object), stack trace: ***
    @ 00007f1d98dfc8b3 folly::symbolizer::(anonymous namespace)::signalHandler(int, siginfo_t*, void*)
                       /src/folly/folly/experimental/symbolizer/SignalHandler.cpp:431
    @ 00007f1d95e6c89f (unknown)
    @ 00007f1d8f73de1e containsWatcher.part.3
                       /src/zookeeper/zookeeper-client/zookeeper-client-c/src/zk_hashtable.c:152
    @ 00007f1d8f73e806 pathHasWatcher
                       /src/zookeeper/zookeeper-client/zookeeper-client-c/src/zk_hashtable.c:456
    @ 00007f1d8f7382dd aremove_watches
                       /src/zookeeper/zookeeper-client/zookeeper-client-c/src/zookeeper.c:4260
    @ 00007f1d8f738f82 zoo_aremove_watches
                       /src/zookeeper/zookeeper-client/zookeeper-client-c/src/zookeeper.c:5131
","[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",Bug,ZOOKEEPER-3885,Major,Tudor Bosman,Fixed,2020-07-31T14:47:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zoo_aremove_watches segfault: zk_hashtable needs locking!,2020-09-10T10:43:36.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",1.0
Ling Mao,"[<JIRA Component: name='server', id='12312382'>]",2020-07-12T01:48:49.000+0000,Ling Mao," 
{code:java}
1. we can create znodes normally

2020-07-12 09:52:22,663 [myid:localhost:2181] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1420] - Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, session id = 0x10001a6779c0b5a, negotiated timeout = 30000WATCHER::WatchedEvent state:SyncConnected type:None path:null
[zk: localhost:2181(CONNECTED) 0] create /test5
Created /test5
[zk: localhost:2181(CONNECTED) 1] create /test6
Created /test6
[zk: localhost:2181(CONNECTED) 2]                                                                                  
{code}
{code:java}
2. However, ""zkServer.sh status"" doesn't work, after restarting the server, it works

[root@zk-test001 bin]# ./zkServer.sh status
/usr/bin/java
ZooKeeper JMX enabled by default
Using config: /data/software/apache-zookeeper-3.6.1-bin/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost.
Error contacting service. It is probably not running.{code}
{code:java}
GC info

[root@zk-test001 conf]# jstat -gcutil 15206 200
  S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT   
 18.49   0.00  92.26  47.75  95.61  90.82    280   22.735     2    5.363   28.098
 18.49   0.00  92.26  47.75  95.61  90.82    280   22.735     2    5.363   28.098
 18.49   0.00  92.26  47.75  95.61  90.82    280   22.735     2    5.363   28.098
 18.49   0.00  92.26  47.75  95.61  90.82    280   22.735     2    5.363   28.098
 18.49   0.00  92.26  47.75  95.61  90.82    280   22.735     2    5.363   28.098
 18.49   0.00  92.26  47.75  95.61  90.82    280   22.735     2    5.363   28.098
 18.49   0.00  92.26  47.75  95.61  90.82    280   22.735     2    5.363   28.098
 18.49   0.00  92.26  47.75  95.61  90.82    280   22.735     2    5.363   28.098
 18.49   0.00  92.26  47.75  95.61  90.82    280   22.735     2    5.363   28.098
 18.49   0.00  92.26  47.75  95.61  90.82    280   22.735     2    5.363   28.098
 18.49   0.00  92.26  47.75  95.61  90.82    280   22.735     2    5.363   28.098
 18.49   0.00  92.26  47.75  95.61  90.82    280   22.735     2    5.363   28.098
 18.49   0.00  92.26  47.75  95.61  90.82    280   22.735     2    5.363   28.098
 18.49   0.00  92.26  47.75  95.61  90.82    280   22.735     2    5.363   28.098
 18.49   0.00  92.26  47.75  95.61  90.82    280   22.735     2    5.363   28.098
 18.49   0.00  92.26  47.75  95.61  90.82    280   22.735     2    5.363   28.098
 18.49   0.00  92.26  47.75  95.61  90.82    280   22.735     2    5.363   28.098
{code}",[],Bug,ZOOKEEPER-3884,Major,Ling Mao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zkServer.sh status doesn't work when we do a large scale benchmark test,2020-07-12T08:33:42.000+0000,[],1.0
,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='quorum', id='12312379'>]",2020-07-07T20:40:04.000+0000,Bharath B,"Have deployed zookeeper(v3.5.7) included in the kafka(v2.5.0) bundle as containers using kubernetes, where 3 instances of each kafka and zookeeper is deployed. Interaction among kafka brokers, with kafka client and kafka with zookeeper are all TLS based and is working as expected. But zookeeper quorum formation fails with TLS handshake error, as the server name in the https request does not match with any of the SANs in the certificate configured for zookeeper server. Server name in the request is of the form ""x-x-x-x.kubernetes.default.svc.cluster.local"" (where x-x-x-x is the IP address of the POD), and I am unable to understand the reason behind pre-pending FQDN with a IP address. Could anyone please let me know, if I am missing any configuration or the behavior observed is as designed. Like in kafka, we don't have ""ssl.client.auth"" confiuration parameter for zookeeper, so I am not so sure, if it's client or server validation failing during the handshake.

Please find below the extract of the error logs from the zookeeper1 POD

{color:#de350b}ERROR Failed to verify host address: x.x.x.x (org.apache.zookeeper.common.ZKTrustManager){color}
 {color:#de350b}javax.net.ssl.SSLPeerUnverifiedException: Certificate for <x.x.x.x> doesn't match any of the subject alternative names: [zookeeper, zookeeper1, zookeeper2, zookeeper3, zookeeper1.default.svc.cluster.local, zookeeper2.default.svc.cluster.local, zookeeper3.default.svc.cluster.local]{color}",[],Bug,ZOOKEEPER-3882,Critical,Bharath B,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper quorum formation fails when TLS is enabled in k8s env,2020-07-14T11:41:00.000+0000,"[<JIRA Version: name='3.5.7', id='12346098'>]",1.0
,"[<JIRA Component: name='java client', id='12312381'>]",2020-07-06T07:41:48.000+0000,FengGao,"We use curator 4.2.0 and zookeeper 3.4.13 as client, when we call curator PathChildrenCache.start(), It calls Zookeeper.getData(), but getData throw an NPE, please help check is it an BUG? Thanks.

""2020-07-02 00:17:31,697"" [UTC:20200702T001731+0700]|Error||DeployWorkThread-6|CONFIG > Failed to watch config of unit:ae9224e172952800c3b7a,type:global_configuration. com.huawei.itpaas.config.sdk.service.ConfigurationService.doCommonConfigWatcher(ConfigurationService.java:999)""2020-07-02 00:17:31,697"" [UTC:20200702T001731+0700]|Error||DeployWorkThread-6|CONFIG > Failed to watch config of unit:ae9224e172952800c3b7a,type:global_configuration. com.huawei.itpaas.config.sdk.service.ConfigurationService.doCommonConfigWatcher(ConfigurationService.java:999)java.lang.NullPointerException: null at org.apache.zookeeper.server.DataTree.copyStat(DataTree.java:253) ~[zookeeper-3.4.13.fixed.20200331.jar:3.4.13--1] at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1226) ~[zookeeper-3.4.13.fixed.20200331.jar:3.4.13--1] at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:327) ~[curator-framework-4.2.0.jar:4.2.0] at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:316) ~[curator-framework-4.2.0.jar:4.2.0] at org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:64) ~[curator-client-4.2.0.jar:?] at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100) ~[curator-client-4.2.0.jar:?] at org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:313) ~[curator-framework-4.2.0.jar:4.2.0] at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:304) ~[curator-framework-4.2.0.jar:4.2.0] at org.apache.curator.framework.imps.GetDataBuilderImpl$2.forPath(GetDataBuilderImpl.java:145) ~[curator-framework-4.2.0.jar:4.2.0] at org.apache.curator.framework.imps.GetDataBuilderImpl$2.forPath(GetDataBuilderImpl.java:141) ~[curator-framework-4.2.0.jar:4.2.0] at org.apache.curator.framework.recipes.cache.PathChildrenCache.internalRebuildNode(PathChildrenCache.java:633) ~[curator-recipes-4.2.0.jar:4.2.0] at org.apache.curator.framework.recipes.cache.PathChildrenCache.rebuild(PathChildrenCache.java:335) ~[curator-recipes-4.2.0.jar:4.2.0] at org.apache.curator.framework.recipes.cache.PathChildrenCache.start(PathChildrenCache.java:304) ~[curator-recipes-4.2.0.jar:4.2.0] at com.huawei.itpaas.config.sdk.service.ConfigurationService.doCommonConfigWatcher(ConfigurationService.java:965) [com.huawei.itpaas.configservice.sdk-6.1.4-RELEASE.jar:?] at com.huawei.itpaas.config.sdk.impl.ConfigPluginImpl.watchCommonGlobal(ConfigPluginImpl.java:383) [com.huawei.itpaas.configservice.sdk-6.1.4-RELEASE.jar:?] at com.huawei.itpaas.agent.deploy.plugin.configuration.support.ConfigMgmtImpl.watchFiles(ConfigMgmtImpl.java:374) [com.huawei.itpaas.agent.deploy.plugin-6.1.4-RELEASE.jar:?] at com.huawei.itpaas.agent.deploy.plugin.engine.commands.bind.BindCommand.watchFiles(BindCommand.java:196) [com.huawei.itpaas.agent.deploy.plugin-6.1.4-RELEASE.jar:?] at com.huawei.itpaas.agent.deploy.plugin.engine.commands.bind.BindCommand.importSchema(BindCommand.java:187) [com.huawei.itpaas.agent.deploy.plugin-6.1.4-RELEASE.jar:?] at com.huawei.itpaas.agent.deploy.plugin.engine.commands.bind.BindCommand.run(BindCommand.java:70) [com.huawei.itpaas.agent.deploy.plugin-6.1.4-RELEASE.jar:?] at com.huawei.itpaas.agent.deploy.plugin.engine.commands.bind.BindCommand.run(BindCommand.java:1) [com.huawei.itpaas.agent.deploy.plugin-6.1.4-RELEASE.jar:?] at com.huawei.itpaas.agent.deploy.engine.run.CommandRunner.runOneCommand(CommandRunner.java:246) [com.huawei.itpaas.agent.deploy.engine-6.1.4-RELEASE.jar:?] at com.huawei.itpaas.agent.deploy.engine.run.CommandRunner.access$1(CommandRunner.java:234) [com.huawei.itpaas.agent.deploy.engine-6.1.4-RELEASE.jar:?] at com.huawei.itpaas.agent.deploy.engine.run.CommandRunner$AsyncScheduler.run(CommandRunner.java:198) [com.huawei.itpaas.agent.deploy.engine-6.1.4-RELEASE.jar:?] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_242] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_242] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]",[],Bug,ZOOKEEPER-3881,Critical,FengGao,Cannot Reproduce,2021-02-15T15:59:04.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,org.apache.zookeeper.ZooKeeper.getData may throw NullPointerException when copyStat with null response stat.,2021-02-15T16:00:18.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",3.0
,"[<JIRA Component: name='c client', id='12312380'>]",2020-07-04T11:09:46.000+0000,David Vujic,"I have tried to follow the [installation guide|https://github.com/apache/zookeeper/tree/master/zookeeper-client/zookeeper-client-c], but cannot understand how to make the *autoreconf* step to work. It is failing at the AM_PATH_CPPUNIT missing. Have tried the suggestions in the guide, but without success. 

The OS I have used are Mac OS X and Elementary OS. Also, went trying using CentOS in a docker container, but cannot figure out how to add all requirements (autoconf missing etc).

Previous builds of the Zookeeper C Client (3.5.x) have worked fine though, I guess something has changed?

For the development of node-zookeeper, this is a blocker.

Do you have any suggestions how to solve this?",[],Bug,ZOOKEEPER-3879,Major,David Vujic,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,C Client: how to build?,2021-01-10T12:16:47.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>]",4.0
Mohammad Arshad,[],2020-07-03T07:23:19.000+0000,Mohammad Arshad,"Clients should be able to connect to ZooKeeper  with or without square bracket around IPV6 in connection string. 
127:0:0:0:0:0:0:1:2181 and [127:0:0:0:0:0:0:1]:2181 both should work.

After ZOOKEEPER-3106 fix connection with  127:0:0:0:0:0:0:1:2181 fails
I think we should support both  with or without square bracket around IPV6.
","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",Bug,ZOOKEEPER-3878,Major,Mohammad Arshad,Fixed,2020-07-13T10:12:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Client connection fails if IPV6 is not enclosed in square brackets,2020-09-10T10:43:36.000+0000,[],2.0
Mohammad Arshad,"[<JIRA Component: name='jmx', id='12312451'>]",2020-07-03T07:00:22.000+0000,Mohammad Arshad,"JMX metrics Bean RemotePeerBean should enclose ipv6 host in square bracket same as LocalPeerBean 
 Changes done in ZOOKEEPER-3057 for LocalPeerBean should also be done for
RemotePeerBean","[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-3877,Minor,Mohammad Arshad,Fixed,2021-03-02T06:08:48.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,JMX Bean RemotePeerBean should enclose IPV6 host in square bracket same as LocalPeerBean ,2021-03-28T08:54:57.000+0000,[],1.0
Mohammad Arshad,[],2020-07-03T06:52:33.000+0000,Mohammad Arshad,"When server configuration has client IP and port in it as below

{code:java}
server.1=127:0:0:0:0:0:0:1:2890:3890:participant;127:0:0:0:0:0:0:1:2181
{code}
Then zkServer.sh status command fails. It is not able to parse the host and ip.","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",Bug,ZOOKEEPER-3876,Minor,Mohammad Arshad,Fixed,2020-07-29T10:32:15.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkServer.sh status command fails when IPV6 is configured,2020-09-10T10:43:45.000+0000,[],1.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2020-07-02T02:50:08.000+0000,Filip Niksic,"Using a [tool|https://github.com/fniksic/zootester] that I wrote for testing ZooKeeper, I discovered the following scenario which causes ZooKeeper to violate sequential consistency.

Initially, start an ensemble with 3 servers called A, B, and C, and initialize 2 znodes called /key0 and /key1 to 0. Stop all servers.
 # Start A and B. Stop A and at the same time initiate setting /key1 to 101 on B. Stop B.
 # Start A and B and stop them. In this step it seems that /key1 == 101 is successfully propagated to A.
 # Start A and C. Initiate a conditional write on A: if /key1 == 101, set /key0 to 200. The write seems to be successful. Stop the servers.
 # Start A, B, and C. Initiate a conditional write on B: if /key1 == 0, set /key1 to 301. Surprisingly, the write succeeds. Stop the servers.

Finally, start all servers and read the values of /key0 and /key1 on all servers. They will be 200 and 301.

Even if we assume that any write can fail, the set of possible values for /key0 and /key1 under sequential consistency consists of (0, 0), (0, 101), (200, 101), and (0, 301). The values (200, 301) should not be possible: if /key0 == 200, then setting /key1 to 101 must have succeeded. On the other hand, if /key1 == 301, then setting /key1 to 101 must have failed, as this write happens before reading /key1 == 0.

The cause of this bug is probably related to the cause of ZOOKEEPER-2832, which was reported 3 years ago and is still open. You will notice that the above scenario is similar to the scenario reported there. Indeed, my tool randomly explores similar scenarios with conditional and unconditional writes under random server crashes, in search for sequential consistency violations.

I have attached a patch with a test that reproduces this bug. The affected version is 3.5.8. I suspect that 3.6.1 is also affected, but unfortunately, I'm having trouble compiling that version.",[],Bug,ZOOKEEPER-3875,Major,Filip Niksic,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Sequential consistency violation,2020-09-03T02:47:00.000+0000,"[<JIRA Version: name='3.5.8', id='12346950'>]",3.0
,"[<JIRA Component: name='java client', id='12312381'>]",2020-06-27T05:57:31.000+0000,Ling Mao,"I also attach the client side logs here
{code:java}
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /bench-watch/12318org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /bench-watch/123182020-06-27 13:42:31,898 [myid:127.0.0.1:2181] - WARN  [pool-1-thread-11-SendThread(127.0.0.1:2181):ClientCnxn$SendThread@1278] - Session 0x1001a2fab59004c for sever localhost/127.0.0.1:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.java.lang.NullPointerException at org.apache.zookeeper.ClientCnxnSocketNIO.enableWrite(ClientCnxnSocketNIO.java:377) at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:353) at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1268) at org.apache.zookeeper.KeeperException.create(KeeperException.java:102) at org.apache.zookeeper.KeeperException.create(KeeperException.java:54) at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1856)
{code}
 
{code:java}
2020-06-27 13:51:04,334 [myid:] - ERROR [pool-1-thread-12:ClientCnxn@1590] - Timeout error occurred for the packet 'clientPath:null serverPath:null finished:false header:: 13886,3  replyHeader:: 0,0,0  request:: '/bench-watch/12815,T  response::  '.2020-06-27 13:51:04,334 [myid:] - ERROR [pool-1-thread-12:ClientCnxn@1590] - Timeout error occurred for the packet 'clientPath:null serverPath:null finished:false header:: 13886,3  replyHeader:: 0,0,0  request:: '/bench-watch/12815,T  response::  '.2020-06-27 13:51:04,334 [myid:] - ERROR [pool-1-thread-29:ClientCnxn@1590] - Timeout error occurred for the packet 'clientPath:null serverPath:null finished:false header:: 13395,3  replyHeader:: 0,0,0  request:: '/bench-watch/12331,T  response::  '.2020-06-27 13:51:04,335 [myid:127.0.0.1:2181] - WARN  [pool-1-thread-12-SendThread(127.0.0.1:2181):ClientCnxn$SendThread@1278] - Session 0x1001a2fab59003d for sever localhost/127.0.0.1:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.java.nio.channels.CancelledKeyException at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:73) at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:77) at org.apache.zookeeper.ClientCnxnSocketNIO.enableWrite(ClientCnxnSocketNIO.java:377) at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:353) at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1268)2020-06-27 13:51:04,336 [myid:127.0.0.1:2181] - INFO  [pool-1-thread-39-SendThread(127.0.0.1:2181):ClientCnxn$SendThread@1159] - Opening socket connection to server localhost/127.0.0.1:2181.2020-06-27 13:51:04,336 [myid:127.0.0.1:2181] - INFO  [pool-1-thread-39-SendThread(127.0.0.1:2181):ClientCnxn$SendThread@1161] - SASL config status: Will not attempt to authenticate using SASL (unknown error)2020-06-27 13:51:04,337 [myid:127.0.0.1:2181] - INFO  [pool-1-thread-39-SendThread(127.0.0.1:2181):ClientCnxn$SendThread@993] - Socket connection established, initiating session, client: /127.0.0.1:62205, server: localhost/127.0.0.1:21812020-06-27 13:51:04,339 [myid:127.0.0.1:2181] - INFO  [pool-1-thread-39-SendThread(127.0.0.1:2181):ClientCnxn$SendThread@1426] - Session establishment complete on server localhost/127.0.0.1:2181, session id = 0x1001a2fab59005a, negotiated timeout = 40000
{code}",[],Bug,ZOOKEEPER-3873,Minor,Ling Mao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ClientCnxnSocketNIO#enableWrite throws NullPointerException,2022-02-03T08:50:20.000+0000,[],2.0
,[],2020-06-25T12:47:14.000+0000,ko christ,"h2. Description

In a nutshell, my dockerized Zookeeper installation stops working on cluster leader changes.

The cluster responds to 4-letter commands but when I force a leader change, the clients timeout like forever. A workaround is to run follow up restarts which resolve the issue, usually when the leader returns to the previous state. This affects the high availability of the cluster.
h2. Example

For example, assuming that a 3-node ZK cluster has the following initial state (*State A*). All Zookeeper clients work fine in this state.
||ZK 1||ZK 2||ZK 3||
|follower|follower|*leader*|

 

and a restart occurs and Zookeeper ends up to this (*State B*)
||ZK 1||ZK 2||ZK 3||
|follower|*leader*|follower|

In State B, all client attempts fail to connect and they timeout, like forever. Follow up leader restarts may resolve the issue, usually (but not always) due to a *return to the previous state A*. 
h2. Affected versions

I have verified that this bug with dockerized Zookeeper in replicated mode on
 * *{{3.5.5}}*
 * *{{3.5.8}}*
 * *{{3.6.1}}*

h2. Reproduce

{color:#de350b}Note: On all the examples below replace tortoise with your hostname.{color}

Deploy a 3-node Zookeeper cluster (could be 5-node) using the official 3.5.8 image.
{code:java}
docker run -d --name=zkcl01 -p 1493:1493 -p 1494:1494 -p 1495:1495 -h tortoise-zkcl01 -e HOSTNAME=tortoise -e ZOO_PORT=1493 -e ZOO_LOG4J_PROP=""INFO,CONSOLE,ROLLINGFILE"" -e ZOO_4LW_COMMANDS_WHITELIST=srvr,ruok,mntr,stat -e ZOO_STANDALONE_ENABLED=False -e ZOO_SERVERS=""server.1=0.0.0.0:1495:1494;1493 server.2=tortoise:1498:1497;1496 server.3=tortoise:1501:1500;1499"" -e ZOO_MY_ID=1 zookeeper:3.5.8
docker run -d --name=zkcl02 -p 1496:1496 -p 1497:1497 -p 1498:1498 -h tortoise-zkcl02 -e HOSTNAME=tortoise -e ZOO_PORT=1496 -e ZOO_LOG4J_PROP=""INFO,CONSOLE,ROLLINGFILE"" -e ZOO_4LW_COMMANDS_WHITELIST=srvr,ruok,mntr,stat -e ZOO_STANDALONE_ENABLED=False -e ZOO_SERVERS=""server.1=tortoise:1495:1494;1493 server.2=0.0.0.0:1498:1497;1496 server.3=tortoise:1501:1500;1499"" -e ZOO_MY_ID=2 zookeeper:3.5.8
docker run -d --name=zkcl03 -p 1499:1499 -p 1500:1500 -p 1501:1501 -h tortoise-zkcl03 -e HOSTNAME=tortoise -e ZOO_PORT=1499 -e ZOO_LOG4J_PROP=""INFO,CONSOLE,ROLLINGFILE"" -e ZOO_4LW_COMMANDS_WHITELIST=srvr,ruok,mntr,stat -e ZOO_STANDALONE_ENABLED=False -e ZOO_SERVERS=""server.1=tortoise:1495:1494;1493 server.2=tortoise:1498:1497;1496 server.3=0.0.0.0:1501:1500;1499"" -e ZOO_MY_ID=3 zookeeper:3.5.8
{code}
 

Monitor cluster's state with the 4-letter {{srvr}} command
{code:java}
watch -n 1 'for i in 1493 1496 1499; do echo $i; echo srvr | nc tortoise $i ; echo; done'{code}
 

Verify that you can connect to the cluster successfully using any client (zkCli.sh in this case)
{code:java}
docker exec -ti zkcl01 bin/zkCli.sh -server tortoise:1493,tortoise:1496,tortoise:1499 ls /
...
...
WatchedEvent state:SyncConnected type:None path:null
[zookeeper]{code}
 

Stop/Start the leader node (based on {{srvr}} output from the previous step) in order to force a leader change.
{code:java}
docker stop zkcl03; sleep 15; docker start zkcl03{code}
 

Verify that the client now fails to connect and they timeout.
{code:java}
docker exec -ti zkcl01 bin/zkCli.sh -server tortoise:1493,tortoise:1496,tortoise:1499 ls /
...
...
closing socket connection and attempting reconnect
KeeperErrorCode = ConnectionLoss for /{code}
 

Finally, -restart- stop/sleep/start the leader a few more times only to verify that the client succeeds usually when the leader goes back to the initial state.

 

This must be a bug unless there is a misconfiguration that I am missing.",[],Bug,ZOOKEEPER-3871,Major,ko christ,Works for Me,2020-06-26T12:20:35.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper clients fail on dockerized Zookeeper leader changes,2020-06-26T12:20:35.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",1.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2020-06-24T17:57:44.000+0000,Fangmin Lv,"The current ReadOnlyZooKeeperServer was implemented with PrepRequestProcessor, which will prepare txn even in read only mode.

Also, it will fail the write request in ReadOnlyRequestProcessor, which will cause out of order response issue. 

This Jira is going to fix this by replacing the PrepRequestProcessor with CommitProcessor, same as the other learner set up.",[],Bug,ZOOKEEPER-3870,Major,Fangmin Lv,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Avoid out of order response and txns created with ReadOnlyZooKeeperServer,2020-10-01T03:56:07.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>]",2.0
,"[<JIRA Component: name='c client', id='12312380'>]",2020-06-18T20:29:27.000+0000,Tudor Bosman,"Using the multithreaded C library.

Due to a race condition between `zookeeper_close` and `do_completion`, it is possible for `zookeeper_close` to close the handle without running the pending completions, causing a completion leak (not calling the client's completion callbacks) and a memory leak.

`zookeeper_close` sets `close_requested` here: [https://github.com/apache/zookeeper/blob/master/zookeeper-client/zookeeper-client-c/src/zookeeper.c#L3752]; after that point, the completion thread can exit at any time: [https://github.com/apache/zookeeper/blob/8da9c723ac1a889c989ecefada722ed858049537/zookeeper-client/zookeeper-client-c/src/mt_adaptor.c#L473]

But, even after `zookeeper_close`, completions can still be added to `completions_to_process` from two different places: the IO thread (which is still running) and `zookeeper_close` itself, via `free_completions`: [https://github.com/apache/zookeeper/blob/master/zookeeper-client/zookeeper-client-c/src/zookeeper.c#L3760]

I have a fix (I'll update this issue with the pull request) that uses a separate `terminate_completion` flag in `adaptor_threads` instead of `zh->close_requested` to make the IO thread exit.",[],Bug,ZOOKEEPER-3868,Major,Tudor Bosman,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Completion leak in zookeeper_close,2020-07-29T17:38:55.000+0000,"[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",1.0
,[],2020-06-16T11:24:11.000+0000,Mrinal,"Unable to change permission of zookeper node having read access 

1.open zookeeper-shell.sh with corresponding IP

getAcl /zookeeper
'world,'anyone
: r

while changing the same its saying Authentication is not valid

setAcl /zookeeper world:anyone:cdrwa
Authentication is not valid : /zookeeper

 ",[],Bug,ZOOKEEPER-3866,Minor,Mrinal,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Unable to change permission of zookeper node having read access,2021-01-06T11:21:15.000+0000,[],4.0
Mate Szalay-Beko,[],2020-06-16T08:39:56.000+0000,Mate Szalay-Beko,The current branch-3.5 can not be used to build Curator 4.3. In the fix of ZOOKEEPER-3829 I accidentally changed a public constructor of ZooKeeper server. I didn’t realize Curator is using it for testing (and maybe other Apache projects are doing so as well).,"[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",Bug,ZOOKEEPER-3865,Blocker,Mate Szalay-Beko,Fixed,2020-06-19T07:55:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,fix backward-compatibility for ZooKeeperServer constructor,2020-09-10T10:43:39.000+0000,[],1.0
Jie Huang,"[<JIRA Component: name='server', id='12312382'>]",2020-06-15T23:37:51.000+0000,Jie Huang,"These Ops are not read operations. They will modify the state, ",[],Bug,ZOOKEEPER-3864,Minor,Jie Huang,Won't Fix,2020-06-24T02:59:18.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Reject create/renew/close global session in RO mode,2020-08-29T16:00:24.000+0000,[],2.0
Jie Huang,"[<JIRA Component: name='server', id='12312382'>]",2020-06-15T23:19:59.000+0000,Jie Huang,"ReadOnlyZooKeeperServer is using the default SessionTrackerImpl, which tracks and expires the global sessions, which should be tracked and expired only by the leader. This diff changes the code to use LearnerSessionTracker, which only tracks and expires local session.","[<JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3863,Minor,Jie Huang,Fixed,2020-09-17T05:36:05.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Do not track global sessions in ReadOnlyZooKeeperServer,2021-03-28T08:55:25.000+0000,[],2.0
,"[<JIRA Component: name='other', id='12333125'>]",2020-06-15T08:04:03.000+0000,hexiaoxiao,!搜狗截图20200615155052.png!,[],Bug,ZOOKEEPER-3861,Major,hexiaoxiao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,log.zxid file is very large size. I want to know the reason and solution.,2020-06-16T01:36:10.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",1.0
Tamas Penzes,[],2020-06-10T15:19:47.000+0000,Tamas Penzes,Since the parent pom's version was forgotten to be synchronized when the cherry-pick got done the branch-3.6 doesn't build at the moment.,"[<JIRA Version: name='3.6.2', id='12347809'>]",Bug,ZOOKEEPER-3857,Blocker,Tamas Penzes,Fixed,2020-06-10T16:07:17.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeper 3.6 doesn't build after Curator test committed,2020-09-10T10:43:52.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2020-05-24T02:14:57.000+0000,Zhuqi Jin,"We tested upgrading a single-node zookeeper from branch-3.4/branch-3.5 to branch-3.6, but the upgraded node failed to start.

The error message is shown as following:
{code:java}
2020-05-24 00:24:24,996 [myid:1] - ERROR [main:ZooKeeperServerMain@90] - Unexpected exception, exiting abnormally
java.io.IOException: No snapshot found, but there are log entries. Something is broken!
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:281)
        at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:285)
        at org.apache.zookeeper.server.ZooKeeperServer.loadData(ZooKeeperServer.java:484)
        at org.apache.zookeeper.server.ZooKeeperServer.startdata(ZooKeeperServer.java:655)
        at org.apache.zookeeper.server.NIOServerCnxnFactory.startup(NIOServerCnxnFactory.java:758)
        at org.apache.zookeeper.server.ServerCnxnFactory.startup(ServerCnxnFactory.java:130)
        at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:159)
        at org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:112)
        at org.apache.zookeeper.server.ZooKeeperServerMain.main(ZooKeeperServerMain.java:67)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:140)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:90)
2020-05-24 00:24:24,999 [myid:1] - INFO  [main:ZKAuditProvider@42] - ZooKeeper audit is disabled.
2020-05-24 00:24:25,001 [myid:1] - ERROR [main:ServiceUtils@42] - Exiting JVM with code 1 {code}
The error can be reproduced through the following steps:
 # Step1: Start a single-node zookeeper (compiled from either branch-3.4 or branch-3.5) with the following configuration(zoo.cfg):

{code:java}
tickTime=2000
initLimit=10
syncLimit=5
dataDir=/tmp/zookeeper
clientPort=2181
server.1=localhost:2888:3888{code}
 # Step2: Use a zookeeper stress testing tool - zk-smoketool ([https://github.com/phunt/zk-smoketest.git]) - to test this node. We invoked create, set, and get operations in zk-smoketool but not delete operation, so that generated data are left on disk.
 # Step3: Upgrade the node to branch-3.6 with the same configuration. After upgraded, as the log suggested, zookeeper failed to start.

We learned about ZOOKEEPER-3056 and ZOOKEEPER-3513, and added 
{code:java}
zookeeper.snapshot.trust.empty=true {code}
to branch-3.6's configuration(zoo.cfg), but it ran into the same failure.",[],Bug,ZOOKEEPER-3848,Major,Zhuqi Jin,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper upgrade fails due to missing snapshots on branch-3.6,2020-11-11T21:18:02.000+0000,"[<JIRA Version: name='3.6.2', id='12347809'>]",3.0
Mate Szalay-Beko,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2020-05-21T07:33:40.000+0000,kaushik srinivas,"With 
*reconfigEnabled = false (not explicitly setting, relying on the default value).*
 
Install 3 zookeeper servers with 3 zk information in all the 3 zookeeper quorum servers.
 
Do a rolling scale up of cluster from 3 to 5 with below steps.
 
1. Install 4th zookeeper with servers list of 1,2,3,4,5
2. Install 5th zookeeper with servers list of 1,2,3,4,5
3. Do a rolling restart of servers 1 2 & 3 with servers list of 1,2,3,4,5.
 
Result/Behavior: quorum is lost.
 
With this https://issues.apache.org/jira/browse/ZOOKEEPER-2819
description points at a PR [https://github.com/apache/zookeeper/pull/292]
which should have this issue of rolling restart fixed without dynamic reconfiguration feature enabled.
 
We still see quorum loss issues without dynamic reconfig feature enabled.
 
 ","[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",Bug,ZOOKEEPER-3842,Blocker,kaushik srinivas,Duplicate,2020-06-03T08:02:20.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Rolling scale up of zookeeper cluster does not work with reconfigEnabled=false,2020-09-10T10:43:49.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",4.0
Andor Molnar,"[<JIRA Component: name='server', id='12312382'>]",2020-05-18T13:09:14.000+0000,Andor Molnar,"This is the same issue as reported in

https://issues.apache.org/jira/browse/HTTPCLIENT-1906

For performance reasons we use a copy-and-pasted version of the HostnameVerifier, as a consequence we don't pick up these fixes automatically.

 ","[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",Bug,ZOOKEEPER-3832,Major,Andor Molnar,Fixed,2020-05-21T12:15:25.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZKHostnameVerifier rejects valid certificates with subjectAltNames,2020-09-10T10:43:44.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",1.0
Mate Szalay-Beko,"[<JIRA Component: name='server', id='12312382'>]",2020-05-15T17:18:00.000+0000,Keli Wang,"I have a zookeeper cluster with 3 nodes, node3 is the leader of the cluster.

 
{code:java}
server.1=node1
server.2=node2
server.3=node3 # current leader{code}
With dynamic reconfiguration disabled, I scale this cluster to 4 nodes with 2 steps:
 # Start node4 with new config, now node4 is a follower.
 # Modify config and restart node1, node2 and node3 one by one.

The new cluster config is:
{code:java}
server.1=node1
server.2=node2
server.3=node3 
server.4=node4 # current leader
{code}
After restart, node4 is the leader of this cluster. But I cannot connect to this cluster using zkCli now, zkCli keeps in CONNECTING status.

If I restart node4, node3 will be the new leader, and now I can connected to cluster using zkCli again.

After some digging, I find node4's Leader#allowedToCommit field is false, so this cluster won't commit any new proposals.

 

I have attached a zookeeper cluster to reproduce this problem. The cluster in the attachment can run in one single machine.

 ","[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",Bug,ZOOKEEPER-3830,Major,Keli Wang,Duplicate,2020-06-03T08:03:37.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"After add a new node, zookeeper cluster won't commit any proposal if this new node is leader",2020-09-10T10:43:55.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",3.0
Mate Szalay-Beko,"[<JIRA Component: name='server', id='12312382'>]",2020-05-15T03:12:54.000+0000,benwang li,"It's easy to reproduce this bug.
{code:java}
//代码占位符
 
Step 1. Deploy 3 nodes  A,B,C with configuration A,B,C .
Step 2. Deploy node ` D` with configuration  `A,B,C,D` , cluster state is ok now.
Step 3. Restart nodes A,B,C with configuration A,B,C,D, then the leader will be D, cluster hangs, but it can accept `mntr` command, other command like `ls /` will be blocked.

Step 4. Restart nodes D, cluster state is back to normal now.
 
{code}
 

We have looked into the code of 3.5.6 version, and we found it may be the issue of  `workerPool` .

The `CommitProcessor` shutdown and make `workerPool` shutdown, but `workerPool` still exists. It will never work anymore, yet the cluster still thinks it's ok.

 

I think the bug may still exist in master branch.

We have tested it in our machines by reset the `workerPool` to null. If it's ok, please assign this issue to me, and then I'll create a PR. 

 

 

 ","[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",Bug,ZOOKEEPER-3829,Major,benwang li,Fixed,2020-06-03T07:59:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Zookeeper refuses request after node expansion,2020-09-10T10:43:46.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",7.0
,"[<JIRA Component: name='java client', id='12312381'>]",2020-05-15T01:09:24.000+0000,Aishwarya Soni,"I have configured 5 nodes zookeeper cluster using 3.6.1 version in a docker containerized environment. As a part of some destructive testing, I restarted zookeeper leader. Now, re-election happened and all 5 nodes (containers) are back in good state with new leader. But when I login to one of the container and go inside zk Cli (./zkCli.sh) and run the cmd *ls /* I see below error,
 {color:#000000} {color}
 *{color:#000000}[zk: localhost:2181(CONNECTING) 1]{color}* 

*{color:#000000}[zk: localhost:2181(CONNECTING) 1] ls /{color}*

*{color:#000000}2020-05-14 23:48:26,556 [myid:localhost:2181] - WARN  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1229] - Client session timed out, have not heard from server in 30001ms for session id 0x0{color}*

*{color:#000000}2020-05-14 23:48:26,556 [myid:localhost:2181] - WARN  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1272] - Session 0x0 for sever localhost/127.0.0.1:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.{color}*

*{color:#000000}org.apache.zookeeper.ClientCnxn$SessionTimeoutException: Client session timed out, have not heard from server in 30001ms for session id 0x0{color}*

 *{color:#000000}at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1230){color}*

*{color:#000000}KeeperErrorCode = ConnectionLoss for /{color}*

*{color:#000000}[zk: localhost:2181(CONNECTING) 2] 2020-05-14 23:48:28,089 [myid:localhost:2181] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1154] - Opening socket connection to server localhost/127.0.0.1:2181.{color}*

*{color:#000000}2020-05-14 23:48:28,089 [myid:localhost:2181] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1156] - SASL config status: Will not attempt to authenticate using SASL (unknown error){color}*

*{color:#000000}2020-05-14 23:48:28,090 [myid:localhost:2181] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@986] - Socket connection established, initiating session, client: /127.0.0.1:60384, server: localhost/127.0.0.1:2181{color}*

*{color:#000000}2020-05-14 23:48:58,119 [myid:localhost:2181] - WARN  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1229] - Client session timed out, have not heard from server in 30030ms for session id 0x0{color}*

*{color:#000000}2020-05-14 23:48:58,120 [myid:localhost:2181] - WARN  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1272] - Session 0x0 for sever localhost/127.0.0.1:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.{color}*

*{color:#000000}org.apache.zookeeper.ClientCnxn$SessionTimeoutException: Client session timed out, have not heard from server in 30030ms for session id 0x0{color}*

 *{color:#000000}at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1230){color}*

*{color:#000000}2020-05-14 23:49:00,003 [myid:localhost:2181] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1154] - Opening socket connection to server localhost/127.0.0.1:2181.{color}*

*{color:#000000}2020-05-14 23:49:00,004 [myid:localhost:2181] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1156] - SASL config status: Will not attempt to authenticate using SASL (unknown error){color}*

*{color:#000000}2020-05-14 23:49:00,004 [myid:localhost:2181] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@986] - Socket connection established, initiating session, client: /127.0.0.1:32936, server: localhost/127.0.0.1:2181{color}*

*{color:#000000}2020-05-14 23:49:30,032 [myid:localhost:2181] - WARN  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1229] - Client session timed out, have not heard from server in 30029ms for session id 0x0{color}*

*{color:#000000}2020-05-14 23:49:30,033 [myid:localhost:2181] - WARN  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1272] - Session 0x0 for sever localhost/127.0.0.1:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.{color}*

*{color:#000000}org.apache.zookeeper.ClientCnxn$SessionTimeoutException: Client session timed out, have not heard from server in 30029ms for session id 0x0{color}*

 *{color:#000000}at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1230){color}*

*{color:#000000}2020-05-14 23:49:31,230 [myid:localhost:2181] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1154] - Opening socket connection to server localhost/127.0.0.1:2181.{color}*

*{color:#000000}2020-05-14 23:49:31,230 [myid:localhost:2181] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1156] - SASL config status: Will not attempt to authenticate using SASL (unknown error){color}*

*{color:#000000}2020-05-14 23:49:31,230 [myid:localhost:2181] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@986] - Socket connection established, initiating session, client: /127.0.0.1:33766, server: localhost/127.0.0.1:2181{color}*

{color:#000000}Does anyone know what could possibly be wrong? For reference: https://issues.apache.org/jira/browse/ZOOKEEPER-2164{color}

This behavior is observed on all the nodes when the leader is restarted. All is good when a follower is restarted.",[],Bug,ZOOKEEPER-3828,Major,Aishwarya Soni,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zookeeper clients gets connection timeout when the leader node is restarted,2021-07-16T21:36:27.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.5.7', id='12346098'>, <JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",15.0
,"[<JIRA Component: name='server', id='12312382'>]",2020-05-12T08:46:43.000+0000,Aldan Brito,"upgrade of zookeeper from 3.4.14 to 3.5.7 

We faced the snapshot issue which is described in https://issues.apache.org/jira/browse/ZOOKEEPER-3056

After setting the property ""snapshot.trust.empty=true"" the upgrade was successful.

while reverting the ""snapshot.trust.empty=false"" flag and restart of the zookeeper pods, one of the zookeeper server is failing with the similar stack trace no snapshot  found.
{code:java}
{""type"":""log"", ""host"":""zk-testzk-0"", ""level"":""ERROR"", ""neid"":""zookeeper-4636c00bfc3849e0be179bc71cef17f8"", ""system"":""zookeeper"", ""time"":""2020-05-12T08:32:17.685Z"", ""timezone"":""UTC"", ""log"":{""message"":""main - org.apache.zookeeper.server.quorum.QuorumPeer - Unable to load database on disk""}}
java.io.IOException: No snapshot found, but there are log entries. Something is broken!
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:240)
        at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:240)
        at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:901)
        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:887)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:205)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:123)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:82)
{""type"":""log"", ""host"":""zk-testzk-0"", ""level"":""ERROR"", ""neid"":""zookeeper-4636c00bfc3849e0be179bc71cef17f8"", ""system"":""zookeeper"", ""time"":""2020-05-12T08:32:17.764Z"", ""timezone"":""UTC"", ""log"":{""message"":""main - org.apache.zookeeper.server.quorum.QuorumPeerMain - Unexpected exception, exiting abnormally""}}
java.lang.RuntimeException: Unable to run quorum server
        at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:938)
        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:887)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:205)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:123)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:82)
Caused by: java.io.IOException: No snapshot found, but there are log entries. Something is broken!
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:240)
        at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:240)
        at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:901)
{code}",[],Bug,ZOOKEEPER-3826,Critical,Aldan Brito,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,upgrade from 3.4.x to 3.5.x,2020-06-25T16:22:49.000+0000,"[<JIRA Version: name='3.5.7', id='12346098'>]",4.0
,"[<JIRA Component: name='java client', id='12312381'>]",2020-05-12T00:07:42.000+0000,Rhys Yarranton,"StaticHostProvider.updateServerList contains address matching like this:
{code:java}
        for (InetSocketAddress addr : shuffledList) {
            if (addr.getPort() == myServer.getPort()
                    && ((addr.getAddress() != null
                            && myServer.getAddress() != null && addr
                            .getAddress().equals(myServer.getAddress())) || addr
                            .getHostString().equals(myServer.getHostString()))) {
                myServerInNewConfig = true;
                break;
            }
        }
{code}
 

The addresses in shuffledList are unresolved, while the current server address in myServer is a resolved address (coming from a socket).  If the connect string is expressed in terms of IP addresses instead of host names, the two won't match even when they represent the same server.

On the unresolved addresses, getAddress() is null, and getHostString() is something like 1.2.3.4.  On the resolved address, getAddress() is not null, and getHostString() is (normally) the canonical host name corresponding to the IP address.

As a result, this method tends to return true (reconfig) when it should not.  The calling method, ZooKeeper.updateServerList then closes the connection.

This might be written off as not too serious, except that Curator calls this method when there is a connection state change.  (Sometimes many times.)  What we observe is that when the client has to reconnect, _e.g._, if there is a server failure, when it reconnects the socket gets closed right away.  It goes into a cycle of death until the session dies and a new one is created.  (This doesn't seem like very nice behaviour on Curator's behalf, but that's what's out there.)

As a workaround, we implemented a custom HostProvider to filter out calls to updateServerList which don't actually change the list.

As a permanent fix, instead of passing the current host based on the socket remote address, may need to remember the unresolved address that was used to connect.  (Or use the original strings.)

Filed this against 3.5.5.  Based on source control, it looks this still in exists on master at time of writing.",[],Bug,ZOOKEEPER-3825,Major,Rhys Yarranton,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,StaticHostProvider.updateServerList address matching fails when connectString uses IP addresses,2020-06-24T06:49:56.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",1.0
,"[<JIRA Component: name='kerberos', id='12329415'>, <JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2020-05-11T13:12:16.000+0000,Rajkiran Sura,"With 'DynamicReconfig' feature in v3.5.6, ideally the servers can be added and removed without restarting ZooKeeper service on any of the nodes.

But, with Keberos (GSSAPI via SASL) enabled quorum authentication/authorization, this is not possible. Because, when you try to add a new server, it won't be able to connect to any of the members in the ensemble and the data won't be synced. This is because all the members reject it based on authorization. For this to make it work, we need to do 'reconfig', then restart leader, the new member and rest of the members.

Is this the expected behavior with Quorum-auth + DynamicReconfig? Or am I missing something here.

This is our basic quorum-auth config:
{quote}quorum.auth.serverRequireSasl=true
 quorum.auth.kerberos.servicePrincipal=zookeeper/_HOST
 quorum.auth.enableSasl=true
 quorum.auth.learner.saslLoginContext=QuorumLearner
 quorum.auth.learnerRequireSasl=true
 quorum.cnxn.threads.size=20
 quorum.auth.server.saslLoginContext=QuorumServer
{quote}
FTR: I raised this question in [ZooKeeper-user forum|http://zookeeper-user.578899.n2.nabble.com/ZooKeeper-dynamic-reconfig-issue-when-Quorum-authn-authz-is-enabled-td7584927.html] and both Mate and Enrico suspect this to be a bug.

Also this is easily reproducible in a Kerbers (GSSAPI via SASL) enabled quorum based ensemble.

 

Regards,

Rajkiran

 ",[],Bug,ZOOKEEPER-3824,Major,Rajkiran Sura,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper dynamic reconfig doesn't work with GSSAPI/SASL enabled Quorum authn/z,2020-05-28T09:49:45.000+0000,"[<JIRA Version: name='3.5.6', id='12345243'>]",2.0
,[],2020-05-07T20:22:20.000+0000,Sebastian Schmitz,"Hello,

after Zookeeper 3.6.1 solved the issue with leader-election containing the IP and so causing it to fail in separate networks, like in our docker-setup I updated from 3.4.14 to 3.6.1 in Dev- and Test-Environments. It all went smoothly and ran for one day. This night I had a new Update of the environment as we deploy as a whole package of all containers (Kafka, Zookeeper, Mirrormaker etc.) we also replace the Zookeeper-Containers with latest ones. In this case, there was no change, the containers were just removed and deployed again. As the config and data of zookeeper is not stored inside the containers that's not a problem but this night it broke the whole clusters of Zookeeper and so also Kafka was down.
 * zookeeper_node_1 was stopped and the container removed and created again
 * zookeeper_node_1 starts up and the election takes place
 * zookeeper_node_2 is elected as leader again
 * zookeeper_node_2 is stopped and the container removed and created again
 * zookeeper_node_3 is elected as the leader while zookeeper_node_2 is down
 * zookeeper_node_2 starts up and zookeeper_node_3 remains leader

And from there all servers just report

2020-05-07 14:07:57,187 [myid:3] - WARN  [NIOWorkerThread-2:NIOServerCnxn@364] - Unexpected exception2020-05-07 14:07:57,187 [myid:3] - WARN  [NIOWorkerThread-2:NIOServerCnxn@364] - Unexpected exceptionEndOfStreamException: Unable to read additional data from client, it probably closed the socket: address = /z.z.z.z:46060, session = 0x2014386bbde0000 at org.apache.zookeeper.server.NIOServerCnxn.handleFailedRead(NIOServerCnxn.java:163) at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:326) at org.apache.zookeeper.server.NIOServerCnxnFactory$IOWorkRequest.doWork(NIOServerCnxnFactory.java:522) at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:154)  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)  at java.base/java.lang.Thread.run(Unknown Source)

and don't recover.

I was able to recover the cluster in Test-Environment by stopping and starting all the zookeeper-nodes. The cluster in dev is still in that state and I'm checking the logs to find out more...

The full logs of the deployment of Zookeeper and Kafka that started at 02:00 are attached. The first time in local NZ-time and the second one is UTC. the IPs I replaced are x.x.x.x for node_1, y.y.y.y for node_2 and z.z.z.z for node_3

The Kafka-Servers are running on the same machine. Which means that the EndOfStreamEceptions could also be connections from Kafka as I don't think that zookeeper_node_3 establish a session with itself?

 

Edit:
 I just found some interesting log from Test-Environment:
 zookeeper_node_1: 2020-05-07 14:10:29,418 [myid:1] INFO  [NIOWorkerThread-6:ZooKeeperServer@1375] Refusing session request for client /f.f.f.f:42012 as it has seen zxid 0xc600000000 our last zxid is 0xc5000028f8 client must try another server
 zookeeper_node_2: 2020-05-07 14:10:29,680 [myid:2] INFO  [NIOWorkerThread-4:ZooKeeperServer@1375] Refusing session request for client /f.f.f.f:51506 as it has seen zxid 0xc600000000 our last zxid is 0xc5000028f8 client must try another server
 These entried are repeated there before the EndOfStreamException shows up...
 I found that was set by zookeeper_node_3:
 zookeeper_node_3: 2020-05-07 14:09:44,495 [myid:3] INFO  [QuorumPeer[myid=3](plain=0.0.0.0:2181)(secure=disabled):Leader@1501] Have quorum of supporters, sids: [[1, 3],[1, 3]]; starting up and setting last processed zxid: 0xc600000000
 zookeeper_node_3: 2020-05-07 14:10:12,587 [myid:3] INFO  [LearnerHandler-/z.z.z.z:60156:LearnerHandler@800] Synchronizing with Learner sid: 2 maxCommittedLog=0xc5000028f8 minCommittedLog=0xc500002704 lastProcessedZxid=0xc600000000 peerLastZxid=0xc5000028f8
 It looks like this update of zxid didn't reach nodes 1 and 2 and so they refuse connections from clients?

I also added the full logs for our test-environment now",[],Bug,ZOOKEEPER-3822,Critical,Sebastian Schmitz,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper 3.6.1 EndOfStreamException,2020-06-25T10:16:51.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>]",5.0
,"[<JIRA Component: name='build-infrastructure', id='12333105'>, <JIRA Component: name='c client', id='12312380'>]",2020-05-06T18:32:53.000+0000,Dmitry Wagin,"Since  3.5.7 version release archive does not contain generated zookeeper.jute.c, zookeeper.jute.h",[],Bug,ZOOKEEPER-3820,Minor,Dmitry Wagin,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Missing zookeeper.jute.c, zookeeper.jute.h in release archive",2020-11-21T21:41:59.000+0000,"[<JIRA Version: name='3.5.7', id='12346098'>, <JIRA Version: name='3.6.1', id='12346764'>]",5.0
,"[<JIRA Component: name='server', id='12312382'>]",2020-05-06T15:27:45.000+0000,Bozhen Liu,"We run our static race detector on ZooKeeper and discovered three methods in class org/apache/zookeeper/server/DataTree may have concurrent accesses on ephemerals without proper lock protection. The four methods are createNode(), deleteNode(), killSession() and deserialize(). They all read and write the ephemerals in the similar way without common lock from get() to put():
{code:java}
public void deserialize(InputArchive ia, String tag) {
  ...
  HashSet<String> list = ephemerals.get(eowner); 
  if (list == null){ 
    list = new HashSet<String>(); 
    ephemerals.put(eowner, list); 
  }
  list.add(path);//createNode() has lock on list  
}

void killSession(long session, long zxid) {
  HashSet<String> list = ephemerals.remove(session);
  ...
}
{code}
Both createNode() and deserialize() can add to the list of a session, one with lock, but the other has no lock.

Both createNode() and deleteNode() can update the list of a session with lock, which can be executed concurrently. However, killSession() can be called after deleteNode(), which will remove the mapping that added previously by createNode(). Is this a proper operation? or is there other forced sequences to avoid this?

This is similar to ZOOKEEPER-3102.

The possible traces that lead to the race are (v3.5.4, probably the same in v3.6.1):

=> Race: org/apache/zookeeper/server/DataTree.java/util/Map (org/apache/zookeeper/server/DataTree:478 (TID: 2190) , org/apache/zookeeper/server/DataTree:1215 (TID: 9592) )
 Trace 1st node: (TID: 2190) 
 -> Call org/apache/zookeeper/server/quorum/QuorumPeerMain.initializeAndRun from org/apache/zookeeper/server/quorum/QuorumPeerMain.main (line 82)
 -> Call org/apache/zookeeper/server/quorum/QuorumPeerMain.runFromConfig from org/apache/zookeeper/server/quorum/QuorumPeerMain.initializeAndRun (line 123)
 *> Thread (153) created by Application,Lorg/apache/zookeeper/server/quorum/QuorumPeerMain>.runFromConfig (line 200)
 -> Call java/lang/Thread.start from org/apache/zookeeper/server/quorum/QuorumPeer.start (line 862)
 -> Call org/apache/zookeeper/server/quorum/QuorumPeer.run from java/lang/Thread.start (line -1)
 *> Thread (2190) created by Application,Lorg/apache/zookeeper/server/quorum/QuorumPeer>.run (line 1167)
 -> Call org/apache/zookeeper/server/PrepRequestProcessor.run from java/lang/Thread.start (line -1)
 -> Call org/apache/zookeeper/server/PrepRequestProcessor.pRequest from org/apache/zookeeper/server/PrepRequestProcessor.run (line 145)
 -> Call org/apache/zookeeper/server/FinalRequestProcessor.processRequest from org/apache/zookeeper/server/PrepRequestProcessor.pRequest (line 906)
 -> Call org/apache/zookeeper/server/ZooKeeperServer.processTxn from org/apache/zookeeper/server/FinalRequestProcessor.processRequest (line 104)
 -> Call org/apache/zookeeper/server/ZooKeeperServer.processTxn from org/apache/zookeeper/server/ZooKeeperServer.processTxn (line 1190)
 -> Call org/apache/zookeeper/server/ZKDatabase.processTxn from org/apache/zookeeper/server/ZooKeeperServer.processTxn (line 1199)
 -> Call org/apache/zookeeper/server/DataTree.processTxn from org/apache/zookeeper/server/ZKDatabase.processTxn (line 434)
 -> Call org/apache/zookeeper/server/DataTree.createNode from org/apache/zookeeper/server/DataTree.processTxn (line 786)
 => Read on org/apache/zookeeper/server/DataTree.java/util/Map(ephemerals) in org/apache/zookeeper/server/DataTree.createNode (line 478)
 Trace 2st node: (TID: 9592) 
 -> Call org/apache/zookeeper/server/quorum/QuorumPeerMain.initializeAndRun from org/apache/zookeeper/server/quorum/QuorumPeerMain.main (line 82)
 -> Call org/apache/zookeeper/server/quorum/QuorumPeerMain.runFromConfig from org/apache/zookeeper/server/quorum/QuorumPeerMain.initializeAndRun (line 123)
 *> Thread (153) created by Application,Lorg/apache/zookeeper/server/quorum/QuorumPeerMain>.runFromConfig (line 200)
 -> Call java/lang/Thread.start from org/apache/zookeeper/server/quorum/QuorumPeer.start (line 862)
 -> Call org/apache/zookeeper/server/quorum/QuorumPeer.run from java/lang/Thread.start (line -1)
 -> Call org/apache/zookeeper/server/quorum/Leader.lead from org/apache/zookeeper/server/quorum/QuorumPeer.run (line 1227)
 *> Thread (6402) created by Application,Lorg/apache/zookeeper/server/quorum/Leader>.lead (line 448)
 -> Call org/apache/zookeeper/server/quorum/Leader$LearnerCnxAcceptor.run from java/lang/Thread.start (line -1)
 *> Thread (7676) created by Application,Lorg/apache/zookeeper/server/quorum/Leader$LearnerCnxAcceptor>.run (line 386)
 -> Call org/apache/zookeeper/server/quorum/LearnerHandler.run from java/lang/Thread.start (line -1)
 -> Call org/apache/zookeeper/server/quorum/Leader.processAck from org/apache/zookeeper/server/quorum/LearnerHandler.run (line 559)
 -> Call org/apache/zookeeper/server/quorum/Leader.tryToCommit from org/apache/zookeeper/server/quorum/Leader.processAck (line 863)
 -> Call org/apache/zookeeper/server/quorum/QuorumPeer.processReconfig from org/apache/zookeeper/server/quorum/Leader.tryToCommit (line 779)
 -> Call org/apache/zookeeper/server/quorum/QuorumPeer.restartLeaderElection from org/apache/zookeeper/server/quorum/QuorumPeer.processReconfig (line 1871)
 -> Call org/apache/zookeeper/server/quorum/QuorumPeer.startLeaderElection from org/apache/zookeeper/server/quorum/QuorumPeer.restartLeaderElection (line 1530)
 -> Call org/apache/zookeeper/server/quorum/QuorumPeer.getLastLoggedZxid from org/apache/zookeeper/server/quorum/QuorumPeer.startLeaderElection (line 917)
 -> Call org/apache/zookeeper/server/quorum/QuorumPeer.loadDataBase from org/apache/zookeeper/server/quorum/QuorumPeer.getLastLoggedZxid (line 1009)
 -> Call org/apache/zookeeper/server/ZKDatabase.loadDataBase from org/apache/zookeeper/server/quorum/QuorumPeer.loadDataBase (line 867)
 -> Call org/apache/zookeeper/server/persistence/FileTxnSnapLog.restore from org/apache/zookeeper/server/ZKDatabase.loadDataBase (line 240)
 -> Call org/apache/zookeeper/server/persistence/FileSnap.deserialize from org/apache/zookeeper/server/persistence/FileTxnSnapLog.restore (line 200)
 -> Call org/apache/zookeeper/server/persistence/FileSnap.deserialize from org/apache/zookeeper/server/persistence/FileSnap.deserialize (line 87)
 -> Call org/apache/zookeeper/server/util/SerializeUtils.deserializeSnapshot from org/apache/zookeeper/server/persistence/FileSnap.deserialize (line 122)
 -> Call org/apache/zookeeper/server/DataTree.deserialize from org/apache/zookeeper/server/util/SerializeUtils.deserializeSnapshot (line 141)
 => Write to org/apache/zookeeper/server/DataTree.java/util/Map(ephemerals) in org/apache/zookeeper/server/DataTree.deserialize (line 1215)
 --------------------------------------------------------------------------------------------------------------------------------
 => Race: org/apache/zookeeper/server/DataTree.java/util/Map (org/apache/zookeeper/server/DataTree:478 (TID: 2190) , org/apache/zookeeper/server/DataTree:1005 (TID: 2835) )
 Trace 1st node: (TID: 2190) 
 -> Call org/apache/zookeeper/server/quorum/QuorumPeerMain.initializeAndRun from org/apache/zookeeper/server/quorum/QuorumPeerMain.main (line 82)
 -> Call org/apache/zookeeper/server/quorum/QuorumPeerMain.runFromConfig from org/apache/zookeeper/server/quorum/QuorumPeerMain.initializeAndRun (line 123)
 *> Thread (153) created by Application,Lorg/apache/zookeeper/server/quorum/QuorumPeerMain>.runFromConfig (line 200)
 -> Call java/lang/Thread.start from org/apache/zookeeper/server/quorum/QuorumPeer.start (line 862)
 -> Call org/apache/zookeeper/server/quorum/QuorumPeer.run from java/lang/Thread.start (line -1)
 *> Thread (2190) created by Application,Lorg/apache/zookeeper/server/quorum/QuorumPeer>.run (line 1167)
 -> Call org/apache/zookeeper/server/PrepRequestProcessor.run from java/lang/Thread.start (line -1)
 -> Call org/apache/zookeeper/server/PrepRequestProcessor.pRequest from org/apache/zookeeper/server/PrepRequestProcessor.run (line 145)
 -> Call org/apache/zookeeper/server/FinalRequestProcessor.processRequest from org/apache/zookeeper/server/PrepRequestProcessor.pRequest (line 906)
 -> Call org/apache/zookeeper/server/ZooKeeperServer.processTxn from org/apache/zookeeper/server/FinalRequestProcessor.processRequest (line 104)
 -> Call org/apache/zookeeper/server/ZooKeeperServer.processTxn from org/apache/zookeeper/server/ZooKeeperServer.processTxn (line 1190)
 -> Call org/apache/zookeeper/server/ZKDatabase.processTxn from org/apache/zookeeper/server/ZooKeeperServer.processTxn (line 1199)
 -> Call org/apache/zookeeper/server/DataTree.processTxn from org/apache/zookeeper/server/ZKDatabase.processTxn (line 434)
 -> Call org/apache/zookeeper/server/DataTree.createNode from org/apache/zookeeper/server/DataTree.processTxn (line 786)
 => Read on org/apache/zookeeper/server/DataTree.java/util/Map(ephemerals) in org/apache/zookeeper/server/DataTree.createNode (line 478)
 Trace 2st node: (TID: 2835) 
 -> Call org/apache/zookeeper/server/quorum/QuorumPeerMain.initializeAndRun from org/apache/zookeeper/server/quorum/QuorumPeerMain.main (line 82)
 -> Call org/apache/zookeeper/server/quorum/QuorumPeerMain.runFromConfig from org/apache/zookeeper/server/quorum/QuorumPeerMain.initializeAndRun (line 123)
 *> Thread (153) created by Application,Lorg/apache/zookeeper/server/quorum/QuorumPeerMain>.runFromConfig (line 200)
 -> Call java/lang/Thread.start from org/apache/zookeeper/server/quorum/QuorumPeer.start (line 862)
 -> Call org/apache/zookeeper/server/quorum/QuorumPeer.run from java/lang/Thread.start (line -1)
 *> Thread (2190) created by Application,Lorg/apache/zookeeper/server/quorum/QuorumPeer>.run (line 1167)
 -> Call org/apache/zookeeper/server/quorum/QuorumPeer$1.run from java/lang/Thread.start (line -1)
 -> Call org/apache/zookeeper/server/quorum/ReadOnlyZooKeeperServer.startup from org/apache/zookeeper/server/quorum/QuorumPeer$1.run (line 1157)
 -> Call org/apache/zookeeper/server/ZooKeeperServer.startup from org/apache/zookeeper/server/quorum/ReadOnlyZooKeeperServer.startup (line 70)
 -> Call org/apache/zookeeper/server/quorum/ReadOnlyZooKeeperServer.setupRequestProcessors from org/apache/zookeeper/server/ZooKeeperServer.startup (line 453)
 *> Thread (2835) created by Application,Lorg/apache/zookeeper/server/quorum/ReadOnlyZooKeeperServer>.setupRequestProcessors (line 59)
 -> Call org/apache/zookeeper/server/PrepRequestProcessor.run from java/lang/Thread.start (line -1)
 -> Call org/apache/zookeeper/server/PrepRequestProcessor.pRequest from org/apache/zookeeper/server/PrepRequestProcessor.run (line 145)
 -> Call org/apache/zookeeper/server/FinalRequestProcessor.processRequest from org/apache/zookeeper/server/PrepRequestProcessor.pRequest (line 906)
 -> Call org/apache/zookeeper/server/ZooKeeperServer.processTxn from org/apache/zookeeper/server/FinalRequestProcessor.processRequest (line 104)
 -> Call org/apache/zookeeper/server/ZooKeeperServer.processTxn from org/apache/zookeeper/server/ZooKeeperServer.processTxn (line 1190)
 -> Call org/apache/zookeeper/server/ZKDatabase.processTxn from org/apache/zookeeper/server/ZooKeeperServer.processTxn (line 1199)
 -> Call org/apache/zookeeper/server/DataTree.processTxn from org/apache/zookeeper/server/ZKDatabase.processTxn (line 434)
 -> Call org/apache/zookeeper/server/DataTree.killSession from org/apache/zookeeper/server/DataTree.processTxn (line 854)
 => Write to org/apache/zookeeper/server/DataTree.java/util/Map(ephemerals) in org/apache/zookeeper/server/DataTree.killSession (line 1005)
 --------------------------------------------------------------------------------------------------------------------------------
 => Race: org/apache/zookeeper/server/DataTree.java/util/Map (org/apache/zookeeper/server/DataTree:478 (TID: 2190) , org/apache/zookeeper/server/DataTree:481 (TID: 13154) )
 Trace 1st node: (TID: 2190) 
 -> Call org/apache/zookeeper/server/quorum/QuorumPeerMain.initializeAndRun from org/apache/zookeeper/server/quorum/QuorumPeerMain.main (line 82)
 -> Call org/apache/zookeeper/server/quorum/QuorumPeerMain.runFromConfig from org/apache/zookeeper/server/quorum/QuorumPeerMain.initializeAndRun (line 123)
 *> Thread (153) created by Application,Lorg/apache/zookeeper/server/quorum/QuorumPeerMain>.runFromConfig (line 200)
 -> Call java/lang/Thread.start from org/apache/zookeeper/server/quorum/QuorumPeer.start (line 862)
 -> Call org/apache/zookeeper/server/quorum/QuorumPeer.run from java/lang/Thread.start (line -1)
 *> Thread (2190) created by Application,Lorg/apache/zookeeper/server/quorum/QuorumPeer>.run (line 1167)
 -> Call org/apache/zookeeper/server/PrepRequestProcessor.run from java/lang/Thread.start (line -1)
 -> Call org/apache/zookeeper/server/PrepRequestProcessor.pRequest from org/apache/zookeeper/server/PrepRequestProcessor.run (line 145)
 -> Call org/apache/zookeeper/server/FinalRequestProcessor.processRequest from org/apache/zookeeper/server/PrepRequestProcessor.pRequest (line 906)
 -> Call org/apache/zookeeper/server/ZooKeeperServer.processTxn from org/apache/zookeeper/server/FinalRequestProcessor.processRequest (line 104)
 -> Call org/apache/zookeeper/server/ZooKeeperServer.processTxn from org/apache/zookeeper/server/ZooKeeperServer.processTxn (line 1190)
 -> Call org/apache/zookeeper/server/ZKDatabase.processTxn from org/apache/zookeeper/server/ZooKeeperServer.processTxn (line 1199)
 -> Call org/apache/zookeeper/server/DataTree.processTxn from org/apache/zookeeper/server/ZKDatabase.processTxn (line 434)
 -> Call org/apache/zookeeper/server/DataTree.createNode from org/apache/zookeeper/server/DataTree.processTxn (line 786)
 => Read on org/apache/zookeeper/server/DataTree.java/util/Map(ephemerals) in org/apache/zookeeper/server/DataTree.createNode (line 478)
 Trace 2st node: (TID: 13154) 
 -> Call org/apache/zookeeper/server/quorum/QuorumPeerMain.initializeAndRun from org/apache/zookeeper/server/quorum/QuorumPeerMain.main (line 82)
 -> Call org/apache/zookeeper/server/quorum/QuorumPeerMain.runFromConfig from org/apache/zookeeper/server/quorum/QuorumPeerMain.initializeAndRun (line 123)
 *> Thread (153) created by Application,Lorg/apache/zookeeper/server/quorum/QuorumPeerMain>.runFromConfig (line 200)
 -> Call java/lang/Thread.start from org/apache/zookeeper/server/quorum/QuorumPeer.start (line 862)
 -> Call org/apache/zookeeper/server/quorum/QuorumPeer.run from java/lang/Thread.start (line -1)
 -> Call org/apache/zookeeper/server/quorum/Observer.observeLeader from org/apache/zookeeper/server/quorum/QuorumPeer.run (line 1201)
 -> Call org/apache/zookeeper/server/quorum/Learner.syncWithLeader from org/apache/zookeeper/server/quorum/Observer.observeLeader (line 74)
 -> Call org/apache/zookeeper/server/ZooKeeperServer.startup from org/apache/zookeeper/server/quorum/Learner.syncWithLeader (line 559)
 -> Call org/apache/zookeeper/server/quorum/ObserverZooKeeperServer.setupRequestProcessors from org/apache/zookeeper/server/ZooKeeperServer.startup (line 453)
 *> Thread (8653) created by Application,Lorg/apache/zookeeper/server/quorum/ObserverZooKeeperServer>.setupRequestProcessors (line 97)
 -> Call java/lang/Thread.start from org/apache/zookeeper/server/quorum/CommitProcessor.start (line 263)
 -> Call org/apache/zookeeper/server/quorum/CommitProcessor.run from java/lang/Thread.start (line -1)
 -> Call org/apache/zookeeper/server/quorum/CommitProcessor.sendToNextProcessor from org/apache/zookeeper/server/quorum/CommitProcessor.run (line 180)
 -> Call org/apache/zookeeper/server/WorkerService.schedule from org/apache/zookeeper/server/quorum/CommitProcessor.sendToNextProcessor (line 272)
 *> Thread (13154) created by Application,Lorg/apache/zookeeper/server/WorkerService>.schedule (line 128)
 -> Call org/apache/zookeeper/server/WorkerService$ScheduledWorkRequest.run from java/lang/Thread.start (line -1)
 -> Call org/apache/zookeeper/server/quorum/CommitProcessor$CommitWorkRequest.doWork from org/apache/zookeeper/server/WorkerService$ScheduledWorkRequest.run (line 162)
 -> Call org/apache/zookeeper/server/FinalRequestProcessor.processRequest from org/apache/zookeeper/server/quorum/CommitProcessor$CommitWorkRequest.doWork (line 297)
 -> Call org/apache/zookeeper/server/ZooKeeperServer.processTxn from org/apache/zookeeper/server/FinalRequestProcessor.processRequest (line 104)
 -> Call org/apache/zookeeper/server/ZooKeeperServer.processTxn from org/apache/zookeeper/server/ZooKeeperServer.processTxn (line 1190)
 -> Call org/apache/zookeeper/server/ZKDatabase.processTxn from org/apache/zookeeper/server/ZooKeeperServer.processTxn (line 1199)
 -> Call org/apache/zookeeper/server/DataTree.processTxn from org/apache/zookeeper/server/ZKDatabase.processTxn (line 434)
 -> Call org/apache/zookeeper/server/DataTree.createNode from org/apache/zookeeper/server/DataTree.processTxn (line 786)
 => Write to org/apache/zookeeper/server/DataTree.java/util/Map(ephemerals) in org/apache/zookeeper/server/DataTree.createNode (line 481)",[],Bug,ZOOKEEPER-3819,Minor,Bozhen Liu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Potential Races on DataTree,2020-05-08T15:54:40.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.1', id='12346764'>]",2.0
lujie,"[<JIRA Component: name='security', id='12329414'>]",2020-05-06T06:39:45.000+0000,Aishwarya Soni,"I am configuring SSL on Zookeeper 3.5.5 branch and have removed the clientPort config from zoo.cfg and adding onlysecureClientPort. Also, I have removed it from my server ensemble connection string in zoo.cfg.dynamic file as it results in a port binding issue on the port 2181 if we keep it in both the files.

But, in zkServer.sh, it checks if the clientPort is set in the *status* cmd else it throws exit 1 and terminates the process. How to overcome this situation? We cannot see the clientPort in zoo.cfg as it would enable mixed mode which we do not want when we enable SSL.

Also, I am using zkServer.sh status output as a healthcheck for our containerized zookeeper to see if thee quorum is established or not as in cluster mode, zookeeper can finally run either in follower or leader state (ignoring intermediate state changes). So as the status output throws exit 1, the healthcheck is also failing.","[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",Bug,ZOOKEEPER-3818,Critical,Aishwarya Soni,Fixed,2020-05-12T10:07:03.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,fix zkServer.sh status command to support SSL-only server,2020-09-10T10:43:47.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",3.0
Mate Szalay-Beko,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2020-05-02T06:54:21.000+0000,Rajkiran Sura,"Hello,

We recently upgraded our 5 node ZooKeeper ensemble from v3.4.8 to v3.5.6. Encountered no issues as such.

This is how the ZooKeeper config looks like:
{quote}tickTime=2000
dataDir=/zookeeper-data/
initLimit=5
syncLimit=2
maxClientCnxns=2048
autopurge.snapRetainCount=3
autopurge.purgeInterval=1
4lw.commands.whitelist=stat, ruok, conf, isro, mntr
authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider
requireClientAuthScheme=sasl
quorum.cnxn.threads.size=20
quorum.auth.enableSasl=true
quorum.auth.kerberos.servicePrincipal= zookeeper/_HOST
quorum.auth.learnerRequireSasl=true
quorum.auth.learner.saslLoginContext=QuorumLearner
quorum.auth.serverRequireSasl=true
quorum.auth.server.saslLoginContext=QuorumServer
server.17=node1.foo.bar.com:2888:3888;2181
server.19=node2.foo.bar.com:2888:3888;2181
server.20=node3.foo.bar.com:2888:3888;2181
server.21=node4.foo.bar.com:2888:3888;2181
server.22=node5.bar.com:2888:3888;2181
{quote}
Post upgrade, we had to migrate server.22 on the same node, but with *FOO*.bar.com domain name due to kerberos referral issues. And, we used different server-identifier, i.e., *23* when we migrated. So, here is how the new config looked like:
{quote}server.17=node1.foo.bar.com:2888:3888;2181
server.19=node2.foo.bar.com:2888:3888;2181
server.20=node3.foo.bar.com:2888:3888;2181
server.21=node4.foo.bar.com:2888:3888;2181
*server.23=node5.{color:#00875a}foo{color}.bar.com:2888:3888;2181*
{quote}
We restarted all the nodes in the ensemble with the above updated config. And the migrated node joined the quorum successfully and was serving all clients directly connected to it, without any issues.

Recently, when a leader election happened, server.*23*=node5.foo.bar.com(migrated node) was chosen as Leader (as it has highest ID). But then, ZooKeeper was unable to serve any clients and *all* the servers were _somehow still_ trying to establish a channel to 22 (old DNS name: node5.bar.com) and were throwing below error in a loop:
{quote}{{2020-05-02 01:43:03,026 [myid:23] - WARN [WorkerSender[myid=23]:QuorumPeer$QuorumServer@196] - Failed to resolve address: node4.bar.com}}
{{java.net.UnknownHostException: node5.bar.com: Name or service not known}}
{{ at java.base/java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)}}
{{ at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:929)}}
{{ at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1515)}}
{{ at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:848)}}
{{ at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1505)}}
{{ at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1364)}}
{{ at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1298)}}
{{ at java.base/java.net.InetAddress.getByName(InetAddress.java:1248)}}
{{ at org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer.recreateSocketAddresses(QuorumPeer.java:194)}}
{{ at org.apache.zookeeper.server.quorum.QuorumPeer.recreateSocketAddresses(QuorumPeer.java:774)}}
{{ at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:701)}}
{{ at org.apache.zookeeper.server.quorum.QuorumCnxManager.toSend(QuorumCnxManager.java:620)}}
{{ at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.process(FastLeaderElection.java:477)}}
{{ at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.run(FastLeaderElection.java:456)}}
{{ at java.base/java.lang.Thread.run(Thread.java:834)}}
{{2020-05-02 01:43:03,026 [myid:23] - WARN [WorkerSender[myid=23]:QuorumCnxManager@679] - Cannot open channel to 22 at election address node5.bar.com:3888}}
{{java.net.UnknownHostException: node5.bar.com}}
{{ at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:220)}}
{{ at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:403)}}
{{ at java.base/java.net.Socket.connect(Socket.java:591)}}
{{ at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:650)}}
{{ at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:714)}}
{{ at org.apache.zookeeper.server.quorum.QuorumCnxManager.toSend(QuorumCnxManager.java:620)}}
{{ at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.process(FastLeaderElection.java:477)}}
{{ at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.run(FastLeaderElection.java:456)}}
{{ at java.base/java.lang.Thread.run(Thread.java:834)}}
{quote}
Fetching config from live ZooKeeper znode also doesn't show ""*22*"" being a member of the ensemble. Its not clear how ""22"" is still coming into the picture.
{quote}In [4]: zk.get('/zookeeper/config')
Out[4]:
('server.17=node1.foo.bar.com:2888:3888:participant;0.0.0.0:2181\n

server.19=node2.foo.bar.com:2888:3888:participant;0.0.0.0:2181\n

server.20=node3.foo.bar.com:2888:3888:participant;0.0.0.0:2181\n

server.21=node4.foo.bar.com:2888:3888:participant;0.0.0.0:2181\n

server.23=node5.foo.bar.com:2888:3888:participant;0.0.0.0:2181\n

version=0',
 ZnodeStat(czxid=0, mzxid=0, ctime=0, mtime=1588399290245, version=-1, cversion=0, aversion=-1, ephemeralOwner=0, dataLength=360, numChildren=0, pzxid=0))
{quote}
We suspected some weird caching issue and restarted ZooKeeper across all the nodes but that didn't help. So, whenever node5 becomes the Leader, ID:22 is popping up. We even rebooted node5 and that hasn't helped too.

We also looked at '/zookeeper/config' content from snapshot files and did not find any reference to ID:22.

Any help would be greatly appreciated.

NOTE: dynamic config is disabled.

Thanks,
Rajkiran","[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",Bug,ZOOKEEPER-3814,Major,Rajkiran Sura,Duplicate,2020-06-03T08:01:16.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeper config propagates even with disabled dynamic reconfig,2020-09-10T10:43:40.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",3.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2020-04-29T09:31:13.000+0000,Mykola Khazanovych,"On the page: 

[https://cwiki.apache.org/confluence/display/ZOOKEEPER/Index]

there is a hyperlinked text '[ZooKeeper Recipes|http://hadoop.apache.org/zookeeper/docs/current/recipes.html]' with the link

[http://hadoop.apache.org/zookeeper/docs/current/recipes.html]

on the click it returns 404:
{noformat}
Not Found
The requested URL was not found on this server. {noformat}
 ",[],Bug,ZOOKEEPER-3812,Minor,Mykola Khazanovych,,,The issue is open and ready for the assignee to start work on it.,Open,0.0, ZooKeeper Recipes hyperlink is broken on Intro page,2020-04-29T09:31:13.000+0000,[],1.0
Norbert Kalmár,[],2020-04-23T15:01:02.000+0000,Norbert Kalmár,We accidently had a netty-codec-4.1.49 versioned license file added instead of 4.1.48. Since 4.1.49 was just released a day ago (it's not even seen in mvn central) why not upgrade instead of fixing the licence file name.,[],Bug,ZOOKEEPER-3804,Minor,Norbert Kalmár,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,upgrade netty to 4.1.49,2020-04-23T15:02:49.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.7', id='12346098'>]",1.0
,[],2020-04-20T21:41:38.000+0000,Vova Vysotskyi,"For the case when {{TestingServer.start()}} and {{TestingServer.close()}} methods are running in different threads (but {{TestingServer.close()}} is executed after {{TestingServer.start()}} is complited), {{FileTxnSnapLog.fastForwardFromEdits()}} throws NPE, since {{FileTxnSnapLog.close()}} was already called in {{TestingZooKeeperMain.internalRunFromConfig()}} method.

Such a case may be observed in unit tests when start and close methods are called in methods annotated with {{@Before}} and {{@After}} annotations.

Here is a simple test which helps to reproduce this issue:
{code:java}
  @Test
  public void testNPE() throws Exception {
    for (int i = 0; i < 100; i++) {
      TestingServer testingServer = new TestingServer();
      Thread thread = new Thread(() -> {
        try {
          testingServer.start();
        } catch (Exception e) {
          throw new RuntimeException(e);
        }
      });
      thread.start();
      thread.join();
      testingServer.close();
    }
  }
{code}

The stack trace is the following:
{noformat}
java.lang.NullPointerException
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.fastForwardFromEdits(FileTxnSnapLog.java:269)
	at org.apache.zookeeper.server.ZKDatabase.fastForwardDataBase(ZKDatabase.java:251)
	at org.apache.zookeeper.server.ZooKeeperServer.shutdown(ZooKeeperServer.java:583)
	at org.apache.zookeeper.server.ZooKeeperServer.shutdown(ZooKeeperServer.java:546)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.shutdown(NIOServerCnxnFactory.java:929)
	at org.apache.curator.test.TestingZooKeeperMain.close(TestingZooKeeperMain.java:178)
	at org.apache.curator.test.TestingZooKeeperServer.stop(TestingZooKeeperServer.java:118)
	at org.apache.curator.test.TestingZooKeeperServer.close(TestingZooKeeperServer.java:130)
	at org.apache.curator.test.TestingServer.close(TestingServer.java:178)
	at org.apache.drill.exec.coord.zk.TestZookeeperClient.testNPE(TestZookeeperClient.java:109)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at mockit.integration.junit4.JUnit4TestRunnerDecorator.executeTestMethod(JUnit4TestRunnerDecorator.java:157)
	at mockit.integration.junit4.JUnit4TestRunnerDecorator.invokeExplosively(JUnit4TestRunnerDecorator.java:71)
	at mockit.integration.junit4.FakeFrameworkMethod.invokeExplosively(FakeFrameworkMethod.java:29)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
{noformat}

Looks like this NPE is a regression after ZOOKEEPER-2845, where instead of using a local variable of {{txnLog}} was used class field.",[],Bug,ZOOKEEPER-3803,Major,Vova Vysotskyi,Fixed,2020-06-25T06:48:12.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,FileTxnSnapLog.fastForwardFromEdits() throws NPE if TestingServer is started from another thread,2020-10-19T08:02:33.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",6.0
Christopher Tubbs,"[<JIRA Component: name='build', id='12312383'>]",2020-04-18T10:27:02.000+0000,Christopher Tubbs,"To improve the stability of Jenkins links, the views have been updated to use a single letter prefix.

So, the pom.xml should be updated to point to https://builds.apache.org/view/Z/view/ZooKeeper/ ","[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",Bug,ZOOKEEPER-3801,Major,Christopher Tubbs,Fixed,2020-08-29T15:52:06.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Fix Jenkins link in pom,2020-09-10T10:43:43.000+0000,[],1.0
Enrico Olivelli,"[<JIRA Component: name='build', id='12312383'>]",2020-04-18T07:10:21.000+0000,Enrico Olivelli,"fatjat and full-build profiles conflict, because both enable the fatjar module.
but with full-build the fatjar is not actually built.

This is an example of error


{code:java}
[eolivelli@localhost zookeeper]$ mvn clean -Pfull-build,fatjar
[INFO] Scanning for projects...
[ERROR] [ERROR] Project 'org.apache.zookeeper:zookeeper-contrib-fatjar:3.6.1-SNAPSHOT' is duplicated in the reactor @ 
[ERROR] Project 'org.apache.zookeeper:zookeeper-contrib-fatjar:3.6.1-SNAPSHOT' is duplicated in the reactor -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, pl
{code}

The minimal fix is to enable fatjar even with full-build profile.
The full-build profile is meant to build all of the modules, and it is used during the release process as well","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.6.1', id='12346764'>]",Bug,ZOOKEEPER-3797,Blocker,Enrico Olivelli,Fixed,2020-04-18T17:16:18.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Conflict between fatjar and full-build Maven profiles in branch-3.6,2020-04-18T17:18:54.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.6.1', id='12346764'>]",1.0
Michael Han,"[<JIRA Component: name='server', id='12312382'>]",2020-04-10T17:55:06.000+0000,Michael Han,"When RequestThrottler is not enabled or is enabled but configured incorrectly, ZooKeeper server will stop throttling. This is a serious bug as without request throttling, it's fairly easy to overwhelm ZooKeeper which leads to all sorts of issues. 

This is a regression introduced in ZOOKEEPER-3243, where the total number of queued requests in request processing pipeline is not taking into consideration when deciding whether to throttle or not, or only taken into consideration conditionally based on RequestThrottler's configurations. We should make sure always taking into account the number of queued requests in request processing pipeline before making throttling decisions.","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.1', id='12346764'>]",Bug,ZOOKEEPER-3793,Critical,Michael Han,Fixed,2020-04-12T09:16:17.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Request throttling is broken when RequestThrottler is disabled or configured incorrectly.,2021-10-23T18:20:51.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",2.0
Norbert Kalmár,"[<JIRA Component: name='documentation', id='12312422'>]",2020-04-10T09:16:35.000+0000,Zili Chen,"After ZOOKEEPER-3369 we generate document in {{apidocs/}} instead of {{api/}} before. Moreover, the document site cannot proper linked to apidocs in 3.6.0(but in 3.5.7, thought it is wrong in {{website}} branch, it is manually corrected in {{asf-site}} branch}}.

I propose we generate single aggregated document and place it under {{api/}} folder, as well as bumping website for the update.","[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.6.2', id='12347809'>]",Bug,ZOOKEEPER-3792,Major,Zili Chen,Fixed,2020-05-21T10:13:07.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Reconcile document site in 3.5.7 & 3.6.0,2021-06-30T04:03:38.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.7', id='12346098'>]",3.0
,"[<JIRA Component: name='metric system', id='12334405'>]",2020-04-04T05:53:25.000+0000,Karthik Prasad,"Enabled Prometheus Metrics Server  on zookeeper that is running on  OpenJDK-11.0.5 with minimal modules enabled. Server crashed with below error.

 

 
{code:java}
2020-04-04 05:17:32,310 [myid:1] - INFO  [main:AbstractConnector@330] - Started ServerConnector@7c711375{HTTP/1.1,[http/1.1]} {0.0.0.0:7000} 2020-04-04 05:17:32,310 [myid:1] - INFO  [main:AbstractConnector@330] - Started ServerConnector@7c711375{HTTP/1.1,[http/1.1]} {0.0.0.0:7000} 2020-04-04 05:17:32,311 [myid:1] - INFO  [main:Server@399] - Started @897ms2020-04-04 05:17:32,321 [myid:1] - INFO  [main:ServerMetrics@62] - ServerMetrics initialized with provider org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider@5efa40fe2020-04-04 05:17:32,334 [myid:1] - INFO  [main:AbstractConnector@380] - Stopped ServerConnector@7c711375{HTTP/1.1,[http/1.1]} {0.0.0.0:7000} 2020-04-04 05:17:32,335 [myid:1] - INFO  [main:ContextHandler@1016] - Stopped o.e.j.s.ServletContextHandler@4c163e3{/,null,UNAVAILABLE}Exception in thread ""main"" java.lang.NoClassDefFoundError: sun/misc/Unsafe at io.prometheus.client.Striped64.getUnsafe(Striped64.java:316) at io.prometheus.client.Striped64.<clinit>(Striped64.java:296) at io.prometheus.client.Summary$Child.<init>(Summary.java:260) at io.prometheus.client.Summary$Child.<init>(Summary.java:198) at io.prometheus.client.Summary.newChild(Summary.java:159) at io.prometheus.client.Summary.newChild(Summary.java:81) at io.prometheus.client.SimpleCollector.labels(SimpleCollector.java:76) at io.prometheus.client.SimpleCollector.initializeNoLabelsChild(SimpleCollector.java:107) at io.prometheus.client.Summary.<init>(Summary.java:92) at io.prometheus.client.Summary$Builder.create(Summary.java:136) at io.prometheus.client.Summary$Builder.create(Summary.java:95) at io.prometheus.client.SimpleCollector$Builder.register(SimpleCollector.java:245) at org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider$PrometheusSummary.<init>(PrometheusMetricsProvider.java:348) at org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider$Context.lambda$getSummary$2(PrometheusMetricsProvider.java:235) at java.base/java.util.concurrent.ConcurrentHashMap.computeIfAbsent(Unknown Source) at org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider$Context.getSummary(PrometheusMetricsProvider.java:231) at org.apache.zookeeper.server.ServerMetrics.<init>(ServerMetrics.java:70) at org.apache.zookeeper.server.ServerMetrics.metricsProviderInitialized(ServerMetrics.java:63) at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:161) at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:136) at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:90)Caused by: java.lang.ClassNotFoundException: sun.misc.Unsafe at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source) at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source) at java.base/java.lang.ClassLoader.loadClass(Unknown Source) ... 21 more  
{code}
 

When I enabled jdk.unsupported module, zookeeper server started and able to access prometheus metrics. However jdk.unsupported is deprecated.",[],Bug,ZOOKEEPER-3784,Minor,Karthik Prasad,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,PrometheusProvider uses deprecated sun.misc.Unsafe,2020-04-08T03:09:35.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",3.0
Zili Chen,[],2020-04-01T15:23:36.000+0000,Zili Chen,"After Py3, {{filter}} return a {{filter object}} instead of {{list object}}, which causes

Traceback (most recent call last):
  File ""zk-merge-pr.py"", line 533, in <module>
    main()
  File ""zk-merge-pr.py"", line 519, in main
    resolve_jira_issues(commit_title, merged_refs, jira_comment)
  File ""zk-merge-pr.py"", line 329, in resolve_jira_issues
    resolve_jira_issue(merge_branches, comment, jira_id)
  File ""zk-merge-pr.py"", line 312, in resolve_jira_issue
    jira_fix_versions = [get_version_json(v) for v in fix_versions]
  File ""zk-merge-pr.py"", line 312, in <listcomp>
    jira_fix_versions = [get_version_json(v) for v in fix_versions]
  File ""zk-merge-pr.py"", line 310, in get_version_json
    return filter(lambda v: v.name == version_str, versions)[0].raw
TypeError: 'filter' object is not subscriptable

We can replace filter with list comprehension to fix it.","[<JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3782,Major,Zili Chen,Fixed,2020-04-08T12:05:29.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Replace filter with list comprehension for returning list in zk-merge-pr.py,2021-03-28T08:55:29.000+0000,[],1.0
,[],2020-04-01T14:54:08.000+0000,Weichu Liu,"Hi,

In our deployment, we are using zookeepers as Kafka quorum.

Recently we upgraded Zookeeper from 3.4.14 to 3.5.7.
During the upgrade, we enabled snapshot.trust.empty and removed it after.
However, about 25% of zookeepers (all on 3.5.7) are not creating snapshot files, so they keep crashing on reboot with ERROR.

{noformat}
ERROR [main:QuorumPeer@937] - Unable to load database on disk
<http://java.io|java.io>.IOException: No snapshot found, but there are log entries. Something is broken!
{noformat}

Nothing suspecious did I find from the zookeeper log.

I managed to reproduce the issue with the data from our servers, I put them in the attachment.","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-3781,Major,Weichu Liu,Fixed,2021-03-06T19:43:24.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Zookeeper 3.5.7 not creating snapshot,2021-12-10T05:06:32.000+0000,[],4.0
Norbert Kalmár,[],2020-04-01T11:14:13.000+0000,Norbert Kalmár,"We removed the getRevision() function in 3.5.6:
https://github.com/apache/zookeeper/commit/96880c0b2ba39f1841f8bdc4a0119a467b462d03#diff-1d42dde48d42e3baf819e22b56a880a7

Some projects (like HBase for example) are actually using this method. (Although not relevant anymore, since we are on git).

3.5.6, 3.5.7 and 3.6.0 does not have this method anymore. However, if someone is using this function, to make upgrading easier, we should restore it.
","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",Bug,ZOOKEEPER-3780,Critical,Norbert Kalmár,Fixed,2020-04-07T13:41:40.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,restore Version.getRevision() to be backward compatible,2020-05-11T15:41:10.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.6', id='12345243'>, <JIRA Version: name='3.5.7', id='12346098'>]",2.0
,[],2020-04-01T05:28:43.000+0000,Jaikiran Pai,"When running on Java 14, the 3.4.x version of `org.apache.zookeeper.Zookeeper` which creates a client to connect to a Zookeeper server, fails to connect against the server.

This appears to be due to the use of `InetSocketAddress.toString()` to parse the host name of an unresolved address in StaticHostProvider[1]. The issue doesn't show up in 3.5.x versions of Zookeeper. But given that 3.4.x isn't that old, I thought I'll raise this here and see if it can be fixed and whether a new 3.4.x release is warranted.

Furthermore, there was a discussion here[2] which explains the whole details. There's even a discussion in the openjdk net-dev mailing list[3] to see if there are other similar usages in Zookeeper which might be impacted by this change in the JDK 14. I don't have any real experience with this library, so if someone more knowledgable knows whether or not there are other similar usages in this project, please do reply to the net-dev discussion on the openjdk mailing list.

 

 [1] [https://github.com/apache/zookeeper/blob/branch-3.4.14/zookeeper-server/src/main/java/org/apache/zookeeper/client/StaticHostProvider.java#L135]

[2] [https://github.com/quarkusio/quarkus/issues/8212#issuecomment-605403656]

[3] https://mail.openjdk.java.net/pipermail/net-dev/2020-March/013725.html",[],Bug,ZOOKEEPER-3779,Major,Jaikiran Pai,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper client 3.4.x fails to connect when using Java 14,2021-01-14T06:26:55.000+0000,"[<JIRA Version: name='3.4.14', id='12343587'>]",8.0
Mate Szalay-Beko,[],2020-04-01T04:24:14.000+0000,Hoang Dang,"I upgrade our cluster from 3.5.7 to 3.6.0. I make small change in config for metricsProvider (prometheus) which I guess won't affect the our cluster's functions. But we get following error log: 
{code:java}
2020-04-01 04:04:57,892 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):Follower@292] - shutdown Follower
2020-04-01 04:04:57,892 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):QuorumPeer@863] - Peer state changed: looking
2020-04-01 04:04:57,892 [myid:1] - WARN  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):QuorumPeer@1501] - PeerState set to LOOKING
2020-04-01 04:04:57,892 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):QuorumPeer@1371] - LOOKING
2020-04-01 04:04:57,892 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):FastLeaderElection@931] - New election. My id = 1, proposed zxid=0x140000044b
2020-04-01 04:04:57,894 [myid:1] - INFO  [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:1, n.state:LOOKING, n.leader:1, n.round:$
2020-04-01 04:04:57,895 [myid:1] - INFO  [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:2, n.state:FOLLOWING, n.leader:3, n.roun$
2020-04-01 04:04:57,896 [myid:1] - INFO  [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:3, n.state:LEADING, n.leader:3, n.round:$
2020-04-01 04:04:57,896 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):QuorumPeer@857] - Peer state changed: following
2020-04-01 04:04:57,897 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):QuorumPeer@1453] - FOLLOWING
2020-04-01 04:04:57,897 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):ZooKeeperServer@1246] - minSessionTimeout set to 4000
2020-04-01 04:04:57,897 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):ZooKeeperServer@1255] - maxSessionTimeout set to 40000
2020-04-01 04:04:57,897 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):ResponseCache@45] - Response cache size is initialized with value 400.
2020-04-01 04:04:57,897 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):ResponseCache@45] - Response cache size is initialized with value 400.
2020-04-01 04:04:57,897 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):RequestPathMetricsCollector@111] - zookeeper.pathStats.slotCapacity = 60
2020-04-01 04:04:57,897 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):RequestPathMetricsCollector@112] - zookeeper.pathStats.slotDuration = 15
2020-04-01 04:04:57,897 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):RequestPathMetricsCollector@113] - zookeeper.pathStats.maxDepth = 6
2020-04-01 04:04:57,897 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):RequestPathMetricsCollector@114] - zookeeper.pathStats.initialDelay = 5
2020-04-01 04:04:57,898 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):RequestPathMetricsCollector@115] - zookeeper.pathStats.delay = 5
2020-04-01 04:04:57,898 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):RequestPathMetricsCollector@116] - zookeeper.pathStats.enabled = false
2020-04-01 04:04:57,898 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):ZooKeeperServer@1470] - The max bytes for all large requests are set to 104857600
2020-04-01 04:04:57,898 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):ZooKeeperServer@1484] - The large request threshold is set to -1
2020-04-01 04:04:57,898 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):ZooKeeperServer@329] - Created server with tickTime 2000 minSessionTimeout 4000 maxSes$
2020-04-01 04:04:57,898 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):Follower@75] - FOLLOWING - LEADER ELECTION TOOK - 5 MS
2020-04-01 04:04:57,899 [myid:1] - INFO  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):QuorumPeer@863] - Peer state changed: following - discovery
2020-04-01 04:04:57,900 [myid:1] - WARN  [QuorumPeer[myid=1](plain=[0:0:0:0:0:0:0:0]:2181)(secure=disabled):Follower@129] - Exception when following the leader
java.lang.IllegalArgumentException
        at java.base/java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1295)
        at java.base/java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1181)
        at java.base/java.util.concurrent.Executors.newFixedThreadPool(Executors.java:92)
        at org.apache.zookeeper.server.quorum.Learner.connectToLeader(Learner.java:275)
        at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:87)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1455)
{code}
 

 After checking the code [here|https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Learner.java]
{code:java}
        if (self.isMultiAddressReachabilityCheckEnabled()) {
            // even if none of the addresses are reachable, we want to try to establish connection
            // see ZOOKEEPER-3758
            addresses = multiAddr.getAllReachableAddressesOrAll();
        } else {
            addresses = multiAddr.getAllAddresses();
        }

        ExecutorService executor = Executors.newFixedThreadPool(addresses.size());  
{code}
I guess there's something wrong with *multiAddress.reachabilityCheckEnabled*. So I decide to turn it *off (false)*. After that, I can start our cluster as expected.

So could you please:
 * Update the document [here |http://zookeeper.apache.org/doc/r3.6.0/zookeeperAdmin.html] for _multiAddress.reachabilityCheckEnabled_ because it has effect even if _multiAddress.enabled=false_ (which is default)
 * Check the code in Learner.java to make sure _addresses.size()_ is always larger than 0","[<JIRA Version: name='3.6.1', id='12346764'>]",Bug,ZOOKEEPER-3778,Major,Hoang Dang,Duplicate,2020-04-01T06:41:09.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Cannot upgrade from 3.5.7 to 3.6.0 due to multiAddress.reachabilityCheckEnabled,2020-04-01T06:41:09.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",2.0
Mate Szalay-Beko,[],2020-03-31T20:13:56.000+0000,Philipp,"We had working Zookeeper 3.4.8 that came with Kafka. 3 machines cluster. Now I'm trying to upgrade it to 3.6.0. I didn't touch the data directory, just set up 3.6.0 Zookeeper in the new directory and updated config. Stopped all Zookeepers. Now I've started eache Zk in turn with some delay. And they are not forming up quorum, stuck seemingly forever, sometimes trying to elect and sometimes throwing nonsensical errors.

For example, on node 1 (sorry don't know how to format this properly in jira):

Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,133 [myid:1] - INFO [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:2, n.state:LOOKING, n.leader:2, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,133 [myid:1] - INFO [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:1, n.state:LOOKING, n.leader:2, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,134 [myid:1] - INFO [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:3, n.state:LEADING, n.leader:3, n.round:0x4a, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,335 [myid:1] - INFO [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:2, n.state:LOOKING, n.leader:2, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,536 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@857] - Peer state changed: following
Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,536 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@1453] - FOLLOWING
Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,536 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):ZooKeeperServer@1246] - minSessionTimeout set to 6000
Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,536 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):ZooKeeperServer@1255] - maxSessionTimeout set to 60000
Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,536 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):ResponseCache@45] - Response cache size is initialized with value 400.
Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,536 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):RequestPathMetricsCollector@111] - zookeeper.pathStats.slotCapacity = 60
Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,536 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):RequestPathMetricsCollector@112] - zookeeper.pathStats.slotDuration = 15
Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,536 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):RequestPathMetricsCollector@113] - zookeeper.pathStats.maxDepth = 6
Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,536 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):RequestPathMetricsCollector@114] - zookeeper.pathStats.initialDelay = 5
Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,537 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):RequestPathMetricsCollector@115] - zookeeper.pathStats.delay = 5
Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,537 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):RequestPathMetricsCollector@116] - zookeeper.pathStats.enabled = false
Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,537 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):ZooKeeperServer@1470] - The max bytes for all large requests are set to 104857600
Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,537 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):ZooKeeperServer@1484] - The large request threshold is set to -1
Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,537 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):ZooKeeperServer@329] - Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 clientPortListenBacklog -1 datadir /kafka/kafka_zookeeper_data/logdir/version-2 snapdir /kafka/kafka_zookeeper_data/version-2
Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,537 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):Follower@75] - FOLLOWING - LEADER ELECTION TOOK - 92790 MS
Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,537 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@863] - Peer state changed: following - discovery
Mar 31 20:05:21 java[24055]: 2020-03-31 20:05:21,736 [myid:1] - INFO [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:FOLLOWING; n.sid:2, n.state:LOOKING, n.leader:2, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:22 java[24055]: 2020-03-31 20:05:22,538 [myid:1] - INFO [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:FOLLOWING; n.sid:2, n.state:LOOKING, n.leader:2, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:22 java[24055]: 2020-03-31 20:05:22,539 [myid:1] - WARN [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):Follower@129] - Exception when following the leader
Mar 31 20:05:22 java[24055]: java.lang.IllegalArgumentException
Mar 31 20:05:22 java[24055]: at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1307)
Mar 31 20:05:22 java[24055]: at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1195)
Mar 31 20:05:22 java[24055]: at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:89)
Mar 31 20:05:22 java[24055]: at org.apache.zookeeper.server.quorum.Learner.connectToLeader(Learner.java:275)
Mar 31 20:05:22 java[24055]: at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:87)
Mar 31 20:05:22 java[24055]: at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1455)
Mar 31 20:05:22 java[24055]: 2020-03-31 20:05:22,539 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):Follower@292] - shutdown Follower
Mar 31 20:05:22 java[24055]: 2020-03-31 20:05:22,539 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@863] - Peer state changed: looking
Mar 31 20:05:22 java[24055]: 2020-03-31 20:05:22,539 [myid:1] - WARN [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@1501] - PeerState set to LOOKING
Mar 31 20:05:22 java[24055]: 2020-03-31 20:05:22,539 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@1371] - LOOKING
Mar 31 20:05:22 java[24055]: 2020-03-31 20:05:22,539 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):FastLeaderElection@931] - New election. My id = 1, proposed zxid=0x10002b653
Mar 31 20:05:22 java[24055]: 2020-03-31 20:05:22,539 [myid:1] - INFO [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:1, n.state:LOOKING, n.leader:1, n.round:0x4c, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:22 java[24055]: 2020-03-31 20:05:22,542 [myid:1] - INFO [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:3, n.state:LEADING, n.leader:3, n.round:0x4a, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:22 java[24055]: 2020-03-31 20:05:22,544 [myid:1] - INFO [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:2, n.state:LEADING, n.leader:2, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:22 java[24055]: 2020-03-31 20:05:22,744 [myid:1] - INFO [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:1, n.state:LOOKING, n.leader:1, n.round:0x4c, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:22 java[24055]: 2020-03-31 20:05:22,744 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):FastLeaderElection@966] - Notification time out: 400
Mar 31 20:05:22 java[24055]: 2020-03-31 20:05:22,745 [myid:1] - INFO [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:2, n.state:LEADING, n.leader:2, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:22 java[24055]: 2020-03-31 20:05:22,746 [myid:1] - INFO [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:3, n.state:LEADING, n.leader:3, n.round:0x4a, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0{code}

----------
On node 2 in the same time:

Mar 31 20:04:39 java[23556]: 2020-03-31 20:04:39,764 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LEADING; n.sid:1, n.state:LOOKING, n.leader:1, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:21 java[23556]: 2020-03-31 20:05:21,130 [myid:2] - WARN  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@1471] - Unexpected exception
Mar 31 20:05:21 java[23556]: java.lang.InterruptedException: Timeout while waiting for epoch from quorum
Mar 31 20:05:21 java[23556]:         at org.apache.zookeeper.server.quorum.Leader.getEpochToPropose(Leader.java:1425)
Mar 31 20:05:21 java[23556]:         at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:599)
Mar 31 20:05:21 java[23556]:         at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1468)
Mar 31 20:05:21 java[23556]: 2020-03-31 20:05:21,131 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):Leader@792] - Shutting down
Mar 31 20:05:21 java[23556]: 2020-03-31 20:05:21,131 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):Leader@798] - Shutdown called. For the reason Forcing shutdown
Mar 31 20:05:21 java[23556]: 2020-03-31 20:05:21,131 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@863] - Peer state changed: looking
Mar 31 20:05:21 java[23556]: 2020-03-31 20:05:21,131 [myid:2] - WARN  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@1501] - PeerState set to LOOKING
Mar 31 20:05:21 java[23556]: 2020-03-31 20:05:21,131 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@1371] - LOOKING
Mar 31 20:05:21 java[23556]: 2020-03-31 20:05:21,131 [myid:2] - WARN  [LearnerCnxAcceptorHandler-/172.30.2.252:2888:Leader$LearnerCnxAcceptor$LearnerCnxAcceptorHandler@523] - Exception while shutting down acceptor.
Mar 31 20:05:21 java[23556]: java.net.SocketException: Socket closed
Mar 31 20:05:21 java[23556]:         at java.net.PlainSocketImpl.socketAccept(Native Method)
Mar 31 20:05:21 java[23556]:         at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)
Mar 31 20:05:21 java[23556]:         at java.net.ServerSocket.implAccept(ServerSocket.java:545)
Mar 31 20:05:21 java[23556]:         at java.net.ServerSocket.accept(ServerSocket.java:513)
Mar 31 20:05:21 java[23556]:         at org.apache.zookeeper.server.quorum.Leader$LearnerCnxAcceptor$LearnerCnxAcceptorHandler.acceptConnections(Leader.java:510)
Mar 31 20:05:21 java[23556]:         at org.apache.zookeeper.server.quorum.Leader$LearnerCnxAcceptor$LearnerCnxAcceptorHandler.run(Leader.java:493)
Mar 31 20:05:21 java[23556]:         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
Mar 31 20:05:21 java[23556]:         at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Mar 31 20:05:21 java[23556]:         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
Mar 31 20:05:21 java[23556]:         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
Mar 31 20:05:21 java[23556]:         at java.lang.Thread.run(Thread.java:745)
Mar 31 20:05:21 java[23556]: 2020-03-31 20:05:21,131 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):FastLeaderElection@931] - New election. My id = 2, proposed zxid=0x10002b653
Mar 31 20:05:21 java[23556]: 2020-03-31 20:05:21,133 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:2, n.state:LOOKING, n.leader:2, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:21 java[23556]: 2020-03-31 20:05:21,134 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:1, n.state:LOOKING, n.leader:2, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:21 java[23556]: 2020-03-31 20:05:21,134 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:3, n.state:LEADING, n.leader:3, n.round:0x4a, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:21 java[23556]: 2020-03-31 20:05:21,335 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):FastLeaderElection@966] - Notification time out: 400
Mar 31 20:05:21 java[23556]: 2020-03-31 20:05:21,335 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:2, n.state:LOOKING, n.leader:2, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:21 java[23556]: 2020-03-31 20:05:21,336 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:3, n.state:LEADING, n.leader:3, n.round:0x4a, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:21 java[23556]: 2020-03-31 20:05:21,736 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):FastLeaderElection@966] - Notification time out: 800
Mar 31 20:05:21 java[23556]: 2020-03-31 20:05:21,736 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:2, n.state:LOOKING, n.leader:2, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:21 java[23556]: 2020-03-31 20:05:21,737 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:1, n.state:FOLLOWING, n.leader:2, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:21 java[23556]: 2020-03-31 20:05:21,737 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:3, n.state:LEADING, n.leader:3, n.round:0x4a, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,538 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):FastLeaderElection@966] - Notification time out: 1600
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,538 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:1, n.state:FOLLOWING, n.leader:2, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,539 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@857] - Peer state changed: leading
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,539 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@1465] - LEADING
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,539 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):ZooKeeperServer@1246] - minSessionTimeout set to 6000
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,539 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):ZooKeeperServer@1255] - maxSessionTimeout set to 60000
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,539 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):ResponseCache@45] - Response cache size is initialized with value 400.
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,539 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):RequestPathMetricsCollector@111] - zookeeper.pathStats.slotCapacity = 60
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,539 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):RequestPathMetricsCollector@112] - zookeeper.pathStats.slotDuration = 15
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,539 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):RequestPathMetricsCollector@113] - zookeeper.pathStats.maxDepth = 6
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,540 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LEADING; n.sid:1, n.state:LOOKING, n.leader:1, n.round:0x4c, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,540 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):RequestPathMetricsCollector@114] - zookeeper.pathStats.initialDelay = 5
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,541 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):RequestPathMetricsCollector@115] - zookeeper.pathStats.delay = 5
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,541 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):RequestPathMetricsCollector@116] - zookeeper.pathStats.enabled = false
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,541 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):ZooKeeperServer@1470] - The max bytes for all large requests are set to 104857600
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,541 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):ZooKeeperServer@1484] - The large request threshold is set to -1
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,541 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):ZooKeeperServer@329] - Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 clientPortListenBacklog -1 datadir /kafka/kafka_zookeeper_data/logdir/version-2 snapdir /kafka/kafka_zookeeper_data/version-2
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,541 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):Leader@581] - LEADING - LEADER ELECTION TOOK - 1410 MS
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,541 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@863] - Peer state changed: leading - discovery
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,541 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):FileTxnSnapLog@470] - Snapshotting: 0x10002b653 to /kafka/kafka_zookeeper_data/version-2/snapshot.10002b653
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,544 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LEADING; n.sid:2, n.state:LOOKING, n.leader:2, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,544 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LEADING; n.sid:2, n.state:LEADING, n.leader:2, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,544 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LEADING; n.sid:3, n.state:LEADING, n.leader:3, n.round:0x4a, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:22 java[23556]: 2020-03-31 20:05:22,568 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):ZooKeeperServer@519] - Snapshot taken in 26 ms

----------------------
node 3:

Mar 31 20:05:21 java[20867]: 2020-03-31 20:05:21,133 [myid:3] - INFO  [WorkerReceiver[myid=3]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LEADING; n.sid:2, n.state:LOOKING, n.leader:2, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:21 java[20867]: 2020-03-31 20:05:21,134 [myid:3] - INFO  [WorkerReceiver[myid=3]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LEADING; n.sid:1, n.state:LOOKING, n.leader:2, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:21 java[20867]: 2020-03-31 20:05:21,335 [myid:3] - INFO  [WorkerReceiver[myid=3]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LEADING; n.sid:2, n.state:LOOKING, n.leader:2, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:21 java[20867]: 2020-03-31 20:05:21,736 [myid:3] - INFO  [WorkerReceiver[myid=3]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LEADING; n.sid:2, n.state:LOOKING, n.leader:2, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:22 java[20867]: 2020-03-31 20:05:22,540 [myid:3] - INFO  [WorkerReceiver[myid=3]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LEADING; n.sid:1, n.state:LOOKING, n.leader:1, n.round:0x4c, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:22 java[20867]: 2020-03-31 20:05:22,544 [myid:3] - INFO  [WorkerReceiver[myid=3]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LEADING; n.sid:2, n.state:LOOKING, n.leader:2, n.round:0x4b, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:22 java[20867]: 2020-03-31 20:05:22,745 [myid:3] - INFO  [WorkerReceiver[myid=3]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LEADING; n.sid:1, n.state:LOOKING, n.leader:1, n.round:0x4c, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
Mar 31 20:05:23 java[20867]: 2020-03-31 20:05:23,147 [myid:3] - INFO  [WorkerReceiver[myid=3]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LEADING; n.sid:1, n.state:LOOKING, n.leader:1, n.round:0x4c, n.peerEpoch:0x1, n.zxid:0x10002b653, message format version:0x2, n.config version:0x0
","[<JIRA Version: name='3.6.1', id='12346764'>]",Bug,ZOOKEEPER-3776,Major,Philipp,Duplicate,2020-04-02T16:53:42.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Cluster stuck not forming up quorum ,2020-04-02T17:00:15.000+0000,[],2.0
Shireen Nagdive,"[<JIRA Component: name='leaderElection', id='12312378'>]",2020-03-30T19:33:29.000+0000,Lasaro Camargos,"method run of QuorumCnxManager throws the following exception:

if (length <= 0 || length > PACKETMAXSIZE) {
     throw new IOException(""Received packet with invalid packet: "" + length);
 }

Instead of the current string, the cause should be ""Received packet with invalid length: """,[],Bug,ZOOKEEPER-3775,Trivial,Lasaro Camargos,,,This issue is being actively worked on at the moment by the assignee.,In Progress,0.0,Wrong message in IOException,2020-04-09T07:57:01.000+0000,[],2.0
,[],2020-03-28T04:42:10.000+0000,Jinjiang Ling,"When I upgrade my zookeeper cluster to 3.5.7, a *nessus scan* pinged the cluster because the 8080 port of JettyAdminServer allows HTTP TRACE method.","[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",Bug,ZOOKEEPER-3772,Major,Jinjiang Ling,Fixed,2020-05-05T07:22:18.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,JettyAdminServer should not allow HTTP TRACE method,2020-09-10T10:43:45.000+0000,"[<JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",2.0
Mate Szalay-Beko,"[<JIRA Component: name='leaderElection', id='12312378'>]",2020-03-24T18:49:05.000+0000,Lasaro Camargos,"In a cluster with three nodes, node3 is the leader and the other nodes are followers.

If I stop node3, the other two nodes do not finish the leader election.

This is happening with ZK 3.5.7,  openjdk version ""12.0.2"" 2019-07-16, and this config

 

tickTime=2000
 initLimit=30
 syncLimit=3
 dataDir=/company/service/data
 dataLogDir=/company/service/log
 clientPort=2181
 snapCount=100000
 autopurge.snapRetainCount=3
 autopurge.purgeInterval=1
 skipACL=yes
 preAllocSize=65536
 maxClientCnxns=0
 4lw.commands.whitelist=*
 admin.enableServer=false

server.1=companydemo1.snc4.companyinc.com:3000:4000
 server.2=companydemo2.snc4.companyinc.com:3000:4000
 server.3=companydemo3.snc4.companyinc.com:3000:4000

 

Could you have a look at the logs and help me figure this out? It seems like node 1 is not getting notifications back from node2, but I don't see anything wrong with the network so I am wondering if bugs like  ZOOKEEPER-3756 could be causing it.

 

In the logs, node3 is killed at 11:17:14

node2 is killed at 11:17:50 2 and node 1 at 11:18:02 

 

 

 ","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",Bug,ZOOKEEPER-3769,Major,Lasaro Camargos,Fixed,2020-04-07T07:09:17.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,fast leader election does not end if leader is taken down,2020-05-11T15:41:05.000+0000,"[<JIRA Version: name='3.5.7', id='12346098'>]",3.0
,[],2020-03-17T00:51:53.000+0000,Jinjiang Ling,"when I upgrade zookeeper from 3.4.13 to 3.5.7 in my application, I find the function processCmd in ZooKeeperMain.java is just like blow
{code:java}
protected boolean processCmd(MyCommandOptions co) throws CliException, IOException, InterruptedException {
    boolean watch = false;
    try {
        watch = processZKCmd(co);
        exitCode = ExitCode.EXECUTION_FINISHED.getValue();
    } catch (CliException ex) {
        exitCode = ex.getExitCode();
        System.err.println(ex.getMessage());
    }
    return watch;
}
{code}
it throws {color:#FF0000}CliException {color}which has been caught in the funciton, so I think it can be removed.","[<JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",Bug,ZOOKEEPER-3760,Major,Jinjiang Ling,Fixed,2020-03-26T17:47:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,remove a useless throwing CliException,2020-05-11T15:41:19.000+0000,"[<JIRA Version: name='3.5.7', id='12346098'>]",1.0
,[],2020-03-16T14:45:04.000+0000,Agostino Sarubbo,"The start script misses a way to configure a java_rmi port, see also:
https://issues.apache.org/jira/browse/KAFKA-8658
[https://github.com/apache/kafka/pull/7088/commits/d02e14da8752a08bfe4f837d1cfea2c7b51e07af]",[],Bug,ZOOKEEPER-3759,Minor,Agostino Sarubbo,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,A way to configure the jmx rmi port,2020-03-16T14:45:04.000+0000,[],1.0
Mate Szalay-Beko,"[<JIRA Component: name='server', id='12312382'>]",2020-03-16T09:52:59.000+0000,Agostino Sarubbo,"Hello,
 we have a cluster with 5 zookeeper servers. We tried the update from 3.5.7 to 3.6.0 but it does not work.

We got the following:
{code:java}
2020-03-16 10:40:45,514 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):QuorumPeer@863] - Peer state changed: looking 2020-03-16 10:40:45,514 [myid:1] - WARN  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):QuorumPeer@1501] - PeerState set to LOOKING 2020-03-16 10:40:45,514 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):QuorumPeer@1371] - LOOKING 2020-03-16 10:40:45,514 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):FastLeaderElection@931] - New election. My id = 1, proposed zxid=0x0 2020-03-16 10:40:45,515 [myid:1] - INFO  [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:1, n.state:LOOKING , n.leader:1, n.round:0x1b, n.peerEpoch:0x0, n.zxid:0x0, message format version:0x2, n.config version:0x0 2020-03-16 10:40:45,517 [myid:1] - INFO  [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:2, n.state:FOLLOWI NG, n.leader:4, n.round:0x1a, n.peerEpoch:0x5c, n.zxid:0x5b00000004, message format version:0x2, n.config version:0x0 2020-03-16 10:40:45,517 [myid:1] - INFO  [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:3, n.state:FOLLOWI NG, n.leader:4, n.round:0x1a, n.peerEpoch:0x5c, n.zxid:0x5b00000004, message format version:0x2, n.config version:0x0 2020-03-16 10:40:45,517 [myid:1] - INFO  [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:5, n.state:FOLLOWI NG, n.leader:4, n.round:0x1a, n.peerEpoch:0x5c, n.zxid:0x5b00000004, message format version:0x2, n.config version:0x0 2020-03-16 10:40:45,518 [myid:1] - INFO  [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@376] - Notification: my state:LOOKING; n.sid:4, n.state:LEADING , n.leader:4, n.round:0x1a, n.peerEpoch:0x5c, n.zxid:0x5b00000004, message format version:0x2, n.config version:0x0 2020-03-16 10:40:45,518 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):QuorumPeer@857] - Peer state changed: following 2020-03-16 10:40:45,518 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):QuorumPeer@1453] - FOLLOWING 2020-03-16 10:40:45,518 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):ZooKeeperServer@1246] - minSessionTimeout set to 4000 2020-03-16 10:40:45,518 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):ZooKeeperServer@1255] - maxSessionTimeout set to 40000 2020-03-16 10:40:45,519 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):ResponseCache@45] - Response cache size is initialized with value 400. 2020-03-16 10:40:45,519 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):ResponseCache@45] - Response cache size is initialized with value 400. 2020-03-16 10:40:45,519 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):RequestPathMetricsCollector@111] - zookeeper.pathStats.slotCapacity = 60 2020-03-16 10:40:45,519 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):RequestPathMetricsCollector@112] - zookeeper.pathStats.slotDuration = 15 2020-03-16 10:40:45,519 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):RequestPathMetricsCollector@113] - zookeeper.pathStats.maxDepth = 6 2020-03-16 10:40:45,519 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):RequestPathMetricsCollector@114] - zookeeper.pathStats.initialDelay = 5 2020-03-16 10:40:45,519 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):RequestPathMetricsCollector@115] - zookeeper.pathStats.delay = 5 2020-03-16 10:40:45,519 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):RequestPathMetricsCollector@116] - zookeeper.pathStats.enabled = false 2020-03-16 10:40:45,519 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):ZooKeeperServer@1470] - The max bytes for all large requests are set t o 104857600 2020-03-16 10:40:45,519 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):ZooKeeperServer@1484] - The large request threshold is set to -1 2020-03-16 10:40:45,519 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):ZooKeeperServer@329] - Created server with tickTime 2000 minSessionTim eout 4000 maxSessionTimeout 40000 clientPortListenBacklog -1 datadir /opt/loway/zookeeper/logs/version-2 snapdir /opt/loway/zookeeper/data/version-2 2020-03-16 10:40:45,519 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):Follower@75] - FOLLOWING - LEADER ELECTION TOOK - 4 MS 2020-03-16 10:40:45,519 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):QuorumPeer@863] - Peer state changed: following - discovery 2020-03-16 10:40:46,521 [myid:1] - WARN  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):Follower@129] - Exception when following the leader java.lang.IllegalArgumentException        at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1314)        at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1202)        at java.util.concurrent.Executors.newFixedThreadPool(Executors.java:89)        at org.apache.zookeeper.server.quorum.Learner.connectToLeader(Learner.java:275)        at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:87)        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1455) 2020-03-16 10:40:46,521 [myid:1] - INFO  [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=0.0.0.0:2281):Follower@292] - shutdown Follower{code}","[<JIRA Version: name='3.6.1', id='12346764'>]",Bug,ZOOKEEPER-3758,Major,Agostino Sarubbo,Fixed,2020-03-19T16:10:45.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Update from 3.5.7 to 3.6.0 does not work,2020-04-02T16:53:42.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",3.0
,"[<JIRA Component: name='leaderElection', id='12312378'>]",2020-03-14T16:33:42.000+0000,Alex Kaiser,"Short overview:

If you have a large snapCount (we are using 10,000,000) you can end up with a very large transaction log (ours are between 1GB - 1.5 GB), which can cause the sync between a newly elected leader and it's followers to take 20+ seconds.  This stems from the code (FileTxnIterator.getStorageSize()) in most cases returning 0 even if the transaction log is 1GB.

 

Long Explanation:

A few years ago we had some trouble with our zookeeper cluster having many shortish (100-500ms) pauses during our peak traffic times.  These ended up resulting from the master taking a snap shot.  To solve this we upped the snapCount to 10,000,000 so that we weren't taking snapshots nearly as often.  We also made changes to reduce the size of our snapshots (from around 2.5 GB to ~500 MB).

I don't remember what version of zookeeper we were using originally, but this was all working fine using 3.4.10, but we started to have problems when we upgraded to 3.5.6 around 3 months ago.  We have a fairly high transaction rate and thus end up hitting the zxid overflow about once a month, which will cause a leader election.  When we were on 3.4.10, this was fine because leader election and syncing would happen within 2-4 seconds, which was low enough for us to be able to basically ignore it.  However after we upgraded to 3.5.6 the pauses we saw took between 15 - 30 seconds which were unacceptable for us.

For now to solve this I set zookeeper.forceSnapshotSync=true (yes, I know the comments say this is only supposed to be used for testing), which causes syncing using snapshots (only 10-50 MB) instead of the transaction log (1-1.5 GB).

 

Technical details:

I tried taking a look at the code and I think I know why this happens.  From what I learned, it looks like when a follower needs to sync with a leader on the leader LearnerHandler.syncFollower() gets called.  It goes through a big if statement, but at one point it will call db.getProposalsFromTxnLog(peerLastZxid, sizeLimit).  That peerLastZxid could be some very old zxid if the follower hadn't taken a snapshot in a long time (i.e. has a large snapCount) and the sizeLimit will generally be 0.33 * snapshot size (in my case around 10 MB).

Inside of that getProposalsFromTxnLog it will create a TxnIterator and then call getStorageSize() on it.  The problem comes from the fact that this call to getStorageSize() will usually return with 0.  The reason that happens is because the FileTxnIterator class has a ""current"" log file that it is reading, this.logFile, and a list of files that it would still have to iterate through, this.storedFiles.  The getStroageSize() function though only looks at the storedFiles list, so if the iterator has one large transaction log as the ""current"" log file and nothing in the storedFiles list, then this method will return 0 even though there is a huge transaction log to sync.

One other side affect of this problem is that even bouncing a follower can cause long (5-10 second) pauses as the leader will hold a read lock on the transaction log while it syncs up with the follower.

While I know what the problem is I don't know what the best solution is.  I'm willing to work on the solution, but I would appreciate suggestions.  One possible solution would be to include the this.logFile in the getStorageSize() calculation, however this could cause the estimate to over estimate the amount of data that is in the iterator (possibly by a lot) and I don't know what the consequences of doing that is.  I'm not quite sure what is a good way to get an accurate estimate.

 ",[],Bug,ZOOKEEPER-3757,Minor,Alex Kaiser,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Transaction log sync can take 20+ seconds after leader election when there is a large snapCount,2020-03-14T16:33:42.000+0000,"[<JIRA Version: name='3.5.6', id='12345243'>]",2.0
Mate Szalay-Beko,"[<JIRA Component: name='leaderElection', id='12312378'>]",2020-03-11T20:43:33.000+0000,Dai Shi,"Not sure if this is the place to ask, please close if it's not.

I am seeing some behavior that I can't explain since upgrading to 3.5:

In a 5 member quorum, when server 3 is the leader and each server has this in their configuration: 
{code:java}
server.1=100.71.255.254:2888:3888:participant;2181
server.2=100.71.255.253:2888:3888:participant;2181
server.3=100.71.255.252:2888:3888:participant;2181
server.4=100.71.255.251:2888:3888:participant;2181
server.5=100.71.255.250:2888:3888:participant;2181{code}
If servers 1 or 2 are restarted, they fail to rejoin the quorum with this in the logs:
{code:java}
2020-03-11 20:23:35,720 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):QuorumPeer@1175] - LOOKING
2020-03-11 20:23:35,721 [myid:2] - INFO  [QuorumPeer[myid=2](plain=0.0.0.0:2181)(secure=disabled):FastLeaderElection@885] - New election. My id =  2, proposed zxid=0x1b8005f4bba
2020-03-11 20:23:35,733 [myid:2] - INFO  [WorkerSender[myid=2]:QuorumCnxManager@438] - Have smaller server identifier, so dropping the connection: (3, 2)
2020-03-11 20:23:35,734 [myid:2] - INFO  [0.0.0.0/0.0.0.0:3888:QuorumCnxManager$Listener@924] - Received connection request 100.126.116.201:36140
2020-03-11 20:23:35,735 [myid:2] - INFO  [WorkerSender[myid=2]:QuorumCnxManager@438] - Have smaller server identifier, so dropping the connection: (4, 2)
2020-03-11 20:23:35,740 [myid:2] - INFO  [WorkerSender[myid=2]:QuorumCnxManager@438] - Have smaller server identifier, so dropping the connection: (5, 2)
2020-03-11 20:23:35,740 [myid:2] - INFO  [0.0.0.0/0.0.0.0:3888:QuorumCnxManager$Listener@924] - Received connection request 100.126.116.201:36142
2020-03-11 20:23:35,740 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection@679] - Notification: 2 (message format version), 2 (n.leader), 0x1b8005f4bba (n.zxid), 0x1 (n.round), LOOKING (n.state), 2 (n.sid), 0x1b8 (n.peerEPoch), LOOKING (my state)0 (n.config version)
2020-03-11 20:23:35,742 [myid:2] - WARN  [SendWorker:3:QuorumCnxManager$SendWorker@1143] - Interrupted while waiting for message on queue
java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2088)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.pollSendQueue(QuorumCnxManager.java:1294)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.access$700(QuorumCnxManager.java:82)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:1131)
2020-03-11 20:23:35,744 [myid:2] - WARN  [SendWorker:3:QuorumCnxManager$SendWorker@1153] - Send worker leaving thread  id 3 my id = 2
2020-03-11 20:23:35,745 [myid:2] - WARN  [RecvWorker:3:QuorumCnxManager$RecvWorker@1230] - Interrupting SendWorker{code}
The only way I can seem to get them to rejoin the quorum is to restart the leader.

However, if I remove server 4 and 5 from the configuration of server 1 or 2 (so only servers 1, 2, and 3 remain in the configuration file), then they can rejoin the quorum fine. Is this expected and am I doing something wrong? Any help or explanation would be greatly appreciated. Thank you.","[<JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",Bug,ZOOKEEPER-3756,Major,Dai Shi,Fixed,2020-03-23T15:20:55.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Members failing to rejoin quorum,2020-05-20T07:06:58.000+0000,"[<JIRA Version: name='3.5.6', id='12345243'>, <JIRA Version: name='3.5.7', id='12346098'>]",5.0
Jean-Baptiste Onofré,"[<JIRA Component: name='build', id='12312383'>]",2020-03-09T07:30:48.000+0000,Amichai Rothman,"Trying to upgrade Aries RSA from ZooKeeper 3.4.14 to 3.6.0, I found that itests (pax exam) fail - only to discover that the new ZooKeeper release jar is now missing all OSGi headers.

ZooKeeper releases are generally few and far in between, which makes it disappointing to see such regression bugs - it means other projects must remain on ancient versions of ZK for a long time.

It would be great if the OSGi headers could be added back and a new minor release cut expeditiously, or alternatively a separate zookeeper-osgi bundle be distributed in addition to the standard one if there are issues with the standard one for some reason.

 ",[],Bug,ZOOKEEPER-3754,Major,Amichai Rothman,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper unusable in OSGi - missing headers,2021-08-10T09:31:18.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",1.0
Damien Diederen,"[<JIRA Component: name='documentation', id='12312422'>]",2020-03-09T05:01:18.000+0000,Vik Gamov,"Hi 

it looks like the documentation website isn't working properly. All links are 404.

 

[https://zookeeper.apache.org/doc/r3.4.8/zookeeperAdmin.html#sc_zkCommands|file:///Users/viktor/projects/confluent/ps-recommendations/src/docs/asciidoc/whitepapers/#linkCheck33]",[],Bug,ZOOKEEPER-3753,Major,Vik Gamov,Duplicate,2020-03-09T07:47:14.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Documentation website is broken,2020-03-09T07:47:34.000+0000,[],1.0
Patrick D. Hunt,"[<JIRA Component: name='security', id='12329414'>]",2020-03-08T02:19:36.000+0000,Patrick D. Hunt,"owasp is flagging jackson-databind again due to CVE-2020-9547, CVE-2020-9548, CVE-2020-9546

We need to update to 2.9.10.4",[],Bug,ZOOKEEPER-3750,Blocker,Patrick D. Hunt,Won't Do,2020-04-30T10:19:25.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"update jackson-databind to address CVE-2020-9547, CVE-2020-9548, CVE-2020-9546",2020-04-30T10:19:25.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.5.7', id='12346098'>]",1.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2020-03-08T00:59:04.000+0000,Lisa Beal,"[https://zookeeper.apache.org/documentation.html] links all dead, rendering all Zookeeper documentation inaccessible.","[<JIRA Version: name='3.5.7', id='12346098'>]",Bug,ZOOKEEPER-3749,Minor,Lisa Beal,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,https://zookeeper.apache.org/documentation.html links all dead,2020-03-09T07:47:47.000+0000,"[<JIRA Version: name='3.5.7', id='12346098'>]",2.0
Zili Chen,[],2020-03-07T16:47:51.000+0000,Zili Chen,"there is no link to the KEYS file - this is essential for
verifying sigs.

Also the links to release artifacts must not use downloads.a.o, they must
use the mirror system.","[<JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3748,Major,Zili Chen,Fixed,2020-03-14T16:49:30.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Resolve release requirements in download page,2021-03-28T08:54:00.000+0000,[],1.0
Mate Szalay-Beko,[],2020-03-06T18:24:56.000+0000,SledgeHammer,"I have updated my 3.6.0 zkServer.cmd to include the new ""-Dzookeeper.sessionRequireClientSASLAuth=true"" flag. However, I am still able to connect with anonymous Kafka clients. Am I missing something?",[],Bug,ZOOKEEPER-3747,Major,SledgeHammer,Cannot Reproduce,2020-03-09T16:36:47.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zookeeper.sessionRequireClientSASLAuth not working,2020-03-09T17:19:06.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",2.0
Zili Chen,[],2020-03-05T10:56:48.000+0000,Zili Chen,,[],Bug,ZOOKEEPER-3746,Major,Zili Chen,Fixed,2020-03-07T13:35:57.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Move the download page to downloads.apache.org ,2020-03-07T13:36:21.000+0000,[],1.0
Zili Chen,[],2020-03-05T10:54:17.000+0000,Zili Chen,,"[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.1', id='12346764'>]",Bug,ZOOKEEPER-3745,Major,Zili Chen,Fixed,2020-03-14T16:52:16.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Update copyright notices from 2019 to 2020,2021-03-28T08:55:30.000+0000,[],1.0
Christopher Tubbs,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='server', id='12312382'>]",2020-02-25T18:51:23.000+0000,Christopher Tubbs,"For better support building on newer JDKs, the unsupported class, com.sun.nio.file.SensitivityWatchEventModifier, must not be used.

I will submit a PR for this.","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",Bug,ZOOKEEPER-3739,Major,Christopher Tubbs,Fixed,2020-03-20T10:00:59.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Remove use of com.sun.nio.file.SensitivityWatchEventModifier,2020-05-11T15:41:21.000+0000,[],2.0
Christopher Tubbs,"[<JIRA Component: name='build', id='12312383'>]",2020-02-25T17:38:14.000+0000,Christopher Tubbs,"properties-maven-plugin uses an older version of plexus utils, that fails to read ENV variables properly when a variable is multi-line.

This bug causes it to be difficult to build ZooKeeper in some environments (Fedora, with a default bash 4 shell, for example).

Since ZooKeeper only uses this plugin to get the git commit id, the plugin can be removed, and replaced with a more specific plugin that can achieve the same job with simpler config (https://github.com/koraktor/mavanagaiata)

I'm working on a PR for this, which will come shortly.","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.1', id='12346764'>]",Bug,ZOOKEEPER-3738,Major,Christopher Tubbs,Fixed,2020-04-15T15:34:42.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Avoid use of broken codehaus properties-maven-plugin,2022-01-28T08:37:05.000+0000,[],2.0
Christopher Tubbs,"[<JIRA Component: name='jmx', id='12312451'>, <JIRA Component: name='server', id='12312382'>]",2020-02-25T01:20:46.000+0000,Christopher Tubbs,"Apache Accumulo is trying to switch to using log4j2 only. However, this seems impossible, because ZooKeeper has a hard-coded dependency on log4j 1.2 for some sort of jmx thing. The following is the error and stack trace I get whenever I remove log4j 1.2 from the class path and try to run a test instance of ZooKeeper as part of Accumulo's build test suite.

{code}
2020-02-24T20:10:03,682 [jmx.ManagedUtil] ERROR: Problems while registering log4j jmx beans!
java.lang.ClassNotFoundException: org.apache.log4j.jmx.HierarchyDynamicMBean
	at jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581) ~[?:?]
	at jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178) ~[?:?]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:521) ~[?:?]
	at java.lang.Class.forName0(Native Method) ~[?:?]
	at java.lang.Class.forName(Class.java:315) ~[?:?]
	at org.apache.zookeeper.jmx.ManagedUtil.registerLog4jMBeans(ManagedUtil.java:72) ~[zookeeper-3.5.7.jar:3.5.7]
	at org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:94) ~[zookeeper-3.5.7.jar:3.5.7]
	at org.apache.zookeeper.server.ZooKeeperServerMain.main(ZooKeeperServerMain.java:64) ~[zookeeper-3.5.7.jar:3.5.7]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]
	at org.apache.accumulo.start.Main.lambda$execMainClass$1(Main.java:167) ~[accumulo-start-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:834) [?:?]
{code}

I know previous work has been done on ZOOKEEPER-850 and ZOOKEEPER-1371 to eliminate the use of log4j in the source, but this work does not appear to be complete, since it is still required at runtime (at least, for the server... but maybe for the client too... it's hard to tell from the way Accumulo runs its test suite, and I'm not super familiar with ZK internals).","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",Bug,ZOOKEEPER-3737,Major,Christopher Tubbs,Fixed,2020-03-01T06:56:54.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Unable to eliminate log4j1 transitive dependency,2020-05-11T15:41:20.000+0000,"[<JIRA Version: name='3.4.14', id='12343587'>, <JIRA Version: name='3.5.7', id='12346098'>]",4.0
,[],2020-02-24T22:15:42.000+0000,Shubham S,"Hi, I am building a zookeeper docker image from official docker images. I am currently using Zookeeper 3.4.14. I have 3 containers, each running a zookeeper server and I am setting the environment variable as shown in the docker-compose.yml file 

environment:

  ZOO_AUTOPURGE_PURGEINTERVAL: 24
  ZOO_AUTOPURGE_SNAPRETAINCOUNT: 3

I can clearly see the values reflecting back in my zoo.cfg.

 
{code:java}
cat 
/conf/zoo.cfg 
clientPort=xxxx 
dataDir=/data 
dataLogDir=/datalog 
tickTime=2000 
initLimit=5 
syncLimit=2 
autopurge.snapRetainCount=3 
autopurge.purgeInterval=24 
maxClientCnxns=60 
server.1=zoo1:xxxx:xxxx server.2=zoo2:xxxx:xxxx server.3=0.0.0.0:xxxx:xxxx
{code}
 

Getting in the container and doing a printenv, I can see the values reflecting back as well.

 
{code:java}
printenv 
ZOO_DATA_LOG_DIR=/datalog 
HOSTNAME=0536b195f621 
JAVA_HOME=/usr/local/openjdk-8 
ZOO_DATA_DIR=/data 
JAVA_BASE_URL=https://github.com/AdoptOpenJDK/openjdk8-upstream-binaries/releases/download/jdk8u232-b09/OpenJDK8U-jre_ 
ZOO_INIT_LIMIT=5 
PWD=/datalog/version-2 
JAVA_URL_VERSION=8u232b09 
ZOO_AUTOPURGE_SNAPRETAINCOUNT=3 
HOME=/root 
LANG=C.UTF-8 
ZOO_SYNC_LIMIT=2 
ZOO_SERVERS=server.1=zoo1:xxxx:xxxx server.2=zoo2:xxxx:xxxx server.3=0.0.0.0:xxxx:xxxx 
SHLVL=1 
ZOO_MY_ID=3 
ZOO_MAX_CLIENT_CNXNS=60 
ZOO_TICK_TIME=2000 
ZOO_CONF_DIR=/conf PATH=/usr/local/openjdk-8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/zookeeper-3.4.14/bin 
ZOOCFGDIR=/conf 
ZOO_AUTOPURGE_PURGEINTERVAL=24 
JAVA_VERSION=8u232 
ZOO_LOG_DIR=/logs 
OLDPWD=/zookeeper-3.4.14 _=/usr/bin/printenv
{code}
 

 

I can also clearly see the purge task being completed as well

 
{code:java}
020-02-18T16:49:56.605549689Z 2020-02-18 16:49:56,604 [myid:3] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started. 2020-02-18T16:49:56.636000804Z 2020-02-18 16:49:56,635 [myid:3] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed. 2020-02-19T16:49:56.606280261Z 2020-02-19 16:49:56,605 [myid:3] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started. 2020-02-19T16:49:56.657389039Z 2020-02-19 16:49:56,657 [myid:3] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed. 2020-02-20T16:49:56.605362615Z 2020-02-20 16:49:56,604 [myid:3] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started. 2020-02-20T16:49:56.612265088Z 2020-02-20 16:49:56,611 [myid:3] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed. 2020-02-21T16:49:56.605773207Z 2020-02-21 16:49:56,604 [myid:3] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started. 2020-02-21T16:49:56.643037255Z 2020-02-21 16:49:56,642 [myid:3] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed. 2020-02-22T16:49:56.605712054Z 2020-02-22 16:49:56,605 [myid:3] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started. 2020-02-22T16:49:56.661826480Z 2020-02-22 16:49:56,661 [myid:3] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed. 2020-02-23T16:49:56.606569211Z 2020-02-23 16:49:56,604 [myid:3] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started. 2020-02-23T16:49:56.629269327Z 2020-02-23 16:49:56,628 [myid:3] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed. 2020-02-24T16:49:56.605299157Z 2020-02-24 16:49:56,604 [myid:3] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started. 2020-02-24T16:49:56.606483941Z 2020-02-24 16:49:56,606 [myid:3] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
{code}
 

But neither the logs nor the snapshots are being deleted.

I have redeployed the entire stack and even build a new image from the official docker images but I still am getting the same result.

 

Using the following command does work correctly but I don't want to do it manually.
./zkCleanup.sh -n 3

 

Can someone help me out? ",[],Bug,ZOOKEEPER-3736,Major,Shubham S,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper auto purge process does not purge files,2020-04-21T13:29:01.000+0000,"[<JIRA Version: name='3.4.14', id='12343587'>]",4.0
,"[<JIRA Component: name='server', id='12312382'>]",2020-02-24T01:35:26.000+0000,Ling Mao,"{code:java}
} else if (digestFromLoadedSnapshot.zxid != 0 && zxid > digestFromLoadedSnapshot.zxid) {
    RATE_LOGGER.rateLimitLog(""The txn 0x{} of snapshot digest does not ""
            + ""exist."", Long.toHexString(digestFromLoadedSnapshot.zxid));
}
{code}
the printed log likes this:
{code:java}
Message:The txn 0x{} of snapshot digest does not exist. Value:fa4e00000082
{code}
*1. 0x{}* takes no effort

*2. RATE_LOGGER.rateLimitLog* doesn't use like the ordinary LOG",[],Bug,ZOOKEEPER-3735,Minor,Ling Mao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,fix the bad format of RATE_LOGGER,2020-03-05T09:48:15.000+0000,[],2.0
Mate Szalay-Beko,"[<JIRA Component: name='c client', id='12312380'>]",2020-02-13T16:45:25.000+0000,Vladislav Tyulbashev,"Zookeeper C Client periodically resolves server names since https://issues.apache.org/jira/browse/ZOOKEEPER-1355

After dns resolution ip addresses are checked against previous set of addresses. However, currently only several bytes are checked (it is assumed, that all addresses are ipv4 length bytes).

Case:
 1) zookeeper server operates only by ipv6
 2) client connects to it by some hostname (zookeeper-1.news.yandex.ru, for example)
 3) container with zookeeper server dies, new container is up, and zookeeper-1.news.yandex.ru now points to new address
 4) several bits in ipv6 address are changed
 5) zookeeper client ignores changes in address, because of incorrect strcmp size and first bytes were equal
 6) zookeeper client now can't reconnect to zookeeper without manual intervention, because it tries old address

This is proposed fix: [https://github.com/apache/zookeeper/pull/1252]","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",Bug,ZOOKEEPER-3726,Major,Vladislav Tyulbashev,Fixed,2020-04-11T07:07:06.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,invalid ipv6 address comparison in C client,2020-05-11T15:41:18.000+0000,"[<JIRA Version: name='3.5.6', id='12345243'>]",1.0
Mate Szalay-Beko,[],2020-02-13T08:36:17.000+0000,Antoine DESSAIGNE,"Hello everyone,

We noticed that with Zookeeper 3.5.6, it fails to establish quorum on a new deployment on a regular basis (approx 50% of the time)

We were able to reduce the reproduction steps to the bare minimum we could. Consider the following docker-compose.yml file
{noformat}
version: '2'
services:
  orchestrator1.cameltest.int:
    image: zookeeper:3.5.6
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=0.0.0.0:2888:3888 server.2=orchestrator2.cameltest.int:2888:3888
  orchestrator2.cameltest.int:
    image: zookeeper:3.5.6
    environment:
      ZOO_MY_ID: 2
      ZOO_SERVERS: server.1=orchestrator1.cameltest.int:2888:3888 server.2=0.0.0.0:2888:3888
{noformat}
When launching a brand new cluster with it (with {{docker-compose up}}, no previous data) it fails half of the time with 3.5.6 and never in 3.4.14.

You'll find attached 3 logs:
* a failure one using 3.5.6
* a success one using 3.5.6
* a success one 3.4.14

I don't think it's related to some docker/docker-compose issue (as it's working using 3.4.14 on the same server)

I'll try to check each intermediate release to pin a more specific version.

Unfortunately, I don't know yet my way in the Zookeeper code, what can I do to help? Thanks!

PS: Yes, it's strange to have 2 servers as they're both required to work, but it's the smallest repro-case",[],Bug,ZOOKEEPER-3725,Major,Antoine DESSAIGNE,Duplicate,2020-03-09T12:05:06.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper fails to establish quorum with 2 servers using 3.5.6,2020-03-09T12:05:06.000+0000,"[<JIRA Version: name='3.5.6', id='12345243'>]",6.0
,"[<JIRA Component: name='java client', id='12312381'>]",2020-02-13T06:01:42.000+0000,Deepak Vilakkat,"[https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/ClientCnxn.java#L439]

This makes scaling zookeeper an issue without notifying all clients that they need to increase the sessionTimeout to a large value. Already had a production outage when a client in an asia data-center was trying to write to a zookeeper server in america for cross-colo announcements. The session timeout was kept at 5000ms and was working all the while but the cluster size was increased which made this value less than 200ms. Since its technically impossible to connect with this value, we increased session timeout.

 

Shouldn't there be a floor value like 5 seconds, below which this value shouldn't drop. Theoretically this calculation can make connecting over Local network also timeout in some use cases.

 

This was also discussed in [http://zookeeper-user.578899.n2.nabble.com/How-to-modify-Client-Connection-timer-td7583017.html#a7583019] and I am trying to understand if there is some other catch for this implementation.",[],Bug,ZOOKEEPER-3724,Major,Deepak Vilakkat,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,[Java Client] - Calculation of connectionTimeout needs improvement.,2020-02-13T06:01:42.000+0000,[],1.0
Damien Diederen,"[<JIRA Component: name='c client', id='12312380'>]",2020-02-07T15:45:16.000+0000,Damien Diederen,"The C client included in {{release-3.5.7-rc0}} and {{release-3.5.7-rc1}} suffers from a few issues:

# It configures, but ""forgets"" to build the C code in the {{full-build}} profile;
# Compilation actually fails with GCC 8.3, as the the {{Makefile}} uses {{-Werror}} and the compiler detects a couple possible buffer overruns;
# The {{WIN32}} branch of the code does not compile because of a change in socket representation.

This should probably be set to ""blocker,"" but I don't know if the C client is supposed to block a release.  Oh, and the first issue, at least, also existed in 3.5.6—and it seems nobody complained :)

A ""pull request"" is in the works.","[<JIRA Version: name='3.5.7', id='12346098'>]",Bug,ZOOKEEPER-3719,Major,Damien Diederen,Fixed,2020-02-09T13:55:28.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,C Client compilation issues in 3.5.7-rc,2020-02-27T13:13:39.000+0000,"[<JIRA Version: name='3.5.6', id='12345243'>]",1.0
Norbert Kalmár,[],2020-02-07T13:28:50.000+0000,Norbert Kalmár,"As [~eolivelli] pointed out:

{quote}
I see differences between the contents of the source tarball and the
git tag (using Meld, as suggested by Patrick some month ago), namely:
- there is not checkstyleSuppressions.xml file, and mvn
checkstyle:check fails (it is not bound to the default lifecycle, so
mvn clean install still works)
- there are "".c"" generated files, they should not be part of the source release
- there is not ""dev"" directory
- there is not .travis.yml file
{quote}

Note: only affects branch-3.5.","[<JIRA Version: name='3.5.7', id='12346098'>]",Bug,ZOOKEEPER-3718,Major,Norbert Kalmár,Fixed,2020-02-10T07:14:36.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Generated source tarball is missing some files,2020-05-07T10:26:52.000+0000,"[<JIRA Version: name='3.5.6', id='12345243'>]",1.0
,[],2020-02-04T18:30:11.000+0000,Baohua,"*Issue*:

When use swarm service to create a zookeeper cluster with service VIP and enable healthcheck in the zookeeper docker image (even directly return 0), the client cannot connect to the server (status is connecting, and then retry), but the server status will report everything is OK.

*Root cause*:

When swarm service is started, the service VIP does not work until the healthcheck is passed. At that time, seems the client service is in an abnormal status that even after healthcheck is passed, and the zookeeper cluster is normal, no client can connect to the zookeeper.

*Solution (potential)*:

After the cluster status becomes OK, need to reset the client service to allow connection.",[],Bug,ZOOKEEPER-3717,Major,Baohua,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper won't work with swarm service VIP when enabling healthcheck,2020-02-04T20:21:44.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.4.14', id='12343587'>, <JIRA Version: name='3.5.6', id='12345243'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='security', id='12329414'>, <JIRA Component: name='server', id='12312382'>]",2020-02-03T19:05:11.000+0000,Patrick D. Hunt,"OWASP dependency-check is failing 

upgrade netty 4.1.42 to address CVE-2019-20444 CVE-2019-20445

[ERROR] netty-transport-4.1.42.Final.jar: CVE-2019-20445, CVE-2019-20444

We need to upgrade to netty 4.1.45 (current latest) or later","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.7', id='12346098'>]",Bug,ZOOKEEPER-3716,Blocker,Patrick D. Hunt,Fixed,2020-02-04T09:35:57.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,upgrade netty 4.1.42 to address CVE-2019-20444 CVE-2019-20445,2020-02-14T15:23:31.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.5.6', id='12345243'>]",2.0
Pierre Yin,"[<JIRA Component: name='server', id='12312382'>]",2020-02-03T10:50:34.000+0000,Pierre Yin,"The Follower/Observer may load snapshot from disk or leader in some scenarios. During the snapshot loading, the follower/observer may lose the connection from leader when the network is broken.In current design, follower/observer would switch into ReadOnly mode immediately when the network connection from leader is broken. So follower/observer may become ReadOnlyZooKeeperServer before the ZKDatabase initialization of snapshot loading is finished. The time window between follower/observer ReadOnly mode's successful switch and the ZkDatabase's full snapshot loading is unsafe. 

The unsafe window may confuse Curator's NodeCache. If NodeCache's underlying reconnection hit the unsafe window, it may get NoNode KeeperException for the specified path and clear the NodeCache. When the unsafe window is elapsed, NodeCache can see the data again.

This behavior is not correct. From client's view, it gets a null value for a short period 
when the server ensemble network is broken. Curator NodeCache is often used as configuration's source. Returning null is confusing and introduces logical issues  for configuration scenario.

I think the better behavior should be that reject all the reconnecting during the unsafe window. NodeCache still keep the old data when reconnection is rejected. This behavior makes sense.

I will send my patch later. Hope someone can help to review it.",[],Bug,ZOOKEEPER-3713,Major,Pierre Yin,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ReadOnlyZooKeeperServer should not expose the uninitialized ZKDatabase to client during the snapshot loading.,2021-02-07T16:05:49.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.14', id='12343587'>, <JIRA Version: name='3.5.6', id='12345243'>]",1.0
Damien Diederen,"[<JIRA Component: name='server', id='12312382'>]",2020-02-02T15:11:56.000+0000,Damien Diederen,"The {{SaslServer}} instance held a {{ServerCnxn}} is not explicitly {{disposed()}} when the connection is closed.  This means that we are relying on the GC finalizer to release associated resources.

While this does not seem to be problematic in practice, it would be better to explicitly {{dispose()}} the object at {{close()}} time.  This is unlikely to make a difference for managed providers, but {{-Dsun.security.jgss.native=true}} can potentially change the game.

----

(For reference, in case somebody searches for this.)

This came up while investigating a file descriptor leak related to the use of the native Sun provider.  The issue turned out *not* to be due to the missing dispose, but seems to be caused by a long-standing bug in the MIT Kerberos replay cache:

https://github.com/xrootd/xrootd/issues/414

{quote}
Actually, this is a bug in the kerberos library as we really do close the cache but the descriptor may still leak. This is a known issue and has been fixed in various version of kerberos but apparently not in the version being used here. The only mitigation is to not export tickets (which is not necessary).
{quote}

The problem exists in MIT Kerberos 1.7.1, but will be fixed in 1.8—which replaces the problematic component by a new implementation:

{noformat}
commit e8a35f6962ce2d048616fb7457bff2d90398ca48
Author: Greg Hudson <ghudson@mit.edu>
Date: Wed May 15 01:01:34 2019 -0400

    Use file2 replay cache by default
    
    Remove the existing default replay cache implementation and replace it
    with a wrapper around the file2 replay cache code. Change the
    filename to krb5_EUID.rcache2, ignoring the residual (and therefore
    the server principal name). On Windows, use the local appdata
    directory if KRB5RCACHEDIR is not set in the environment.
    
    ticket: 8786
{noformat}
",[],Bug,ZOOKEEPER-3711,Minor,Damien Diederen,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Dispose SaslServer instances after use,2020-02-03T08:37:05.000+0000,[],1.0
Ling Mao,"[<JIRA Component: name='tests', id='12312427'>]",2020-01-31T04:07:27.000+0000,Ling Mao,,"[<JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3710,Trivial,Ling Mao,Fixed,2020-01-31T11:01:42.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,[trivial bug] fix compile error in PurgeTxnTest introduced by ZOOKEEPER-3231,2021-03-28T08:54:35.000+0000,[],1.0
,"[<JIRA Component: name='leaderElection', id='12312378'>]",2020-01-28T01:48:24.000+0000,Suhas Dantkale,"Scenario:
1. 5 node ensemble-(SID 1,2,3,4,5). 5 is the current Leader.
2. Test brings down 5's ZK process.
3. Leadership election begins. First each SID votes itself to be the leader as expected.
4. SID 1 and SID 2 gets notification from SID 3 before they get Notification from SID 4. They update their vote to propose 3 as the Leader as expected and send notifications.
5. SID 3 receives the notification from 1, 2 and itself and its Election predicate is successfully terminated and it goes to LEADING state, comes out of FLE and goes to the next phase.
6. SID 2 meantime goes to FOLLOWING state , comes out of FLE and goes to the next phase(NEWLEADER sending etc).

so far so good.
7. Meantime (somewhere after step 4) SID 1 receives notification from  SID 4 and since SID 4 > SID 3(and zxid is same), SID 1 changes its mind and updates its proposal - now to elect 4 as leader and sends notification.
8. SID 4 is trying to elect itself as leader. And even though SID 2 and SID 3 are out of election, the SID 4 can not get out of election because - not enough number of nodes are following 3(Only 1 is following 3).
9. SID 2 is also stuck in FLE like SID 4.

So, in summary SID 1 and 4 are stuck in FLE (in lookForLeader()) and SID 2 and SID 3 are stuck in the next phase because SID 3's NEWLEADER is not responded by the quorum.
",[],Bug,ZOOKEEPER-3707,Major,Suhas Dantkale,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Leadership Election gets stuck in 5 node ensemble,2020-01-30T19:05:28.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",4.0
Pierre Yin,"[<JIRA Component: name='java client', id='12312381'>]",2020-01-25T08:28:45.000+0000,Pierre Yin,"The close method of ZooKeeper may cause the leak of SendThread when the network is broken.

When the network is broken, the SendThread of ZooKeeper client falls into the continuous reconnecting scenario. But there is an unsafe point which is just at the moment before startConnect() during the continuous reconnecting. If SendThread.close() in another thread hit the unsafe point, startConnect() would sleep some time and force to change state to States.CONNECTING although SendThread.close() already set state to States.CLOSED. In this case, the SendThread would be never be dead and nobody would change the state again.

In normal case, ZooKeeper.close() would be blocked forever to wait closeSession packet is finished until the network broken is recovered. But if user set the request timeout, ZooKeeper.close() would break the block waiting within timeout and invoke SendThread.close() to change state to CLOSED. That's why SendThread.close() can hit the unsafe point.

Set request timeout is a very common practice. 

I propose a patch and send it out later.

Maybe someone can help to review it.

 

Thanks

 

 ","[<JIRA Version: name='3.8.0', id='12349587'>]",Bug,ZOOKEEPER-3706,Major,Pierre Yin,Fixed,2020-03-03T02:13:59.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZooKeeper.close() would leak SendThread when the network is broken,2021-03-17T09:21:42.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.14', id='12343587'>, <JIRA Version: name='3.5.6', id='12345243'>]",6.0
,"[<JIRA Component: name='server', id='12312382'>]",2020-01-20T20:41:06.000+0000,Craig Condit,"We have been running Zookeeper 3.5.6 on several clusters for a while now, and have noticed (pretty consistently) that the new Admin Server seems to stop responding (hangs) after the ZK service has been up and running for a while. I'm not sure what causes this, but it seems to happen fairly reliably after some time (sometimes 10 minutes or more). This manifests as curl (or any other HTTP client) hanging while attempting to access any URL from the admin server port, even the top level which normally just returns a generic Jetty 404 error.

Possibly this was triggered by a Jetty version update?",[],Bug,ZOOKEEPER-3702,Major,Craig Condit,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,AdminServer stops responding (possible deadlock),2020-01-20T22:13:53.000+0000,"[<JIRA Version: name='3.5.6', id='12345243'>]",1.0
Andor Molnar,[],2020-01-20T10:02:31.000+0000,Ivan Kelly," We ran into a situation where the cluster ended up with split brain when the log disk filled up on a node.

The ZK cluster(3 node) in question was being used as the metadata store for Pulsar. There was an outage in the Pulsar cluster, where two the ZK nodes filled up there log disks, causing the cluster to lose quorum. Once we rectified the full disk situation and restarted the nodes everything seemed to work, but we started getting a lot of log messages about UpdateMetadataLoop retrying. UpdateMetadataLoop is used to update bookkeeper ledger metadata. If it sees a write conflict it rereads the znode, checks whether the update needs to happen, applies it and writes. These retries were flooding the log on a subset of the brokers. It turned out that it was reading a znode with version 0, but when it tried the setData with version set to 0 it was failing because the znode had a version of 2 (there were many instances of this). After investigating this, we saw that the znode had a different stat and value on ZK-1 to that on ZK-0 & ZK-2.

We resolved the situation by deleting the log and snapshots from ZK-1 and restarting, at which point everything went back to normal. Had ZK-1 managed to become leader we would have been in a lot of trouble, but thankfully this didn't happen.

For the sequence of events that led to split brain, I'll refer to the following code.
{code}
public class FileTxnSnapLog {
    ...
    public boolean truncateLog(long zxid) throws IOException {
        // close the existing txnLog and snapLog
        close();

        // truncate it
        FileTxnLog truncLog = new FileTxnLog(dataDir);
        boolean truncated = truncLog.truncate(zxid);
        truncLog.close();

        // re-open the txnLog and snapLog
        // I'd rather just close/reopen this object itself, however that 
        // would have a big impact outside ZKDatabase as there are other
        // objects holding a reference to this object.
        txnLog = new FileTxnLog(dataDir);
        snapLog = new FileSnap(snapDir);

        return truncated;
    }

    public void close() throws IOException {
        txnLog.close();
        snapLog.close();
    }
}

public class FileSnap implements SnapShot {
    ...
    public synchronized void serialize(DataTree dt, Map<Long, Integer> sessions, File snapShot)
            throws IOException {
        if (!close) {
            // actual snapshot code
        }
    }

    @Override
    public synchronized void close() throws IOException {
        close = true;
    }
}
{code}

The sequence of events that lead to the failure are:

| 2020-01-04 01:56:56Z | ZK-2 fails to write to its transaction log due to disk full. ZK-2 is still participating in leader election. ZK-2 becomes a follower of ZK-1. ZK-1 sends TRUNC to ZK-2. truncLog.truncate on ZK-2 throws an exception because of the disk being full, and leaves the process in a broken state. |
|2020-01-04 02:35:23Z | ZK-2 removes 9 transaction logs from disk (bringing it from 100% to 19%). It doesn't recover because its in a broken state. |
|2020-01-09 08:57:33Z| ZK-1 fails to write to its transaction log due to disk full. Restarts as follower. Goes into loop of dropping from quorum (because it can't update transaction log)|
|2020-01-09 08:59:33Z |ZK-1 receives snapshot from leader (ZK-0) (at 1e00000000). ZK-1 persists snapshot, but fails to add subsequent transations to log due to lack of space. ZK-1 drops from quorum.|
|2020-01-09 09:00:12Z |ZK-1 joins quorum as follower. 1e00000000 is close enough to leader to receive TRUNC(1d0000001d). TRUNC fails because txnLog can't flush on close() in trunateLog. ZK-1 goes into loop, dropping and joining quorum.|
|2020-01-09 09:39:00Z |ZK-1 runs purgeTxnLog. Process doesn't recover due to truncation exception having broken FileTxnSnapLog.|
|2020-01-09 19:28:37Z |ZK-1 is restarted. ZK-1 joins quorum as follower. ZK-1 receives TRUNC(1d0000001d). In this case, txnLog.close() can succeed because there's nothing to flush. snapLog is closed. truncLog.truncate fails with ""java.io.IOException: No log files found to truncate! This could happen if you still have snapshots from an old setup or log files were deleted accidentally or dataLogDir was changed in zoo.cfg."". It's true that there are no log files to truncate because the snapshot is at 1e00000000 which was received from the leader at 08:59 and nothing has been logged since. In any case, FileTxnSnapLog is in another inconsistent state. snapLog is closed. txnLog is closed, but nothing was ever written to it, so it looks like brand new.|
|2020-01-09 19:29:04Z| ZK-2 is restarted. ZK-2 & ZK-0 are now in a good state, so they can make progress. Transactions start to be logged.|
|2020-01-09 19:33:16Z| ZK-1 joins the quorum. As progress has been made, it receives a SNAP from the leader at 6b30001183a. It writes a snapshot, which ultimately calls FileSnap#serialize. Nothing hits the snapshot disk, because FileSnap is in closed state since 19:28. ZK-1 starts logging transactions to its log disk.|
|2020-01-09 19:42:00Z |We do a rolling restart of the cluster.|
|2020-01-09 19:45:11Z |ZK-1 loads the last snapshot that has been persisted to disk (1e00000000), and applies all log entries with zxid greater than the snapshot (6b30001183a onwards). |
|2020-01-09 19:47:35Z |ZK-2 & ZK-1 form a quorum, ZK-2 leading. ZK-1 reports its lastZxid as 6b30001b32f and gets a DIFF from ZK-2.|

From this point on, the cluster has split brain. ZK-1 is missing all transaction between 1e00000000 and 6bf0001183a. 

There's a couple of failures in the code that could stop this problem. 
- An exception in truncateLog should nuke the process. Even without the split brain occurring, the processes limped on in a broken state for days and required human intervention to get going again.
- snapLog and txnLog should be defensively nulled after they're closed. 
- FileSnap#serialize should not fail silently if close=true. This is really bad. It should at least throw an exception.


The issue occurred with 3.4.13 running on a kubernetes cluster. The bad code paths still exist on master.","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.5.7', id='12346098'>]",Bug,ZOOKEEPER-3701,Blocker,Ivan Kelly,Fixed,2021-01-06T11:33:57.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Split brain on log disk full,2021-03-28T08:55:21.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>, <JIRA Version: name='3.5.6', id='12345243'>]",5.0
Patrick D. Hunt,"[<JIRA Component: name='security', id='12329414'>]",2020-01-18T19:20:32.000+0000,Patrick D. Hunt,"owasp is flagging 
https://builds.apache.org/view/S-Z/view/ZooKeeper/job/zookeeper-master-maven-owasp/329/console

> [ERROR] jackson-databind-2.9.10.1.jar: CVE-2019-20330

""FasterXML jackson-databind 2.x before 2.9.10.2 lacks certain net.sf.ehcache blocking""

I don't believe we use ""ehcache"" but we should upgrade asap.","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.5.7', id='12346098'>]",Bug,ZOOKEEPER-3699,Blocker,Patrick D. Hunt,Fixed,2020-01-23T10:20:01.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,upgrade jackson-databind to address CVE-2019-20330,2020-02-14T15:23:38.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.5.6', id='12345243'>]",2.0
Mate Szalay-Beko,[],2020-01-17T10:15:03.000+0000,Mate Szalay-Beko,"During testing RC for 3.6.0, we found that ZooKeeper cluster with large number of ensemble members (e.g. 23) can not start properly. We see a lot of warnings in the log:
{code:java}
2020-01-15 20:02:13,431 [myid:13] - WARN
 [ListenerHandler-phunt-MBP13.local/192.168.1.91:4193:QuorumCnxManager@691]
- None of the addresses (/192.168.1.91:4190) are reachable for sid 10
java.net.NoRouteToHostException: No valid address among [/192.168.1.91:4190]
{code}
 and also:
{code:java}
2020-01-17 11:02:26,177 [myid:4] - WARN  [Thread-2531:QuorumCnxManager$SendWorker@1269] - destination address /127.0.0.1 not reachable anymore, shutting down the SendWorker for sid 6
{code}
The exceptions are happening when the new MultiAddress feature tries to filter the unreachable hosts from the address list. This involves the calling of the InetAddress.isReachable method with a default timeout of 500ms, which goes down to a native call in java and basically try to do a ping (an ICMP echo request) to the host. Naturally, the localhost should be always reachable. For some reason, this call gets failed (timeouted or simly refused) on mac if we have many ensemble members. I tested with 9 members and the cluster started properly. With 11-13-15 members it took more and more time to get the cluster to start, and the ""NoRouteToHostException"" started to appear in the logs. After around 1 minute the 15 ensemble members cluster started, but obviously this is not good this way. (I also tried with JDK 11 but the I found the same behaviour)

 

On linux, I haven't been able to reproduce the problem. I tried with 5, 9, 15 and 23 ensemble members and the quorum always seems to start properly in a few seconds. (I used OpenJDK 1.8.232 on Ubuntu 18.04)

*Update*:

On mac, we we have the ICMP rate limit set to 250 by default. You can turn this off using the following command: sudo sysctl -w net.inet.icmp.icmplim=0
 (see [https://krypted.com/mac-os-x/disable-icmp-rate-limiting-os-x/])

Using the above command before starting the 23 ensemble members cluster locally seems to solve the problem for me. (can someone verify?) The question is if this workaround is enough or not.

As far as I can tell, the current code will generate {{2*A*(M-1)}} ICMP calls in each ZooKeeper server during startup, if {{'M'}} is the number of ensemble members and {{'A'}} is the number of election addresses provided for each member. This is not that high, if each ZooKeeper server is started on a different machine, but if we start a lot of ZooKeeper servers on a single machine, then it can quickly go beyond the predefined limit of 250 for mac.","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3698,Major,Mate Szalay-Beko,Fixed,2020-01-23T12:44:13.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,NoRouteToHostException when starting large ZooKeeper cluster on localhost,2021-03-28T08:55:08.000+0000,[],2.0
,"[<JIRA Component: name='c client', id='12312380'>]",2020-01-13T14:53:55.000+0000,Jeremy Sowden,"{{zoo_amulti}} only initializes request objects if {{rc == ZOK}}, but it unconditionally calls {{free_duplicate_path}}.  For example:

{noformat}
             case ZOO_CHECK_OP: {
                struct CheckVersionRequest req;
                rc = rc < 0 ? rc : CheckVersionRequest_init(zh, &req,
                                        op->check_op.path, op->check_op.version);
                rc = rc < 0 ? rc : serialize_CheckVersionRequest(oa, ""req"", &req);
                enter_critical(zh);
                entry = create_completion_entry(zh, h.xid, COMPLETION_VOID, op_result_void_completion, result, 0, 0);
                leave_critical(zh);
                free_duplicate_path(req.path, op->check_op.path);
                break;
            }
{noformat}

This means that if there is a marshalling error in one operation, for all the later operations, the request will be initialized, the value of {{req.path}} will be undefined, and {{free_duplicate_path}} may attempt to free an invalid pointer.",[],Bug,ZOOKEEPER-3697,Minor,Jeremy Sowden,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zoo_amulti can attempt to free invalid memory after marshalling errors.,2020-02-13T08:49:05.000+0000,"[<JIRA Version: name='3.4.14', id='12343587'>, <JIRA Version: name='3.5.6', id='12345243'>]",1.0
,"[<JIRA Component: name='security', id='12329414'>, <JIRA Component: name='server', id='12312382'>]",2020-01-06T18:22:09.000+0000,Ron Dagostino,Setting zookeeper.ssl.clientAuth currently has no impact; a client certificate is currently always required.,"[<JIRA Version: name='3.5.7', id='12346098'>]",Bug,ZOOKEEPER-3674,Major,Ron Dagostino,Fixed,2020-02-18T15:14:59.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zookeeper.ssl.clientAuth ignored,2020-08-16T21:55:32.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.5.6', id='12345243'>]",5.0
,[],2020-01-06T02:45:03.000+0000,jx,"when one broker restart, zk repeated forever

1. Getting a snapshot from leader

2. Snapshotting to disk

3. cause Connection reset

4. shutdown Follower

 

Does get snapshot from leader or snapshot to disk cause synclimit timeout ?
{code:java}
// code placeholder
2020-01-05 22:56:31,168 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:Environment@100] - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2020-01-05 22:56:31,169 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:Environment@100] - Server environment:java.io.tmpdir=/tmp
2020-01-05 22:56:31,169 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:Environment@100] - Server environment:java.compiler=<NA>
2020-01-05 22:56:31,169 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:Environment@100] - Server environment:os.name=Linux
2020-01-05 22:56:31,169 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:Environment@100] - Server environment:os.arch=amd64
2020-01-05 22:56:31,169 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:Environment@100] - Server environment:os.version=3.10.104-1-tlinux2_kvm_guest-0022.tl2
2020-01-05 22:56:31,169 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:Environment@100] - Server environment:user.name=user_00
2020-01-05 22:56:31,169 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:Environment@100] - Server environment:user.home=/home/user_00
2020-01-05 22:56:31,170 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:Environment@100] - Server environment:user.dir=/usr/local/services/zookeeper-3_4_12-V8-32-400-cluster-001-0.0
2020-01-05 22:56:31,171 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:ZooKeeperServer@173] - Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 datadir /data/zookeeper/version-2 snapdir /data/zookeeper/version-2
2020-01-05 22:56:31,183 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:Follower@65] - FOLLOWING - LEADER ELECTION TOOK - 81
2020-01-05 22:56:31,185 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:QuorumPeer$QuorumServer@184] - Resolved hostname: 100.94.122.151 to address: /100.94.122.151
2020-01-05 22:56:31,190 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:Learner@336] - Getting a snapshot from leader 0xb1a0a15a6
2020-01-05 22:57:19,023 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:FileTxnSnapLog@296] - Snapshotting: 0xb1a0a15a6 to /data/zookeeper/version-2/snapshot.b1a0a15a6
2020-01-05 22:57:53,554 [myid:3] - WARN  [QuorumPeer[myid=3]/0.0.0.0:2181:Learner@387] - Got zxid 0xb1a0a15a7 expected 0x1
2020-01-05 22:57:53,596 [myid:3] - WARN  [QuorumPeer[myid=3]/0.0.0.0:2181:Follower@90] - Exception when following the leader
java.net.SocketException: Connection reset
        at java.net.SocketInputStream.read(SocketInputStream.java:210)
        at java.net.SocketInputStream.read(SocketInputStream.java:141)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
        at java.io.DataInputStream.readFully(DataInputStream.java:195)
        at java.io.DataInputStream.readFully(DataInputStream.java:169)
        at org.apache.jute.BinaryInputArchive.readBuffer(BinaryInputArchive.java:94)
        at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:87)
        at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:99)
        at org.apache.zookeeper.server.quorum.Learner.readPacket(Learner.java:153)
        at org.apache.zookeeper.server.quorum.Learner.syncWithLeader(Learner.java:380)
        at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:83)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:981)
2020-01-05 22:57:53,615 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:Follower@169] - shutdown called
java.lang.Exception: shutdown Follower
        at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:169)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:985)
2020-01-05 22:57:53,615 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:FollowerZooKeeperServer@140] - Shutting down
2020-01-05 22:57:53,615 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:QuorumPeer@909] - LOOKING
2020-01-05 22:57:53,616 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:FastLeaderElection@813] - New election. My id =  3, proposed zxid=0xb1a0a15a6
2020-01-05 22:57:53,617 [myid:3] - INFO  [WorkerReceiver[myid=3]:FastLeaderElection@595] - Notification: 1 (message format version), 3 (n.leader), 0xb1a0a15a6 (n.zxid), 0x2 (n.round), LOOKING (n.state), 3 (n.sid), 0xb (n.peerEpoch) LOOKING (my state)
2020-01-05 22:57:53,618 [myid:3] - INFO  [WorkerReceiver[myid=3]:FastLeaderElection@595] - Notification: 1 (message format version), 2 (n.leader), 0xa0000001b (n.zxid), 0x1 (n.round), FOLLOWING (n.state), 1 (n.sid), 0xb (n.peerEpoch) LOOKING (my state)
2020-01-05 22:57:53,618 [myid:3] - INFO  [WorkerReceiver[myid=3]:FastLeaderElection@595] - Notification: 1 (message format version), 2 (n.leader), 0xa0000001b (n.zxid), 0x1 (n.round), LEADING (n.state), 2 (n.sid), 0xb (n.peerEpoch) LOOKING (my state)
2020-01-05 22:57:53,619 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:QuorumPeer@979] - FOLLOWING
2020-01-05 22:57:53,619 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:ZooKeeperServer@173] - Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 datadir /data/zookeeper/version-2 snapdir /data/zookeeper/version-2
2020-01-05 22:57:53,619 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:Follower@65] - FOLLOWING - LEADER ELECTION TOOK - 3
2020-01-05 22:57:53,619 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:QuorumPeer$QuorumServer@184] - Resolved hostname: 100.94.122.151 to address: /100.94.122.151
2020-01-05 22:57:53,628 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:Learner@336] - Getting a snapshot from leader 0xb1a0a4842
2020-01-05 22:58:34,196 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:FileTxnSnapLog@296] - Snapshotting: 0xb1a0a4842 to /data/zookeeper/version-2/snapshot.b1a0a4842
2020-01-05 22:59:03,670 [myid:3] - WARN  [QuorumPeer[myid=3]/0.0.0.0:2181:Learner@387] - Got zxid 0xb1a0a4843 expected 0x1
2020-01-05 22:59:03,692 [myid:3] - WARN  [QuorumPeer[myid=3]/0.0.0.0:2181:Follower@90] - Exception when following the leader
java.net.SocketException: Connection reset
        at java.net.SocketInputStream.read(SocketInputStream.java:210)
        at java.net.SocketInputStream.read(SocketInputStream.java:141)
{code}",[],Bug,ZOOKEEPER-3673,Major,jx,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Getting a snapshot from leader cause Connection reset shutdown Follower and repeated forever,2020-01-06T02:45:03.000+0000,"[<JIRA Version: name='3.4.12', id='12342040'>]",1.0
,"[<JIRA Component: name='java client', id='12312381'>]",2020-01-05T09:07:24.000+0000,BambuBet,"BambuBet merupakan Situs Judi Online terepecaya masa kini di karenakan [#BambuBet]http://www.bambubet.org/ memliki Agent Marketing yang tersebar di seluruh indonesia untuk memasarkan Producknya ke  Seluruh Indonesia. Tentunya tidak hanya memiliki pasaran yang tersebar di seluruh Indonesia. Bambubet juga memiliki Pelayanan yang bagus , Sperti Proses Deposit dan withdraw yang cepat. tidak hanya itu BambuBet juga memiliki Pelayanan seperti memberikan Bocoran togel Besok atau pun Bocoran Togel Hari ini. Tentunya Sebagai [#Bandar Togel Terpercaya]http://www.bambubet.org/ di Indonesia BambuBet tidak pernah bermain-main untuk memasarakan Produknya ke Indonesia. Segala Fasilitas dan apapun yang di inginkan para member akan di Sediakan dan di layani dengan Baik Oleh BambuBet. http://www.bambubet.org/",[],Bug,ZOOKEEPER-3672,Major,BambuBet,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,BambuBet,2022-02-03T08:50:23.000+0000,"[<JIRA Version: name='3.4.14', id='12343587'>]",4.0
Sujith Simon,"[<JIRA Component: name='java client', id='12312381'>]",2019-12-27T12:46:54.000+0000,bright.zhou,,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.5.7', id='12346098'>]",Bug,ZOOKEEPER-3667,Major,bright.zhou,Fixed,2020-01-20T07:42:56.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,set jute.maxbuffer hexadecimal number throw parseInt error,2020-02-14T15:23:30.000+0000,"[<JIRA Version: name='3.5.6', id='12345243'>]",3.0
Enrico Olivelli,[],2019-12-23T06:51:35.000+0000,Ling Mao,"mvn clean package -Dmaven.test.skip=true -U
{code:java}
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 35.080 s
[INFO] Finished at: 2019-12-23T14:46:29+08:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project zookeeper-recipes: Could not resolve dependencies for project org.apache.zookeeper:zookeeper-recipes:pom:3.7.0-SNAPSHOT: Could not find artifact org.apache.zookeeper:zookeeper:jar:tests:3.7.0-SNAPSHOT in spring-snapshot (http://repo.spring.io/snapshot) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :zookeeper-recipes

{code}",[],Bug,ZOOKEEPER-3661,Major,Ling Mao,Not A Problem,2019-12-24T16:26:53.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Failed to execute goal on project zookeeper-recipes,2019-12-27T02:55:37.000+0000,[],2.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2019-12-20T08:22:54.000+0000,Fangmin Lv,"During DIFF sync, the txns will be applied to learner's DataTree but it won't be added into the in memory committed txns cache in ZkDatabase. If this server became new leader later, and when other servers try to sync with it, it may cause data inconsistency due to part of txns are missing.

This is not a problem if we fully shutdown the ZkDB and reload from disk, but the current behavior in 3.5 and 3.6 will not fully shutdown the DB, which is a nice optimization to reduce the unavailable time with large snapshot.

Internally, we have another version of 'Retain DB' implementation, and we caught this issue with the digest feature we just upstreamed, and have fixed that internally. Just realized we haven't upstreamed that, and this is the Jira for that issue, will send a PR for this soon.",[],Bug,ZOOKEEPER-3658,Major,Fangmin Lv,Invalid,2020-01-16T20:44:25.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Potential data inconsistency due to txns gap in committedLog when ZkDB not fully shutdown,2020-01-16T20:44:26.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.6', id='12345243'>]",1.0
,[],2019-12-19T00:36:24.000+0000,Eric Hammerle,"This issues was introduced in [ZOOKEEPER-3311|https://github.com/apache/zookeeper/pull/851]. The lastFlushTime used to decide the batch window is not updated correctly for the observer case when the nextProcessor is always be null.

This can cause observers to fall behind and their sync queue to grow indefinitely.","[<JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3656,Major,Eric Hammerle,Fixed,2020-01-07T19:39:46.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,SyncRequestProcessor doesn't update lastFlushTime correctly on observers,2021-03-28T08:55:33.000+0000,[],2.0
,"[<JIRA Component: name='server', id='12312382'>]",2019-12-18T20:36:51.000+0000,Kishor Patil,"Depending on localhost and /etc/hosts config, this test can fail.. So trying make it more resilient.

I have put up the patch on https://github.com/apache/zookeeper/pull/1188",[],Bug,ZOOKEEPER-3655,Minor,Kishor Patil,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Fix QuorumSSLTest to remove hardcoded localhost and port,2020-03-02T12:31:41.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.5.6', id='12345243'>]",1.0
Damien Diederen,"[<JIRA Component: name='c client', id='12312380'>]",2019-12-18T14:56:40.000+0000,Damien Diederen,"The {{Makefile.am}} distributed with the C client defines some per-target {{\*_CFLAGS}} and {{\*_CXXFLAGS}} variables.  These however, do not reference {{AM_CFLAGS}} (resp. AM_CXXFLAGS}}, which means that some options (notably {{-Wall}}) are missing when building subsets of the code.

Dixit the [Automake docs|https://www.gnu.org/software/automake/manual/html_node/Program-and-Library-Variables.html]:

{quote}
In compilations with per-target flags, the ordinary ‘AM_’ form of
the flags variable is _not_ automatically included in the
compilation (however, the user form of the variable _is_ included).
So for instance, if you want the hypothetical ‘maude’ compilations
to also use the value of ‘AM_CFLAGS’, you would need to write:

     maude_CFLAGS = ... your flags ... $(AM_CFLAGS)
{quote}

Restoring the flags, however, causes compilation failures (in the library) and a slew of new warnings (in the tests) which had not been noticed because of the missing options.  These errors/warnings have to be fixed before the flags can be tightened up.

(I have a preliminary patch, and am planning to submit a ""pull request"" soon.)","[<JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3654,Major,Damien Diederen,Fixed,2020-03-25T11:10:40.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Incorrect *_CFLAGS handling in Automake,2021-03-28T08:54:13.000+0000,[],1.0
Sujith Simon,"[<JIRA Component: name='audit', id='12336158'>]",2019-12-17T13:12:36.000+0000,Sujith Simon,"When the Audit Log feature is enabled in a standalone zookeeper setup, an error pops up which states ""Failed to audit log request"" with an EndOfFile exception due to an issue with the  deserialization in the AuditHelper.java when the request.request.slice() returns an empty pointer.","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3653,Major,Sujith Simon,Fixed,2019-12-19T08:55:12.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Audit Log feature fails in a stand alone zookeeper setup,2021-03-28T08:54:54.000+0000,[],1.0
,"[<JIRA Component: name='java client', id='12312381'>]",2019-12-16T14:31:06.000+0000,Sylvain Wallez,"ZOOKEEPER-2111 introduced {{synchronized(state)}} statements in {{ClientCnxn}} and {{ClientCnxn.SendThread}} to coordinate insertion in {{outgoingQueue}} and draining it when the client connection isn't alive.

There are several issues with this approach:
 - the value of the {{state}} field is not stable, meaning we don't always synchronize on the same object.
 - the {{state}} field is an enum value, which are global objects. So in an application with several ZooKeeper clients connected to different servers, this causes some contention between clients.

An easy fix is change those {{synchronized(state)}} statements to {{synchronized(outgoingQueue)}} since it is local to each client and is what we want to coordinate.

I'll be happy to prepare a PR with the above change if this is deemed to be the correct way to fix it.

 

Another issue that makes contention worse is {{ClientCnxnSocketNIO.cleanup()}} that is called from within the above synchronized block and contains {{Thread.sleep(100)}}. Why is this sleep statement needed, and can we remove it?

 ","[<JIRA Version: name='3.7.1', id='12350030'>, <JIRA Version: name='3.6.4', id='12350076'>, <JIRA Version: name='3.9.0', id='12351304'>, <JIRA Version: name='3.8.1', id='12351398'>]",Bug,ZOOKEEPER-3652,Major,Sylvain Wallez,Fixed,2022-03-30T18:30:04.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Improper synchronization in ClientCnxn,2022-04-05T06:44:05.000+0000,"[<JIRA Version: name='3.5.6', id='12345243'>]",5.0
Mate Szalay-Beko,[],2019-12-16T10:25:39.000+0000,Mate Szalay-Beko,"NettyServerCnxnFactoryTest is flaky, it fails from time to time on jenkins.

e.g. [https://builds.apache.org/view/ZK%20All/job/zookeeper-master-maven/557/org.apache.zookeeper$zookeeper/testReport/org.apache.zookeeper.server/NettyServerCnxnFactoryTest/testOutstandingHandshakeLimit/]

 
{code:java}
INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running org.apache.zookeeper.server.NettyServerCnxnFactoryTest
[ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed:
7.339 s <<< FAILURE! - in
org.apache.zookeeper.server.NettyServerCnxnFactoryTest
[ERROR]
testOutstandingHandshakeLimit(org.apache.zookeeper.server.NettyServerCnxnFactoryTest)
 Time elapsed: 6.569 s  <<< FAILURE!
java.lang.AssertionError:

Expected: is <true>
     but: was <false>
at
org.apache.zookeeper.server.NettyServerCnxnFactoryTest.testOutstandingHandshakeLimit(NettyServerCnxnFactoryTest.java:142)
{code}","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3651,Major,Mate Szalay-Beko,Fixed,2019-12-16T21:13:20.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,NettyServerCnxnFactoryTest is flaky,2021-03-28T08:54:10.000+0000,"[<JIRA Version: name='3.5.6', id='12345243'>]",1.0
,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2019-12-16T08:07:00.000+0000,Pavel Lobach,"This is a follow-up ticket for [https://github.com/alexguan/node-zookeeper-client/issues/100]

I found that above nodejs ZK client (it's 100% pure JS implementation) starts XID counter from 0 in requests which leads to really strange behaviour when throttling happens on ZK server side - please check it out for more details - that's interesting.

Above client will be fixed I hope, but actually, problem is still valid for native Java (partially) and C clients' implementation:

Java client: Actually here I see change was made to avoid xid overflow (ZOOKEEPER-3253), but I don't see it's merged into latest 3.4.14 release - is there any plans for this change to make into 3.4.x release?

C client: overflow is not checked and it has another problem with starting value, here is the code for single-threaded implementation (MT variant uses same logic):
{code:c}
// make sure the static xid is initialized before any threads started
__attribute__((constructor)) int32_t get_xid()
{
    static int32_t xid = -1;
    if (xid == -1) {
        xid = time(0);
    }
    return fetch_and_add(&xid,1);
}
{code}
starting value is chosen to be time(0) which is current Unix epoch time. It will overflow in the future on its own, making C client (and all implementations using this library as a dependency) completely out of order some day (and I can even tell you exact date :)). And as the time passes, this window (time() .. overflow) shrinks every day, making range available for xid values smaller and smaller... so problems will start happen earlier for clients making large number of requests without session reestablishment.

One more thing to note here:

Why XID=0 is considered as invalid value (check above ticket for details)? It's not stated anywhere, except ZK server code itself which decrements queued requests only for positive XID (below excerpt is from tip of the master branch):
{code:java}
    // will be called from zkServer.processPacket
    public void decrOutstandingAndCheckThrottle(ReplyHeader h) {
        if (h.getXid() <= 0) {
            return;
        }
        if (!zkServer.shouldThrottle(outstandingCount.decrementAndGet())) {
            enableRecv();
        }
    }
{code}
Apart from that, requests with xid=0 are getting through fine. Can we change condition to h.getXid() < 0 ?

And last one: is there some documentation fo ZK wire protocol, explaining fields' meaning and allowed values? I did not find one and had to reverse engineer the logic...
 Maybe it's a good idea to create it, so there will not be such misunderstandings and discrepancies in clients' implementations?",[],Bug,ZOOKEEPER-3650,Major,Pavel Lobach,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zero or overflown xid disrupts the session ,2020-02-17T03:15:34.000+0000,"[<JIRA Version: name='3.4.14', id='12343587'>, <JIRA Version: name='3.5.6', id='12345243'>]",3.0
,"[<JIRA Component: name='build', id='12312383'>]",2019-12-12T21:17:47.000+0000,Prachi Prakash,"After building successfully I was trying to run the ZooKeeperServerMain class with the zoo_sample.cfg and getting the following exception:

Exception in thread ""main"" java.lang.NoClassDefFoundError: com/codahale/metrics/ReservoirException in thread ""main"" java.lang.NoClassDefFoundError: com/codahale/metrics/Reservoir at org.apache.zookeeper.metrics.impl.DefaultMetricsProvider$DefaultMetricsContext.lambda$getSummary$2(DefaultMetricsProvider.java:126) at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660) at org.apache.zookeeper.metrics.impl.DefaultMetricsProvider$DefaultMetricsContext.getSummary(DefaultMetricsProvider.java:122) at org.apache.zookeeper.server.ServerMetrics.<init>(ServerMetrics.java:74) at org.apache.zookeeper.server.ServerMetrics.<clinit>(ServerMetrics.java:44) at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:132) at org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:112) at org.apache.zookeeper.server.ZooKeeperServerMain.main(ZooKeeperServerMain.java:67)Caused by: java.lang.ClassNotFoundException: *com.codahale.metrics.Reservoir* at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 8 more



Can anyone guide on how to rectify this, it uses 

<dropwizard.version>3.2.5</dropwizard.version> not the 4.x 

 

 ",[],Bug,ZOOKEEPER-3647,Trivial,Prachi Prakash,Won't Fix,2020-02-17T03:27:48.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"Exception in thread ""main"" java.lang.NoClassDefFoundError: com/codahale/metrics/Reservoir",2021-11-22T08:58:48.000+0000,[],4.0
Mate Szalay-Beko,[],2019-12-11T11:06:44.000+0000,Mate Szalay-Beko,"Whenever you quit from the C command line client, you got a warning on the server logs like:

```
2019-12-11 10:28:40,973 [myid:] - WARN  [NIOWorkerThread-6:NIOServerCnxn@364] - Unexpected exception
EndOfStreamException: Unable to read additional data from client, it probably closed the socket: address = /0:0:0:0:0:0:0:1:51026, session = 0x1000012817e001c
	at org.apache.zookeeper.server.NIOServerCnxn.handleFailedRead(NIOServerCnxn.java:163)
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:326)
	at org.apache.zookeeper.server.NIOServerCnxnFactory$IOWorkRequest.doWork(NIOServerCnxnFactory.java:522)
	at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:154)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
``` 

This happens no matter how you exit from the C command-line client. E.g.:
- using Ctrl-C
- using the `quit` command
- using the `--cmd` option and executing a single command",[],Bug,ZOOKEEPER-3645,Minor,Mate Szalay-Beko,Duplicate,2019-12-12T14:03:52.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,C CLI should close session properly,2019-12-12T14:03:52.000+0000,"[<JIRA Version: name='3.5.6', id='12345243'>]",2.0
Michael Han,"[<JIRA Component: name='server', id='12312382'>]",2019-12-10T15:38:07.000+0000,Manikumar,"We have tried to upgrade single node *standalone* ZK server from 3.4.14 to 3.5.6.  There were no snapshot files, so as suggested in ZOOKEEPER-3056, we have set snapshot.trust.empty to true. After server startup, when we tried to list the znodes, we found that znodes are missing.

Steps to reproduce:
 # Start a single node ZK 3.4.14 server and create few znodes
 # Upgrade the server to 3.5.6 with  snapshot.trust.empty=true config
 # try to list the znodes using zkShell

Looking into the [source code|https://github.com/apache/zookeeper/blob/release-3.5.6/zookeeper-server/src/main/java/org/apache/zookeeper/server/persistence/FileTxnSnapLog.java#L224], looks like we are not reading transaction log if there are no snapshot files and snapshot.trust.empty is set to true.

ZK 3.5.6 logs:
{quote}[2019-12-07 12:13:35,007] INFO Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 datadir /var/lib/zookeeper/version-2 snapdir /var/lib/zookeeper/version-2
  (org.apache.zookeeper.server.ZooKeeperServer)
 [2019-12-07 12:13:35,012] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
 [2019-12-07 12:13:35,014] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 1 selector thread(s), 12 worker threads, and 64 kB direct buffers. (org.apache
 .zookeeper.server.NIOServerCnxnFactory)
 [2019-12-07 12:13:35,017] INFO binding to port [0.0.0.0/0.0.0.0:2181|http://0.0.0.0/0.0.0.0:2181] (org.apache.zookeeper.server.NIOServerCnxnFactory)
 [2019-12-07 12:13:35,027] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)
 [2019-12-07 12:13:35,029] DEBUG Created new input stream /var/lib/zookeeper/version-2/log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
 [2019-12-07 12:13:35,031] DEBUG Created new input archive /var/lib/zookeeper/version-2/log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
 [2019-12-07 12:13:35,035] DEBUG EOF exception java.io.EOFException: Failed to read /var/lib/zookeeper/version-2/log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
 [2019-12-07 12:13:35,035] WARN No snapshot found, but there are log entries. This should only be allowed during upgrading. (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
 [2019-12-07 12:13:35,035] INFO Snapshotting: 0x0 to /var/lib/zookeeper/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
 [2019-12-07 12:13:35,036] INFO Snapshotting: 0x0 to /var/lib/zookeeper/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
 [2019-12-07 12:13:35,050] INFO Using checkIntervalMs=60000 maxPerMinute=10000 (org.apache.zookeeper.server.ContainerManager)
 [2019-12-07 12:15:07,137] DEBUG Accepted socket connection from /[127.0.0.1:38888|http://127.0.0.1:38888/] (org.apache.zookeeper.server.NIOServerCnxnFactory)
 [2019-12-07 12:15:07,144] DEBUG Session establishment request from client /[127.0.0.1:38888|http://127.0.0.1:38888/] client's lastZxid is 0x0 (org.apache.zookeeper.server.ZooKeeperServer)
 [2019-12-07 12:15:07,145] DEBUG Adding session 0x100006e15fb0000 (org.apache.zookeeper.server.SessionTrackerImpl)
 [2019-12-07 12:15:07,148] TRACE SessionTrackerImpl — Adding session 0x100006e15fb0000 30000 (org.apache.zookeeper.server.SessionTrackerImpl)
 [2019-12-07 12:15:07,149] DEBUG Client attempting to establish new session: session = 0x100006e15fb0000, zxid = 0x0, timeout = 30000, address = /[127.0.0.1:38888|http://127.0.0.1:38888/] (org.apache.zookeeper.server.ZooKeeperServer)
 [2019-12-07 12:15:07,155] TRACE :Psessionid:0x100006e15fb0000 type:createSession cxid:0x0 zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a (org.apache.zookeeper.server.PrepRequestProcessor)
 [2019-12-07 12:15:07,155] TRACE SessionTrackerImpl — Existing session 0x100006e15fb0000 30000 (org.apache.zookeeper.server.SessionTrackerImpl)
 [2019-12-07 12:15:07,155] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
 [2019-12-07 12:15:07,170] DEBUG Processing request:: sessionid:0x100006e15fb0000 type:createSession cxid:0x0 zxid:0x1 txntype:-10 reqpath:n/a (org.apache.zookeeper.server.FinalRequestProcessor)
{quote}","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.5.7', id='12346098'>]",Bug,ZOOKEEPER-3644,Blocker,Manikumar,Fixed,2020-01-05T21:19:10.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Data loss after upgrading standalone ZK server 3.4.14 to 3.5.6 with snapshot.trust.empty=true,2020-02-14T15:23:44.000+0000,"[<JIRA Version: name='3.5.6', id='12345243'>]",6.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2019-12-09T21:13:28.000+0000,Alex Mirgorodskiy,"If the leader crashes after sending a SNAP sync to a learner, but before sending the NEWLEADER message, the learner will not save the snapshot to disk. But it will advance its lastProcessedZxid to that from the snapshot (call it Zxid X)

A new leader will get elected, and it will resync our learner again immediately. But this time, it will use the incremental DIFF method, starting from Zxid X. A DIFF-based resync does not trigger snapshots, so the learner is still holding the original snapshot purely in memory. If the learner restarts after that, it will silently lose all the data up to Zxid X.

An easy way to reproduce is to insert System.exit into LearnerHandler.java right before sending the NEWLEADER message (on the one instance that is currently running the leader, but not the others):
{noformat}
             LOG.debug(""Sending NEWLEADER message to "" + sid);
+            if (leader.self.getId() == 1 && sid == 3) {
+               LOG.debug(""Bail when server.1 resyncs server.3"");
+               System.exit(0);
+            }
{noformat}
If I remember right, the repro steps are as follows. Run with that patch in a 4-instance ensemble where server.3 is an Observer, the rest are voting members, and server.1 is the current Leader. Start server.3 after the other instances are up. It will get the initial snapshot from server.1 and server.1 will stop immediately because of the patch. Say, server.2 takes over as the new Leader. Server.3 will receive a Diff resync from server.2, but will skip persisting the snapshot. A subsequent restart of server.3 will make that instance come up with a blank data tree.

The above steps assumed that server.3 is an Observer, but it can presumably happen for voting members too. Just need a 5-instance ensemble.

Our workaround is to take the snapshot unconditionally on receiving NEWLEADER:
{noformat}
-                   if (snapshotNeeded) {
+                   // Take the snapshot unconditionally. The first leader may have crashed
+                   // after sending us a SNAP, but before sending NEWLEADER. The second leader will
+                   // send us a DIFF, and we'd still like to take a snapshot, even though
+                   // the upstream code used to skip it.
+                   if (true || snapshotNeeded) {
                        zk.takeSnapshot();
                    }
{noformat}
This is what 3.4.x series used to do. But I assume it is not the ideal fix, since it essentially disables the ""snapshotNeeded"" optimization.","[<JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3642,Major,Alex Mirgorodskiy,Fixed,2020-11-23T11:53:32.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Data inconsistency when the leader crashes right after sending SNAP sync,2021-03-28T08:54:06.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.5.6', id='12345243'>]",4.0
,"[<JIRA Component: name='c client', id='12312380'>]",2019-12-09T09:22:35.000+0000,Damien Diederen,"ZOOKEEPER-3635 changed the versioning scheme for the C client from integer-valued {{ZOO_\{MAJOR,MINOR,PATCH\}_VERSION}} definitions to a single string-valued {{#define ZOO_VERSION ""3.6.0""}}.

This causes the Perl and Python contribs to fail to build.

(I'm looking into it.)","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3641,Major,Damien Diederen,Fixed,2019-12-10T11:21:00.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,New ZOO_VERSION define breaks Perl & Python contribs,2019-12-10T11:22:30.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",2.0
,[],2019-12-06T07:32:31.000+0000,sunfeifei,"2019-12-01 21:46:16,537 [myid:161] - ERROR [CommitProcessor:161:CommitProcessor@148] - Unexpected exception causing CommitProcessor to exit
java.lang.NullPointerException
	at org.apache.zookeeper.server.ZKDatabase.addCommittedProposal(ZKDatabase.java:250)
	at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:120)
	at org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:74)
2019-12-04 21:46:16,537 [myid:161] - INFO  [CommitProcessor:161:CommitProcessor@150] - CommitProcessor exited loop!



2019-12-01 21:46:36,616 [myid:161] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@841] - Refusing session request for client /192.168.40.20:14973 as it has seen zxid 0x1119e271e9 our last zxid is 0x1119e271e7 client must try another server
2019-12-01 21:46:36,617 [myid:161] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@841] - Refusing session request for client /192.168.53.50:51241 as it has seen zxid 0x1119e271e9 our last zxid is 0x1119e271e7 client must try another server
2019-12-01 21:46:36,631 [myid:161] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@841] - Refusing session request for client /192.168.164.94:32532 as it has seen zxid 0x1119e274a5 our last zxid is 0x1119e271e7 client must try another server",[],Bug,ZOOKEEPER-3639,Major,sunfeifei,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Unexpected exception causing CommitProcessor to exit,2019-12-12T02:37:44.000+0000,[],1.0
,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='server', id='12312382'>]",2019-12-05T10:58:28.000+0000,Ling Mao,,[],Bug,ZOOKEEPER-3637,Minor,Ling Mao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Fix haveDelivered wrong implementation,2019-12-28T14:48:21.000+0000,[],2.0
,[],2019-11-29T07:52:59.000+0000,zechao zheng,"h4. Question

After a large number of znodes are created, ZooKeeper servers in the ZooKeeper cluster become faulty and cannot be automatically recovered or restarted.

Logs of the followe:
2016-06-23 08:00:18,763 | WARN  | QuorumPeer[myid=26](plain=/10.16.9.138:24002)(secure=disabled) | Exception when following the leader | org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:93)
java.net.SocketTimeoutException: Read timed out
    at java.net.SocketInputStream.socketRead0(Native Method)
    at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
    at java.net.SocketInputStream.read(SocketInputStream.java:170)
    at java.net.SocketInputStream.read(SocketInputStream.java:141)
    at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
    at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
    at java.io.DataInputStream.readInt(DataInputStream.java:387)
    at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
    at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:83)
    at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:99)
    at org.apache.zookeeper.server.quorum.Learner.readPacket(Learner.java:156)
    at org.apache.zookeeper.server.quorum.Learner.registerWithLeader(Learner.java:276)
    at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:75)
    at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1094)2016-06-23 08:00:18,763 | WARN  | QuorumPeer[myid=26](plain=/10.16.9.138:24002)(secure=disabled) | Exception when following the leader | org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:93)
java.net.SocketTimeoutException: Read timed out
    at java.net.SocketInputStream.socketRead0(Native Method)
    at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
    at java.net.SocketInputStream.read(SocketInputStream.java:170)
    at java.net.SocketInputStream.read(SocketInputStream.java:141)
    at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
    at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
    at java.io.DataInputStream.readInt(DataInputStream.java:387)
    at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
    at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:83)
    at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:99)
    at org.apache.zookeeper.server.quorum.Learner.readPacket(Learner.java:156)
    at org.apache.zookeeper.server.quorum.Learner.registerWithLeader(Learner.java:276)
    at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:75)
    at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1094)
Logs of the leader:
016-06-23 07:30:57,481 | WARN  | QuorumPeer[myid=25](plain=/10.16.9.136:24002)(secure=disabled) | Unexpected exception | org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1108)
java.lang.InterruptedException: Timeout while waiting for epoch to be acked by quorum
    at org.apache.zookeeper.server.quorum.Leader.waitForEpochAck(Leader.java:1221)
    at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:487)
    at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1105)
2016-06-23 07:30:57,482 | INFO  | QuorumPeer[myid=25](plain=/10.16.9.136:24002)(secure=disabled) | Shutdown called | org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:623)
java.lang.Exception: shutdown Leader! reason: Forcing shutdown
    at org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:623)
    at org.apache.zookeeper.server.quorum.QuorumPeer.stopLeader(QuorumPeer.java:1149)
    at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1110)",[],Bug,ZOOKEEPER-3634,Critical,zechao zheng,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,why zookeeper huge snapshot cause waitEpockAck timeout?,2021-01-06T11:13:52.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",4.0
Mate Szalay-Beko,[],2019-11-28T16:08:11.000+0000,Mate Szalay-Beko,"*thanks for Mike Smotritsky for reporting this bug!*

when only secureClientPort is defined in the config and there is no regular clientPort, then both the {{stat}} and the {{conf}} commands result in 500 Server Error caused by NullPointerExceptions. The problem is that no {{serverCnxFactory}} is defined in the {{ZooKeeperServer}} in this case, we have only {{secureServerCnxnFactory}}.

see the attached stacktraces about the exceptions (reproduced on the current master branch)

The stat and conf admin commands should actually provide info about both secure and unsecure connections, and should handle the case when any of these are missing.","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.7', id='12346098'>]",Bug,ZOOKEEPER-3633,Major,Mate Szalay-Beko,Fixed,2019-12-03T09:02:20.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,AdminServer commands throw NPE when only secure client port is used,2020-02-14T15:23:49.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.5.6', id='12345243'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2019-11-22T10:27:51.000+0000,Andor Molnar,"branch34_openjdk8 has been failing since build #504. No patch has been submitted at that time, so it must be some infra problem.

The following job has failed:

[https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper_branch34_openjdk8/504/]

with the following error message:
{noformat}
javax.security.sasl.SaslException: Failed to initialize authentication mechanism using SASL [Caused by javax.security.auth.login.LoginException: SASL-authentication failed because the specified JAAS configuration section 'QuorumLearnerInvalid' could not be found.]
	at org.apache.zookeeper.server.quorum.auth.SaslQuorumAuthLearner.<init>(SaslQuorumAuthLearner.java:72)
	at org.apache.zookeeper.server.quorum.QuorumCnxManagerTest.createAndStartManager(QuorumCnxManagerTest.java:739)
	at org.apache.zookeeper.server.quorum.QuorumCnxManagerTest.createAndStartManager(QuorumCnxManagerTest.java:728)
	at org.apache.zookeeper.server.quorum.QuorumCnxManagerTest.testAuthLearnerBadCredToAuthRequiredServerWithHigherSid(QuorumCnxManagerTest.java:286)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:55)
	at java.util.concurrent.FutureTask.run(java.base@9-internal/FutureTask.java:266)
	at java.lang.Thread.run(java.base@9-internal/Thread.java:804)
Caused by: javax.security.auth.login.LoginException: SASL-authentication failed because the specified JAAS configuration section 'QuorumLearnerInvalid' could not be found.
	at org.apache.zookeeper.server.quorum.auth.SaslQuorumAuthLearner.<init>(SaslQuorumAuthLearner.java:63){noformat}",[],Bug,ZOOKEEPER-3628,Major,Andor Molnar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Build failing on branch-3.4 with Java 8,2019-11-22T10:27:51.000+0000,"[<JIRA Version: name='3.4.14', id='12343587'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2019-11-20T07:26:29.000+0000,Siddhesh Ghadi,"Build fails on travis as well as on local env for ppc64le due to some flaky tests. Very few times build passes. Also when the failing tests are ran individually they pass.

 

Travis logs:

Failed build: [https://api.travis-ci.org/v3/job/613931066/log.txt]

Passed build: [https://api.travis-ci.org/v3/job/614361405/log.txt]",[],Bug,ZOOKEEPER-3623,Major,Siddhesh Ghadi,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Build failure due to flaky tests on ppc64le,2019-11-20T18:23:45.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2019-11-17T02:38:42.000+0000,Kelly Schoenhofen,"Using 3.5.6 I have quorum tls working, but I'm being asked to tighten up from the default of AES128 & TLS 1.2, I've tried the following in the zoo.cfg:

ssl.quorum.protocol=TLSv1.3

This is apparently not supported yet - is this dependent on the version of openssl on the system, or is this just not an option I can specify? Where can I find the list of protocols that are recognized? If 1.3 is not yet available, not the end of the world.

-ssl.quorum.ciphersuites=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384-

-This is not a recognized cipher, neither is AES256/SHA256. The above cipher _should_ be available though, and is the stronger successor to AES128/SHA256.-

-I have the suspicion that I'm setting it wrong, because if I set it to the cipher it defaults to when unset:-

-ssl.quorum.ciphersuites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256-

-Gives me this when cluster members try to connect:-

-2019-11-16 19:39:33,731 [myid:1] - INFO [xxx/x.x.x.x:3888:UnifiedServerSocket$UnifiedSocket@273] - Accepted TLS connection from xxx/x.x.x.x:40822 - NONE - SSL_NULL_WITH_NULL_NULL-
 -2019-11-16 19:39:33,732 [myid:1] - WARN [xxx/x.x.x.x:3888:QuorumCnxManager@542] - Exception reading or writing challenge: {}-

 

(the only alteration I made to the above snippet is changing the machine names to xxx and ip's to x.x.x.x, I altered it in no other way)

So two questions:

1) is tls 1.3 an option?

2) what is the cipher list? I would like an aes256 option. 

Update: So I removed all my changes and I kept getting the the SSL_NULL_WITH_NULL_NULL error. I tore everything down, put it all back together, and still got SSL_NULL. Started again with just the first two nodes, very slowly, picking over the log, and then I noticed the initial error that the name in the cert didn't match the name of the server. When I set up the reverse lookup zone in DNS on Friday, I had set the FQDN properly, but over the weekend (while zk hummed along fine) the zone populated and overwrote everything with just the machine names, removing the FQDN. Hence the name not matching. 
I manually added the FQDN to the entries, rebooted the servers, and they started working.
Since I was getting SSL_NULL when I got off of trying TLSv1.3 and just trying AES256-SHA384, I tried that again, and it works fine:

2019-11-16 22:20:09,346 [myid:2] - INFO [LearnerHandler-/x.x.x.x:43548:UnifiedServerSocket$UnifiedSocket@273] - Accepted TLS connection from xxx/x.x.x.x:43548 - TLSv1.2 - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384

So, this is less of a bug and more of a request - is TLS 1.3 an option, and how can I get a cipher list? I have AES256-SHA384 so that's acceptable to the SecOps where I work. ",[],Bug,ZOOKEEPER-3622,Minor,Kelly Schoenhofen,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper 3.5.6 Quorum TLS protocol issues,2019-11-18T01:39:26.000+0000,"[<JIRA Version: name='3.5.6', id='12345243'>]",4.0
Ling Mao,"[<JIRA Component: name='server', id='12312382'>]",2019-11-16T07:46:08.000+0000,Ling Mao,,[],Bug,ZOOKEEPER-3621,Major,Ling Mao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,fix the potential bug in the process of clean-up of session when clock skew,2019-11-16T07:46:08.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",1.0
Ling Mao,"[<JIRA Component: name='security', id='12329414'>, <JIRA Component: name='server', id='12312382'>]",2019-11-15T05:28:44.000+0000,Vrinda Davda,"I was able to add one user with /crdwa/ access.
 The moment I add another user with read-only access- /r/. The first user - /user1/
 gets overridden with read-only access. Please see the output below :

 
{code:java}
WatchedEvent state:SyncConnected type:None path:null
[zk: localhost:2181(CONNECTED) 0]  addauth digest user1:password1
[zk: localhost:2181(CONNECTED) 1] setAcl /newznode auth:user1:password1:crwad
cZxid = 0xe
ctime = Thu Nov 07 13:29:43 IST 2019
mZxid = 0xe
mtime = Thu Nov 07 13:29:43 IST 2019
pZxid = 0xe
cversion = 0
dataVersion = 0
aclVersion = 1
ephemeralOwner = 0x0
dataLength = 8
numChildren = 0
[zk: localhost:2181(CONNECTED) 2] getAcl /newznode
'digest,'user1:XDkd2dsEuhc9ImU3q8pa8UOdtpI=
: cdrwa
[zk: localhost:2181(CONNECTED) 3] addauth digest user2:password2
[zk: localhost:2181(CONNECTED) 4] setAcl /newznode auth:user2:password2:r
cZxid = 0xe
ctime = Thu Nov 07 13:29:43 IST 2019
mZxid = 0xe
mtime = Thu Nov 07 13:29:43 IST 2019
pZxid = 0xe
cversion = 0
dataVersion = 0
aclVersion = 2
ephemeralOwner = 0x0
dataLength = 8
numChildren = 0
zk: localhost:2181(CONNECTED) 5] getAcl /newznode
'digest,'user1:XDkd2dsEuhc9ImU3q8pa8UOdtpI=
: r
'digest,'user2:lo/iTtNMP+gEZlpUNaCqLYO3i5U=
: r
{code}
 

If setAcl for both the users at the same time. I get both users duplicated, one with readonly and another with cdrwa permissions, as below:

 
{code:java}
[zk: localhost:2181(CONNECTED) 1] getAcl /zk_test
'world,'anyone
: cdrwa
[zk: localhost:2181(CONNECTED) 2]  addauth digest user1:password1
[zk: localhost:2181(CONNECTED) 3] addauth digest user2:password2
[zk: localhost:2181(CONNECTED) 5] 
setAcl /zk_test auth:user2:password2:r,auth:user1:password1:cdrwa  
cZxid = 0x2
ctime = Wed Nov 13 20:14:08 IST 2019
mZxid = 0x2
mtime = Wed Nov 13 20:14:08 IST 2019
pZxid = 0x2
cversion = 0
dataVersion = 0
aclVersion = 2
ephemeralOwner = 0x0
dataLength = 7
numChildren = 0
[zk: localhost:2181(CONNECTED) 7] getAcl /zk_test
'digest,'user1:XDkd2dsEuhc9ImU3q8pa8UOdtpI=
: r
'digest,'user2:lo/iTtNMP+gEZlpUNaCqLYO3i5U=
: r
'digest,'user1:XDkd2dsEuhc9ImU3q8pa8UOdtpI=
: cdrwa
'digest,'user2:lo/iTtNMP+gEZlpUNaCqLYO3i5U=
: cdrwa
{code}
 ","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-3617,Major,Vrinda Davda,,,This issue is being actively worked on at the moment by the assignee.,In Progress,0.0,ZK digest ACL permissions gets overridden,2022-02-03T08:36:24.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.5', id='12343268'>]",4.0
Sujith Simon,"[<JIRA Component: name='server', id='12312382'>]",2019-11-12T18:14:48.000+0000,Scott Guminy,"I was using ZooKeeper client in WebSphere Liberty and attempting to configure SSL/TLS for client connections.

To do so, I must add the system property {{zookeeper.client.secure=true}}.  In WebSphere Liberty, java system properties are placed in a file called bootstrap.properties - each property on a separate line.  I accidentally added a space at the end of the line.  When {{ZKConfig.getBoolean()}} attempted to convert this string to a {{boolean}}, it returned {{false}} due to the space at the end.

{{ZKConfig.getBoolean()}} should trim the string before attempting to convert to a boolean.","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.5.7', id='12346098'>]",Bug,ZOOKEEPER-3613,Minor,Scott Guminy,Fixed,2020-01-20T08:10:25.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZKConfig fails to return proper value on getBoolean() when user accidentally includes spaces at the end of the value,2020-02-14T15:23:45.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",3.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2019-11-12T12:10:34.000+0000,Kevin Moultry,The zookeeper package package in zookeeper-docs\skin the lib prototype.js in version 1.4.0_pre4. There is a known security vulnerability CVE-2008-7220. Can you please upgrade to 1.6.0.2 or higher. Thanks.,[],Bug,ZOOKEEPER-3612,Major,Kevin Moultry,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,CLONE - Update lib prototype.js: 1.4.0_pre4 due to security vulnerability,2019-12-20T05:35:21.000+0000,[],2.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2019-11-07T17:20:55.000+0000,DW,The zookeeper package package in zookeeper-docs\skin the lib prototype.js in version 1.4.0_pre4. There is a known security vulnerability CVE-2008-7220. Can you please upgrade to 1.6.0.2 or higher. Thanks.,[],Bug,ZOOKEEPER-3610,Major,DW,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Update lib prototype.js: 1.4.0_pre4 due to security vulnerability,2019-11-13T08:16:15.000+0000,"[<JIRA Version: name='3.4.14', id='12343587'>]",1.0
,"[<JIRA Component: name='contrib', id='12312700'>]",2019-11-07T17:13:43.000+0000,DW,The zookeeper package package in zookeeper-contrib\zookeeper-contrib-loggraph\src\resources\webapp\org\apache\zookeeper\graph\resources the lib yui-min.js in version 3.1.0. There is a known security vulnerability CVE-2013-4939. Can you please upgrade to higher version. Thanks.,[],Bug,ZOOKEEPER-3609,Major,DW,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Update lib yui-min: 3.1.0 due to security vulnerability,2019-11-07T17:18:48.000+0000,"[<JIRA Version: name='3.4.14', id='12343587'>]",1.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2019-11-06T07:33:50.000+0000,Jiafu Jiang,"I will describe the problem by a detail example.

1. Suppose we have three zk servers: zk1, zk2, and zk3. zk1 and zk2 are online, zk3 is offline, zk1 is the leader.

2. In TRUNC sync, zk1 sends a TRUNC request to zk2, then sends the remaining proposals in the committedLog. *When the follower zk2 receives the proposals, it applies them directly into the datatree, but not the committedLog.*

3. After the data sync phase, zk1 may continue to send zk2 more committed proposals, and they will be applied to both the datatree and the committedLog of zk2.

4. Then zk1 fails, zk3 restarts successfully, zk2 becomes the leader.

5. The leader zk2 sends a TRUNC request to zk3, then the remaining proposals from the committedLog. But since some proposals, which are from the leader zk1 in TRUNC sync(as I describe above), are not in the committedLog, they will not be sent to zk3.

6. Now data inconsistency happens between zk2 and zk3, since some data may exist in zk2's datatree, but not zk3's datatree.",[],Bug,ZOOKEEPER-3607,Critical,Jiafu Jiang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Potential data inconsistency due to the inconsistency between ZKDatabase.committedLog and dataTree in Trunc sync.,2020-03-13T06:44:17.000+0000,"[<JIRA Version: name='3.4.14', id='12343587'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2019-11-02T14:18:54.000+0000,Jordan Zimmerman,ZOOKEEPER-3242 add a connection throttle. It gets set in the main constructor but not the alternate constructor. This is breaking Apache Curator's testing framework. It should also be set in the alternate constructor to avoid an NPE.,"[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3605,Major,Jordan Zimmerman,Fixed,2019-11-06T14:21:43.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZOOKEEPER-3242 add a connection throttle. Default constructor needs to set it,2019-11-06T19:09:28.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",2.0
,"[<JIRA Component: name='build', id='12312383'>]",2019-11-01T14:06:55.000+0000,Suraj Kumar Agrahari,"I downloaded zookeeper 3.5.6 and was unable to start because the zookeeper_server.pid file was not being created.

The problem was in the line *ZOO_DATADIR=""$(echo -e ""${ZOO_DATADIR}"" | sed -e 's/^[[:space:]]*//' -e 's/[[:space:]]*$//')""*

 

After removing -e option from the echo command, it started successfully.",[],Bug,ZOOKEEPER-3603,Major,Suraj Kumar Agrahari,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper (3.5.6) not starting on mac,2019-11-18T05:26:03.000+0000,"[<JIRA Version: name='3.5.6', id='12345243'>]",2.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2019-10-29T19:23:57.000+0000,Fangmin Lv," 
There was a regression introduced after inline the write in CommitProcessor with changes intorduced in ZOOKEEPER-3359, which didn't wait the in-flight write to finish before calling shutdown on the nextProcessor.
 
So it's possible that CommitProcessor thread and QuorumPeer thread will update the DataTree concurrently if we're doing fastForwardDataBase at the end of ZooKeeperServer.shutdown, which will cause inconsistent issue.
 
This JIRA is going to make sure we wait on the CommitProcessor to shutdown gracefully before calling shutdown on next processor, and exit if we cannot finish it gracefully to avoid potential inconsistency.",[],Bug,ZOOKEEPER-3598,Critical,Fangmin Lv,,,"This issue was once resolved, but the resolution was deemed incorrect. From here issues are either marked assigned or resolved.",Reopened,0.0, Fix potential data inconsistency issue due to CommitProcessor not gracefully shutdown,2019-11-18T12:41:52.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2019-10-23T04:40:29.000+0000,Alex Mirgorodskiy,"We've run into a problem where one of the zookeeper instances lost most of its data after its zk process has been restarted. We suspect an interaction between dynamic reconfiguration and snapshot-based resync of that instance. Details and some amateurish analysis are below. We can also upload transaction logs, if need be.

We have a 6-instance ensemble running version 3.5.5 with 3 quorum members and 3 observers. One of the observers (Instance 6) saw its db shrink from 3162 znodes down to 10 after that instance restarted:

> 2019-10-13T16:44:19.060+0000 [.zk-monitor-0] Monitor command mntr: zk_version 3.5.5-afd10a8846b22a34c5a818034bb22e99dd44587b, built on 09/16/2019 18:31 GMT
> zk_znode_count 3162
> --
> 2019-10-13T16:48:32.713+0000 [.zk-monitor-0] Monitor command mntr: zk_version 3.5.5-afd10a8846b22a34c5a818034bb22e99dd44587b, built on 09/16/2019 18:31 GMT
> zk_znode_count 10

Contrast it with Instance 1 that was the leader at the time, and whose znode_count remained stable around 3000:

> 2019-10-13T16:44:48.625+0000 [.zk-monitor-0] Monitor command mntr: zk_version 3.5.5-afd10a8846b22a34c5a818034bb22e99dd44587b, built on 09/16/2019 18:31 GMT
> zk_znode_count 3178
> --
> ...
> --
> 2019-10-13T16:48:48.731+0000 [.zk-monitor-0] Monitor command mntr: zk_version 3.5.5-afd10a8846b22a34c5a818034bb22e99dd44587b, built on 09/16/2019 18:31 GMT
> zk_znode_count 3223

It appears that the problem had happened 30 minutes earlier, when Instance 6 got resynced from the leader via the Snap method, yet skipped creating an on-disk snapshot. The end result was that the in-memory state was fine, but there was only the primordial snapshot.0 on disk, and transaction logs only started after the missing snapshot:

$ ls -l version-2
> total 1766
> -rw-r--r-- 1 daautomation daautomation 1 Oct 13 09:14 acceptedEpoch
> -rw-r--r-- 1 daautomation daautomation 1 Oct 13 10:12 currentEpoch
> -rw-r--r-- 1 daautomation daautomation 2097168 Oct 13 09:44 log.6000002e0
> -rw-r--r-- 1 daautomation daautomation 1048592 Oct 13 10:09 log.600001f1b
> -rw-r--r-- 1 daautomation daautomation 4194320 Oct 13 12:16 log.600003310
> -rw-r--r-- 1 daautomation daautomation 770 Oct 13 09:14 snapshot.0

So the zk reboot wiped out most of the state.

Dynamic reconfig might be relevant here. Instance 6 started as an observer, got removed, and immediately re-added as a participant. Instance 2 went the other way, from participant to observer:

> 2019-10-13T16:14:19.323+0000 ZK reconfig: removing node 6
> 2019-10-13T16:14:19.359+0000 ZK reconfig: adding server.6=10.80.209.138:2888:3888:participant;0.0.0.0:2181
> 2019-10-13T16:14:19.399+0000 ZK reconfig: adding server.2=10.80.209.131:2888:3888:observer;0.0.0.0:2181

Looking at the logs, Instance 6 started and received a resync snapshot from the leader right before the dynamic reconfig:

> 2019-10-13T16:14:19.284+0000 [.QuorumPeer[myid=6](plain=/0.0.0.0:2181)(secure=disabled)] Getting a snapshot from leader 0x6000002dd
> ...
> 2019-10-13T16:14:19.401+0000 [.QuorumPeer[myid=6](plain=/0.0.0.0:2181)(secure=disabled)] Got zxid 0x6000002de expected 0x1

Had it processed the NEWLEADER packet afterwards, it would've persisted the snapshot locally. But there's no NEWLEADER message in the Instance 6 log. Instead, there's a ""changes proposed in reconfig"" exception, likely a result of the instance getting dynamically removed and re-added as a participant:

> 2019-10-13T16:14:19.467+0000 [.QuorumPeer[myid=6](plain=/0.0.0.0:2181)(secure=disabled)] Becoming a non-voting participant
> 2019-10-13T16:14:19.467+0000 [.QuorumPeer[myid=6](plain=/0.0.0.0:2181)(secure=disabled)] Exception when observing the leaderjava.lang.Exception: changes proposed in reconfig\n\tat org.apache.zookeeper.server.quorum.Learner.syncWithLeader(Learner.java:506)\n\tat org.apache.zookeeper.server.quorum.Observer.observeLeader(Observer.java:74)\n\tat org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1258)

Perhaps the NEWLEADER packet was still in the socket, but sitting behing INFORMANDACTIVATE, whose exception prevented us from processing NEWLEADER?

Also, it may or may not be related, but this area got changed recently as part of https://issues.apache.org/jira/browse/ZOOKEEPER-3104.",[],Bug,ZOOKEEPER-3591,Major,Alex Mirgorodskiy,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Inconsistent resync with dynamic reconfig,2019-12-20T20:19:49.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",2.0
,"[<JIRA Component: name='java client', id='12312381'>]",2019-10-22T17:59:39.000+0000,Aristotelhs,"After some reworking on the Zookeeper Sasl implementation (https://issues.apache.org/jira/browse/ZOOKEEPER-3156) the knob of zookeeper.sasl.client.canonicalize.hostname was introduced in order to disable the host name canonicalization. However in ZKClientConfig in handleBackwardCompatibility() this option is not included, which I assume is due to omission. This creates an issue if the zookeeper library is hidden behind another library that does provide an interface to change this value.

Therefore it should also be set by the system properties like the rest of the variables.","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.7', id='12346098'>]",Bug,ZOOKEEPER-3590,Minor,Aristotelhs,Fixed,2019-11-22T22:43:14.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Zookeeper is unable to set the zookeeper.sasl.client.canonicalize.hostname using system variable,2020-02-14T15:23:48.000+0000,"[<JIRA Version: name='3.5.6', id='12345243'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2019-10-22T07:53:03.000+0000,Pierre Yin,"ZOOKEEPER-3104 describes one critical data inconsistency risk.

The risk also exists in 3.4 branch.

In our 3.4.13 production cluster, the data inconsistency happens for many times.

After digging some transaction logs and snapshot, we believe that ZOOKEEPER-3104 is the main risk to contributes to our data inconsistency.

The risk probability maybe higher than we can consider in real product environment.  The serialization of big DataTree may leads to a big risk time window in the high write traffic situation. Any failure during the risk time window would cause the data inconsistency. 

The data inconsistency is almost unacceptable in zookeeper semantic.

This issue is already fixed in 3.6. But I think it is very necessary to backport ZOOKEEPER-3104 to branch-3.4, especially in the situation that the migration from 3.4 to 3.5 actually take more effort to evaluate the compatibility risk in real product environment.

I will have submit a github pull request to fix it. Can anyone help us to review it please ?

Many thanks.

 ",[],Bug,ZOOKEEPER-3589,Critical,Pierre Yin,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,3.4-branch has potential data inconsistency caused by ZOOKEEPER-3104,2020-11-23T11:52:54.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>, <JIRA Version: name='3.4.14', id='12343587'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2019-10-14T17:27:08.000+0000,Javi Roman,"Fresh download from release site:

cd apache-zookeeper-3.5.5

mv clean install
{code:java}
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Apache ZooKeeper 3.5.5 ............................. SUCCESS [  2.854 s]
[INFO] Apache ZooKeeper - Documentation ................... SUCCESS [  2.991 s]
[INFO] Apache ZooKeeper - Jute ............................ SUCCESS [  9.815 s]
[INFO] Apache ZooKeeper - Server .......................... FAILURE [  0.253 s]
[INFO] Apache ZooKeeper - Client .......................... SKIPPED
[INFO] Apache ZooKeeper - Recipes ......................... SKIPPED
[INFO] Apache ZooKeeper - Recipes - Election .............. SKIPPED
[INFO] Apache ZooKeeper - Recipes - Lock .................. SKIPPED
[INFO] Apache ZooKeeper - Recipes - Queue ................. SKIPPED
[INFO] Apache ZooKeeper - Assembly 3.5.5 .................. SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 16.172 s
[INFO] Finished at: 2019-10-14T19:23:04+02:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.codehaus.mojo:properties-maven-plugin:1.0.0:read-project-properties (default) on project zookeeper: Circular property definition: env.BASH_FUNC__module_raw%%=() {  unset _mlshdbg;
[ERROR]  if [ ""${MODULES_SILENT_SHELL_DEBUG:-0}"" = '1' ]; then
[ERROR]  case ""$-"" in 
[ERROR]  *v*x*)
[ERROR]  set +vx;
[ERROR]  _mlshdbg='vx'
[ERROR]  ;;
[ERROR]  *v*)
[ERROR]  set +v;
[ERROR]  _mlshdbg='v'
[ERROR]  ;;
[ERROR]  *x*)
[ERROR]  set +x;
[ERROR]  _mlshdbg='x'
[ERROR]  ;;
[ERROR]  *)
[ERROR]  _mlshdbg=''
[ERROR]  ;;
[ERROR]  esac;
[ERROR]  fi;
[ERROR]  unset _mlre _mlIFS;
[ERROR]  if [ -n ""${IFS+x}"" ]; then
[ERROR]  _mlIFS=$IFS;
[ERROR]  fi;
[ERROR]  IFS=' ';
[ERROR]  for _mlv in ${MODULES_RUN_QUARANTINE:-};
[ERROR]  do
[ERROR]  if [ ""${_mlv}"" = ""${_mlv##*[!A-Za-z0-9_]}"" -a ""${_mlv}"" = ""${_mlv#[0-9]}"" ]; then
[ERROR]  if [ -n ""`eval 'echo ${'$_mlv'+x}'`"" ]; then
[ERROR]  _mlre=""${_mlre:-}${_mlv}_modquar='`eval 'echo ${'$_mlv'}'`' "";
[ERROR]  fi;
[ERROR]  _mlrv=""MODULES_RUNENV_${_mlv}"";
[ERROR]  _mlre=""${_mlre:-}${_mlv}='`eval 'echo ${'$_mlrv':-}'`' "";
[ERROR]  fi;
[ERROR]  done;
[ERROR]  if [ -n ""${_mlre:-}"" ]; then
[ERROR]  eval `eval ${_mlre}/usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash '""$@""'`;
[ERROR]  else
[ERROR]  eval `/usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash ""$@""`;
[ERROR]  fi;
[ERROR]  _mlstatus=$?;
[ERROR]  if [ -n ""${_mlIFS+x}"" ]; then
[ERROR]  IFS=$_mlIFS;
[ERROR]  else
[ERROR]  unset IFS;
[ERROR]  fi;
[ERROR]  unset _mlre _mlv _mlrv _mlIFS;
[ERROR]  if [ -n ""${_mlshdbg:-}"" ]; then
[ERROR]  set -$_mlshdbg;
[ERROR]  fi;
[ERROR]  unset _mlshdbg;
[ERROR]  return $_mlstatus
[ERROR] } -> MODULES_SILENT_SHELL_DEBUG:-0=null -> IFS+x=null -> MODULES_RUN_QUARANTINE:-=null -> _mlv=null -> _mlv##*[!A-Za-z0-9_]=null -> _mlv=null
[ERROR] -> [Help 1]
{code}
mvn -version


 Apache Maven 3.5.4 (Red Hat 3.5.4-5)
 Maven home: /usr/share/maven
 Java version: 1.8.0_222, vendor: Oracle Corporation, runtime: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.222.b10-0.fc30.x86_64/jre
 Default locale: en_US, platform encoding: UTF-8
 OS name: ""linux"", version: ""5.2.18-200.fc30.x86_64"", arch: ""amd64"", family: ""unix""",[],Bug,ZOOKEEPER-3580,Major,Javi Roman,Not A Problem,2019-10-20T17:07:19.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Maven Build error: Circular property definition,2019-10-20T17:07:19.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",1.0
Zili Chen,"[<JIRA Component: name='java client', id='12312381'>]",2019-10-14T10:47:14.000+0000,Ling Mao,"When we use the native java client
{code:java}
try {
  zk = new ZooKeeper(connectString, (int) sessionTimeout, null);
} catch (IOException e) {
  throw new DBException(""Creating connection failed."");
}
{code}
We will get the following, this issue had existed in all the zookeeper releases for a long time
{code:java}
2019-10-14 18:41:49 ERROR ClientCnxn:537 - Error while calling watcher2019-10-14 18:41:49 ERROR ClientCnxn:537 - Error while calling watcherjava.lang.NullPointerException at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:535) at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:510)2019-10-14 18:41:50 ERROR ClientCnxn:537 - Error while calling watcherjava.lang.NullPointerException at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:535) at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:510)
{code}","[<JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3579,Minor,Ling Mao,Fixed,2020-04-16T10:52:00.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,handle NPE gracefully when the watch parameter of zookeeper java client is null,2021-03-28T08:54:02.000+0000,[],2.0
,"[<JIRA Component: name='kerberos', id='12329415'>, <JIRA Component: name='security', id='12329414'>]",2019-10-14T06:02:21.000+0000,Ahshan,"Although i'm able to authenticate successfully with the kerberoes account *""zookeeper/kafka-d1.eng.company.com@COMPANY.COM"" , i still happen to encounter*  AUTH_FAILED during client Authentication

Following is the verification made from my end :
 # Checked DNS ( Both Forward and Backward)

nslookup kafka-d1.eng.company.com
 Server: 172.16.2.3
 Address: 172.16.2.3#53

Name: kafka-d1.eng.company.com
 Address: 10.14.61.17

Reverse DNS

nslookup 10.14.61.17
 Server: 172.16.2.3
 Address: 172.16.2.3#53

17.61.14.10.in-addr.arpa name = kafka-d1.eng.company.com.

 

2. Kerberoes Authentication

kinit -kt /etc/keytabs/zookeeper.keytab -V zookeeper/kafka-d1.eng.company.com
 Using default cache: /tmp/krb5cc_0
 Using principal: zookeeper/kafka-d1.eng.company.com@COMPANY.COM
 Using keytab: /etc/keytabs/zookeeper.keytab
 Authenticated to Kerberos v5

 

Below is the krb5 configuration File:

cat /etc/krb5.conf
 [libdefaults]
 default_realm = COMPANY.COM
 dns_lookup_kdc = true
 dns_lookup_realm = true
 ticket_lifetime = 86400
 renew_lifetime = 604800
 forwardable = true
 default_tgs_enctypes = aes256-cts
 default_tkt_enctypes = aes256-cts
 permitted_enctypes = aes256-cts
 udp_preference_limit = 1
 kdc_timeout = 3000
 ignore_acceptor_hostname = true
 [realms]
 COMPANY.COM =

{ kdc = srv-ussc-dc01e.company.com admin_server = srv-exxx.company.com kdc = srv-exxxe.company.com }

[domain_realm]
 kafka-d1.eng.company.com = COMPANY.COM

 

export JVMFLAGS=-Djava.security.auth.login.config=/usr/share/zookeeper/conf/client_jaas.conf -Dsun.security.krb5.debug=true

 

cat /usr/share/zookeeper/conf/client_jaas.conf
Client {
 com.sun.security.auth.module.Krb5LoginModule required
 useKeyTab=true
 debug=true
 keyTab=""/etc/keytabs/zookeeper.keytab""
 storeKey=true
 useTicketCache=false
 principal=""zookeeper/kafka-d1.eng.company.com@COMPANY.COM;
};

*Error Message :[^zoo.cfg][^zookeeper_server.log]*
{noformat}
./zkCli.sh -server kafka-d1.eng.company.com:2181
Connecting to kafka-d1.eng.company.com:2181
2019-10-14 02:08:16,625 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT
2019-10-14 02:08:16,628 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=kafka-d1.eng.company.com
2019-10-14 02:08:16,628 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.8.0_201
2019-10-14 02:08:16,630 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation
2019-10-14 02:08:16,630 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/opt/jdk1.8.0_201/jre
2019-10-14 02:08:16,630 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/usr/share/zookeeper/bin/../build/classes:/usr/share/zookeeper/bin/../build/lib/*.jar:/usr/share/zookeeper/bin/../lib/slf4j-log4j12-1.6.1.jar:/usr/share/zookeeper/bin/../lib/slf4j-api-1.6.1.jar:/usr/share/zookeeper/bin/../lib/netty-3.10.5.Final.jar:/usr/share/zookeeper/bin/../lib/log4j-1.2.16.jar:/usr/share/zookeeper/bin/../lib/jline-0.9.94.jar:/usr/share/zookeeper/bin/../zookeeper-3.4.10.jar:/usr/share/zookeeper/bin/../src/java/lib/*.jar:/usr/share/zookeeper/bin/../conf:
2019-10-14 02:08:16,630 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2019-10-14 02:08:16,631 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/tmp
2019-10-14 02:08:16,631 [myid:] - INFO  [main:Environment@100] - Client environment:java.compiler=<NA>
2019-10-14 02:08:16,631 [myid:] - INFO  [main:Environment@100] - Client environment:os.name=Linux
2019-10-14 02:08:16,631 [myid:] - INFO  [main:Environment@100] - Client environment:os.arch=amd64
2019-10-14 02:08:16,631 [myid:] - INFO  [main:Environment@100] - Client environment:os.version=3.10.0-327.el7.x86_64
2019-10-14 02:08:16,631 [myid:] - INFO  [main:Environment@100] - Client environment:user.name=root
2019-10-14 02:08:16,631 [myid:] - INFO  [main:Environment@100] - Client environment:user.home=/root
2019-10-14 02:08:16,631 [myid:] - INFO  [main:Environment@100] - Client environment:user.dir=/usr/share/zookeeper-3.4.10/bin
2019-10-14 02:08:16,632 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=kafka-d1.eng.company.com:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@306a30c7
Welcome to ZooKeeper!
JLine support is enabled
Debug is  true storeKey true useTicketCache false useKeyTab true doNotPrompt false ticketCache is null isInitiator true KeyTab is /etc/keytabs/zookeeper.keytab refreshKrb5Config is false principal is zookeeper/kafka-d1.eng.company.com@COMPANY.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is false
[zk: kafka-d1.eng.company.com:2181(CONNECTING) 0] principal is zookeeper/kafka-d1.eng.company.com@COMPANY.COM
Will use keytab
Commit Succeeded 2019-10-14 02:08:16,971 [myid:] - INFO  [main-SendThread(kafka-d1.eng.company.com:2181):Login@295] - Client successfully logged in.
2019-10-14 02:08:16,973 [myid:] - INFO  [Thread-1:Login$1@128] - TGT refresh thread started.
2019-10-14 02:08:16,975 [myid:] - INFO  [Thread-1:Login@303] - TGT valid starting at:        Mon Oct 14 02:08:16 EDT 2019
2019-10-14 02:08:16,976 [myid:] - INFO  [Thread-1:Login@304] - TGT expires:                  Mon Oct 14 12:08:16 EDT 2019
2019-10-14 02:08:16,976 [myid:] - INFO  [Thread-1:Login$1@183] - TGT refresh sleeping until: Mon Oct 14 10:08:57 EDT 2019
2019-10-14 02:08:16,977 [myid:] - INFO  [main-SendThread(kafka-d1.eng.company.com:2181):SecurityUtils$1@124] - Client will use GSSAPI as SASL mechanism.
2019-10-14 02:08:16,988 [myid:] - INFO  [main-SendThread(kafka-d1.eng.company.com:2181):ClientCnxn$SendThread@1032] - Opening socket connection to server kafka-d1.eng.company.com/10.14.61.17:2181. Will attempt to SASL-authenticate using Login Context section 'Client'
2019-10-14 02:08:16,994 [myid:] - INFO  [main-SendThread(kafka-d1.eng.company.com:2181):ClientCnxn$SendThread@876] - Socket connection established to kafka-d1.eng.company.com/10.14.61.17:2181, initiating session
2019-10-14 02:08:17,002 [myid:] - INFO  [main-SendThread(kafka-d1.eng.company.com:2181):ClientCnxn$SendThread@1299] - Session establishment complete on server kafka-d1.eng.company.com/10.14.61.17:2181, sessionid = 0x16dc8cbdb3b0002, negotiated timeout = 30000WATCHER::WatchedEvent state:SyncConnected type:None path:null
2019-10-14 02:08:17,024 [myid:] - ERROR [main-SendThread(kafka-d1.eng.company.com:2181):ZooKeeperSaslClient@247] - SASL authentication failed using login context 'Client'.WATCHER::WatchedEvent state:AuthFailed type:None path:null{noformat}
 

 ",[],Bug,ZOOKEEPER-3576,Major,Ahshan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper Fails with AUTH_FAILED state  with SASL,2019-10-14T07:44:28.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",1.0
,"[<JIRA Component: name='c client', id='12312380'>]",2019-10-07T23:46:22.000+0000,Ronald Fenner,"I'm trying to compile the source and getting this error

make all-am
 make[1]: Entering directory `/home/ec2-user/zookeeper/zookeeper-client/zookeeper-client-c'
 /bin/sh ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I./include -I./tests -I./generated -Wall -Werror -Wdeclaration-after-statement -g -O2 -D_GNU_SOURCE -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c -o zookeeper.lo `test -f 'src/zookeeper.c' || echo './'`src/zookeeper.c
 libtool: compile: gcc -DHAVE_CONFIG_H -I. -I./include -I./tests -I./generated -Wall -Werror -Wdeclaration-after-statement -g -O2 -D_GNU_SOURCE -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c src/zookeeper.c -fPIC -DPIC -o .libs/zookeeper.o
 src/zookeeper.c: In function 'print_completion_queue':
 src/zookeeper.c:2542:5: error: null argument where non-null required (argument 1) [-Werror=nonnull]
 fprintf(LOGSTREAM,""Completion queue: "");
 ^~~~~~~
 src/zookeeper.c:2544:9: error: null argument where non-null required (argument 1) [-Werror=nonnull]
 fprintf(LOGSTREAM,""empty\n"");
 ^~~~~~~
 src/zookeeper.c:2550:9: error: null argument where non-null required (argument 1) [-Werror=nonnull]
 fprintf(LOGSTREAM,""%d,"",cptr->xid);
 ^~~~~~~
 src/zookeeper.c:2553:5: error: null argument where non-null required (argument 1) [-Werror=nonnull]
 fprintf(LOGSTREAM,""end\n"");
 ^~~~~~~
 cc1: all warnings being treated as errors
 make[1]: *** [zookeeper.lo] Error 1
 make[1]: Leaving directory `/home/ec2-user/zookeeper/zookeeper-client/zookeeper-client-c'
 make: *** [all] Error 2

 

Looking through the code in include/zookeeper_log.h at line 30 LOGSTREAM is defined as NULL. This cause the above error.

In the 3.4.x branch it was getLogStream().

I believe this for the 3.5 branch should be zoo_get_log_stream()

 

 ",[],Bug,ZOOKEEPER-3569,Major,Ronald Fenner,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Compile error due to LOGSTREAM being null when passed to fprintf,2019-10-08T05:03:19.000+0000,[],2.0
,"[<JIRA Component: name='tests', id='12312427'>]",2019-10-01T16:00:44.000+0000,Patrick D. Hunt,"I ran the mvn tests on 3.5 branch and then submitted a PR. I didn't notice that one of the snapfiles got caught up in the process, see this commit:

https://github.com/phunt/zookeeper/commit/44c7f93398aa47feea444afd2aaea4592324284e

something seems borked with mvn test - modified (generated/etc...) files should be in target not the mainline code.

See discussion here:

https://github.com/apache/zookeeper/pull/1102#issuecomment-537090502",[],Bug,ZOOKEEPER-3565,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,snapfile modified out of target directory during mvn testing,2019-10-01T16:00:44.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",0.0
,"[<JIRA Component: name='java client', id='12312381'>]",2019-10-01T02:07:46.000+0000,Hongcai Deng,"recently i found some fullgc occur on my java app. i did heapdump and found that

!image-2019-10-01-10-02-28-228.png!

EventThread ate too much memory. I dig into zk code, found that

 
{code:java}
class EventThread extends ZooKeeperThread {
    private final LinkedBlockingQueue<Object> waitingEvents =
        new LinkedBlockingQueue<Object>();

    // code lines
}{code}
waitingEvents not set a boundary. is this for some reason?

 ",[],Bug,ZOOKEEPER-3564,Critical,Hongcai Deng,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,org.apache.zookeeper.ClientCnxn EventThread Memory Problem,2020-05-27T12:30:12.000+0000,[],6.0
,"[<JIRA Component: name='security', id='12329414'>]",2019-09-30T18:25:35.000+0000,Patrick D. Hunt,"The mvn dependency check is failing on 3.4 and 3.5:

3.4:
[ERROR] netty-3.10.6.Final.jar: CVE-2019-16869

3.5:
[ERROR] netty-transport-4.1.29.Final.jar: CVE-2019-16869
","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.6', id='12345243'>]",Bug,ZOOKEEPER-3563,Blocker,Patrick D. Hunt,Fixed,2019-10-08T13:17:12.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,dependency check failing on 3.4 and 3.5 branches - CVE-2019-16869 on Netty,2019-10-16T18:59:07.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.4.14', id='12343587'>]",1.0
Colm O hEigeartaigh,[],2019-09-25T18:37:52.000+0000,Colm O hEigeartaigh,Jackson should be updated to the latest version to pick up a fix for CVE-2019-14540,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.6', id='12345243'>]",Bug,ZOOKEEPER-3559,Major,Colm O hEigeartaigh,Fixed,2019-09-27T11:22:05.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Update Jackson to 2.9.10,2019-10-16T18:59:18.000+0000,[],2.0
Enrico Olivelli,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='c client', id='12312380'>]",2019-09-19T18:13:25.000+0000,Enrico Olivelli,"During the Rc0 VOTE of 3.5.6 we found that the 'configure' file inside the source tarball does not have the right permissions.

","[<JIRA Version: name='3.5.6', id='12345243'>]",Bug,ZOOKEEPER-3552,Blocker,Enrico Olivelli,Fixed,2019-09-20T08:20:56.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Source tarbal for branch-3.5 does not set execution permissions to ""configure"" file",2019-10-16T18:58:57.000+0000,"[<JIRA Version: name='3.5.6', id='12345243'>]",1.0
,[],2019-09-17T10:02:41.000+0000,yeshuangshuang,"i found a bug,the log.xxx is not auto purge as 65MB, it increases 1GB",[],Bug,ZOOKEEPER-3549,Major,yeshuangshuang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zookeeper auto purge bug,2020-03-07T04:55:05.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",2.0
Jordan Zimmerman,"[<JIRA Component: name='server', id='12312382'>]",2019-09-16T13:21:49.000+0000,Sylvain Wallez,"{{ContainerManager}} does not delete containers whose cversion is zero to avoid situations where a container would be deleted before the application had the chance to create children.

This caused issues in our application where the process stopped between container creation and child creation: the containers were never deleted.

To avoid this while giving applications the time to create children, empty containers with a cversion of zero should be deleted after a grace period, e.g. not when they are first collected, but the second time.","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3546,Major,Sylvain Wallez,Fixed,2019-11-25T14:38:11.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Containers that never have children stay forever,2019-12-03T18:37:13.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.5.5', id='12343268'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2019-09-10T05:16:34.000+0000,hu xiaodong,"In the method 'org.apache.zookeeper.client.ZooKeeperSaslClient#respondToServer', 
{code:java}
 LOG.error(""SASL authentication failed using login context '""
 + this.getLoginContext()
 + ""' with exception: {}"", e); {code}
// I think '{}' above is wrong. It's redundant.

 

!image-2019-09-10-14-02-30-306.png!",[],Bug,ZOOKEEPER-3541,Minor,hu xiaodong,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Wrong placeholder '{}' in logs.,2019-09-18T13:30:19.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",1.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2019-09-09T16:41:53.000+0000,Fangmin Lv,"When dynamically replace a server with IPv4/IPv6 with the same port, the server will complain about 'address already in use', and cause the client port not available anymore.","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3540,Major,Fangmin Lv,Fixed,2019-09-10T16:51:46.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Client port unavailable after binding the same client port during reconfig,2019-09-11T00:58:01.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",2.0
,"[<JIRA Component: name='build', id='12312383'>]",2019-09-06T03:13:47.000+0000,Mohammad Arshad,"On windows maven command {code}mvn clean install -DskipTests{code} creates corrupted tarballs.
In zookeeper-assembly/pom.xml <tarLongFileMode>posix</tarLongFileMode> causing the problem.  Many use Windows as development environment. it would be better if we can make tarLongFileMode property configurable or select based on OS.

",[],Bug,ZOOKEEPER-3536,Minor,Mohammad Arshad,Invalid,2019-09-20T10:28:43.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,On Windows maven build generates corrupted tarball,2019-09-20T10:28:43.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",3.0
,[],2019-09-03T17:08:26.000+0000,Karolos Antoniadis,"Hello ZooKeeper developers,

there are cases during *leader election*, where there is non-stop communication between observers and participants. 
This communication occurs as follows: 
- an observer sends a notification to a participant
- the participant responds
- an observer sends another notification and so on and so forth ...

It is possible that an observer-participant pair exchange hundreds of notification messages in the span of one second. As a consequence, the system is burdened with unnecessary load, and the logs are filled with useless information as can be seen below:

 
{noformat}
2019-09-03 16:37:22,630 [myid:4] - INFO [WorkerReceiver[myid=4]:FastLeaderElection@692] - Notification: my state:LOOKING; n.sid:2, n.state:LOOKING, n.leader:3, n.round:0x2, n.peerEpoch:0x1, n.zxid:0x0, message format version:0x2, n.config version:0x100000000
2019-09-03 16:37:22,632 [myid:4] - INFO [WorkerReceiver[myid=4]:FastLeaderElection@692] - Notification: my state:LOOKING; n.sid:1, n.state:LOOKING, n.leader:3, n.round:0x2, n.peerEpoch:0x1, n.zxid:0x0, message format version:0x2, n.config version:0x100000000
2019-09-03 16:37:22,633 [myid:4] - INFO [WorkerReceiver[myid=4]:FastLeaderElection@692] - Notification: my state:LOOKING; n.sid:2, n.state:LOOKING, n.leader:3, n.round:0x2, n.peerEpoch:0x1, n.zxid:0x0, message format version:0x2, n.config version:0x100000000
2019-09-03 16:37:22,635 [myid:4] - INFO [WorkerReceiver[myid=4]:FastLeaderElection@692] - Notification: my state:LOOKING; n.sid:1, n.state:LOOKING, n.leader:3, n.round:0x2, n.peerEpoch:0x1, n.zxid:0x0, message format version:0x2, n.config version:0x100000000
2019-09-03 16:37:22,635 [myid:4] - INFO [WorkerReceiver[myid=4]:FastLeaderElection@692] - Notification: my state:LOOKING; n.sid:2, n.state:LOOKING, n.leader:3, n.round:0x2, n.peerEpoch:0x1, n.zxid:0x0, message format version:0x2, n.config version:0x100000000{noformat}
 

 
h4. Why does the non-stop communication bug occur?

This bug stems from the fact that when a participant receives a notification from an observer, the participant responds right away, as can be seen [here|https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/FastLeaderElection.java#L325] - it is even written in the comments. Now, when the observer receives back the message from the participant there are 2 cases that could lead to non-stop communication:
1) The observer has a greater {{logicalclock}} than the participant and both the observer and the participant are in a {{LOOKING}} state. In such a case, the observer responds right away to the participant as can be seen [here|https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/FastLeaderElection.java#L392]. 
2) The observer is {{OBSERVING}} while the participant is still {{LOOKING}}, then the non-stop communication ensues due to the code in [here|https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/FastLeaderElection.java#L413].  
h4.  
h4. How can we reproduce this non-stop communication bug?

It is not trivial to reproduce this bug, although we saw it occurring in the wild. To reproduce this bug, we provide a script that utilizes docker and that can be used to easily debug ZK code. The script starts a ZK cluster with 3 participants (P1, P2, P3) and 2 observers (O1, O2). The script together with instructions on how to use it can be found [here|https://github.com/insumity/zookeeper_debug_tool].

 

Using the script, there are at least 2 ways to reproduce the bug:
1) We can artificially delay the leader election by introducing the following code in {{FastLeaderElection}} (in [here|https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/FastLeaderElection.java#L1006]).

 
{code:java}
// Verify if there is any change in the proposed leader
int time = finalizeWait;
if (self.getId() >= 1 && self.getId() <= 3) {
    time = 2000;
}{code}
 

and changing the immediate succeeding line:
{code:java}
while ((n = recvqueue.poll(finalizeWait, TimeUnit.MILLISECONDS)) != null) {code}
to 

 
{code:java}
while ((n = recvqueue.poll(time, TimeUnit.MILLISECONDS)) != null) { 
{code}
Now, if we run a ZK cluster and force a leader election by killing the leader, we see the non-stop communication occurring. The reason is that  as a result of this delay the observer restarts (increments its {{logicalclock}}), tries to connect to the previous leader, but fails since the previous leader is crashed, and the observer restarts by incrementing {{logicalclock}} once more and hence starting the non-stop communication.


2) Another way to reproduce the bug is by creating a network partition that partitions P1 from P2, P3, O2 but that still keeps participant P1 connected to observer O1. In such a case, the non-stop communication ensues since O1 is {{OBSERVING}} while P1 remains in a {{LOOKING}} state. To reproduce this bug, using the above script, someone just has to do:
 *  wait till the ZK cluster starts running
 *  in your local machine do ./create_np_case_3.sh (attached file in this issue)
 *  force a leader election by restarting the leader (most likely the leader is server 3)


It is true that scenario 2 is slightly unrealistic. However, the first scenario where leader election takes too much time to complete is pretty realistic.  Whenever we saw this non-stop communication bug, it was because leader election took too long to complete. For instance, it could occur if there is some type of split-vote during LE and the elected leader times out while
{noformat}
waiting for epoch from quorum {noformat}
[here|https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Leader.java#L1350].

 
h4. 
How can we fix this issue?

One idea would be that before an observer starts observing a leader, it verifies that the leader is up and running using a check similar to {{checkLeader}} as is done [here|https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/FastLeaderElection.java#L1037].
This will prevent from having non-stop communication between observers and participants during long leader elections, since observers do not try to connect to an already failed leader, and hence they will not increase their {{logicalclock}}. However, this fix on its own does not solve the 2nd way to reproduce the bug that was described above.

Best Regards,
Karolos

 ",[],Bug,ZOOKEEPER-3534,Minor,Karolos Antoniadis,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Non-stop communication between participants and observers.,2019-09-04T23:40:26.000+0000,[],3.0
Chang Lou,[],2019-09-02T21:02:01.000+0000,Chang Lou,"During our ZooKeeper fault injection testing, we observed that sometimes the ZK cluster could hang (requests time out, node status shows ok). After inspecting the issue, we believe this is caused by I/O (serializing ACLCache) inside a critical section. The bug is essentially similar to what is described in ZooKeeper-2201.

org.apache.zookeeper.server.DataTree#serialize calls the aclCache.serialize when doing dataree serialization, however, org.apache.zookeeper.server.ReferenceCountedACLCache#serialize could get stuck at OutputArchieve.writeInt due to potential network/disk issues. This can cause the system experiences hanging issues similar to ZooKeeper-2201 (any attempt to create/delete/modify the DataNode will cause the leader to hang at the beginning of the request processor chain). The root cause is the lock contention between:
 * org.apache.zookeeper.server.DataTree#serialize -> org.apache.zookeeper.server.ReferenceCountedACLCache#serialize 
 * PrepRequestProcessor#getRecordForPath -> org.apache.zookeeper.server.DataTree#getACL(org.apache.zookeeper.server.DataNode) -> org.apache.zookeeper.server.ReferenceCountedACLCache#convertLong

When the snapshot gets stuck in acl serialization, it would prevent all other operations to ReferenceCountedACLCache. Since getRecordForPath calls ReferenceCountedACLCache#convertLong, any op triggering getRecordForPath will cause the leader to hang at the beginning of the request processor chain:
{code:java}
org.apache.zookeeper.server.ReferenceCountedACLCache.convertLong(ReferenceCountedACLCache.java:87)
org.apache.zookeeper.server.DataTree.getACL(DataTree.java:734)
   - locked org.apache.zookeeper.server.DataNode@4a062b7d
org.apache.zookeeper.server.ZKDatabase.aclForNode(ZKDatabase.java:371)
org.apache.zookeeper.server.PrepRequestProcessor.getRecordForPath(PrepRequestProcessor.java:170)
   - locked java.util.ArrayDeque@3f7394f7
org.apache.zookeeper.server.PrepRequestProcessor.pRequest2Txn(PrepRequestProcessor.java:417)
org.apache.zookeeper.server.PrepRequestProcessor.pRequest(PrepRequestProcessor.java:757)
org.apache.zookeeper.server.PrepRequestProcessor.run(PrepRequestProcessor.java:145)
{code}
Similar to ZooKeeper-2201, the leader can still send out heartbeats so the cluster will not recover until the network/disk issue resolves.  

Steps to reproduce this bug:
 # start a cluster with 1 leader and n followers
 # manually create some ACLs, to enlarge the window of dumping acls so it would be more likely to hang at serializing ACLCache when delay happens. (we wrote a script to generate such workloads, see attachments)
 # inject long network/disk write delays and run some benchmarks to trigger snapshots
 # once stuck, you should observe new requests to the cluster would fail.

Essentially the core problem is the OutputArchive write should not be kept inside this synchronization block. So a straightforward solution is to move writes out of sync block: do a copy inside the sync block and perform vulnerable network writes afterwards. The patch for this solution is attached and verified.  Another more systematic fix is perhaps replacing all synchronized methods in the ReferenceCountedACLCache with ConcurrentHashMap. 

We double checked that the issue remains in the latest version of master branch (68c21988d55c57e483370d3ee223c22da2d1bbcf). 

Attachments are 1) patch for fix and regression test 2) scripts to generate workloads to fill ACL cache","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3531,Critical,Chang Lou,Fixed,2019-10-09T14:39:04.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Synchronization on ACLCache cause cluster to hang when network/disk issues happen during datatree serialization,2019-10-09T19:28:00.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.5.5', id='12343268'>]",4.0
,"[<JIRA Component: name='server', id='12312382'>]",2019-08-30T04:56:48.000+0000,wanglei,"1.Node1:1566815238 (myid)、Node2:1566815239 (myid)、Node3:1566815240 (myid)

2.After a election, Node3 become the new leader, begin to sync with followers

 

*2019-08-27 04:26:09,521 [myid:1566815240] - INFO [NIOServerCxn.Factory:/172.28.8.123:9880:ZooKeeperServer@910][] - Refusing session request for not-read-only client /172.28.0.3:38994*
 *2019-08-27 04:26:09,609 [myid:1566815240] - INFO [QuorumPeer[myid=1566815240]/172.28.8.123:9880:ZooKeeperServer@502][] - shutting down*
 *2019-08-27 04:26:09,609 [myid:1566815240] - INFO [QuorumPeer[myid=1566815240]/172.28.8.123:9880:SessionTrackerImpl@226][] - Shutting down*
 *2019-08-27 04:26:09,609 [myid:1566815240] - INFO [QuorumPeer[myid=1566815240]/172.28.8.123:9880:PrepRequestProcessor@769][] - Shutting down*
 *2019-08-27 04:26:09,609 [myid:1566815240] - INFO [ReadOnlyRequestProcessor:1566815240:ReadOnlyRequestProcessor@111][] - ReadOnlyRequestProcessor exited loop!*
 *2019-08-27 04:26:09,610 [myid:1566815240] - INFO [ProcessThread(sid:1566815240 cport:-1)::PrepRequestProcessor@144][] - PrepRequestProcessor exited loop!*
 *2019-08-27 04:26:09,610 [myid:1566815240] - INFO [QuorumPeer[myid=1566815240]/172.28.8.123:9880:FinalRequestProcessor@430][] - shutdown of request processor complete*
 *2019-08-27 04:26:09,613 [myid:1566815240] - INFO [QuorumPeer[myid=1566815240]/172.28.8.123:9880:QuorumPeer@992][] - LEADING*
 *2019-08-27 04:26:09,615 [myid:1566815240] - INFO [QuorumPeer[myid=1566815240]/172.28.8.123:9880:Leader@64][] - TCP NoDelay set to: true*
 *2019-08-27 04:26:09,616 [myid:1566815240] - INFO [QuorumPeer[myid=1566815240]/172.28.8.123:9880:ZooKeeperServer@174][] - Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 300000 datadir /opt/fusionplatform/data/zookeeper/data/version-2 snapdir /opt/fusionplatform/data/zookeeper/data/version-2*
 *2019-08-27 04:26:09,616 [myid:1566815240] - INFO [QuorumPeer[myid=1566815240]/172.28.8.123:9880:Leader@380][] - {color:#FF0000}LEADING - LEADER ELECTION TOOK - 15297{color}*
 *2019-08-27 04:26:09,956 [myid:1566815240] - INFO [NIOServerCxn.Factory:/172.28.8.123:9880:NIOServerCnxnFactory@222][] - Accepted socket connection from /172.28.0.3:39012*
 *2019-08-27 04:26:09,956 [myid:1566815240] - WARN [NIOServerCxn.Factory:/172.28.8.123:9880:NIOServerCnxn@383][] - Exception causing close of session 0x0: ZooKeeperServer not running*
 *2019-08-27 04:26:09,974 [myid:1566815240] - INFO [NIOServerCxn.Factory:/172.28.8.123:9880:NIOServerCnxnFactory@222][] - Accepted socket connection from /172.28.0.2:50732*
 *2019-08-27 04:26:09,974 [myid:1566815240] - WARN [NIOServerCxn.Factory:/172.28.8.123:9880:NIOServerCnxn@383][] - Exception causing close of session 0x0: ZooKeeperServer not running*
 *2019-08-27 04:26:10,513 [myid:1566815240] - INFO [NIOServerCxn.Factory:/172.28.8.123:9880:NIOServerCnxnFactory@222][] - Accepted socket connection from /172.28.0.5:60010*
 *2019-08-27 04:26:10,514 [myid:1566815240] - WARN [NIOServerCxn.Factory:/172.28.8.123:9880:NIOServerCnxn@383][] - Exception causing close of session 0x0: ZooKeeperServer not running*
 *2019-08-27 04:26:10,516 [myid:1566815240] - INFO [NIOServerCxn.Factory:/172.28.8.123:9880:NIOServerCnxnFactory@222][] - Accepted socket connection from /172.28.0.5:60020*
 *2019-08-27 04:26:10,517 [myid:1566815240] - WARN [NIOServerCxn.Factory:/172.28.8.123:9880:NIOServerCnxn@383][] - Exception causing close of session 0x0: ZooKeeperServer not running*
 *2019-08-27 04:26:10,530 [myid:1566815240] - INFO [NIOServerCxn.Factory:/172.28.8.123:9880:NIOServerCnxnFactory@222][] - Accepted socket connection from /172.28.0.5:60024*
 *2019-08-27 04:26:10,531 [myid:1566815240] - WARN [NIOServerCxn.Factory:/172.28.8.123:9880:NIOServerCnxn@383][] - Exception causing close of session 0x0: ZooKeeperServer not running*
 *2019-08-27 04:26:10,619 [myid:1566815240] - INFO [LearnerHandler-/172.28.0.2:59666:LearnerHandler@346][] - Follower sid: 1566815238 : info : org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@6f38a687*
 *2019-08-27 04:26:10,747 [myid:1566815240] - INFO [LearnerHandler-/172.28.0.2:59666:LearnerHandler@401][] -{color:#FF0000} Synchronizing with Follower sid: 1566815238 maxCommittedLog=0x3 minCommittedLog=0x9000002d9 peerLastZxid=0x9000004ca{color}*
 *2019-08-27 04:26:10,747 [myid:1566815240] - INFO [LearnerHandler-/172.28.0.2:59666:LearnerHandler@410][] - leader and follower are in sync, zxid=0x9000004ca*
 *2019-08-27 04:26:10,748 [myid:1566815240] - INFO [LearnerHandler-/172.28.0.2:59666:LearnerHandler@475][] - Sending DIFF*
 *2019-08-27 04:26:10,811 [myid:1566815240] - INFO [SessionTracker:SessionTrackerImpl@163][] - SessionTrackerImpl exited loop!*
 *2019-08-27 04:26:10,833 [myid:1566815240] - INFO [LearnerHandler-/172.28.0.2:59666:LearnerHandler@535][] - Received NEWLEADER-ACK message from 1566815238*
 *2019-08-27 04:26:10,833 [myid:1566815240] - INFO [QuorumPeer[myid=1566815240]/172.28.8.123:9880:Leader@964][] - Have quorum of supporters, sids: [ 1566815238,1566815240 ]; starting up and setting last processed zxid: 0xa00000000*
 *2019-08-27 04:26:11,160 [myid:1566815240] - INFO [SyncThread:1566815240:FileTxnLog@216][] - Creating new log file: log.a00000001*

{color:#FF0000}maxCommittedLog=0x3 minCommittedLog=0x9000002d9 peerLastZxid=0x9000004ca{color}

{color:#ff0000}*why maxCommittedLog < minCommittedLog?*{color}

2.Node 2(follower) get a Trunc message form leader.The leader zxid of the Trunc message is 0x3. So Node3 truncat the  transaction log(the zxid which is bigger than 0x3 will be deleted). At last, the data in Node2 is inconsistent.

 

*2019-08-27 04:26:14,225 [myid:1566815239] - INFO [WorkerReceiver[myid=1566815239]:FastLeaderElection@595][] - Notification: 1 (message format version), 1566815240 (n.leader), 0x9000004ca (n.zxid), 0x1 (n.round), FOLLOWING (n.state), 1566815238 (n.sid), 0xa (n.peerEpoch) LOOKING (my state)*
 *2019-08-27 04:26:14,226 [myid:1566815239] - INFO [WorkerReceiver[myid=1566815239]:FastLeaderElection@595][] - Notification: 1 (message format version), 1566815240 (n.leader), 0x9000004ca (n.zxid), 0x1 (n.round), FOLLOWING (n.state), 1566815238 (n.sid), 0xa (n.peerEpoch) FOLLOWING (my state)*
 *2019-08-27 04:26:14,226 [myid:1566815239] - INFO [QuorumPeer[myid=1566815239]/172.28.8.122:9880:QuorumPeer@980][] - FOLLOWING*
 *2019-08-27 04:26:14,226 [myid:1566815239] - INFO [Thread-1:QuorumPeer$1@936][] - Interrupted while attempting to start ReadOnlyZooKeeperServer, not started*
 *2019-08-27 04:26:14,229 [myid:1566815239] - INFO [QuorumPeer[myid=1566815239]/172.28.8.122:9880:Learner@86][] - TCP NoDelay set to: true*
 *2019-08-27 04:26:14,229 [myid:1566815239] - INFO [QuorumPeer[myid=1566815239]/172.28.8.122:9880:ZooKeeperServer@174][] - Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 300000 datadir /opt/fusionplatform/data/zookeeper/data/version-2 snapdir /opt/fusionplatform/data/zookeeper/data/version-2*
 *2019-08-27 04:26:14,230 [myid:1566815239] - INFO [QuorumPeer[myid=1566815239]/172.28.8.122:9880:Follower@65][] - {color:#FF0000}FOLLOWING - LEADER ELECTION TOOK - 36{color}*
 *2019-08-27 04:26:14,232 [myid:1566815239] - INFO [QuorumPeer[myid=1566815239]/172.28.8.122:9880:QuorumPeer$QuorumServer@185][] - Resolved hostname: 172.28.8.123 to address: /172.28.8.123*
 *2019-08-27 04:26:14,346 [myid:1566815239] - WARN [QuorumPeer[myid=1566815239]/172.28.8.122:9880:Learner@349][] - {color:#FF0000}Truncating log to get in sync with the leader 0x3{color}*
 *2019-08-27 04:26:14,371 [myid:1566815239] - INFO [QuorumPeer[myid=1566815239]/172.28.8.122:9880:DataTree@715][] - type: create, sessionid:0x10000080a040001 cxid:0x4 zxid:0x3 reqpath:/cps*
 *2019-08-27 04:26:14,374 [myid:1566815239] - WARN [QuorumPeer[myid=1566815239]/172.28.8.122:9880:Learner@387][] - Got zxid 0xa00000001 expected 0x1*

 ",[],Bug,ZOOKEEPER-3526,Major,wanglei,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,data inconsistency due to mistaken TRUNC caused by maxCommittedLog is much less than minCommittedLog when in readonly mode,2022-02-11T11:31:56.000+0000,"[<JIRA Version: name='3.4.14', id='12343587'>]",4.0
Mate Szalay-Beko,[],2019-08-28T14:08:10.000+0000,Mate Szalay-Beko,"We had an earlier fix in ZOOKEEPER-3470 for this test, but it looks failing again. 

I haven't found any failures on the [zookeeper trunk job|https://builds.apache.org/view/ZK%20All/job/ZooKeeper-trunk], but it does fail from time to time on the [precommit maven jobs |https://builds.apache.org/job/PreCommit-ZOOKEEPER-github-pr-build-maven/org.apache.zookeeper$zookeeper/1171/testReport/junit/org.apache.zookeeper.server.quorum/LearnerMetricsTest/testLearnerMetricsTest/] or on [zookeeper-master-maven|https://builds.apache.org/view/ZK%20All/job/zookeeper-master-maven/lastCompletedBuild/org.apache.zookeeper$zookeeper/testReport/org.apache.zookeeper.server.quorum/LearnerMetricsTest/testLearnerMetricsTest/]. 

Can this be maven related somehow? would be strange...",[],Bug,ZOOKEEPER-3524,Major,Mate Szalay-Beko,Duplicate,2019-09-20T09:13:32.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0, LearnerMetricsTest.testLearnerMetricsTest is flaky again,2019-09-20T09:13:32.000+0000,[],1.0
Zili Chen,"[<JIRA Component: name='jute', id='12312385'>]",2019-08-25T02:51:29.000+0000,Zili Chen,"jute generate {{equals}} as follow


{code:java}
    String genJavaEquals(String fname, String peer) {
        return ""    ret = "" + fname + "".equals("" + peer + "");\n"";
    }
{code}

if {{fname}} is null at the runtime, then a {{NullPointerException}} would be thrown, see [this report|https://builds.apache.org/job/PreCommit-ZOOKEEPER-github-pr-build-maven/org.apache.zookeeper$zookeeper/1167/testReport/junit/org.apache.zookeeper.server/PrepRequestProcessorTest/testPRequest/] for instance.

Java already solved this problem by using {{java.util.Objetcs.equals}}, I address this issue along with ZOOKEEPER-3290 in GH-839. But I need input from CPP and CSharp side.

BTW, is there anybody use jute's CSharp version or even CPP version?",[],Bug,ZOOKEEPER-3521,Critical,Zili Chen,Won't Fix,2022-03-14T05:07:45.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,equals generate by jute potentially cause NPE,2022-03-14T05:07:45.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2019-08-25T02:43:56.000+0000,Zili Chen,"We trickily set a diagnosis msg as {{path}} of {{KeeperException}} in {{CreateMode}}(see code snippet below) because {{KeeperException}} doesn't have a dedicated error message field. It is good to have a {{diagnosis}} field beside {{path}} for error message beyond a path.


{code:java}
// CreateMode#L136
String errMsg = ""Received an invalid flag value: "" + flag + "" to convert to a CreateMode"";
LOG.error(errMsg);
throw new KeeperException.BadArgumentsException(errMsg);
{code}
",[],Bug,ZOOKEEPER-3520,Major,Zili Chen,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,KeeperException is good to have a error message field besides path,2019-08-25T02:43:56.000+0000,[],2.0
Patrick D. Hunt,"[<JIRA Component: name='security', id='12329414'>]",2019-08-25T01:50:30.000+0000,Patrick D. Hunt,"owasp check is flagging jackson-databind 2.9.9.1 - upgrade to 2.9.9.3

CVE-2019-14379, CVE-2019-14439, CVE-2019-12384","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.6', id='12345243'>]",Bug,ZOOKEEPER-3518,Blocker,Patrick D. Hunt,Fixed,2019-08-25T19:34:27.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,owasp check flagging jackson-databind 2.9.9.1,2019-10-16T18:58:53.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",1.0
,"[<JIRA Component: name='jmx', id='12312451'>]",2019-08-23T20:43:16.000+0000,Rohit Singh," 
{code:java}
-Dcom.sun.management.jmxremote.authenticate=true -Dcom.sun.management.jmxremote.port=9992 -Dcom.sun.management.jmxremote.rmi.port=9993 -Dcom.sun.management.jmxremote.password.file=/zookeeper/zookeeper-3.4.8/conf/jmxremote-password -Dcom.sun.management.jmxremote.access.file=/zookeeper/zookeeper-3.4.8/conf/jmxremote-access -Dcom.sun.management.jmxremote.ssl=true -Djavax.net.ssl.keyStore=/opt/zookeeper/certificate.ks -Djavax.net.ssl.keyStorePassword=YmM1NTkwZTVlZDg0 -Djavax.net.ssl.trustStore=/opt/zookeeper/serviceCA.ts -Djavax.net.ssl.trustStorePassword=YmM1NTkwZTVlZDg0 -Dcom.sun.management.jmxremote.registry.ssl=true -Dzookeeper.jmx.log4j.disable= -Djava.rmi.server.hostname=<hostname> org.apache.zookeeper.server.quorum.QuorumPeerMain
{code}
When zookeeper is brought with above options following error is seen
{code:java}
Error: Exception thrown by the agent : java.lang.IllegalArgumentException: Expected word at end of line [readwrite ]
{code}
However when Dcom.sun.management.jmxremote.authenticate=false is set to false then zookeeper  starts without any errors, but remote authentication is disabled and ssl works.
{code:java}
-Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=9992 -Dcom.sun.management.jmxremote.rmi.port=9993 -Dcom.sun.management.jmxremote.password.file=/zookeeper/zookeeper-3.4.8/conf/jmxremote-password -Dcom.sun.management.jmxremote.access.file=/zookeeper/zookeeper-3.4.8/conf/jmxremote-access -Dcom.sun.management.jmxremote.ssl=true -Djavax.net.ssl.keyStore=/opt/zookeeper/certificate.ks -Djavax.net.ssl.keyStorePassword=YzJhZjIxN2Q2ODQ4 -Djavax.net.ssl.trustStore=/opt/zookeeper/serviceCA.ts -Djavax.net.ssl.trustStorePassword=YzJhZjIxN2Q2ODQ4 -Dcom.sun.management.jmxremote.registry.ssl=true -Dzookeeper.jmx.log4j.disable= -Djava.rmi.server.hostname=<hostname> org.apache.zookeeper.server.quorum.QuorumPeerMain
{code}
Is this behavior expected. 

 

 ",[],Bug,ZOOKEEPER-3516,Major,Rohit Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper not working with enabling ssl and remote authentication,2019-08-23T20:45:45.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2019-08-23T16:13:15.000+0000,Holger Herbert,values for keystorepass and truststtorePass are stored clear in server.XML,[],Bug,ZOOKEEPER-3515,Critical,Holger Herbert,Not A Problem,2020-05-22T15:44:20.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,server.xml howing keystore and truststore passwords as clear text,2020-05-22T15:44:20.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2019-08-21T08:18:04.000+0000,Stephan Huttenhuis,"In ZOOKEEPER-2325 a check was added that requires a snapshot when loading data. We have been running 3-node ensembles on Zookeeper 3.4.13 for about 5 months for use with Solr Cloud. During this time some ensembles created a few snapshots but other didn't generate any. Because of this upgrading to e.g. 3.5.5 fails.

Either it is perfectly possible for Zookeeper data to have no snapshots or something is going wrong with generating snapshots. The ensembles are straightforward.
 - The following stack occurs:
{noformat}
java.io.IOException: No snapshot found, but there are log entries. Something is broken!
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:211)
	at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:240)
	at org.apache.zookeeper.server.ZooKeeperServer.loadData(ZooKeeperServer.java:290)
	at org.apache.zookeeper.server.ZooKeeperServer.startdata(ZooKeeperServer.java:450)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.startup(NIOServerCnxnFactory.java:764)
	at org.apache.zookeeper.server.ServerCnxnFactory.startup(ServerCnxnFactory.java:98)
	at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:144)
	at org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:106)
	at org.apache.zookeeper.server.ZooKeeperServerMain.main(ZooKeeperServerMain.java:64)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:128)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:82)
{noformat}

 - The zoo.cfg
{noformat}
# The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just
# example sakes.
dataDir=/data/zookeeper/data
# the port at which the clients will connect
clientPort=2181

server.1=myserver1:2888:3888
server.2=myserver2:2888:3888
server.3=myserver3:2888:3888
{noformat}
 

 - The contents of /data/zookeeper/data/version-2
{noformat}
-rw-r--r-- 1 zookeeper zookeeper    1 Aug  7 21:50 acceptedEpoch
-rw-r--r-- 1 zookeeper zookeeper    1 Aug  8 20:38 currentEpoch
-rw-r--r-- 1 zookeeper zookeeper  65M Apr  1 14:44 log.1
-rw-r--r-- 1 zookeeper zookeeper  65M May 15 23:30 log.100000001
-rw-r--r-- 1 zookeeper zookeeper  65M Jul  3 23:21 log.100001645
-rw-r--r-- 1 zookeeper zookeeper  65M Aug  8 20:37 log.300000802
-rw-r--r-- 1 zookeeper zookeeper  65M Aug 20 13:58 log.70000062a
-rw-r--r-- 1 zookeeper zookeeper  65M Apr  4 21:22 log.f0
{noformat}",[],Bug,ZOOKEEPER-3513,Major,Stephan Huttenhuis,Duplicate,2019-12-04T14:16:10.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper upgrade fails due to missing snapshots,2019-12-04T14:16:11.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",4.0
,[],2019-08-15T08:18:38.000+0000,Damien Diederen,"As mentioned in https://github.com/apache/zookeeper/pull/1054#discussion_r314208678 :

There is a {{sleep 3}} statement in {{zkServer.sh restart}}.  I am unable to unearth the history of that particular line, but I believe part—if not all—of that {{sleep}} should be part of {{zkServer.sh stop}}.

I frequently observe {{FAILED TO START}} errors in the C test suite; the logs consistently show that those are caused by {{java.net.BindException: Address already in use}}.  Adding a simple {{sleep 1}} before {{echo STOPPED}} ""fixes"" it for me.  I will submit an initial PR with the corresponding change and a commit message akin to:

----

ZOOKEEPER-XXXX: Make zkServer.sh stop more reliable

Kill is asynchronous, and without the sleep, the server's TCP port can still be busy when the next server is started—causing flaky runs of the C client's test suite.

(It would probably be better to spin a few times, probing with ps -p.)

----

As noted above, the sleep is far from optimal, an adaptive mechanism would be better—but I do not want to make the first iteration too complicated.","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.6', id='12345243'>]",Bug,ZOOKEEPER-3510,Minor,Damien Diederen,Fixed,2019-08-23T10:16:23.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Frequent 'zkServer.sh stop' failures when running C test suite,2019-10-16T18:59:09.000+0000,[],2.0
,"[<JIRA Component: name='security', id='12329414'>, <JIRA Component: name='server', id='12312382'>]",2019-08-13T02:34:07.000+0000,xiaoqin.fu,"In org.apache.zookeeper.server.persistence.FileTxnSnapLog, the statement LOG.debug don't have LOG controls:
    public void processTransaction(TxnHeader hdr,DataTree dt,
            Map<Long, Integer> sessions, Record txn)
        throws KeeperException.NoNodeException {  
		......
        if (rc.err != Code.OK.intValue()) {
            LOG.debug(""Ignoring processTxn failure hdr:"" + hdr.getType()
                    + "", error: "" + rc.err + "", path: "" + rc.path);
        }  
		......
    } 

Sensitive information about hdr type or rc path was leaked. The conditional statement LOG.isDebugEnabled() should be added:
    public void processTransaction(TxnHeader hdr,DataTree dt,
            Map<Long, Integer> sessions, Record txn)
        throws KeeperException.NoNodeException {  
		......
        if (rc.err != Code.OK.intValue()) {
        	if (LOG.isDebugEnabled())
				LOG.debug(""Ignoring processTxn failure hdr:"" + hdr.getType()
                    + "", error: "" + rc.err + "", path: "" + rc.path);
        }  
		......
    } ",[],Bug,ZOOKEEPER-3504,Major,xiaoqin.fu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,An information leakage from FileTxnSnapLog to log:,2019-08-13T08:50:34.000+0000,"[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.4.12', id='12342040'>, <JIRA Version: name='3.4.13', id='12342973'>, <JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.4.14', id='12343587'>]",2.0
Zili Chen,"[<JIRA Component: name='build', id='12312383'>]",2019-08-07T11:49:02.000+0000,Mohammad Arshad,"Currently in zookeeper-jute project jute generated source  code are put in target\classes folder. In eclipse when project is refreshed/cleaned this folder content will get deleted which results in compilation error in other projects
","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.6', id='12345243'>]",Bug,ZOOKEEPER-3498,Major,Mohammad Arshad,Fixed,2019-08-08T15:05:03.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,In zookeeper-jute project generated source should not be in target\classes folder,2019-10-16T18:59:16.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",3.0
Mohammad Arshad,[],2019-08-07T08:26:31.000+0000,Mohammad Arshad,"*Problem:*
ZooKeeper server fails to start, logs following error
{code:java}
Exception in thread ""main"" java.io.IOException: Unreasonable length = 1001025
         at org.apache.jute.BinaryInputArchive.checkLength(BinaryInputArchive.java:127)
         at org.apache.jute.BinaryInputArchive.readBuffer(BinaryInputArchive.java:92)
{code}
This indicates that one of the transactions size is more than the configured 	jute.maxbuffer values. But how transaction more than jute.maxbuffer size is allowed to write? 


*Analysis:*
At ZooKeeper server jute.maxbuffer specifies the maximum size of a transaction. By default it is 1 MB at the server
jute.maxbuffer is used for following:
# Size sanity check of incoming request. Incoming requests size must not be more than jute.maxbuffer
# Size sanity check of the transaction while reading from transaction or snapshot file. Transaction size must not be more than jute.maxbuffer+1024
# Size sanity check of transaction while reading data from the leader. Transaction size must not be more than jute.maxbuffer+1024

Request size sanity check is done in the beginning of a request processing but later request processing adds additional information into request then writes to transaction file. This additional information size is not considered in sanity check. This is how transaction larger than jute.maxbuffer are accepted into ZooKeeper.  

If this additional information size is less than 1024 Bytes then it is OK as ZooKeeper already takes care of it. 
But if this additional information size is more than 1024 bytes it allows the request, But while reading from transaction/snapshot file and while reading from leader it fails and make the ZooKeeper service unavailable  

+Example:+
Suppose incoming request size is 1000000 Bytes
Configured jute.maxbuffer is 1000000
After processing the request ZooKeeper server adds 1025 more bytes
In this case, request will be processed successfully, and 1000000+1025 bytes will be written to transaction file
But while reading from the transaction log 1000000+1025 bytes cannot be read as max allowed length is 1000000(effectively 1000000+1024).

*Solutions:*
If incoming request size sanity check is done after populating all additional information then this problem is solved. But doing sanity check in the later stage of request processing will defeat the purpose of sanity check itself. So this we can not do

Currently additional information size is constant 1024 Bytes [Code Reference|https://github.com/apache/zookeeper/blob/branch-3.5/zookeeper-jute/src/main/java/org/apache/jute/BinaryInputArchive.java#L126]. We should increase this value and make it more reasonable. I propose to make this additional information size to same as the jute.maxbuffer. Also make additional information size configurable.

","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.7', id='12346098'>]",Bug,ZOOKEEPER-3496,Critical,Mohammad Arshad,Fixed,2019-09-26T06:56:44.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Transaction larger than jute.maxbuffer makes ZooKeeper unavailable,2020-02-14T15:23:32.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.4.14', id='12343587'>]",5.0
,[],2019-08-02T14:51:44.000+0000,Matthew Hertz,"Hi,

We have a 3 node Zookeeper cluster. There are a number of znode's on the leader that are not visible on the followers.
{code:java}
$ zkCli -server <server 1> (follower) 
[zk: <server 1>(CONNECTED) 0] get /pyMkdProducer/SNAP/lock/c4a62c9fdfdc412fac3818bbb2af3a0f__lock__0000000040
abcd.company.com:<built-in function getpid>
cZxid = 0xf00061d68
ctime = Thu Nov 01 12:40:33 GMT 2018
mZxid = 0xf00061d68
mtime = Thu Nov 01 12:40:33 GMT 2018
pZxid = 0xf00061d68
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x500be5318d60407
dataLength = 58
numChildren = 0
{code}
{code:java}
$ zkCli -server <server 2> (leader)
[zk: <server2>(CONNECTED) 0] get /pyMkdProducer/SNAP/lock/c4a62c9fdfdc412fac3818bbb2af3a0f__lock__0000000040 Node does not exist: /pyMkdProducer/SNAP/lock/c4a62c9fdfdc412fac3818bbb2af3a0f__lock__0000000040
{code}
{code:java}
$ zkCli -server <server 3> (follower)
[zk: <server3>(CONNECTED) 0] get /pyMkdProducer/SNAP/lock/c4a62c9fdfdc412fac3818bbb2af3a0f__lock__0000000040
abcd.company.com:<built-in function getpid>
cZxid = 0xf00061d68
ctime = Thu Nov 01 12:40:33 GMT 2018
mZxid = 0xf00061d68
mtime = Thu Nov 01 12:40:33 GMT 2018
pZxid = 0xf00061d68
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x500be5318d60407
dataLength = 58
numChildren = 0
{code}
These nodes are ephemeral nodes. The sessions no longer exist. There are 6 znodes in this 'inconsistent' state. The cluster is currently connected - there are no networking partitions currently.

We're at a loss for how to both debug and fix this. Restarting the Zookeeper followers presumably will not help? Are all nodes ever force-synced from the leader?

Help would be appreciated. If any more information would be helpful it can be provided, however we will likely have to resolve this issue one way or another in the near future.

Thanks

 ",[],Bug,ZOOKEEPER-3490,Major,Matthew Hertz,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper followers not reflecting writes (after months),2019-08-02T14:52:30.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",1.0
,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='security', id='12329414'>]",2019-08-02T11:10:04.000+0000,xiaoqin.fu,"   In org.apache.zookeeper.ClientCnxn$SendThread, statements LOG.warn(....) don't have LOG configuration controls.	
        void readResponse(ByteBuffer incomingBuffer) throws IOException {
            ......
			LOG.warn(""Got server path "" + event.getPath()
					+ "" which is too short for chroot path ""
					+ chrootPath);
			......					
        }
	Sensitive information about event path and chroot path may be leaked. The LOG.isWarnEnabled() conditional statement should be added:
	    void readResponse(ByteBuffer incomingBuffer) throws IOException {
            ......
			if (LOG.isWarnEnabled())
				LOG.warn(""Got server path "" + event.getPath()
					+ "" which is too short for chroot path ""
					+ chrootPath);
			......					
        }",[],Bug,ZOOKEEPER-3489,Major,xiaoqin.fu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Possible information leakage to log without LOG configuration control LOG.isWarnEnabled(),2019-08-06T03:19:12.000+0000,[],1.0
,"[<JIRA Component: name='security', id='12329414'>, <JIRA Component: name='server', id='12312382'>]",2019-08-02T11:06:53.000+0000,xiaoqin.fu,"In org.apache.zookeeper.server.ZooKeeperServer, statements LOG.info(....) don't have LOG configuration controls.
    public ZooKeeperServer(FileTxnSnapLog txnLogFactory, int tickTime,
            int minSessionTimeout, int maxSessionTimeout, ZKDatabase zkDb) {    
		......
        LOG.info(""Created server with tickTime "" + tickTime
                + "" minSessionTimeout "" + getMinSessionTimeout()
                + "" maxSessionTimeout "" + getMaxSessionTimeout()
                + "" datadir "" + txnLogFactory.getDataDir()
                + "" snapdir "" + txnLogFactory.getSnapDir());   
		......
    } 
	public void finishSessionInit(ServerCnxn cnxn, boolean valid)     
		......
            if (!valid) {
                LOG.info(""Invalid session 0x""
                        + Long.toHexString(cnxn.getSessionId())
                        + "" for client ""
                        + cnxn.getRemoteSocketAddress()
                        + "", probably expired"");
                cnxn.sendBuffer(ServerCnxnFactory.closeConn);
            } else {
                LOG.info(""Established session 0x""
                        + Long.toHexString(cnxn.getSessionId())
                        + "" with negotiated timeout "" + cnxn.getSessionTimeout()
                        + "" for client ""
                        + cnxn.getRemoteSocketAddress());
                cnxn.enableRecv();
            }		   
		......	
	}	
	Sensitive information about DataDir, SnapDir, SessionId and RemoteSocketAddress may be leaked. It is better to add LOG.isInfoEnabled() conditional statements:
	    public ZooKeeperServer(FileTxnSnapLog txnLogFactory, int tickTime,
            int minSessionTimeout, int maxSessionTimeout, ZKDatabase zkDb) {    
		......
		if (LOG.isInfoEnabled())  
			LOG.info(""Created server with tickTime "" + tickTime
                + "" minSessionTimeout "" + getMinSessionTimeout()
                + "" maxSessionTimeout "" + getMaxSessionTimeout()
                + "" datadir "" + txnLogFactory.getDataDir()
                + "" snapdir "" + txnLogFactory.getSnapDir());   
		......
    } 
	public void finishSessionInit(ServerCnxn cnxn, boolean valid) {     
		......
            if (!valid) {				
				if (LOG.isInfoEnabled())  
					LOG.info(""Invalid session 0x""
                        + Long.toHexString(cnxn.getSessionId())
                        + "" for client ""
                        + cnxn.getRemoteSocketAddress()
                        + "", probably expired"");
                cnxn.sendBuffer(ServerCnxnFactory.closeConn);
            } else {			
				if (LOG.isInfoEnabled())  
					LOG.info(""Established session 0x""
                        + Long.toHexString(cnxn.getSessionId())
                        + "" with negotiated timeout "" + cnxn.getSessionTimeout()
                        + "" for client ""
                        + cnxn.getRemoteSocketAddress());
                cnxn.enableRecv();
            }		   
		......	
	}
	The LOG.isInfoEnabled() conditional statement already exists in org.apache.zookeeper.server.persistence.FileTxnLog:
	public synchronized boolean append(TxnHeader hdr, Record txn) throws IOException {		
	{	......
			   if(LOG.isInfoEnabled()){
					LOG.info(""Creating new log file: "" + Util.makeLogName(hdr.getZxid()));
			   }
		......
	}	",[],Bug,ZOOKEEPER-3488,Major,xiaoqin.fu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Possible information leakage to log without LOG configuration control LOG.isInfoEnabled(),2019-08-13T08:06:19.000+0000,"[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.4.12', id='12342040'>, <JIRA Version: name='3.4.13', id='12342973'>, <JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.4.14', id='12343587'>]",1.0
Karolos Antoniadis,"[<JIRA Component: name='leaderElection', id='12312378'>]",2019-07-31T01:23:33.000+0000,Karolos Antoniadis,"There seems to be a problem with the logging of leader election times: the logged times are much smaller than the actual time it took for the leader election to complete.

This bug can be easily reproduced by following these steps:

1) Run a ZK cluster of 3 servers

2) Kill the server that is currently the leader

3) The log files of the remaining 2 servers contain false leader election times

 

In the attached files you can see the log files of the remaining 2 serve. For brevity, I removed the parts before and after the leader election from the log files.

For example, in {{server1.txt}} we can see that:

 
{code:java}
2019-07-31 00:57:31,852 [myid:1] - WARN [QuorumPeer[myid=1](plain=/0.0.0.0:2791)(secure=disabled):QuorumPeer@1318] - PeerState set to LOOKING
2019-07-31 00:57:31,853 [myid:1] - INFO [QuorumPeer[myid=1](plain=/0.0.0.0:2791)(secure=disabled):QuorumPeer@1193] - LOOKING
2019-07-31 00:57:31,853 [myid:1] - INFO [QuorumPeer[myid=1](plain=/0.0.0.0:2791)(secure=disabled):FastLeaderElection@885] - New election. My id = 1, proposed zxid=0x100000001
[...]
2019-07-31 00:57:32,272 [myid:1] - INFO [QuorumPeer[myid=1](plain=/0.0.0.0:2791)(secure=disabled):Follower@69] - FOLLOWING - LEADER ELECTION TOOK - 1 MS{code}
Leader election supposedly took only 1ms, but in reality it took (32,272 - 31,853) = 419ms!

The reason for this bug seems to be the introduction of this line
{code:java}
start_fle = Time.currentElapsedTime();{code}
(seen here [https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/QuorumPeer.java#L1402]) 

back in this commit [https://github.com/apache/zookeeper/commit/5428cd4bc963c2e653a260c458a8a8edf3fa08ef].

 

 

 

 ","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3479,Minor,Karolos Antoniadis,Fixed,2019-08-02T18:09:13.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Logging false leader election times,2020-07-23T05:12:36.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",3.0
Karolos Antoniadis,[],2019-07-30T11:56:34.000+0000,Lara Catipovic,"Hello ZooKeeper Community,

Could you please help me with at least clarifying a few doubts related to ZooKeeper 3.4.10?
 We have 2 servers in our system, one with 2 Zookeeper servers and the one with 3 - meaning that in case of failure of the server with 3 Zookeeper servers, the quorum cannot be achieved.

*Server 11*
 Zookeeper server 10
 Zookeeper server 11
 Zookeeper server 12

*Server 12*
 Zookeeper server 20
 Zookeeper server 21 -> Leader at the beginning of the procedure

As we were changing something in the configuration, it was needed to restart our servers, and to keep the quorum up, we restarted servers one by one (first on the one with 3 servers and then the other with 2 servers).
 During the restart of the one with 3 servers, the quorum was not lost - since we restarted one by one.
 Then we tried to restart the servers on the other one where we have 2 Servers deployed, one by one also. 
 The restart was executed in a small amount of time. After we restarted the first server 20 (follower) it joined the quorum with no errors, as expected. 
 *After we restarted the Leader server (21), all followers started to shut down!*

We had the same log on all the followers, but here is the example from the follower 20:
{panel}
Jun 27 14:49:31 [myid: 20]: WARN Connection broken for id 21, my id = 20, error =
 Jun 27 14:49:31 javaOFException
 Jun 27 14:49:31 at java.io.DataInputStream.readInt(Unknown Source)
 Jun 27 14:49:31 at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:1013)
 Jun 27 14:49:31 [myid: 20]: INFO Accepted socket connection from /192.168.1.116:18532
 Jun 27 14:49:31 [myid: 20]: WARN Exception when following the leader
 Jun 27 14:49:31 OFException
 Jun 27 14:49:31 at java.io.DataInputStream.readInt(Unknown Source)
 Jun 27 14:49:31 at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
 Jun 27 14:49:31 at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:83)
 Jun 27 14:49:31 at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:99)
 Jun 27 14:49:31 at org.apache.zookeeper.server.quorum.Learner.readPacket(Learner.java:153)
 Jun 27 14:49:31 at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:85)
 Jun 27 14:49:31 at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:937)
 Jun 27 14:49:31 [myid: 20]: WARN Connection request from old client /192.168.1.116:18532; will be dropped if server is in r-o mode
 Jun 27 14:49:31 [myid: 20]: INFO Notification: 1 (message format version), 12 (n.leader), 0x66000012c7 (n.zxid), 0x19 (n.round), LOOKING (n.state), 12 (n.sid), 0x66 (n.peerEpoch) FOLLOWING (my state)
 Jun 27 14:49:31 [myid: 20]: WARN Interrupting SendWorker
 Jun 27 14:49:31 [myid: 20]: INFO Client attempting to renew session 0xa6b9dc92aa60200 at /192.168.1.116:18532
 Jun 27 14:49:31 [myid: 20]: INFO shutdown called
 Jun 27 14:49:31 java.lang.Exception: shutdown Follower
 Jun 27 14:49:31 at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:166)
 Jun 27 14:49:31 at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:941)
 Jun 27 14:49:31 [myid: 20]: INFO Revalidating client: 0xa6b9dc92aa60200
 Jun 27 14:49:31 [myid: 20]: WARN Interrupted while waiting for message on queue
 Jun 27 14:49:31 java.InterruptedException
 Jun 27 14:49:31 at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(Unknown Source)
 Jun 27 14:49:31 at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
 Jun 27 14:49:31 at java.util.concurrent.ArrayBlockingQueue.poll(Unknown Source)
 Jun 27 14:49:31 at org.apache.zookeeper.server.quorum.QuorumCnxManager.pollSendQueue(QuorumCnxManager.java:1097)
 Jun 27 14:49:31 at org.apache.zookeeper.server.quorum.QuorumCnxManager.access$700(QuorumCnxManager.java:74)
 Jun 27 14:49:31 at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:932)
{panel}
*Is it expected that Leader in case of its restart triggers shut down of all its followers?* 
 This seems to me as an unexpected behavior, but maybe I'm wrong.

 

After this step and the servers are up again, 20 tries to become a Leader and server 21 accepts it and tries to follow the new Leader.
 20 received ACK messages from itself and from 21. 
 There are also notifications sent about a new Leader to all other Zookeeper servers:
{panel}
Jun 27 14:49:31 [myid: 20]: INFO LEADING
 Jun 27 14:49:31 [myid: 20]: INFO Created server with tickTime 1500 minSessionTimeout 3000 maxSessionTimeout 30000 datadir /local/cudb/BCServer/version-2 snapdir /local/cudb/BCServer/version-2
 Jun 27 14:49:31 [myid: 20]: INFO LEADING - LEADER ELECTION TOOK - 213
 Jun 27 14:49:32 [myid: 20]: INFO Notification: 1 (message format version), 20 (n.leader), 0x66000012c7 (n.zxid), 0x19 (n.round), LOOKING (n.state), 12 (n.sid), 0x66 (n.peerEpoch) LEADING (my state)
 Jun 27 14:49:32 [myid: 20]: INFO Notification: 1 (message format version), 20 (n.leader), 0x66000012c7 (n.zxid), 0x19 (n.round), LOOKING (n.state), 10 (n.sid), 0x66 (n.peerEpoch) LEADING (my state)
 Jun 27 14:49:32 [myid: 20]: INFO Notification: 1 (message format version), 20 (n.leader), 0x66000012c7 (n.zxid), 0x19 (n.round), LOOKING (n.state), 11 (n.sid), 0x66 (n.peerEpoch) LEADING (my state)
 Jun 27 14:49:32 [myid: 20]: INFO Notification: 1 (message format version), 21 (n.leader), 0x66000012c7 (n.zxid), 0x1 (n.round), LOOKING (n.state), 21 (n.sid), 0x66 (n.peerEpoch) LEADING (my state)
 Jun 27 14:49:32 [myid: 20]: INFO Notification: 1 (message format version), 21 (n.leader), 0x66000012c7 (n.zxid), 0x19 (n.round), LOOKING (n.state), 21 (n.sid), 0x66 (n.peerEpoch) LEADING (my state)
 Jun 27 14:49:32 [myid: 20]: INFO Follower sid: 21 : info : org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@466717f0
 Jun 27 14:49:32 [myid: 20]: INFO Notification: 1 (message format version), 21 (n.leader), 0x66000012c7 (n.zxid), 0x19 (n.round), LOOKING (n.state), 12 (n.sid), 0x66 (n.peerEpoch) LEADING (my state)
 Jun 27 14:49:32 [myid: 20]: INFO Notification: 1 (message format version), 21 (n.leader), 0x66000012c7 (n.zxid), 0x19 (n.round), LOOKING (n.state), 12 (n.sid), 0x66 (n.peerEpoch) LEADING (my state)
 Jun 27 14:49:32 [myid: 20]: INFO Notification: 1 (message format version), 21 (n.leader), 0x66000012c7 (n.zxid), 0x19 (n.round), LOOKING (n.state), 11 (n.sid), 0x66 (n.peerEpoch) LEADING (my state)
 Jun 27 14:49:32 [myid: 20]: INFO Notification: 1 (message format version), 21 (n.leader), 0x66000012c7 (n.zxid), 0x19 (n.round), LOOKING (n.state), 10 (n.sid), 0x66 (n.peerEpoch) LEADING (my state)
 Jun 27 14:49:32 [myid: 20]: INFO Notification: 1 (message format version), 21 (n.leader), 0x66000012c7 (n.zxid), 0x19 (n.round), LOOKING (n.state), 11 (n.sid), 0x66 (n.peerEpoch) LEADING (my state)
 Jun 27 14:49:32 [myid: 20]: INFO Notification: 1 (message format version), 21 (n.leader), 0x66000012c7 (n.zxid), 0x19 (n.round), LOOKING (n.state), 10 (n.sid), 0x66 (n.peerEpoch) LEADING (my state)
{panel}
 

From the servers 10, 11, 12 (located on the server with 3 ZooKeeper servers) it can be seen they all entered the state 'FOLLOWING' and from this step we would expect the Leader to start leading, and followers to start following:
{panel}
Jun 27 14:49:32 [myid: 12]: INFO FOLLOWING
 Jun 27 14:49:32 [myid: 12]: INFO Created server with tickTime 1500 minSessionTimeout 3000 maxSessionTimeout 30000 datadir
 Jun 27 14:49:32 [myid: 12]: INFO FOLLOWING - LEADER ELECTION TOOK - 1217
{panel}
 

But, servers from our first system (10,11,12) are not able to connect to new Leader (20), and it seems they are trying to connect to old Leader (21) (assuming this is the case since they are all using port 4512 which corresponds to Server 21). This log can be seen on all servers where we have 3 ZooKeeper servers deployed (10,11,12):
{panel}
Jun 27 14:49:38 [myid: 12]: WARN Unexpected exception, tries=0, connecting to /192.168.1.116:4512
 Jun 27 14:49:38 java.net.SocketTimeoutException: connect timed out
 Jun 27 14:49:38 at java.net.PlainSocketImpl.socketConnect(Native Method)
 Jun 27 14:49:38 at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source)
 Jun 27 14:49:38 at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source)
 Jun 27 14:49:38 at java.net.AbstractPlainSocketImpl.connect(Unknown Source)
 Jun 27 14:49:38 at java.net.SocksSocketImpl.connect(Unknown Source)
 Jun 27 14:49:38 at java.net.Socket.connect(Unknown Source)
 Jun 27 14:49:38 at org.apache.zookeeper.server.quorum.Learner.connectToLeader(Learner.java:231)
 Jun 27 14:49:38 at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:71)
 Jun 27 14:49:38 at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:937)
{panel}
*Is my understanding correct, or this port is not indicating they are trying to connect to the wrong server?*

 

Even if the Leader restart provoked the restart of all its followers, seems that in this step all other servers should start to connect to the new leader and form a quorum of followers.
 Instead of that scenario, a few seconds later, timeout occurs while waiting for epoch for quorum (the followers never start following although they all received notifications, and they try to connect to old leader) and the 'new' Leader shuts down again:
{panel}
Jun 27 14:49:39 [myid: 20]: WARN Unexpected exception
 *Jun 27 14:49:39 java.lang.InterruptedException: Timeout while waiting for epoch from quorum*
 Jun 27 14:49:39 at org.apache.zookeeper.server.quorum.Leader.getEpochToPropose(Leader.java:896)
 Jun 27 14:49:39 at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:389)
 Jun 27 14:49:39 at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:950)
 Jun 27 14:49:39 [myid: 20]: INFO Shutting down
 Jun 27 14:49:39 [myid: 20]: INFO Shutdown called
 Jun 27 14:49:39 java.lang.Exception: shutdown Leader! reason: Forcing shutdown
 Jun 27 14:49:39 at org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:517)
 Jun 27 14:49:39 at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:956)
 Jun 27 14:49:39 [myid: 20]: INFO exception while shutting down acceptor: java.net.SocketException: Socket closed
 Jun 27 14:49:39 [myid: 20]: INFO LOOKING
 Jun 27 14:49:39 [myid: 20]: INFO New election. My id = 20, proposed zxid=0x66000012c7
 Jun 27 14:49:39 [myid: 20]: ERROR Unexpected exception causing shutdown
 Jun 27 14:49:39 java.InterruptedException
 Jun 27 14:49:39 at java.lang.Object.wait(Native Method)
 Jun 27 14:49:39 at org.apache.zookeeper.server.quorum.Leader.getEpochToPropose(Leader.java:892)
 Jun 27 14:49:39 at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:358)
 Jun 27 14:49:39 [myid: 20]: INFO Notification: 1 (message format version), 20 (n.leader), 0x66000012c7 (n.zxid), 0x1a (n.round), LOOKING (n.state), 20 (n.sid), 0x66 (n.peerEpoch) LOOKING (my state)
 Jun 27 14:49:39 [myid: 20]: WARN ******* GOODBYE /10.22.0.2:55268 ********
{panel}
 

After 3 unsuccessfull retries from servers 10,11,12, since the quorum can not be achieved, connection times out and followers started to shut down again, After they are up, another election is triggered and new LEADER is now located on the first node (Server that becomes a new leader is 12):
{panel}
Jun 27 14:50:07 [myid: 12]: ERROR Unexpected exception
 Jun 27 14:50:07 java.net.SocketTimeoutException: connect timed out
 Jun 27 14:50:07 at java.net.PlainSocketImpl.socketConnect(Native Method)
 Jun 27 14:50:07 at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source)
 Jun 27 14:50:07 at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source)
 Jun 27 14:50:07 at java.net.AbstractPlainSocketImpl.connect(Unknown Source)
 Jun 27 14:50:07 at java.net.SocksSocketImpl.connect(Unknown Source)
 Jun 27 14:50:07 at java.net.Socket.connect(Unknown Source)
 Jun 27 14:50:07 at org.apache.zookeeper.server.quorum.Learner.connectToLeader(Learner.java:231)
 Jun 27 14:50:07 at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:71)
 Jun 27 14:50:07 at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:937)
 Jun 27 14:50:07 [myid: 12]: WARN Exception when following the leader
 Jun 27 14:50:07 java.net.SocketTimeoutException: connect timed out
 Jun 27 14:50:07 at java.net.PlainSocketImpl.socketConnect(Native Method)
 Jun 27 14:50:07 at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source)
 Jun 27 14:50:07 at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source)
 Jun 27 14:50:07 at java.net.AbstractPlainSocketImpl.connect(Unknown Source)
 Jun 27 14:50:07 at java.net.SocksSocketImpl.connect(Unknown Source)
 Jun 27 14:50:07 at java.net.Socket.connect(Unknown Source)
 Jun 27 14:50:07 at org.apache.zookeeper.server.quorum.Learner.connectToLeader(Learner.java:231)
 Jun 27 14:50:07 at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:71)
 Jun 27 14:50:07 at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:937)
 Jun 27 14:50:07 [myid: 12]: INFO shutdown called
 Jun 27 14:50:07 java.lang.Exception: shutdown Follower
 Jun 27 14:50:07 at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:166)
 Jun 27 14:50:07 at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:941)
 Jun 27 14:50:07 [myid: 12]: INFO Shutting down
 Jun 27 14:50:07 [myid: 12]: INFO LOOKING
 Jun 27 14:50:07 [myid: 12]: INFO New election. My id = 12, proposed zxid=0x66000012c7
 Jun 27 14:50:08 [myid: 12]: INFO LEADING
{panel}
 

After this, all other Zookeeper servers normally start to follow the new leader and everything starts to work just fine.

 

Could you please help me and answer the following questions:
 - is it expected behavior that Leader shutdowns all other servers (followers) during after its own restart?
 -> if this is expected, could you please explain in which situations we can expect this behavior and why?
 - if there was a notification sent about the new leader (20) to all other servers, why they were still connecting to old leader?
 - do you have any recommendations on how to 'fix' this behavior?

Any help will be highly appreciated.
 Thanks in advance!

Kind regards,
 Lara",[],Bug,ZOOKEEPER-3478,Major,Lara Catipovic,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Leader restart shuts down all the followers,2019-09-04T18:14:30.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",3.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2019-07-28T17:50:16.000+0000,Fangmin Lv,"There is a race condition which might be triggered if the client create session, upgrading the session with ephemeral node, then immediately issued close session request before it's removed from local session tracker.
 
The close session request will be treated as a local session close request since it still exists in the local session tracker, which goes through the ZK pipeline and delete the session from both local and global session tracker. Since the session is not tracked anymore, it will leave the ephemeral nodes there.
 
 ","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3471,Critical,Fangmin Lv,Fixed,2019-10-09T14:49:21.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Potential lock unavailable due to dangling ephemeral nodes left during local session upgrading,2019-10-16T22:12:06.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",3.0
,[],2019-07-23T16:48:36.000+0000,Jan-Philip Gehrcke,"Hey, we explore switching from ZooKeeper 3.4.14 to ZooKeeper 3.5.5 in [https://github.com/dcos/dcos].

DC/OS coordinates ZooKeeper via Exhibitor. We are not changing anything w.r.t. Exhibitor for now, and are hoping that we can use ZooKeeper 3.5.5 as a drop-in replacement for 3.4.14. This seems to work fine when Exhibitor uses a so-called static ensemble where the individual ZooKeeper instances are known a priori.

When Exhibitor however discovers individual ZooKeeper instances (""dynamic"" back-end) then I think we observe a regression where ZooKeeper 3.5.5 can get into the following bad state (often, but not always):
 # three ZooKeeper instances find each other, leader election takes place (*expected*)
 # leader election succeeds: two followers, one leader (*expected*)
 # all three ZK instances respond IAMOK to RUOK  (*expected*)
 # all three ZK instances respond to SRVR (one says ""Mode: leader"", the other two say ""Mode: follower"")  (*expected*)
 # all three ZK instances respond to MNTR and show plausible output (*expected*)
 # *{color:#ff0000}Unexpected:{color}* any ZooKeeper client trying to connect to any of the three nodes observes a ""connection timeout"", whereas notably this is *not* a TCP connect() timeout. The TCP connect() succeeds, but then ZK does not seem to send the expected byte sequence to the TCP connection, and the ZK client waits for it via recv() until it hits a timeout condition. Examples for two different clients:
 ## In Kazoo we specifically hit _Connection time-out: socket time-out during read_
 generated here: [https://github.com/python-zk/kazoo/blob/88b657a0977161f3815657878ba48f82a97a3846/kazoo/protocol/connection.py#L249]
 ## In zkCli we see  _Client session timed out, have not heard from server in 15003ms for sessionid 0x0, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:main-SendThread(localhost:2181))_
 # This state is stable, it will last forever (well, at least for multiple hours and we didn't test longer than that).
 # In our system the ZooKeeper clients are crash-looping. They retry. What I have observed is that while they retry the ZK ensemble accumulates outstanding requests, here shown from MNTR output (emphasis mine): 
 zk_packets_received 2008
 zk_packets_sent 127
 zk_num_alive_connections 18
 zk_outstanding_requests *1880*
 # The leader emits log lines confirming session timeout, example:
 _[myid:3] INFO [SessionTracker:ZooKeeperServer@398] - Expiring session 0x2000642b18f0020, timeout of 10000ms exceeded [myid:3] INFO [SessionTracker:QuorumZooKeeperServer@157] - Submitting global closeSession request for session 0x2000642b18f0020_
 # In this state, restarting any one of the two ZK followers results in the same state (clients don't get data from ZK upon connect).
 # In this state, restarting the ZK leader, and therefore triggering a leader re-election, almost immediately results in all clients being able to connect to all ZK instances successfully.",[],Bug,ZOOKEEPER-3466,Major,Jan-Philip Gehrcke,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"ZK cluster converges, but does not properly handle client connections (new in 3.5.5)",2020-08-26T17:16:29.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",13.0
,"[<JIRA Component: name='other', id='12333125'>]",2019-07-10T16:35:31.000+0000,Chandrasekhar,"We have used the minimal binary installation for Zookeeper and every time after repave the zookeeper keeps crashing with following logs...

I have attached the zookeeper crash logs and deployment information. Is this related to one of the NULL Pointer Issues mentioned in https://issues.apache.org/jira/browse/ZOOKEEPER-3009 ?

We are trying to find the exact issue here so our cloud native platform guys can help us further. Kindly let us know how to turn on debugging further.",[],Bug,ZOOKEEPER-3460,Major,Chandrasekhar,Cannot Reproduce,2019-07-24T14:43:51.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper 3.4.13: keeps crashing after a repave in cloudnative environment.,2019-07-24T14:43:51.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2019-07-07T11:55:26.000+0000,Marzieh,"Hi

I configured Zookeeper with four nodes for my Mesos cluster with Marathon. When I ran Flink Json file on Marathon, it was run without problem. But, when I entered IP of my two slaves, just one slave shew Flink UI and another slave shew this error:

 

Service temporarily unavailable due to an ongoing leader election. Please refresh

I checked ""zookeeper.out"" file and it said that :

 

019-07-07 11:48:43,412 [myid:] - INFO [main:QuorumPeerConfig@136] - Reading configuration from: /home/zookeeper-3.4.14/bin/../conf/zoo.cfg
2019-07-07 11:48:43,421 [myid:] - INFO [main:QuorumPeer$QuorumServer@185] - Resolved hostname: 0.0.0.0 to address: /0.0.0.0
2019-07-07 11:48:43,421 [myid:] - INFO [main:QuorumPeer$QuorumServer@185] - Resolved hostname: 10.32.0.3 to address: /10.32.0.3
2019-07-07 11:48:43,422 [myid:] - INFO [main:QuorumPeer$QuorumServer@185] - Resolved hostname: 10.32.0.2 to address: /10.32.0.2
2019-07-07 11:48:43,422 [myid:] - INFO [main:QuorumPeer$QuorumServer@185] - Resolved hostname: 10.32.0.5 to address: /10.32.0.5
2019-07-07 11:48:43,422 [myid:] - WARN [main:QuorumPeerConfig@354] - Non-optimial configuration, consider an odd number of servers.
2019-07-07 11:48:43,422 [myid:] - INFO [main:QuorumPeerConfig@398] - Defaulting to majority quorums
2019-07-07 11:48:43,425 [myid:3] - INFO [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3
2019-07-07 11:48:43,425 [myid:3] - INFO [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 0
2019-07-07 11:48:43,425 [myid:3] - INFO [main:DatadirCleanupManager@101] - Purge task is not scheduled.
2019-07-07 11:48:43,432 [myid:3] - INFO [main:QuorumPeerMain@130] - Starting quorum peer
2019-07-07 11:48:43,437 [myid:3] - INFO [main:ServerCnxnFactory@117] - Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connect$
2019-07-07 11:48:43,439 [myid:3] - INFO [main:NIOServerCnxnFactory@89] - binding to port 0.0.0.0/0.0.0.0:2181
2019-07-07 11:48:43,440 [myid:3] - ERROR [main:QuorumPeerMain@92] - Unexpected exception, exiting abnormally
java.net.BindException: Address already in use
 at sun.nio.ch.Net.bind0(Native Method)
 at sun.nio.ch.Net.bind(Net.java:433)
 at sun.nio.ch.Net.bind(Net.java:425)
 at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
 at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
 at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:67)
 at org.apache.zookeeper.server.NIOServerCnxnFactory.configure(NIOServerCnxnFactory.java:90)
 at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:133)
 at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:114)
 at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:81)

 

I searched a lot and could not find the solution.","[<JIRA Version: name='3.4.14', id='12343587'>]",Bug,ZOOKEEPER-3456,Major,Marzieh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Service temporarily unavailable due to an ongoing leader election. Please refresh,2019-07-17T05:32:22.000+0000,[],2.0
,"[<JIRA Component: name='server', id='12312382'>]",2019-06-25T20:29:29.000+0000,Jonathan Halterman,"While debugging some unexpected ""ACL not available for long"" exceptions we were seeing, I noticed that ReferenceCountedACLCache does not mark aclIndex as volatile, which it should since it appears to be read from multiple threads. This may or may not be the cause of the behavior we're seeing, but should be fixed regardless.",[],Bug,ZOOKEEPER-3445,Critical,Jonathan Halterman,Invalid,2019-06-28T15:16:46.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Concurrency issue in ReferenceCountedACLCache,2019-06-28T15:18:00.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.4.14', id='12343587'>]",2.0
,"[<JIRA Component: name='java client', id='12312381'>]",2019-06-25T08:49:10.000+0000,Bobi Traykov,"When attepmting to start ZooKeeper using the *ZKServer.cmd* batch file, I receive the following error:
{noformat}
D:\apache-zookeeper-3.5.5-bin\bin>call ""D:\jre1.8.0_212""\bin\java ""-Dzookeeper.log.dir=D:\apache-zookeeper-3.5.5-bin\bin\..\logs"" ""-Dzookeeper.root.logger=ALL1,CONSOLE"" ""-Dzookeeper.log.file=zookeeper-ironman-server-TF-AMIR-BASTION.log"" ""-XX:+HeapDumpOnOutOfMemoryError"" ""-XX:OnOutOfMemoryError=cmd /c taskkill /pid %%p /t /f"" -cp ""D:\apache-zookeeper-3.5.5-bin\bin\..\build\classes;D:\apache-zookeeper-3.5.5-bin\bin\..\build\lib\*;D:\apache-zookeeper-3.5.5-bin\bin\..\*;D:\apache-zookeeper-3.5.5-bin\bin\..\lib\*;D:\apache-zookeeper-3.5.5-bin\bin\..\conf"" org.apache.zookeeper.server.quorum.QuorumPeerMain ""D:\apache-zookeeper-3.5.5-bin\bin\..\conf\zoo.cfg""
2019-06-25 08:40:41,391 [myid:] - INFO [main:QuorumPeerConfig@133] - Reading configuration from: D:\apache-zookeeper-3.5.5-bin\bin\..\conf\zoo.cfg
2019-06-25 08:40:41,391 [myid:] - ERROR [main:QuorumPeerMain@89] - Invalid config, exiting abnormally
org.apache.zookeeper.server.quorum.QuorumPeerConfig$ConfigException: Error processing D:\apache-zookeeper-3.5.5-bin\bin\..\conf\zoo.cfg
at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parse(QuorumPeerConfig.java:154)
at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:113)
at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:82)
Caused by: java.lang.IllegalArgumentException: dataDir is not set
at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parseProperties(QuorumPeerConfig.java:368)
at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parse(QuorumPeerConfig.java:150)
... 2 more
Invalid config, exiting abnormally{noformat}
 

 

The *zoo.cfg* is a copy-paste from the *zoo_sample.cfg* - the *dataDir* parameter is pointing to E:/zookeeper - an existing empty directory.

The previous two stable releases start without any issues:

zookeeper-3.4.10.tar.gz
zookeeper-3.4.14.tar.gz

 

Last, but not least, the 3.5.5 release works fine for me on Windows 10 (fully updated), Windows Server 2008 R2 and Windows Server 2012 R2.",[],Bug,ZOOKEEPER-3444,Major,Bobi Traykov,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Latest stable version will not start on Windows Server 2016 (DataCenter edition) and Java 8,2019-06-26T10:40:29.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",2.0
,[],2019-06-24T17:32:03.000+0000,Patrick D. Hunt,"The OWASP job is failing due to a medium priority jackson databind issue.

http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2019-12814

we should upgrade the dependency version - I looked into the issue, should be straightforward, however the new dependency (2.9.9.1) is not yet available from the upstream. Once it is we should upgrade.",[],Bug,ZOOKEEPER-3442,Blocker,Patrick D. Hunt,Duplicate,2019-06-24T18:52:05.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,OWASP jenkins failing due to jackson databind CVE published,2019-09-11T20:33:44.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.4.14', id='12343587'>]",1.0
Enrico Olivelli,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='documentation', id='12312422'>]",2019-06-22T22:37:30.000+0000,Enrico Olivelli,"I see this error on Jenkins as we are missing the exclusion for the images of the docs.

{code:java}
Unapproved licenses:

  /home/jenkins/jenkins-slave/workspace/zookeeper-master-maven/zookeeper-docs/src/main/resources/markdown/images/state_dia.dia
{code}

We should also add this check to the precommit job on Travis (this will be part of the commit) and on CI (this is a manual configuration, to be done after fixing this issue)","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.6', id='12345243'>]",Bug,ZOOKEEPER-3440,Critical,Enrico Olivelli,Fixed,2019-06-28T22:38:39.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Fix Apache RAT check by excluding binary files (images),2019-10-16T18:58:54.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",3.0
,"[<JIRA Component: name='tests', id='12312427'>]",2019-06-21T09:46:51.000+0000,Ling Mao,"[https://builds.apache.org/job/PreCommit-ZOOKEEPER-github-pr-build-maven/org.apache.zookeeper$zookeeper/844/testReport/junit/org.apache.zookeeper.server/PrepRequestProcessorMetricsTest/testPrepRequestProcessorMetrics/]
{code:java}
Error Message
expected:<5> but was:<4>
Stacktrace
java.lang.AssertionError: expected:<5> but was:<4>
	at org.apache.zookeeper.server.PrepRequestProcessorMetricsTest.testPrepRequestProcessorMetrics(PrepRequestProcessorMetricsTest.java:146)

Standard Output
2019-06-21 09:09:37,915 [myid:] - INFO  [main:ZKTestCase$1@60] - STARTING testPrepRequestProcessorMetrics
2019-06-21 09:09:37,917 [myid:] - INFO  [main:JUnit4ZKTestRunner$LoggedInvokeMethod@78] - RUNNING TEST METHOD testPrepRequestProcessorMetrics
2019-06-21 09:09:37,951 [myid:] - ERROR [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@1002] - Failed to process sessionid:0x1 type:setData cxid:0x0 zxid:0x1 txntype:5 reqpath:n/a
java.lang.NullPointerException
	at org.apache.zookeeper.server.PrepRequestProcessor.pRequest2Txn(PrepRequestProcessor.java:521)
	at org.apache.zookeeper.server.PrepRequestProcessor.pRequest(PrepRequestProcessor.java:872)
	at org.apache.zookeeper.server.PrepRequestProcessor.run(PrepRequestProcessor.java:156)
2019-06-21 09:09:37,952 [myid:] - ERROR [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@1015] - Dumping request buffer: 0x00042f666f6f0000ffffffff
2019-06-21 09:09:37,959 [myid:] - INFO  [main:JUnit4ZKTestRunner$LoggedInvokeMethod@99] - TEST METHOD FAILED testPrepRequestProcessorMetrics
java.lang.AssertionError: expected:<5> but was:<4>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.zookeeper.server.PrepRequestProcessorMetricsTest.testPrepRequestProcessorMetrics(PrepRequestProcessorMetricsTest.java:146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:80)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2019-06-21 09:09:37,960 [myid:] - INFO  [main:ZKTestCase$1@75] - FAILED testPrepRequestProcessorMetrics
java.lang.AssertionError: expected:<5> but was:<4>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.zookeeper.server.PrepRequestProcessorMetricsTest.testPrepRequestProcessorMetrics(PrepRequestProcessorMetricsTest.java:146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:80)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2019-06-21 09:09:37,960 [myid:] - INFO  [main:ZKTestCase$1@65] - FINISHED testPrepRequestProcessorMetrics
{code}",[],Bug,ZOOKEEPER-3438,Minor,Ling Mao,Not A Problem,2019-06-22T10:57:55.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Flaky test:org.apache.zookeeper.server.PrepRequestProcessorMetricsTest.testPrepRequestProcessorMetrics,2019-12-19T22:59:51.000+0000,[],1.0
,[],2019-06-20T14:43:14.000+0000,prashant,"Hi

We use 3.5.1-alpha version.

We are seeing session expiry issue in VM set up.

This is running in replicated more (two servers + node mastership as one vote for quorum).

we see client session expired after session timeout (of 10 sec).

This connection was to local zk server. session timeout is 10 sec.

This session got established at 17:40:18 and ZK server expired this at 17:40:57, after 39 seconds of establishment.

in between this time, i see few errors and warnings in zookeeper server logs (as shown below).

I see below errors/warning in between this time before session expiry.

This issue is not very easy to replicate , so far we have seen only twice.

Could you please help me identify root cause and let me know if this is fixed in later release ? Thanks, Prashant

Logs are in below mail:

 

[https://mail-archives.apache.org/mod_mbox/zookeeper-user/201906.mbox/browser]",[],Bug,ZOOKEEPER-3435,Major,prashant,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,client session expired after timeout after errors and warning logs in zk server logs,2019-06-20T14:43:14.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2019-06-19T01:07:43.000+0000,Patrick D. Hunt,zkpython is not building after the migration to maven directory structure.,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.6', id='12345243'>]",Bug,ZOOKEEPER-3433,Major,Patrick D. Hunt,Fixed,2019-06-25T13:51:08.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkpython build broken after maven migration,2019-10-16T18:59:19.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.4.14', id='12343587'>]",1.0
Damien Diederen,"[<JIRA Component: name='c client', id='12312380'>]",2019-06-11T19:11:54.000+0000,Suhas Dantkale,"/* returns:

 * -1 if recv call failed,

 * 0 if recv would block,

 * 1 if success

 */

static int recv_buffer(zhandle_t *zh, buffer_list_t *buff)

{

  int off = buff->curr_offset;

  int rc = 0;

[................]

 if (buff == &zh->primer_buffer && rc == buff->len - 1) ++rc; <====== Handshake prematurely complete.





On non-blocking socket, it's possible that socket has exactly ""buff->len - 1"" bytes to read.
Because of the above line, the Handshake is prematurely completed.
What this can lead to is:
There will be one outstanding byte left on the socket and it might go as part of next message which could get corrupted.

I think this can lead to ZRUNTIMEINCONSISTENCY issues later.","[<JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3426,Blocker,Suhas Dantkale,Fixed,2021-01-13T08:29:16.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZK prime_connection(the Handshake) can complete without reading all the payload.,2021-03-28T08:55:20.000+0000,[],2.0
,"[<JIRA Component: name='build', id='12312383'>]",2019-06-11T09:21:28.000+0000,Wei Xin,,[],Bug,ZOOKEEPER-3424,Trivial,Wei Xin,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,download page file package broken,2019-08-27T13:45:01.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",2.0
,"[<JIRA Component: name='c client', id='12312380'>]",2019-06-08T01:09:54.000+0000,Suhas Dantkale,"With newer ZK C Client (3.5.*) and older ZK server(3.4.*), recv_buffer() could potentially return 0 continuously on non-blocking socket.

Following in the recv_buffer() snippet:-
Here, should  the check be:
if (buff == &zh->primer_buffer && buff->curr_offset + rc == buff->len + sizeof(buff->len)  - 1) ++rc;
instead of
if (buff == &zh->primer_buffer && rc == buff->len - 1) ++rc;

snippet :-

  if (buff->buffer) {
        /* want off to now represent the offset into the buffer */
        off -= sizeof(buff->len);

        rc = recv(zh->fd, buff->buffer+off, buff->len-off, 0);

        /* dirty hack to make new client work against old server
         * old server sends 40 bytes to finish connection handshake,
         * while we're expecting 41 (1 byte for read-only mode data) */
      if (buff == &zh->primer_buffer && rc == buff->len - 1) ++rc;  <====== Problem Line(?)

        switch(rc) {
        case 0:
            errno = EHOSTDOWN;
        case -1:
#ifdef _WIN32
            if (WSAGetLastError() == WSAEWOULDBLOCK) {
#else
            if (errno == EAGAIN) {
#endif
                break;
            }
            return -1;
        default:
            buff->curr_offset += rc;
        }
    }
    return buff->curr_offset == buff->len + sizeof(buff->len);


Probably the given code assumes that recv() operation will read in one go.
But on non-blocking socket, that assumption doesn't hold true.
",[],Bug,ZOOKEEPER-3420,Major,Suhas Dantkale,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"With newer ZK C Client and older ZK server, recv_buffer() could potentially return 0 continuously on non-blocking socket",2019-08-02T12:14:14.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",1.0
Rabi Kumar K C,"[<JIRA Component: name='java client', id='12312381'>]",2019-06-05T09:28:14.000+0000,Ling Mao,"[zk: 127.0.0.1:2180(CONNECTED) 0] sync /c1
Sync is OK
[zk: 127.0.0.1:2180(CONNECTED) 1] sync /c1dsafasdfasdfadsfasd
Node does not exist: /c1dsafasdfasdfadsfasd",[],Bug,ZOOKEEPER-3414,Minor,Ling Mao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,sync api should throw NoNodeException when syncing a path which is not exist,2022-02-03T08:50:17.000+0000,[],3.0
,[],2019-06-03T11:39:21.000+0000,Mykola Rak,"I download files from EU mirror:
{code:java}
mrak@mrak:~/tmp/1$ ls
apache-zookeeper-3.5.5.tar.gz.asc KEYS zookeeper-3.5.5.tar.gz

{code}

try to verify signature:


{code:java}
mrak@mrak:~/tmp/1$ gpg --import ./KEYS
gpg: key E22A746A68E327C1: public key ""Patrick Hunt (ZooKeeper release signing key) <phunt@apache.org>"" imported
gpg: key 7C9476266E1CC7A4: public key ""Benjamin Reed (CODE SIGNING KEY) <breed@apache.org>"" imported
gpg: key 0DFF492D8EE2F25C: public key ""Mahadev Konar (CODE SIGNING KEY) <mahadev@apache.org>"" imported
gpg: key 93FB0254D2C80E32: public key ""Flavio Junqueira (CODE SIGNING KEY) <fpj@apache.org>"" imported
gpg: key C2C0FDE0820F225C: public key ""Michi Mutsuzaki (CODE SIGNING KEY) <michim@apache.org>"" imported
gpg: key BE3B6B9392BC2F2B: public key ""Raul Gutierrez Segales <rgs@apache.org>"" imported
gpg: key A1350C22ADAFD097: public key ""Chris Nauroth (CODE SIGNING KEY) <cnauroth@apache.org>"" imported
gpg: key F5CECB3CB5E9BD2D: ""Rakesh Radhakrishnan (CODE SIGNING KEY) <rakeshr@apache.org>"" not changed
gpg: key 59147497767E7473: ""Michael Han (CODE SIGNING KEY) <hanm@apache.org>"" not changed
gpg: key 15072ED241CF31A9: public key ""Abraham Fine (CODE SIGNING KEY) <afine@apache.org>"" imported
gpg: key BDB2011E173C31A2: 4 signatures not checked due to missing keys
gpg: key BDB2011E173C31A2: ""Abraham Fine <abe@abrahamfine.com>"" 3 new signatures
gpg: key FFE35B7F15DFA1BA: ""Andor Molnar <andor@apache.org>"" not changed
gpg: Total number processed: 12
gpg: imported: 8
gpg: unchanged: 3
gpg: new signatures: 3
gpg: no ultimately trusted keys found

{code}
verifying was failed with error BAD signature:
 
{code:java}
mrak@mrak:~/tmp/1$ gpg --verify ./apache-zookeeper-3.5.5.tar.gz.asc ./zookeeper-3.5.5.tar.gz
gpg: Signature made Fri 03 May 2019 02:08:41 PM CEST
gpg: using RSA key FFE35B7F15DFA1BA
gpg: BAD signature from ""Andor Molnar <andor@apache.org>"" [unknown]

{code}
 

 

 ",[],Bug,ZOOKEEPER-3412,Major,Mykola Rak,Cannot Reproduce,2019-06-06T13:57:32.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Bad signature of tarball 3.5.5 on EU mirror,2019-06-06T13:57:32.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",1.0
Ling Mao,"[<JIRA Component: name='scripts', id='12312384'>]",2019-06-01T13:33:26.000+0000,Ling Mao,"[zk: 127.0.0.1:2180(CONNECTED) 26] create -t 500 /ttl_node

19-5-30 下午06时10分50秒 session 0x10007a75c0c0000 cxid 0x0 zxid 0x6 createSession 30000
Exception in thread ""main"" java.lang.NullPointerException
 at java.lang.String.<init>(String.java:566)
 at org.apache.zookeeper.server.persistence.TxnLogToolkit.getDataStrFromTxn(TxnLogToolkit.java:316)
 at org.apache.zookeeper.server.persistence.TxnLogToolkit.printTxn(TxnLogToolkit.java:272)
 at org.apache.zookeeper.server.persistence.TxnLogToolkit.printTxn(TxnLogToolkit.java:266)
 at org.apache.zookeeper.server.persistence.TxnLogToolkit.dump(TxnLogToolkit.java:217)
 at org.apache.zookeeper.server.persistence.TxnLogToolkit.main(TxnLogToolkit.java:116)

t*xnData.append(createTTLTxn.getPath() + "","" + new String(createTTLTxn.getData()))*
 *.append("","" + createTTLTxn.getAcl() + "","" + createTTLTxn.getParentCVersion())*
 *.append("","" + createTTLTxn.getTtl());*",[],Bug,ZOOKEEPER-3410,Minor,Ling Mao,Resolved,2019-07-12T02:04:18.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,./zkTxnLogToolkit.sh will throw the NPE and stop the process of formatting txn logs due to the data's content is null,2019-07-12T02:04:18.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",2.0
Patrick D. Hunt,[],2019-05-28T21:21:40.000+0000,Patrick D. Hunt,"Owasp job is flagging jackson-databind for update:

CVE-2019-12086	CWE-200 Information Exposure	Medium(5.0)	jackson-databind-2.9.8.jar","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.6', id='12345243'>]",Bug,ZOOKEEPER-3405,Critical,Patrick D. Hunt,Fixed,2019-05-30T13:56:15.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,owasp flagging jackson-databind,2019-10-16T18:58:57.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",1.0
,"[<JIRA Component: name='tests', id='12312427'>]",2019-05-27T20:48:20.000+0000,Andor Molnar,"I've seen a lot of test timeout errors with QuorumSSL tests since I upgraded master to BouncyCastle 1.61 due to a Java 9 warning. The warning has been reported by [~eolivelli] which we tried to solve by the upgrade, but the warning message is still present so I don't see any harm in downgrading to the previous version. 

The timeout errors are very frequent with recent Java versions (11+) and quite rare with Java 8.

I think it's worth a try to downgrade and see if tests will be in a better shape.","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.6', id='12345243'>]",Bug,ZOOKEEPER-3404,Major,Andor Molnar,Fixed,2019-05-28T14:08:13.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,BouncyCastle upgrade to 1.61 might cause flaky test issues,2019-10-16T18:59:14.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",2.0
,[],2019-05-27T10:32:26.000+0000,shriram,"Downloaded Zookeeper version 3.5.5 for the security alert *CVE-2019-0201*. Bundle is not OSGi complaint but version 3.4.14 released for the CVE is OSGi complaint.

 

*CVE reference:*

[https://seclists.org/oss-sec/2019/q2/119]",[],Bug,ZOOKEEPER-3403,Blocker,shriram,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper bundle 3.5.5 is Non OSGi complaint,2021-08-10T05:39:47.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",2.0
Michael Han,"[<JIRA Component: name='server', id='12312382'>]",2019-05-22T01:42:19.000+0000,Michael Han,"Recently we have moved some of our production clusters to the top of the trunk. One issue we found is a performance regression on read and write latency on the clusters where the quorum is also serving traffic. The average read latency increased by 50x, p99 read latency increased by 300x. 

The root cause is a log statement introduced in ZOOKEEPER-3177 (PR711), where we added a LOG.info statement in getGlobalOutstandingLimit. getGlobalOutstandingLimit is on the critical code path for request processing and for each request, it will be called twice (one at processing the packet, one at finalizing the request response). This not only degrades performance of the server, but also bloated the log file, when the QPS of a server is high.

This only impacts clusters when the quorum (leader + follower) is serving traffic. For clusters where only observers are serving traffic no impact is observed.

 

 ","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3399,Major,Michael Han,Fixed,2019-05-25T17:46:55.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Remove logging in getGlobalOutstandingLimit for optimal performance.,2019-05-25T20:27:57.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",3.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2019-05-20T17:43:36.000+0000,Andor Molnar,"The generated API docs are now located in 2 different folders:
 * zookeeper-server
 * zookeeper-jute

But the navbar is still pointing to api/index.html which link is broken and needs to be fixed.",[],Bug,ZOOKEEPER-3397,Major,Andor Molnar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,API docs are incorrectly linked in release artifacts,2019-05-20T17:43:56.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",1.0
,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='server', id='12312382'>]",2019-05-14T01:41:44.000+0000,Jiafu Jiang,"Say we have 3 nodes: zk1, zk2, and zk3, zk3 is the leader.

If the file system of the ZooKeeper data directory of the leader is read-only due to some hardware error, the leader will exit and begin a new election.

But the election will keep looping because the new leader may be zk3 again, but zk3 will fail to write epoch to disk due to read-only file system.

 

Since we have 3 nodes, if only one of them is in problem, should the ZooKeeper cluster be available? If the answer is yes, then we ought to fix this problem.",[],Bug,ZOOKEEPER-3393,Major,Jiafu Jiang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Read-only file system may make the whole ZooKeeper cluster to be unavailable.,2019-05-14T01:41:44.000+0000,"[<JIRA Version: name='3.4.12', id='12342040'>, <JIRA Version: name='3.4.14', id='12343587'>]",1.0
,[],2019-05-13T13:53:04.000+0000,Edgar Pascoal,"Not possible to obtain the release notes in PDF format (broken link):

[https://zookeeper.apache.org/doc/r3.5.4-beta/releasenotes.pdf]

!image-2019-05-13-14-51-18-496.png|width=429,height=125!

 

!image-2019-05-13-14-49-14-464.png|width=528,height=267!",[],Bug,ZOOKEEPER-3390,Trivial,Edgar Pascoal,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Broken Link - Release Notes (PDF),2019-05-13T13:53:04.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>]",1.0
,[],2019-05-13T10:37:47.000+0000,Jiri Ondrusek,"Install Zookeeper and Curator (4.1+) in OSGi.
Some export packages are missing.
Problem could is happening on bot 3.4.x and 3.5.x.","[<JIRA Version: name='3.4.15', id='12344988'>]",Bug,ZOOKEEPER-3389,Minor,Jiri Ondrusek,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper does not export all required packages in OSGi (needed for curator),2019-06-26T11:42:46.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.5', id='12343268'>]",3.0
Jiahongchao,"[<JIRA Component: name='documentation', id='12312422'>]",2019-04-28T09:19:10.000+0000,Jiahongchao,"In website, ""Single System Image"" is ""A client will see the same view of the service regardless of the server that it connects to.""

I want to change it to ""Once connected, a client will see the same view of the service even if it switchs to another server""

Because the old one is a little misleading, if cluster has a outdated follower and a normal follower, I not think a client will see the same view of the service regardless of the server that it connects to at its first connection.","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3373,Minor,Jiahongchao,Fixed,2019-08-05T23:51:58.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"need change description for ""Single System Image"" guarantee in document",2020-11-12T15:56:01.000+0000,"[<JIRA Version: name='3.4.14', id='12343587'>]",2.0
,[],2019-04-17T10:43:36.000+0000,Stig Rohde Døssing,"Pulling in Zookeeper 3.4.14 in a Maven build results in spotbugs-annotations also being pulled in as a dependency.

{quote}
[INFO] \- org.apache.zookeeper:zookeeper:jar:3.4.14:compile
[INFO]    +- com.github.spotbugs:spotbugs-annotations:jar:3.1.9:compile
{quote}

Since spotbugs-annotations is under LGPL license, it would ideally be used only during the build, and not be pulled in when users depend on Zookeeper.
 ",[],Bug,ZOOKEEPER-3367,Major,Stig Rohde Døssing,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Zookeeper 3.4.14 Maven jar pulls in spotbugs-annotations, which is under LGPL license",2019-04-17T10:43:36.000+0000,"[<JIRA Version: name='3.4.14', id='12343587'>]",2.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2019-04-09T16:20:12.000+0000,Fangmin Lv,"The current implementation of Netty enable/disable recv logic may cause the direct buffer OOM because we may enable read a large chunk of packets and disabled again after consuming a single ZK request. We have seen this problem on prod occasionally.
 
Need a more advanced flow control in Netty instead of using AUTO_READ. Have improved it internally by enable/disable recv based on the queuedBuffer size, will upstream this soon.
 
With this implementation, the max Netty queued buffer size (direct memory usage) will be 2 * recv_buffer size. It's not the per message size because in epoll ET mode it will try to read until the socket is empty, and because of SslHandler will trigger another read when it's not a full encrypt packet and haven't issued any decrypt message.","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3356,Major,Fangmin Lv,Fixed,2019-07-23T13:56:13.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Request throttling in Netty is not working as expected and could cause direct buffer OOM issue ,2019-07-24T19:45:05.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",3.0
Andrew Kyle Purtell,[],2019-03-27T20:44:04.000+0000,Andrew Kyle Purtell,"Maven assembly plugin configuration must specify tarLongFileMode of ""posix"", not ""gnu"".

Otherwise if the user or group id is too large the build will fail. For example:
{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:3.1.0:single
(source-package) on project zookeeper: Execution source-package of goal
org.apache.maven.plugins:maven-assembly-plugin:3.1.0:single failed: user id '1754762210'
is too big ( > 2097151 ). -> [Help 1]
{noformat}
A very common problem, many other projects here have had to fix this. ",[],Bug,ZOOKEEPER-3337,Major,Andrew Kyle Purtell,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Maven build failed with user or group id is too big ,2019-04-09T09:47:00.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",5.0
,"[<JIRA Component: name='leaderElection', id='12312378'>]",2019-03-27T12:36:23.000+0000,Simin Oraee,"I am working on a testing tool for distributed systems. I tested Zookeeper, enforcing different possible orderings of events. I encountered some inconsistencies in the election of the leader. Here are the logs of 3 completed executions.

I am wondering if these behaviors are expected or not.

1) More than one node consider themselves leaders:
NodeCrashEvent\{id=1, nodeId=0}
NodeStartEvent\{id=7, nodeId=0}
MessageEvent\{id=8, predecessors=[7], from=0, to=0, leader=0, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=9, predecessors=[8, 7], from=0, to=1, leader=0, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=10, predecessors=[9, 7], from=0, to=2, leader=0, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=5, predecessors=[], from=1, to=0, leader=1, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=12, predecessors=[5, 10, 7], from=0, to=0, leader=1, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=13, predecessors=[12, 5, 7], from=0, to=1, leader=1, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=14, predecessors=[5, 13, 7], from=0, to=2, leader=1, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=11, predecessors=[5], from=1, to=1, leader=1, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=15, predecessors=[11], from=1, to=2, leader=1, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=6, predecessors=[], from=2, to=0, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
Node 1 state: LEADING
Node 1 final vote: Vote\{leader=1, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=17, predecessors=[6, 14, 7], from=0, to=0, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=18, predecessors=[17, 6, 7], from=0, to=1, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=19, predecessors=[18, 6, 7], from=0, to=2, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=20, predecessors=[18], from=1, to=0, leader=1, state=LEADING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=16, predecessors=[6], from=2, to=1, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=22, predecessors=[16, 20], from=1, to=2, leader=1, state=LEADING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=21, predecessors=[16], from=2, to=2, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
Node 0 state: FOLLOWING
Node 0 final vote: Vote\{leader=2, zxid=0, electionEpoch=1, peerEpoch=0}
Node 2 state: LEADING
Node 2 final vote: Vote\{leader=2, zxid=0, electionEpoch=1, peerEpoch=0}

2) There are some nodes that follow nodes other than the leaders:
NodeCrashEvent\{id=1, nodeId=0}
NodeStartEvent\{id=7, nodeId=0}
MessageEvent\{id=8, predecessors=[7], from=0, to=0, leader=0, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=9, predecessors=[8, 7], from=0, to=1, leader=0, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=10, predecessors=[9, 7], from=0, to=2, leader=0, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=5, predecessors=[], from=1, to=0, leader=1, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=12, predecessors=[5, 10, 7], from=0, to=0, leader=1, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=13, predecessors=[12, 5, 7], from=0, to=1, leader=1, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=14, predecessors=[5, 13, 7], from=0, to=2, leader=1, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
Node 0 state: FOLLOWING
Node 0 final vote: Vote\{leader=1, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=11, predecessors=[5], from=1, to=1, leader=1, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=15, predecessors=[11], from=1, to=2, leader=1, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=6, predecessors=[], from=2, to=0, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=17, predecessors=[6, 7], from=0, to=2, leader=1, state=FOLLOWING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=16, predecessors=[6], from=2, to=1, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=19, predecessors=[16, 15], from=1, to=0, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=20, predecessors=[16, 19], from=1, to=1, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=22, predecessors=[16, 20], from=1, to=2, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=21, predecessors=[17, 19, 7], from=0, to=1, leader=1, state=FOLLOWING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=18, predecessors=[16], from=2, to=2, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
Node 1 state: FOLLOWING
Node 1 final vote: Vote\{leader=2, zxid=0, electionEpoch=1, peerEpoch=0}
Node 2 state: LEADING
Node 2 final vote: Vote\{leader=2, zxid=0, electionEpoch=1, peerEpoch=0}

3) There are some nodes that neither following nor leading
NodeCrashEvent\{id=3, nodeId=2}
NodeStartEvent\{id=7, nodeId=2}
MessageEvent\{id=8, predecessors=[7], from=2, to=0, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=9, predecessors=[8, 7], from=2, to=1, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=10, predecessors=[9, 7], from=2, to=2, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=5, predecessors=[], from=1, to=0, leader=1, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=11, predecessors=[5], from=1, to=1, leader=1, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=12, predecessors=[11], from=1, to=2, leader=1, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=13, predecessors=[12, 9], from=1, to=0, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=14, predecessors=[9, 13], from=1, to=1, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=15, predecessors=[9, 14], from=1, to=2, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=4, predecessors=[], from=0, to=0, leader=0, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=16, predecessors=[4], from=0, to=1, leader=0, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=17, predecessors=[16], from=0, to=2, leader=0, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=18, predecessors=[8, 17], from=0, to=0, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=19, predecessors=[8, 18], from=0, to=1, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
Node 2 state: LEADING
Node 2 final vote: Vote\{leader=2, zxid=0, electionEpoch=1, peerEpoch=0}
Node 1 state: FOLLOWING
Node 1 final vote: Vote\{leader=2, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=20, predecessors=[8, 19], from=0, to=2, leader=2, state=LOOKING, zxid=0, electionEpoch=1, peerEpoch=0}
MessageEvent\{id=21, predecessors=[20, 7], from=2, to=0, leader=2, state=LEADING, zxid=0, electionEpoch=1, peerEpoch=1}",[],Bug,ZOOKEEPER-3336,Major,Simin Oraee,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Leader election terminated, two leaders or not following leader or not having state",2019-05-10T10:00:23.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",3.0
,"[<JIRA Component: name='scripts', id='12312384'>]",2019-03-26T07:58:55.000+0000,Ling Mao,"in the zkCli.sh cannot create a node,and no exceptions have been thrown, but use the java client api can create that path: ""/configplatform/12"", I saw the same issue previously.

[zk: (CONNECTED) 2] create /configplatform/12
 [zk: (CONNECTED) 3] create /configplatform/12
 [zk: (CONNECTED) 4] create /configplatform/12
 [zk: (CONNECTED) 5] create /configplatform/12
 [zk: (CONNECTED) 6] create /configplatform/12
 [zk: (CONNECTED) 7] ls /configplatform
 [11, 13, 3, 4, 5, 6, 7, 8, 9, 10]
 [zk: (CONNECTED) 8] delete /configplatform/12
 Node does not exist: /configplatform/12
 [zk: (CONNECTED) 9] get /configplatform/12
 Node does not exist: /configplatform/12",[],Bug,ZOOKEEPER-3334,Major,Ling Mao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,./zkCli.sh cannot create the node,2019-12-04T00:28:35.000+0000,[],3.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2019-03-22T12:19:54.000+0000,wenshuai.zhang,navigation xxx.html file not found. fix to xxx.md,[],Bug,ZOOKEEPER-3330,Major,wenshuai.zhang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,[zookeeper-docs]:the index.md's navigation is broken,2019-03-23T09:46:03.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>]",2.0
Ling Mao,[],2019-03-18T01:52:49.000+0000,Ling Mao,"look at my zoo.cfg which don't have the clientPort value when using the reconfig.
*cat ../conf/zoo.cfg*
reconfigEnabled=true
dataDir=../../zkdata2
syncLimit=5
dataLogDir=../../zkdataLog2
initLimit=10
tickTime=2000
dynamicConfigFile=/data/software/zookeeper/zookeeper-test2/conf/zoo.cfg.dynamic.1f00000000

but look at the cmd:""./zkServer.sh status""，it needs this clientPort value 
STAT=`""$JAVA"" ""-Dzookeeper.log.dir=${ZOO_LOG_DIR}"" ""-Dzookeeper.root.logger=${ZOO_LOG4J_PROP}"" ""-Dzookeeper.log.file=${ZOO_LOG_FILE}"" \
 -cp ""$CLASSPATH"" $JVMFLAGS org.apache.zookeeper.client.FourLetterWordMain \
 $clientPortAddress $clientPort srvr 2> /dev/null \

otw, ./zkServer.sh status will fail.",[],Bug,ZOOKEEPER-3322,Major,Ling Mao,Implemented,2019-06-05T06:04:24.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,./zkServer.sh status failed when reconfig don't write the clientPort into the zoo.cfg,2019-06-05T06:04:24.000+0000,[],1.0
Igor Skokov,"[<JIRA Component: name='leaderElection', id='12312378'>]",2019-03-17T08:21:29.000+0000,Igor Skokov,"When trying to run Zookeeper 3.5.4 cluster on Kubernetes, I found out that in some circumstances Zookeeper node stop listening on leader election port. This cause unavailability of ZK cluster. 
Zookeeper deployed  as StatefulSet in term of Kubernetes and has following dynamic configuration:

{code:java}
zookeeper-0.zookeeper:2182:2183:participant;2181
zookeeper-1.zookeeper:2182:2183:participant;2181
zookeeper-2.zookeeper:2182:2183:participant;2181
{code}


Bind address contains DNS name which generated by Kubernetes for each StatefulSet pod.
These DNS names will become resolvable after container start, but with some delay. That delay cause stopping of leader election port listener in QuorumCnxManager.Listener class.
Error happens in QuorumCnxManager.Listener ""run"" method, it tries to bind leader election port to hostname which not resolvable at this moment. Retry count is hard-coded and equals to 3(with backoff of 1 sec). 

Zookeeper server log contains following errors:

{code:java}
2019-03-17 07:56:04,844 [myid:1] - WARN  [QuorumPeer[myid=1](plain=/0.0.0.0:2181)(secure=disabled):QuorumPeer@1230] - Unexpected exception
java.net.SocketException: Unresolved address
	at java.base/java.net.ServerSocket.bind(ServerSocket.java:374)
	at java.base/java.net.ServerSocket.bind(ServerSocket.java:335)
	at org.apache.zookeeper.server.quorum.Leader.<init>(Leader.java:241)
	at org.apache.zookeeper.server.quorum.QuorumPeer.makeLeader(QuorumPeer.java:1023)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1226)
2019-03-17 07:56:04,844 [myid:1] - WARN  [QuorumPeer[myid=1](plain=/0.0.0.0:2181)(secure=disabled):QuorumPeer@1261] - PeerState set to LOOKING
2019-03-17 07:56:04,845 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0.0.0.0:2181)(secure=disabled):QuorumPeer@1136] - LOOKING
2019-03-17 07:56:04,845 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0.0.0.0:2181)(secure=disabled):FastLeaderElection@893] - New election. My id =  1, proposed zxid=0x0
2019-03-17 07:56:04,846 [myid:1] - INFO  [WorkerReceiver[myid=1]:FastLeaderElection@687] - Notification: 2 (message format version), 1 (n.leader), 0x0 (n.zxid), 0xf (n.round), LOOKING (n.state), 1 (n.sid), 0x0 (n.peerEPoch), LOOKING (my state)0 (n.config version)
2019-03-17 07:56:04,979 [myid:1] - INFO  [zookeeper-0.zookeeper:2183:QuorumCnxManager$Listener@892] - Leaving listener
2019-03-17 07:56:04,979 [myid:1] - ERROR [zookeeper-0.zookeeper:2183:QuorumCnxManager$Listener@894] - As I'm leaving the listener thread, I won't be able to participate in leader election any longer: zookeeper-0.zookeeper:2183
{code}

This error happens on most nodes on cluster start and Zookeeper is unable to form quorum. This will leave cluster in unusable state.
As I can see, error present on branches 3.4 and 3.5. 
I think, this error can be fixed by configurable number of retries(instead of hard-coded value of 3). 
Other way to fix this is removing of max retries at all. Currently, ZK server only stop leader election listener and continue to serve on other ports. Maybe, if leader election halts, we should abort process.
","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.6', id='12345243'>]",Bug,ZOOKEEPER-3320,Major,Igor Skokov,Fixed,2019-08-13T11:36:39.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Leader election port stop listen when hostname unresolvable for some time ,2019-10-16T18:59:11.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.4', id='12340141'>]",5.0
,"[<JIRA Component: name='server', id='12312382'>]",2019-03-15T18:18:58.000+0000,Jie Huang,to make spotbugs happy,[],Bug,ZOOKEEPER-3316,Minor,Jie Huang,Invalid,2019-04-04T23:49:33.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Remove unused code in SyncRequestProcessor,2019-12-19T22:59:52.000+0000,[],1.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2019-03-11T17:16:38.000+0000,Fangmin Lv,"This is a new bug we found on production.

ZooKeeper uses ACL reference id and count to save the space in snapshot. During fuzzy snapshot sync, the reference count may not be updated correctly in case like the znode is already exist.

When ACL reference count reaches 0, it will be deleted from the system, but actually there might be other nodes still using it. And when visiting a node with the deleted ACL id, it will be rejected because it doesn't exist anymore.

Here is the detailed flow for one of the scenario here:
 # Server A starts to have snap sync with leader
 # After serializing the ACL map to Server A, there is a txn T1 to create a node N1 with new ACL_1 which was not exist in ACL map
 # On leader, after this txn, the ACL map will be ID1 -> (ACL_1, COUNT: 1), and data tree N1 -> ID1
 # On server A, it will be empty ACL map, and N1 -> ID1 in fuzzy snapshot
 # When replaying the txn T1, it will skip at the beginning since the node is already exist, which leaves an empty ACL map, and N1 is referencing to a non-exist ACL ID1
 # Node N1 will be not accessible because the ACL not exist, and if it became leader later then all the write requests will be rejected as well with marshalling error.

We're still working on the fix, suggestions are welcome.","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3306,Critical,Fangmin Lv,Fixed,2019-04-29T14:49:48.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Node may not accessible due the the inconsistent ACL reference map after SNAP sync ,2019-04-29T22:50:54.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.13', id='12342973'>]",2.0
Enrico Olivelli,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='contrib', id='12312700'>]",2019-03-10T08:47:13.000+0000,Enrico Olivelli,Loggraph uses Jetty and dependency is missing in branch-3.4.,"[<JIRA Version: name='3.4.15', id='12344988'>]",Bug,ZOOKEEPER-3304,Major,Enrico Olivelli,Fixed,2019-03-14T15:57:24.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"Maven build of ""loggraph"" is broken on branch-3.4",2019-03-14T15:58:06.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",1.0
,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='contrib', id='12312700'>]",2019-03-09T15:29:24.000+0000,Hari Sekhon,"ZooKeeper Perl client zkperl fails to compile on Fedora 29 (compiles ok on CentOS 7 though). I cannot build the project to get the zkperl dependencies to run on Fedora as it is. This happens on various versions of ZooKeeper 3.4.x
{code:java}
# perl Makefile.PL --zookeeper-include=/usr/local/include --zookeeper-lib=/usr/local/lib
Generating a Unix-style Makefile
Writing Makefile for Net::ZooKeeper
Writing MYMETA.yml and MYMETA.json

# make
Skip blib/lib/Net/ZooKeeper.pm (unchanged)
Running Mkbootstrap for ZooKeeper ()
chmod 644 ""ZooKeeper.bs""
""/usr/bin/perl"" -MExtUtils::Command::MM -e 'cp_nonempty' -- ZooKeeper.bs blib/arch/auto/Net/ZooKeeper/ZooKeeper.bs 644
gcc -c  -I/usr/local/include -I. -D_REENTRANT -D_GNU_SOURCE -O2 -g -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -Wp,-D_GLIBCXX_ASSERTIONS -fexceptions -fstack-protector-strong -grecord-gcc-switches -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1 -m64 -mtune=generic -fasynchronous-unwind-tables -fstack-clash-protection -fcf-protection -fwrapv -fno-strict-aliasing -I/usr/local/include -D_LARGEFILE_SOURCE -D_FILE_OFFSET_BITS=64 -g   -DVERSION=\""0.36\"" -DXS_VERSION=\""0.36\"" -fPIC ""-I/usr/lib64/perl5/CORE""   ZooKeeper.c
ZooKeeper.c: In function ‘XS_Net__ZooKeeper_acl_constant’:
ZooKeeper.c:784:7: warning: unused variable ‘RETVAL’ [-Wunused-variable]
  AV * RETVAL;
       ^~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper_CLONE’:
ZooKeeper.c:1089:9: warning: unused variable ‘package’ [-Wunused-variable]
  char * package = (char *)SvPV_nolen(ST(0))
         ^~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper_CLONE_SKIP’:
ZooKeeper.c:1109:9: warning: unused variable ‘package’ [-Wunused-variable]
  char * package = (char *)SvPV_nolen(ST(0))
         ^~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper_TIEHASH’:
ZooKeeper.c:1129:9: warning: unused variable ‘package’ [-Wunused-variable]
  char * package = (char *)SvPV_nolen(ST(0))
         ^~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper_UNTIE’:
ZooKeeper.c:1151:5: warning: unused variable ‘ref_count’ [-Wunused-variable]
  IV ref_count = (IV)SvIV(ST(1))
     ^~~~~~~~~
ZooKeeper.c:1150:17: warning: variable ‘attr_hash’ set but not used [-Wunused-but-set-variable]
  Net__ZooKeeper attr_hash;
                 ^~~~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper_SCALAR’:
ZooKeeper.c:1281:17: warning: variable ‘attr_hash’ set but not used [-Wunused-but-set-variable]
  Net__ZooKeeper attr_hash;
                 ^~~~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper_DELETE’:
ZooKeeper.c:1528:7: warning: unused variable ‘attr_key’ [-Wunused-variable]
  SV * attr_key = ST(1)
       ^~~~~~~~
ZooKeeper.c:1527:17: warning: variable ‘attr_hash’ set but not used [-Wunused-but-set-variable]
  Net__ZooKeeper attr_hash;
                 ^~~~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper_CLEAR’:
ZooKeeper.c:1561:17: warning: variable ‘attr_hash’ set but not used [-Wunused-but-set-variable]
  Net__ZooKeeper attr_hash;
                 ^~~~~~~~~
ZooKeeper.xs: In function ‘XS_Net__ZooKeeper_add_auth’:
ZooKeeper.xs:1206:30: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 3 has type ‘STRLEN’ {aka ‘long unsigned int’} [-Wformat=]
             Perl_croak(aTHX_ ""invalid certificate length: %u"", cert_len);
                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  ~~~~~~~~
ZooKeeper.xs: In function ‘XS_Net__ZooKeeper_create’:
ZooKeeper.xs:1286:30: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 3 has type ‘STRLEN’ {aka ‘long unsigned int’} [-Wformat=]
             Perl_croak(aTHX_ ""invalid data length: %u"", buf_len);
                              ^~~~~~~~~~~~~~~~~~~~~~~~~  ~~~~~~~
ZooKeeper.xs:1321:21: error: format not a string literal and no format arguments [-Werror=format-security]
                     Perl_croak(aTHX_ err);
                     ^~~~~~~~~~
ZooKeeper.xs: In function ‘XS_Net__ZooKeeper_set’:
ZooKeeper.xs:1760:30: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 3 has type ‘STRLEN’ {aka ‘long unsigned int’} [-Wformat=]
             Perl_croak(aTHX_ ""invalid data length: %u"", buf_len);
                              ^~~~~~~~~~~~~~~~~~~~~~~~~  ~~~~~~~
ZooKeeper.xs: In function ‘XS_Net__ZooKeeper_set_acl’:
ZooKeeper.xs:1923:13: error: format not a string literal and no format arguments [-Werror=format-security]
             Perl_croak(aTHX_ err);
             ^~~~~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper__Stat_CLONE’:
ZooKeeper.c:2871:9: warning: unused variable ‘package’ [-Wunused-variable]
  char * package = (char *)SvPV_nolen(ST(0))
         ^~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper__Stat_CLONE_SKIP’:
ZooKeeper.c:2891:9: warning: unused variable ‘package’ [-Wunused-variable]
  char * package = (char *)SvPV_nolen(ST(0))
         ^~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper__Stat_TIEHASH’:
ZooKeeper.c:2911:9: warning: unused variable ‘package’ [-Wunused-variable]
  char * package = (char *)SvPV_nolen(ST(0))
         ^~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper__Stat_UNTIE’:
ZooKeeper.c:2933:5: warning: unused variable ‘ref_count’ [-Wunused-variable]
  IV ref_count = (IV)SvIV(ST(1))
     ^~~~~~~~~
ZooKeeper.c:2932:23: warning: variable ‘attr_hash’ set but not used [-Wunused-but-set-variable]
  Net__ZooKeeper__Stat attr_hash;
                       ^~~~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper__Stat_SCALAR’:
ZooKeeper.c:3065:23: warning: variable ‘attr_hash’ set but not used [-Wunused-but-set-variable]
  Net__ZooKeeper__Stat attr_hash;
                       ^~~~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper__Stat_STORE’:
ZooKeeper.c:3167:7: warning: unused variable ‘attr_val’ [-Wunused-variable]
  SV * attr_val = ST(2)
       ^~~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper__Stat_DELETE’:
ZooKeeper.c:3271:7: warning: unused variable ‘attr_key’ [-Wunused-variable]
  SV * attr_key = ST(1)
       ^~~~~~~~
ZooKeeper.c:3270:23: warning: variable ‘attr_hash’ set but not used [-Wunused-but-set-variable]
  Net__ZooKeeper__Stat attr_hash;
                       ^~~~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper__Stat_CLEAR’:
ZooKeeper.c:3304:23: warning: variable ‘attr_hash’ set but not used [-Wunused-but-set-variable]
  Net__ZooKeeper__Stat attr_hash;
                       ^~~~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper__Watch_CLONE’:
ZooKeeper.c:3405:9: warning: unused variable ‘package’ [-Wunused-variable]
  char * package = (char *)SvPV_nolen(ST(0))
         ^~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper__Watch_CLONE_SKIP’:
ZooKeeper.c:3425:9: warning: unused variable ‘package’ [-Wunused-variable]
  char * package = (char *)SvPV_nolen(ST(0))
         ^~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper__Watch_TIEHASH’:
ZooKeeper.c:3445:9: warning: unused variable ‘package’ [-Wunused-variable]
  char * package = (char *)SvPV_nolen(ST(0))
         ^~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper__Watch_UNTIE’:
ZooKeeper.c:3467:5: warning: unused variable ‘ref_count’ [-Wunused-variable]
  IV ref_count = (IV)SvIV(ST(1))
     ^~~~~~~~~
ZooKeeper.c:3466:24: warning: variable ‘attr_hash’ set but not used [-Wunused-but-set-variable]
  Net__ZooKeeper__Watch attr_hash;
                        ^~~~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper__Watch_SCALAR’:
ZooKeeper.c:3599:24: warning: variable ‘attr_hash’ set but not used [-Wunused-but-set-variable]
  Net__ZooKeeper__Watch attr_hash;
                        ^~~~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper__Watch_DELETE’:
ZooKeeper.c:3803:7: warning: unused variable ‘attr_key’ [-Wunused-variable]
  SV * attr_key = ST(1)
       ^~~~~~~~
ZooKeeper.c:3802:24: warning: variable ‘attr_hash’ set but not used [-Wunused-but-set-variable]
  Net__ZooKeeper__Watch attr_hash;
                        ^~~~~~~~~
ZooKeeper.c: In function ‘XS_Net__ZooKeeper__Watch_CLEAR’:
ZooKeeper.c:3836:24: warning: variable ‘attr_hash’ set but not used [-Wunused-but-set-variable]
  Net__ZooKeeper__Watch attr_hash;
                        ^~~~~~~~~
cc1: some warnings being treated as errors
make: *** [Makefile:335: ZooKeeper.o] Error 1
{code}",[],Bug,ZOOKEEPER-3303,Minor,Hari Sekhon,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper Perl client zkperl doesn't compile on newer RHEL systems ie. Fedora,2021-01-06T11:10:21.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.4.12', id='12342040'>, <JIRA Version: name='3.4.13', id='12342973'>]",3.0
,[],2019-03-09T09:10:06.000+0000,Ling Mao,"What we want is ""setquota [-n|-b] val path""",[],Bug,ZOOKEEPER-3299,Trivial,Ling Mao,Not A Bug,2019-05-11T11:02:33.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"""setquota -n|-b val path"" need a brackets",2019-05-11T11:02:33.000+0000,[],1.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2019-03-07T18:07:41.000+0000,Fangmin Lv,"Recently, on prod ensembles, we saw some peers failed to connect to others due to timed out when connecting to the other's leader election port. This was triggered by a network incident with lots of packet loss.

After investigation, we found it's because we doesn't close the socket explicitly when it timed out during ssl handshake in QuorumCnxManager.connectOne.

The quorum connection manager is handling connections sequentially with a default listen backlog queue size 50, during the network loss, there are socket read timed out, which is syncLimit * tickTime, and almost all the following connect requests in the backlog queue will timed out from the other side before it's being processed. Those timed out learners will try to connect to a different server, and leaves the connect requests on server side without sending the close_notify packet. The server is slowly consuming from these queue with syncLimit * tickTime timeout for each of those requests which haven't sent notify_close packet. Any new connect requests will be queued up again when there is spot in the listen backlog queue, but timed out before the server handles it, and it can never successfully finish any new connection, so it failed to join the quorum. And the peers are leaking FD because all those connections are in CLOSE-WAIT state.
  
 Restarting the servers to drain the listen backlog queue mitigated the issue.

Here are the steps to manually reproduce the issue:
 # issuing two telnet connect to server A in the quorum without sending any packet
 # stop all other servers
 # start those again
 # server A read timed out from those telnet connect request one by one and it cannot join the quorum anymore","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3296,Major,Fangmin Lv,Fixed,2019-06-14T07:09:58.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Cannot join quorum due to Quorum SSLSocket connection not closed explicitly when there is handshake issue,2019-06-14T12:09:39.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",2.0
,[],2019-03-06T00:20:47.000+0000,Hari Sekhon,"ZooKeeper fails to compile on Fedora 29 (compiles ok on CentOS 7 though). I cannot build the project to get the zkperl dependencies to run on Fedora as it is. This happens on various versions of ZooKeeper 3.4.x
{code:java}
cd zookeeper-3.4.8/src/c
./configure
make
make  all-am
make[1]: Entering directory '/github/nagios-plugins/zookeeper-3.4.13/src/c'
/bin/sh ./libtool  --tag=CC   --mode=compile gcc -DHAVE_CONFIG_H -I.  -I./include -I./tests -I./generated   -Wall -Werror  -g -O2 -D_GNU_SOURCE -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c -o zookeeper.lo `test -f 'src/zookeeper.c' || echo './'`src/zookeeper.c
libtool: compile:  gcc -DHAVE_CONFIG_H -I. -I./include -I./tests -I./generated -Wall -Werror -g -O2 -D_GNU_SOURCE -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c src/zookeeper.c  -fPIC -DPIC -o .libs/zookeeper.o
src/zookeeper.c: In function ‘format_endpoint_info’:
src/zookeeper.c:3506:21: error: ‘%d’ directive writing between 1 and 5 bytes into a region of size between 0 and 127 [-Werror=format-overflow=]
     sprintf(buf,""%s:%d"",addrstr,ntohs(port));
                     ^~
src/zookeeper.c:3506:17: note: directive argument in the range [0, 65535]
     sprintf(buf,""%s:%d"",addrstr,ntohs(port));
                 ^~~~~~~
src/zookeeper.c:3506:5: note: ‘sprintf’ output between 3 and 134 bytes into a destination of size 128
     sprintf(buf,""%s:%d"",addrstr,ntohs(port));
     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
cc1: all warnings being treated as errors
make[1]: *** [Makefile:955: zookeeper.lo] Error 1
make[1]: Leaving directory '/github/nagios-plugins/zookeeper-3.4.13/src/c'
make: *** [Makefile:631: all] Error 2
{code}",[],Bug,ZOOKEEPER-3293,Minor,Hari Sekhon,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper fails to compile on newer RHEL systems ie. Fedora,2021-01-06T11:08:44.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.4.12', id='12342040'>, <JIRA Version: name='3.4.13', id='12342973'>]",4.0
,"[<JIRA Component: name='c client', id='12312380'>]",2019-02-27T15:11:15.000+0000,David Vujic,"When building the C client on Windows with CMake:

cmake -DWANT_SYNCAPI=OFF -DCMAKE_GENERATOR_PLATFORM=x64

 

With this input, the header file winports.h will not be added in these files:

*zk_log.c*

*zk_adaptor.h*

Also, I think winports.h should be added to *zookeeper.c*

 

Without winports.h compiling will fail on Windows. Errors are about strtok_r and localtime_r - the Windows mappings in winports.h are missing. 

I am guessing that other important includes are missing too (like Windows Sockets).

 

One solution could be to extract the winports.h include out from the THREADED preprocessor, to a separate one:

#ifdef WIN32

#include ""winport.h""

#endif",[],Bug,ZOOKEEPER-3292,Major,David Vujic,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper C Client for Windows: should include winports.h,2021-01-06T11:04:32.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",2.0
,"[<JIRA Component: name='c client', id='12312380'>]",2019-02-21T22:00:27.000+0000,Christian Czezatke,"*Description:*

The get_xid functions in mt_adaptor.c/st_adaptor.c return a 32 bit signed integer that is initialized to the current unix epoch timestamp on startup.

This counter will eventually wrap around, which is not a problem per se, since the client does not expect XID values to monotonically increase: It just verifies that replies to operations come back in order by checking the XID of a request received against the next XID expected. (zookeeper.c:zookeeper_process).

However, after a wrap-around the XID values will eventually collide with the reserved XIDs ad defined in zk_adaptor.h:
 * The first collision will be with SET_WATCHES_XID (-8): The reply to the request that happens to get tagged with -8 will be misinterpreted as a reply to SET_WATCHES. This causes the client to see a connection timeout.
 * The next collision will be with AUTH_XID (-4): At that point the client will segfault, when mis-interpreting the reply:

#0  0x0000000000407645 in auth_completion_func (zh=0x61d010, rc=0) at src/zookeeper.c:1823
 #1  zookeeper_process (zh=zh@entry=0x61d010, events=<optimized out>) at src/zookeeper.c:2896
 #2  0x000000000040c34c in do_io (v=0x61d010) at src/mt_adaptor.c:451
 #3  0x00007ffff7bc8dc5 in start_thread () from /lib64/libpthread.so.0
 #4  0x00007ffff75f573d in clone () from /lib64/libc.so.6

I hit this with a busy C client that runs for a very long time (months). Also, when a client spins in a tight loop trying to submit more operations even for a short period of time after a connection loss the xid values will increment very fast.

 

*Proposed patch:*

To avoid introducing any additional locking, this can be solved by just masking out the MSB in the xid returned by get_xid. Effectively this prevents the returned XID from ever going negative.

To avoid a race when the static xid variable hits -1 eventually after a wrap, around, I propose to not initialize xid with the result of time(0) on startup. This is not needed. This also means that the get_xid function in mt_adapter.c no longer needs to be flagged as constructor.

 Proposed patch is attached.

 

I ran into this on zookeeper 3.5.4 but other versions are likely affected as well.","[<JIRA Version: name='3.5.10', id='12349434'>]",Bug,ZOOKEEPER-3286,Minor,Christian Czezatke,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,xid wrap-around causes connection loss/segfault when hitting predefined XIDs,2020-11-26T14:25:46.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>]",1.0
,"[<JIRA Component: name='tests', id='12312427'>]",2019-02-21T13:24:58.000+0000,Norbert Kalmár,Some tests fails constantly if run by maven. The failures are waiting for ZK server to start or waiting for client to connect. Looks like something is different in the junit runner. Possibly something is different version on the classpath.,[],Bug,ZOOKEEPER-3284,Major,Norbert Kalmár,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Tests fails on 3.4 if running with surefire,2019-02-21T13:24:58.000+0000,"[<JIRA Version: name='3.4.14', id='12343587'>]",1.0
,"[<JIRA Component: name='java client', id='12312381'>]",2019-02-15T21:49:29.000+0000,Jonathan Park," 
{code:java}
2019-02-15 13:40:21,471 [myid:] - DEBUG [main-SendThread(localhost:2181):ClientCnxn$SendThread@759] - Got auth sessionid:0x168f2c5e9c60017
2019-02-15 13:40:21,472 [myid:] - WARN  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1166] - Session 0x168f2c5e9c60017 for server localhost/0:0:0:0:0:0:0:1:2181, unexpected error, closing socket connection and attempting reconnect
java.io.IOException: Xid out of order. Got Xid -3 with err 0 expected Xid -4 for a packet with details: clientPath:null serverPath:null finished:false header:: -4,8 replyHeader:: 0,0,-4 request:: '/,F response:: v{} 
at org.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:828)
at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:94)
at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:366)
at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1143)
2019-02-15 13:40:22,520 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@1027] - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2019-02-15 13:40:22,521 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@877] - Socket connection established to localhost/127.0.0.1:2181, initiating session
2019-02-15 13:40:22,521 [myid:] - DEBUG [main-SendThread(localhost:2181):ClientCnxn$SendThread@950] - Session establishment request sent on localhost/127.0.0.1:2181
2019-02-15 13:40:22,522 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@1301] - Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x168f2c5e9c60017, negotiated timeout = 30000
2019-02-15 13:40:22,525 [myid:] - DEBUG [main-SendThread(localhost:2181):ClientCnxn$SendThread@742] - Got ping response for sessionid: 0x168f2c5e9c60017 after 235329552ms
{code}
ClientCnxn xid's are tracked as java int's. For long-lived ZK clients this can lead to rollover into the negative xid space. Xid = -4 is treated as a special xid reserved for auth requests. With xid rollover, a normal ZK request can also have xid = -4 but the response will be treated as an auth response making subsequent packet processing error with the exception above. We can reproduce this more readily by changing the starting xid in ClientCnxn from 1 to -100. The ZK client will transparently reconnect and establish a new session but features that depended on the same session persisting will unnecessarily experience a disconnected event.

 

I've attached simple class with a main() method that reproduces the failure quickly against a local ZK server after modifying the initial value of ClientCnxn.xid from 1 to -100. 

 ",[],Bug,ZOOKEEPER-3280,Major,Jonathan Park,Duplicate,2019-02-15T22:05:21.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ClientCnxn xid rollover can break sessions,2019-02-19T21:22:06.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.4.12', id='12342040'>]",5.0
,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='security', id='12329414'>, <JIRA Component: name='tests', id='12312427'>]",2019-02-14T11:25:44.000+0000,Enrico Olivelli,"Seems that the maven build lacks some dependency on branch-3.4,

I have these errors while testing the 3.4.14 RC 1 source tarball

[ERROR] org.apache.zookeeper.server.quorum.auth.ApacheDSMiniKdcTest  Time elapsed: 1.161 s  <<< ERROR!
java.lang.NoClassDefFoundError: jdbm/helper/CachePolicy
Caused by: java.lang.ClassNotFoundException: jdbm.helper.CachePolicy",[],Bug,ZOOKEEPER-3279,Major,Enrico Olivelli,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Maven tests fail on branch-3.4,2019-02-14T11:25:44.000+0000,"[<JIRA Version: name='3.4.14', id='12343587'>]",1.0
,"[<JIRA Component: name='build', id='12312383'>]",2019-02-14T10:43:12.000+0000,Enrico Olivelli,[ERROR] Failed to execute goal pl.project13.maven:git-commit-id-plugin:2.2.5:revision (find-current-git-revision) on project zookeeper-server: .git directory is not found! Please specify a valid [dotGitDirectory] in your pom.xml -> [Help 1],[],Bug,ZOOKEEPER-3278,Major,Enrico Olivelli,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Maven sources are not buildable if not linked to a git respotiory,2019-02-14T10:43:12.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.4.14', id='12343587'>]",1.0
,"[<JIRA Component: name='java client', id='12312381'>]",2019-02-01T01:54:22.000+0000,Jiafu Jiang,"I found that ZooKeeper java client blocked, and the related call stack was shown below:

""Election thread-20"" #20 prio=5 os_prio=0 tid=0x00007f7deeadfd80 nid=0x5ec3 in Object.wait() [0x00007f7ddd5d8000]
 java.lang.Thread.State: WAITING (on object monitor)
 at java.lang.Object.wait(Native Method)
 at java.lang.Object.wait(Object.java:502)
 at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1411)
 - locked <0x00000000e04b63b0> (a org.apache.zookeeper.ClientCnxn$Packet)
 at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1177)
 at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1210)
 at com.sugon.parastor.zookeeper.ZooKeeperClient.exists(ZooKeeperClient.java:643)
 ........

 

And I also found that the block process did not have the SendThread thread. It seems like a normal process with ZooKeeper java client should have a SendThread, like below:

""Thread-0-SendThread(ofs_zk1:2181)"" #23 daemon prio=5 os_prio=0 tid=0x00007f8c540379c0 nid=0x739 runnable [0x00007f8c5ad71000]
 java.lang.Thread.State: RUNNABLE
 at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
 at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
 at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
 at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
 - locked <0x00000000e00287a8> (a sun.nio.ch.Util$3)
 - locked <0x00000000e0028798> (a java.util.Collections$UnmodifiableSet)
 - locked <0x00000000e0028750> (a sun.nio.ch.EPollSelectorImpl)
 at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
 at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:349)
 at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1145)

 

So, will the missing of the SendThread thread cause the blocking of exist method?? I'm not sure.",[],Bug,ZOOKEEPER-3266,Major,Jiafu Jiang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper Java client blocks for a very long time.,2019-02-26T11:47:50.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",3.0
Zsombor Gegesy,"[<JIRA Component: name='build', id='12312383'>]",2019-01-31T22:17:04.000+0000,Zsombor Gegesy,"Building Zookeeper branch-3.4 fails with Ant, if I try:
ant package:

{{package:
    [copy] Copying 1 file to /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/share/zookeeper/recipes/queue
   [mkdir] Created dir: /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/share/zookeeper/recipes/queue/src/test/java
    [copy] Copying 1 file to /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/share/zookeeper/recipes/queue/src/test/java
   [mkdir] Created dir: /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/share/zookeeper/recipes/queue/src/main/java
    [copy] Copying 1 file to /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/share/zookeeper/recipes/queue/src/main/java
   [mkdir] Created dir: /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/share/zookeeper/recipes/queue/src/main/c
    [copy] Copying 15 files to /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/share/zookeeper/recipes/queue/src/main/c
    [copy] Copying 1 file to /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/share/zookeeper
   [mkdir] Created dir: /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/dist-maven
    [copy] Copying 1 file to /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/dist-maven
    [copy] Copying 2 files to /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/dist-maven
    [copy] Copying 1 file to /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/dist-maven
    [copy] Copying 2 files to /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/bin
    [copy] Copying 2 files to /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/libexec
    [copy] Copying 2 files to /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/sbin
    [copy] Copying 3 files to /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/conf
    [copy] Copying 304 files to /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/docs
    [copy] Copying 7 files to /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT
    [copy] Copying 72 files to /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/src
    [copy] Copying 1 file to /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/share/zookeeper/templates/conf
    [copy] Copying 1 file to /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/share/zookeeper/templates/conf

BUILD FAILED
/Users/test/src/zookeeper/build.xml:973: /Users/test/src/zookeeper/build/zookeeper-3.4.14-SNAPSHOT/src/zookeeper-contrib does not exist.
}}

The fileset which tries to locate executables in the contrib area doesn't match anything.","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.4.14', id='12343587'>]",Bug,ZOOKEEPER-3265,Major,Zsombor Gegesy,Fixed,2019-02-04T16:10:25.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Build failure on branch-3.4,2019-04-02T10:40:39.000+0000,"[<JIRA Version: name='3.4.14', id='12343587'>]",2.0
,[],2019-01-29T11:45:29.000+0000,Chintan,I tried to find any existing Jira for this but could not find it. This Jira is for introducing support for Open JDK 11 in zookeeper.,[],Bug,ZOOKEEPER-3260,Major,Chintan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Open JDK 11 support in Zookeeper,2019-01-29T11:45:29.000+0000,[],1.0
Samuel Just,"[<JIRA Component: name='java client', id='12312381'>]",2019-01-22T23:21:28.000+0000,Samuel Just,"Once the cxid value increments to -4, the client will interpret the response as an auth packet rather than as a response to a request and will transparently drop the response and the request will hang.  Similarly, -2 will be seen as a ping and will be dropped hanging the request.  -1 shows up as a WatcherEvent and results in the error below.

 
{quote}2019-01-07T21:58:23.209+00:00 [INFO ] [main-SendThread(mnds1-2-phx.ops.sfdc.net:2181)] [ClientCnxn.java:1381] [:] - Session establishment complete on server mnds1-2-phx.ops.sfdc.net/10.246.244.71:2181, sessionid = 0x267859729d66320, negotiated timeout = 10000
 2019-01-07T21:58:22.281+00:00 20190107215822.281000 [WARN ] [main-SendThread(mnds1-3-phx.ops.sfdc.net:2181)] [ClientCnxn.java:1235] [:] - Session 0x267859729d66320 for server mnds1-3-phx.ops.sfdc.net/10.246.244.69:2181, unexpected error, closing socket connection and attempting reconnect
 java.io.IOException: Unreasonable length = 892612659
 at org.apache.jute.BinaryInputArchive.checkLength(BinaryInputArchive.java:127) ~[zookeeper-3.5.3-beta.jar:3.5.3-beta-8ce24f9e675cbefffb8f21a47e06b42864475a60]
 at org.apache.jute.BinaryInputArchive.readString(BinaryInputArchive.java:81) ~[zookeeper-3.5.3-beta.jar:3.5.3-beta-8ce24f9e675cbefffb8f21a47e06b42864475a60]
 at org.apache.zookeeper.proto.WatcherEvent.deserialize(WatcherEvent.java:66) ~[zookeeper-3.5.3-beta.jar:3.5.3-beta-8ce24f9e675cbefffb8f21a47e06b42864475a60]
 at org.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:839) ~[zookeeper-3.5.3-beta.jar:3.5.3-beta-8ce24f9e675cbefffb8f21a47e06b42864475a60]
 at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:101) ~[zookeeper-3.5.3-beta.jar:3.5.3-beta-8ce24f9e675cbefffb8f21a47e06b42864475a60]
 at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:363) ~[zookeeper-3.5.3-beta.jar:3.5.3-beta-8ce24f9e675cbefffb8f21a47e06b42864475a60]
 at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1214) ~[zookeeper-3.5.3-beta.jar:3.5.3-beta-8ce24f9e675cbefffb8f21a47e06b42864475a60]
{quote}
 ","[<JIRA Version: name='3.4.15', id='12344988'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-3253,Minor,Samuel Just,Fixed,2019-03-08T04:10:40.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"client should not send requests with cxid=-4, -2, or -1",2019-12-16T08:07:41.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",4.0
,[],2019-01-17T07:10:23.000+0000,Dmitrii Apanasevich,"I tend to consider the problem as a serious one. For example, a single run of a bunch of twenty tests leaves ~1Gb of garbage on the system disk (64Mb per test). It's easy to get ""No disc space"".

I've created [a simple example on GitHub|https://github.com/apanasevich/embedded-servers-example[]|https://github.com/apanasevich/embedded-servers-example] that demonstrates the problem. Here is the exception which throws EmbeddedZookeeper#shutdown:
{code:java}
java.nio.file.FileSystemException: C:\Users\D15E0~1.APA\AppData\Local\Temp\kafka-8507336206769425170\version-2\log.1: The process cannot access the file because it is being used by another process

at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
at java.nio.file.Files.delete(Files.java:1126)
at org.apache.kafka.common.utils.Utils$2.visitFile(Utils.java:757)
at org.apache.kafka.common.utils.Utils$2.visitFile(Utils.java:746)
at java.nio.file.Files.walkFileTree(Files.java:2670)
at java.nio.file.Files.walkFileTree(Files.java:2742)
at org.apache.kafka.common.utils.Utils.delete(Utils.java:746)
at kafka.zk.EmbeddedZookeeper.shutdown(EmbeddedZookeeper.scala:63)
at example.TempDirectoriesTest.tearDownTest(TempDirectoriesTest.java:58)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:106)
at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:66)
at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)
at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32)
at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93)
at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:117)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)
at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:155)
at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:137)
at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:404)
at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)
at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)
at java.lang.Thread.run(Thread.java:748)
{code}
 ",[],Bug,ZOOKEEPER-3248,Major,Dmitrii Apanasevich,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,EmbeddedZookeeper does not delete temp directory on shutdown in Windows,2022-03-03T15:52:23.000+0000,[],3.0
Ling Mao,"[<JIRA Component: name='server', id='12312382'>]",2018-12-29T04:05:28.000+0000,Jiafu Jiang,"I read the ZooKeeper source code, and I find the purge task use FileTxnSnapLog#findNRecentSnapshots to find snapshots, but the method does not check whether the snapshots are valid.

Consider a worse case, a ZooKeeper server may have many invalid snapshots, and when a purge task begins, it will use the zxid in the last snapshot's name to purge old snapshots and transaction logs, then we may lost data. 

I think we should use FileSnap#findNValidSnapshots(int) instead of FileSnap#findNRecentSnapshots in FileTxnSnapLog#findNRecentSnapshots, but I am not sure.

 ","[<JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-3231,Major,Jiafu Jiang,Fixed,2021-09-10T03:38:54.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0, Purge task may lost data when the recent snapshots are all invalid,2021-09-10T03:38:54.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.4.13', id='12342973'>]",3.0
,"[<JIRA Component: name='build', id='12312383'>]",2018-12-20T16:08:20.000+0000,Enrico Olivelli,"In a lot of places we are using the default encoding.

Spotbugs is not happy.",[],Bug,ZOOKEEPER-3227,Major,Enrico Olivelli,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Address Spotbugs: DM_DEFAULT_ENCODING issues,2018-12-20T16:09:02.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",1.0
,"[<JIRA Component: name='recipes', id='12313246'>]",2018-12-18T13:55:50.000+0000,Huo Zhu,"recently i used WriteLock in my application, and get following Exception
{code:java}
Exception in thread ""produce 1"" java.lang.IllegalArgumentException: Path must start with / character
at org.apache.zookeeper.common.PathUtils.validatePath(PathUtils.java:51)
at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:851)
at org.apache.zookeeper.recipes.lock.WriteLock$1.execute(WriteLock.java:118)
at org.apache.zookeeper.recipes.lock.WriteLock$1.execute(WriteLock.java:1)
at org.apache.zookeeper.recipes.lock.WriteLock.unlock(WriteLock.java:122)
{code}
the following function is called when tried to lock,  used an existed child node name as inner lock id, which may be conflict with another lock user, and at the same time the lock id is just the node name , no with prefix path,  causing{color:#FF0000} java.lang.IllegalArgumentException{color} in final delete operation. 
{code:java}
private  void findPrefixInChildren(String prefix, ZooKeeper zookeeper, String dir) throws KeeperException, InterruptedException {
            List<String> names = zookeeper.getChildren(dir, false);
            for (String name : names) {
                if (name.startsWith(prefix)) {
                    id = name;
                    if (LOG.isDebugEnabled()) {
                        LOG.debug(""Found id created last time: "" + id);
                    }
                    break;
                }
            }
            if (id == null) {
                id = zookeeper.create(dir + ""/"" + prefix, data,  getAcl(), EPHEMERAL_SEQUENTIAL);
                if (LOG.isDebugEnabled()) {
                    LOG.debug(""Created id: "" + id);
                }
            }

        }
{code}
 ",[],Bug,ZOOKEEPER-3221,Critical,Huo Zhu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,WriteLock in recipes may get wrong child name as lock id,2019-01-16T02:41:52.000+0000,[],3.0
,"[<JIRA Component: name='server', id='12312382'>]",2018-12-18T09:55:22.000+0000,Jiafu Jiang,"We known that ZooKeeper server will call fsync to make sure that log data has been successfully saved to disk. But ZooKeeper server does not call fsync to make sure that a snapshot has been successfully saved, which may cause potential problems. Since a close to a file description does not make sure that data is written to disk, see [http://man7.org/linux/man-pages/man2/close.2.html#notes] for more details.

 

If the snapshot is not successfully  saved to disk, it may lead to data inconsistency. Here is my example, which is also a real problem I have ever met.

1. I deployed a 3-node ZooKeeper cluster: zk1, zk2, and zk3, zk2 was the leader.

2. Both zk1 and zk2 had the log records from log1~logX, X was the zxid.

3. The machine of zk1 restarted, and during the reboot,  log(X+1) ~ log Y are saved to log files of both zk2(leader) and zk3(follower).

4. After zk1 restarted successfully, it found itself to be a follower, and it began to synchronize data with the leader. The leader sent a snapshot(records from log 1 ~ log Y) to zk1, zk1 then saved the snapshot to local disk by calling the method ZooKeeperServer.takeSnapshot. But unfortunately, when the method returned, the snapshot data was not saved to disk yet. In fact the snapshot file was created, but the size was 0.

5. zk1 finished the synchronization and began to accept new requests from the leader. Say log records from log(Y + 1) ~ log Z were accepted by zk1 and  saved to log file. With fsync zk1 could make sure log data was not lost.

6. zk1 restarted again. Since the snapshot's size was 0, it would not be used, therefore zk1 recovered using the log files. But the records from log(X+1) ~ logY were lost ! 

 

Sorry for my poor English.

 ",[],Bug,ZOOKEEPER-3220,Critical,Jiafu Jiang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,The snapshot is not saved to disk and may cause data inconsistency.,2022-03-04T13:29:20.000+0000,"[<JIRA Version: name='3.4.12', id='12342040'>, <JIRA Version: name='3.4.13', id='12342973'>]",5.0
,[],2018-12-17T20:55:57.000+0000,yangoofy,"two participants、one observer，zkclient connect to observer。

Then，close the two participants，the zookeeper server cloesed

Ten seconds later，reopen the two participants，and leader selected

----------------------------------------------------------------------------

But the observer can't connect to the new leader immediately。Because in lookForLeader, the observer use blockingQueue(recvqueue)  to offer/poll notifications，when the recvqueue is empty，poll from recvqueue will be blocked，and timeout is 200ms，400ms，800ms....60s。

For example，09:59:59 observer poll notification，recvqueue was empty and timeout was 60s；10:00:00 two participants reopened and reselected；10:00:59 observer polled notification，connected to the new leader

But the maxSessionTimeout default to 40s。The session expired

-----------------------------------------------------------------------------

Please improve it：observer should connect to the new leader as soon as possible","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3218,Major,yangoofy,Fixed,2019-01-18T02:45:51.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zk server reopened，the interval for observer connect to the new leader is too long，then session expired,2019-01-18T06:28:31.000+0000,[],4.0
Enrico Olivelli,[],2018-12-15T00:26:56.000+0000,Patrick D. Hunt,"https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper-trunk-owasp/204/artifact/build/test/owasp/dependency-check-vulnerability.html

https://nvd.nist.gov/vuln/detail/CVE-2018-8088

We don't use EventData but should consider upgrading.","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.4.14', id='12343587'>]",Bug,ZOOKEEPER-3217,Critical,Patrick D. Hunt,Fixed,2019-01-03T15:34:13.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,owasp job flagging slf4j on trunk,2019-04-02T10:40:28.000+0000,[],1.0
Andor Molnar,[],2018-12-13T20:54:33.000+0000,V,"Java 9 introduces covariant return types which allows one to have different return types if return type in the overridden method is a sub type. Since Java 9, few functions return ByteBuffer, whereas the parent method return Buffer, resulting in causing issues for Java 8 and below since for them the method does not exist.

Steps To Reproduce:
1. Setup ZooKeeper Server with JDK11.
2. Setup ZooKeeper Client with JDK8.
3. Try connecting the client and server.

Results:
Cast ByteBuffer instances to Buffer before calling the method.

 

Notes:
There was a similar bug in the MongoDB community - [https://jira.mongodb.org/browse/JAVA-2559]

 

This is not a contribution.","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.5.8', id='12346950'>, <JIRA Version: name='3.6.2', id='12347809'>]",Bug,ZOOKEEPER-3215,Minor,V,Fixed,2020-04-30T07:30:06.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Handle Java 9/11 additions of covariant return types to java.nio.ByteBuffer methods,2020-05-11T15:41:14.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",5.0
,"[<JIRA Component: name='tests', id='12312427'>]",2018-12-13T02:08:33.000+0000,Ling Mao,"more details in:
https://builds.apache.org/job/PreCommit-ZOOKEEPER-github-pr-build/2901/testReport/junit/org.apache.zookeeper.server.quorum/QuorumPeerMainTest/testLeaderElectionWithDisloyalVoter/",[],Bug,ZOOKEEPER-3214,Minor,Ling Mao,Duplicate,2018-12-13T02:19:54.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Flaky test: org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testLeaderElectionWithDisloyalVoter,2018-12-13T02:22:44.000+0000,[],3.0
,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='server', id='12312382'>]",2018-12-12T07:00:19.000+0000,miaojianlong,"# first i found my spark(2.2.0) turn to standby (HA mode with zk) and i can not restart the service to restore the problem。
 # Then I found that there are three nodes in the /spark/leader_election/ directory, which are 48, 93, and 94. These are temporary sequential nodes, and 48 should have been timed out. And I looked at the transaction log and did have a log of delete 48. But the actual data still exists.

The above phenomenon appears on the two nodes 10.35.104.123 and 10.35.104.125, and only 93 and 94 on 10.35.104.124.

Unable to export logs due to phenomenon in the company intranet",[],Bug,ZOOKEEPER-3213,Major,miaojianlong,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Transaction has delete log bug actually it is not delete,2021-01-06T11:01:46.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",4.0
Tamas Penzes,"[<JIRA Component: name='other', id='12333125'>]",2018-12-11T09:15:05.000+0000,Tamas Penzes,During the website migration the doap.rdf file has been forgotten. Must be put back to its place.,"[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3212,Major,Tamas Penzes,Fixed,2018-12-11T10:06:58.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Fix website with adding doap.rdf back,2018-12-11T10:15:06.000+0000,[],1.0
,"[<JIRA Component: name='server', id='12312382'>]",2018-12-11T03:53:59.000+0000,yeshuangshuang,"1.config--zoo.cfg
server.1=127.0.0.1:2902:2903
2.kernel version
version：Linux localhost.localdomain 3.10.0-123.el7.x86_64 #1 SMP Tue Feb 12 19:44:50 EST 2019 x86_64 x86_64 x86_64 GNU/Linux
JDK：
java version ""1.7.0_181""
OpenJDK Runtime Environment (rhel-2.6.14.5.el7-x86_64 u181-b00)
OpenJDK 64-Bit Server VM (build 24.181-b00, mixed mode)
zk: 3.4.5
3.bug details：
Occasionally，But the recurrence probability is extremely high. At first, the read-write timeout takes about 6s, and after a few minutes, all connections (including long ones) will be CLOSE_WAIT state.

4.:Circumvention scheme: it is found that all connections become close_wait to restart the zookeeper server side actively
","[<JIRA Version: name='3.4.5', id='12321883'>]",Bug,ZOOKEEPER-3211,Major,yeshuangshuang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zookeeper standalone mode，found a high level bug in kernel of centos7.0 ，zookeeper Server's  tcp/ip socket connections(default 60 ) are CLOSE_WAIT ，this lead to zk can't work for client any more,2021-01-06T11:00:16.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",7.0
,[],2018-12-08T09:33:06.000+0000,Stanislav Knot,,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.4.14', id='12343587'>]",Bug,ZOOKEEPER-3210,Trivial,Stanislav Knot,Fixed,2019-01-08T16:23:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Typo in zookeeperInternals doc,2019-04-02T10:40:27.000+0000,[],2.0
Fangmin Lv,[],2018-12-04T20:06:12.000+0000,Fangmin Lv,"File like WatchManager.java and WatchesPathReport.java exist in both org/apache/zookeeper/server and org/apache/zookeeper/server/watch folder, org/apache/zookeeper/server/watch is the right one, looks like we introduced the other one by mistake in ZOOKEEPER-3032.",[],Bug,ZOOKEEPER-3207,Minor,Fangmin Lv,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Watch related code being copied over twice when doing maven migration,2022-02-03T08:50:17.000+0000,[],3.0
,"[<JIRA Component: name='kerberos', id='12329415'>]",2018-12-03T14:23:41.000+0000,Stephane Maarek,"We're using Active Directory, and created service principals this way:

{code}
ktpass -princ ZOOKEEPER/host-1@TEST -mapuser zookeeper -mapOp  add -Target TEST
ktpass -princ ZOOKEEPER/host-2@TEST -mapuser zookeeper -mapOp  add -Target TEST
ktpass -princ ZOOKEEPER/host-3@TEST -mapuser zookeeper -mapOp  add -Target TEST
{code}

Using this format, one is not able to do {code}kinit ZOOKEEPER/host-1@TEST{code}, but one is able to do {code}kinit zookeeper@TEST -S ZOOKEEPER/host-1@TEST{code} to obtain a service ticket. 

In the Kafka project, it is fine for the JAAS file to have {code}principal=""kafka@TEST""{code}, and automatically it seems it acquires the correct service ticket (I""m not sure how).

In zookeeper, things fail when a client tries to connect, due to this line:
https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/util/SecurityUtils.java#L170

It'd be great for Zookeeper server to have the same kind of mechanism as Kafka for accepting client connections. This would allow us to have {code}principal=""zookeeper@TEST""{code} in JAAS. Otherwise, maybe support a JAAS new option so we can explicitly name the service ?

FYI - trying {code}principal=""zookeeper/host-1@TEST""{code} does not work as due to how Active Directory works, it complains that the credentials cannot be found in the database (as we try to authenticate using the service name, not the user name)

I'm attaching some documentation I find relevant: https://serverfault.com/questions/682374/client-not-found-in-kerberos-database-while-getting-initial/683058#683058 
",[],Bug,ZOOKEEPER-3206,Major,Stephane Maarek,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Can't use Active Directory for Kerberos Authentication,2018-12-03T14:24:20.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",0.0
,[],2018-11-24T07:35:38.000+0000,Ankit Kothana,"We are using Zookeeper in our system along with Apache Kafka. However, Zookeeper is not producing any relevant logs (even with lower log levels specified in log4j.properties) in the log file that could help us in identifying what is currently going on in ZK or Kafka cluster. 

Please let us know how to retrieve proper logs from ZK cluster.

Version of ZK : 3.4.13",[],Bug,ZOOKEEPER-3199,Major,Ankit Kothana,Fixed,2018-11-26T09:22:48.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Unable to produce verbose logs of Zookeeper,2018-11-26T09:22:48.000+0000,[],1.0
,"[<JIRA Component: name='c client', id='12312380'>]",2018-11-16T01:16:17.000+0000,Jian Wang,"In the zoo_amulti function (zookeeper.c) , it seems an initialization problem.
{code:java}
struct RequestHeader h = { STRUCT_INITIALIZER(xid, get_xid()), STRUCT_INITIALIZER(type, ZOO_MULTI_OP) };
struct MultiHeader mh = { STRUCT_INITIALIZER(type, -1), STRUCT_INITIALIZER(done, 1), STRUCT_INITIALIZER(err, -1) };
struct oarchive *oa = create_buffer_oarchive();
completion_head_t clist = { 0 };
{code}
variable ""clist"" 's member cond and lock are not initialized correctly. They should be initialized by pthread_cond_init and pthread_mutex_init. Otherwise zoo_amulti would crash when queue_completion was called witch calls pthread_cond_boardcast using clist->cond",[],Bug,ZOOKEEPER-3192,Major,Jian Wang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zoo_multi/zoo_amulti crash,2021-01-06T10:57:53.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",2.0
,[],2018-11-13T07:31:29.000+0000,Ted Dunning," 

There are simultaneous efforts ongoing to support network resilience (3188, blocking this issue) and a new configuration syntax (3166, also blocking this issue) being worked on simultaneously.

This issue captures the fact that the new syntax will need to be supported by the network resilience code, but both features are pre-requisites for that support.

 ",[],Bug,ZOOKEEPER-3189,Major,Ted Dunning,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Support new configuration syntax for resilient network feature,2018-11-13T07:32:29.000+0000,[],1.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2018-11-09T06:27:03.000+0000,cheng pan,"the code given in the documentation
{code:java}
while (true) {
    synchronized (mutex) {
        List<String> list = zk.getChildren(root, true);
        if (list.size() < size) {
            mutex.wait();
        } else {
            return true;
        }
    }
}
{code}
When some nodes are not ready, the code calls mutex.wait() and waits for the watcher event to call mutex.notify() to wake it up. The problem is, we can't guarantee that mutex.notify() will definitely happen after mutex.wait(), which will cause client is stuck.
The solution might be CountDownLatch?",[],Bug,ZOOKEEPER-3186,Major,cheng pan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,bug in barrier example code,2018-11-22T02:06:36.000+0000,[],3.0
,"[<JIRA Component: name='security', id='12329414'>, <JIRA Component: name='server', id='12312382'>]",2018-11-01T08:45:35.000+0000,ZHU CHONG,"1、

Modify configuration file zoo.cfg，set  skipACL=yes.

2、

create  /test  null digest:test:ooOS6Ac+VQuWIVe96Ts+Phqg0LU=:cdrwa 

123 is password ,ooOS6Ac+VQuWIVe96Ts+Phqg0LU= is ciphertext

3、

getAcl /test
 'auth,'
 : cdrwa

4、

Modify configuration file zoo.cfg，set  skipACL=no.

5、

addauth  digest test:123

6、

get /test

Authentication is not valid : /test",[],Bug,ZOOKEEPER-3185,Major,ZHU CHONG,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"After the skipACL flag is opened, the acl of the created node becomes 'auth,'. This will cause the node to be unreadable after closing the skipACL.",2020-03-31T10:33:22.000+0000,"[<JIRA Version: name='3.4.12', id='12342040'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2018-10-25T13:41:10.000+0000,Andor Molnar,"This issue is probably introduced by ZOOKEEPER-2024 where 2 seperate queues have been implemented in CommitProcessor to improve performance. [~abrahamfine] 's analysis is accurate on GitHub: https://github.com/apache/zookeeper/pull/300

He was trying to introduce synchronization between Learner.syncWithLeader() and CommitProcessor to wait for in-flight requests to be committed before accepting client requests.

In the affected unit test ({{testNodeDataChanged}}) there's a race between reconnecting client's setWatches request and updates coming from the leader according to the following logs:

{noformat}
2018-10-25 13:59:58,556 [myid:] - DEBUG [FollowerRequestProcessor:1:CommitProcessor@424] - Processing request:: sessionid:0x10005d8fc4d0000 type:setWatches cxid:0x3 zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2018-10-25 13:59:58,556 [myid:] - DEBUG [CommitProcWorkThread-1:FinalRequestProcessor@91] - Processing request:: sessionid:0x10005d8fc4d0000 type:setWatches cxid:0x3 zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
...
2018-10-25 13:59:58,557 [myid:] - DEBUG [CommitProcWorkThread-1:FinalRequestProcessor@91] - Processing request:: sessionid:0x20005d8f8a40000 type:delete cxid:0x1 zxid:0x100000004 txntype:2 reqpath:n/a
...
2018-10-25 13:59:58,561 [myid:] - DEBUG [CommitProcWorkThread-1:FinalRequestProcessor@91] - Processing request:: sessionid:0x20005d8f8a40000 type:create cxid:0x2 zxid:0x100000005 txntype:1 reqpath:n/a
2018-10-25 13:59:58,561 [myid:127.0.0.1:11231] - DEBUG [main-SendThread(127.0.0.1:11231):ClientCnxn$SendThread@864] - Got WatchedEvent state:SyncConnected type:NodeDeleted path:/test-changed for sessionid 0x10005d8fc4d0000
{noformat}

{{setWatches}} request is processed before {{delete}} and {{create}}, hence the client receives NodeDeleted event.

In the working scenario it looks like:

{noformat}
2018-10-25 14:04:55,247 [myid:] - DEBUG [CommitProcWorkThread-1:FinalRequestProcessor@91] - Processing request:: sessionid:0x20005dd88110000 type:delete cxid:
0x1 zxid:0x100000004 txntype:2 reqpath:n/a
2018-10-25 14:04:55,249 [myid:] - DEBUG [CommitProcWorkThread-1:FinalRequestProcessor@91] - Processing request:: sessionid:0x20005dd88110000 type:create cxid:
0x2 zxid:0x100000005 txntype:1 reqpath:n/a
...
2018-10-25 14:04:56,314 [myid:] - DEBUG [FollowerRequestProcessor:1:CommitProcessor@424] - Processing request:: sessionid:0x10005dd88110000 type:setWatches cxid:0x3 zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2018-10-25 14:04:56,315 [myid:] - DEBUG [CommitProcWorkThread-1:FinalRequestProcessor@91] - Processing request:: sessionid:0x10005dd88110000 type:setWatches cxid:0x3 zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
...
2018-10-25 14:04:56,316 [myid:127.0.0.1:11231] - DEBUG [main-SendThread(127.0.0.1:11231):ClientCnxn$SendThread@842] - Got notification sessionid:0x10005dd88110000
2018-10-25 14:04:56,316 [myid:127.0.0.1:11231] - DEBUG [main-SendThread(127.0.0.1:11231):ClientCnxn$SendThread@864] - Got WatchedEvent state:SyncConnected type:NodeDataChanged path:/test-changed for sessionid 0x10005dd88110000
{noformat}

{{delete}} and {{create}} requests happen way before {{setWatches}} comes in (even before the client connection is established) and client receives NodeDataChanged event only.

Abe's approach unfortunately raises the following concerns:
- modifies CommitProcessor's code which might affect performance and correctness ([~shralex] raised on ZOOKEEPER-2807),
- we experienced deadlocks while testing the patch: https://github.com/apache/zookeeper/pull/300

As a consequence I raised this Jira to capture the experiences and to put the unit test on Ignore list, because currently I'm not sure about whether this is a real issue or a non-backward compatible change in 3.6 with the gain of a huge performance improvement.

Either way I don't want this flaky test to influence contributions, so I'll mark as Ignored on trunk until the issue is resolved.",[],Bug,ZOOKEEPER-3182,Critical,Andor Molnar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Race condition when follower syncing with leader and starting to serve requests,2019-10-04T14:55:14.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",6.0
,[],2018-10-24T04:00:39.000+0000,Akira Ajisaka,"ZOOKEEPER-2355 added a getQuorumPeer method to QuorumPeerMain [https://github.com/apache/zookeeper/blob/release-3.5.3/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeerMain.java#L194]. TestingQuorumPeerMain has an identically named method, which is now unintentionally overridding the one in the base class.

This is fixed by CURATOR-409, however, I'd like this to be fixed in ZooKeeper as well.",[],Bug,ZOOKEEPER-3181,Major,Akira Ajisaka,Not A Problem,2018-11-28T06:27:45.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZOOKEEPER-2355 broke Curator TestingQuorumPeerMain,2018-11-28T06:27:45.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.4.11', id='12339207'>]",3.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2018-10-16T15:24:18.000+0000,Fangmin Lv,"For some reason, the ReadOnlyZooKeeperServer was implemented with PrepRequestProcessor, which is meaningless and error-prone, since all it does is preparing txn, and we shouldn't allow txns being created on non-leader server.
 
This will cause dangling global session on RO observer, because the createSession is being generated, and the code thought it's global session and added to Snapshot.
 ",[],Bug,ZOOKEEPER-3178,Major,Fangmin Lv,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Remove PrepRequestProcessor from RO ZooKeeperServer to avoid txns being created in RO mode,2022-02-03T08:50:27.000+0000,[],1.0
Andor Molnar,"[<JIRA Component: name='tests', id='12312427'>]",2018-10-12T11:58:02.000+0000,Andor Molnar,"*Error Message*

Failed to create TrustManager

*Stacktrace*

org.apache.zookeeper.common.X509Exception$SSLContextException: Failed to create TrustManager
	at org.apache.zookeeper.common.X509Util.createSSLContext(X509Util.java:210)
	at org.apache.zookeeper.common.X509Util.createSSLContext(X509Util.java:163)
	at org.apache.zookeeper.common.X509Util.getDefaultSSLContext(X509Util.java:147)
	at org.apache.zookeeper.common.X509UtilTest.testCreateSSLContextWithoutTrustStorePassword(X509UtilTest.java:184)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:79)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:844)
Caused by: org.apache.zookeeper.common.X509Exception$TrustManagerException: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty
	at org.apache.zookeeper.common.X509Util.createTrustManager(X509Util.java:299)
	at org.apache.zookeeper.common.X509Util.createSSLContext(X509Util.java:207)
Caused by: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty
	at java.base/java.security.cert.PKIXParameters.setTrustAnchors(PKIXParameters.java:200)
	at java.base/java.security.cert.PKIXParameters.<init>(PKIXParameters.java:157)
	at java.base/java.security.cert.PKIXBuilderParameters.<init>(PKIXBuilderParameters.java:130)
	at org.apache.zookeeper.common.X509Util.createTrustManager(X509Util.java:274)","[<JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-3165,Major,Andor Molnar,Fixed,2018-10-16T09:57:33.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Java 9: X509UtilTest.testCreateSSLContextWithoutTrustStorePassword fails,2019-05-20T17:50:49.000+0000,"[<JIRA Version: name='3.5.5', id='12343268'>]",2.0
Andrea Reale,"[<JIRA Component: name='c client', id='12312380'>]",2018-10-09T15:18:00.000+0000,Andrea Reale,"As reported (but never fixed) in the past by ZOOKEEPER-2409, ZOOKEEPER-2038 and (partly) ZOOKEEPER-2878, the C client lock-recipe implementation is broken.

I identified three issues.

The main one (as also reported in the aforementioned reports) is that the logic that goes through the lock waiting list is broken. child_floor uses strcmp and compares the full node name (i.e., sessionID-sequence) rather than only comparing the sequence number. This makes it possible for two different clients to hold the lock at the same time: assume two clients, one associated with session A, the other with session B, with A < B lexicographically. Now assume that at some point a thread in B holds a lock and a thread in A tries to acquire the same lock. A will manage to get the lock because of the wrong comparison function, so now two guys hold the lock.

The second issue is a possible deadlock inside zkr_lock_operation. zkr_lock_operation is always called by holding the mutex associated to the client lock. In some cases, zkr_lock_operaton may decide to give-up locking and call zkr_lock_unlock to release the lock. When this happens, it will try to acquire again the same phtread mutex, which will lead to a deadlock.

The third issue relates to the return value of zkr_lock_lock. According to the API docs, the functions returns 0 when no errors. Then it is up to the invoker to check when the lock is held by calling zkr_lock_isowner. However, the implementation, in case of no error, returns zkr_lock_isowner. This is wrong because it becomes impossible to distinguish an error condition from a success (but not ownerhsip). Instead the API (as described in the docs, btw) should return always 0 when no errors occur.

Shortly I will add the link to a PR fixing the issues.

 ","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.4.14', id='12343587'>]",Bug,ZOOKEEPER-3162,Major,Andrea Reale,Fixed,2018-11-12T22:21:18.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Broken lock semantics in C client lock-recipe,2019-04-02T10:40:10.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.4.13', id='12342973'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2018-09-29T03:47:29.000+0000,Ling Mao,"look at the source code in the ClientCnxnSocketNetty.connect(InetSocketAddress):

{code:java}
public void operationComplete(ChannelFuture channelFuture) throws Exception {
            // this lock guarantees that channel won't be assgined after cleanup().
            connectLock.lock();
                try {
                    //----------------------
                    sendThread.primeConnection();
                    //-----------------------
                    firstConnect.countDown();
                    LOG.info(""channel is connected: {}"", channelFuture.getChannel());
                } finally {
                    connectLock.unlock();
                }
            }
 });
{code}

firstConnect.countDown() will not be executed where sendThread.primeConnection() has thrown an exception,it should be put into finally code block. ",[],Bug,ZOOKEEPER-3158,Trivial,Ling Mao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,firstConnect.countDown() will not be executed where sendThread.primeConnection() has thrown an exception,2019-02-19T10:49:14.000+0000,[],1.0
Robert Joseph Evans,"[<JIRA Component: name='java client', id='12312381'>]",2018-09-26T17:20:50.000+0000,Robert Joseph Evans,"Prior to ZOOKEEPER-2184 the zookeeper client would canonicalize a configured host name before creating the SASL client which is used to create the principal name.  After ZOOKEEPER-2184 that canonicalization does not happen so the principal that the ZK client tries to use when it is configured to talk to a CName is different between 3.4.13 and all previous versions of ZK.

 

For example

 

zk1.mycluster.mycompany.com maps to real-node.mycompany.com.

 

3.4.13 will want the server to have [zookeeper/zk1.mycluster.com@KDC.MYCOMPANY.COM|mailto:zookeeper/zk1.mycluster.com@KDC.MYCOMPANY.COM]

3.4.12 wants the server to have [zookeeper/real-node.mycompany.com@KDC.MYCOMPANY.COM|mailto:zookeeper/real-node.mycompany.com@KDC.MYCOMPANY.COM]

 

This makes 3.4.13 incompatible with many ZK setups currently in existence.  It would be nice to have that resolution be optional because in some cases it might be nice to have a single principal tied to the cname.","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.4.14', id='12343587'>]",Bug,ZOOKEEPER-3156,Blocker,Robert Joseph Evans,Fixed,2018-11-05T18:42:05.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZOOKEEPER-2184 causes kerberos principal to not have resolved host name,2019-10-04T14:55:12.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.13', id='12342973'>, <JIRA Version: name='3.5.5', id='12343268'>]",6.0
Michael Han,"[<JIRA Component: name='build-infrastructure', id='12333105'>]",2018-09-19T18:37:54.000+0000,Michael Han,"When trigger a precommit check Jenkins job directly through the [web interface|https://builds.apache.org/job/PreCommit-ZOOKEEPER-github-pr-build/] , the result can't be relayed back on github, after the job finished. ",[],Bug,ZOOKEEPER-3151,Minor,Michael Han,Workaround,2018-09-19T23:04:15.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Jenkins github integration is broken if retriggering the precommit job through Jenkins admin web page.,2020-01-06T20:08:18.000+0000,[],3.0
,"[<JIRA Component: name='leaderElection', id='12312378'>]",2018-09-14T18:14:52.000+0000,Andrew January,"Steps to reproduce:
 # Have a 3 node cluster set up, with node 2 as the leader, and node 3 zxid ahead of node 1 such that node 3 will be the new leader when node 2 disappears.
 # Shut down node 2 such that it is unreachable and attempts to connect to it yield a socket timeout.
 # Have the remaining two nodes get ""Connection refused"" responses almost immediately if one tries to connect to the other on a port that isn't open.

Expected behaviour:

The remaining nodes reach quorum.

Actual behaviour:

The remaining nodes repeatedly fail to reach quorum, spinning and holding elections until node 2 is brought back.

 

This is because:
 # An election for a new leader starts.
 # Both nodes broadcast notifications to all the other nodes
 # The notifications are sent to node 1 quickly, then it tries to send it to node 2, which takes cnxTimeout (default 5s) before timing out, then sends it to node 3. This results in all the notifications to node 3 taking 5 seconds to arrive.
 # Despite the delays, node 1 and node 3 agree that node 3 should be leader.
 # node 1 sends the message that it will follow node 3, then immediately tries to connect to it as leader.
 # Because of the delay, node 3 hasn't yet received the notification that node 1 is following it, so doesn't start accepting requests.
 # This causes the requests from node 1 to fail quickly with ""Connection refused"".
 # It retries 5 times (pausing a second between each)
 # Because these connection refused are happening at 1/5th of cnxTimeout, node 1 gives up trying to follow node 3 and starts a new election.
 # Node 3 times out waiting for node 1 to acknowledge it as leader, and starts a new election.

 

We can work around the issue by decreasing cnxTimeout to be less than 5. However, it seems like a bad idea to rely on tweaking a value based on network performance, especially as the value is only configurable via JVM args rather than the conf files.",[],Bug,ZOOKEEPER-3149,Minor,Andrew January,Duplicate,2018-09-16T19:13:57.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Unreachable node can prevent remaining nodes from gaining quorum,2018-09-16T19:13:58.000+0000,"[<JIRA Version: name='3.4.12', id='12342040'>]",3.0
Enrico Olivelli,"[<JIRA Component: name='kerberos', id='12329415'>]",2018-09-14T15:01:30.000+0000,Enrico Olivelli,"Branch 3.4 uses Apache Directory Service for Kerberos tests, this is not compatibile with JDK 11.

A simple ""upgrade"" is not enough.

The fix is to port Kerby based tests from branch-3.5 to branch-3.4 and make old tests run *only on JDK6* and new tests with Kerby run on JDK7 onwards.

 

There will be some duplicated code, but branch-3.4 is expected to be sent in be deprecated soon, as 3.5 will be released as ""stabile"".

Those ""old"" test would be dropped in case we decide to drop JDK6 support.

 

Additionally JDK6 VMs cannot download dependencies from Maven Central due to SSL policies:

[ivy:retrieve]     Server access error at url https://repo1.maven.org/maven2/net/minidev/json-smart/ (javax.net.ssl.SSLException: Received fatal alert: protocol_version)
[ivy:retrieve]     Server access error at url https://repo1.maven.org/maven2/net/minidev/json-smart/ (javax.net.ssl.SSLException: Received fatal alert: protocol_version)

 

 

 ","[<JIRA Version: name='3.4.14', id='12343587'>]",Bug,ZOOKEEPER-3148,Critical,Enrico Olivelli,Fixed,2018-09-25T11:25:58.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Fix Kerberos tests on branch 3.4 and JDK11,2019-04-02T10:40:11.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",3.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2018-09-11T23:47:07.000+0000,Fangmin Lv,"This is another issue I found recently, we haven't seen this problem on prod (or maybe we don't notice).

 
Currently, the CloseSession is not idempotent, executing the CloseSession twice won't get the same result.
 
The problem is that closeSession will only check what's the ephemeral nodes associated with that session bases on current states. Nodes deleted during taking fuzzy snapshot won't be deleted again when replay the txn.
 
This looks fine, since it's already gone, but there is problem with the pzxid of the parent node. Snapshot is taken fuzzily, so it's possible that the parent had been serialized while the nodes are being deleted when executing the closeSession Txn. The pzxid will not be updated in the snapshot when replaying the closeSession txn, because doesn't know what's the paths being deleted, so it won't patch the pzxid like what we did in the deleteNode ZOOKEEPER-3125.
 
The inconsistent pzxid will lead to potential watch notification missing when client reconnect with setWatches because of the staleness. 
 
This JIRA is going to fix those issues by adding the CloseSessionTxn, it will record all those nodes being deleted in that CloseSession txn, so that we know which nodes to update when replaying the txn.","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3145,Critical,Fangmin Lv,Fixed,2019-09-11T08:27:50.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Potential watch missing issue due to stale pzxid when replaying CloseSession txn with fuzzy snapshot,2019-09-11T13:30:40.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.13', id='12342973'>]",3.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2018-09-11T23:14:52.000+0000,Fangmin Lv,"Found this issue recently when checking another prod issue, the problem is that the current code will update lastProcessedZxid before it's actually making change for the global sessions in the DataTree.
 
In case there is a snapshot taking in progress, and there is a small time stall between set lastProcessedZxid and update the session in DataTree due to reasons like thread context switch or GC, etc, then it's possible the lastProcessedZxid is actually set to the future which doesn't include the global session change (add or remove).
 
When reload this snapshot and it's txns, it will replay txns from lastProcessedZxid + 1, so it won't create the global session anymore, which could cause data inconsistent.
 
When global sessions are inconsistent, it might have ephemeral inconsistent as well, since the leader will delete all the ephemerals locally if there is no global sessions associated with it, and if someone have snapshot sync with it then that server will not have that ephemeral as well, but others will. It will also have global session renew issue for that problematic session.
 
The same issue exist for the closeSession txn, we need to move these global session update logic before processTxn, so the lastProcessedZxid will not miss the global session here.
 
 ","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3144,Critical,Fangmin Lv,Fixed,2018-09-14T22:08:43.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Potential ephemeral nodes inconsistent due to global session inconsistent with fuzzy snapshot,2018-09-14T22:55:16.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.13', id='12342973'>]",4.0
,"[<JIRA Component: name='server', id='12312382'>]",2018-09-07T09:32:36.000+0000,Nitish Kulkarni,"I am trying to run zookeeper version 2.11-1.1.0 on 32 bit machine. I have installed jdk 1.8.0_181 but zookeeper is not running and displaying following error:

Error: missing {{server' JVM at }}C:\Program Files (x86)\Java\jre1.8.0_181\bin\server\jvm.dll'.
Please install or use the JRE or JDK that contains these missing components.

This is because for jdk1.8.0_181 is not creating server folder which contains jvm.dll.

So please let me know how is zookeeper going to address this issue. Because if this issue is not resolved zookeeper won't run on 32 bit machine.",[],Bug,ZOOKEEPER-3139,Major,Nitish Kulkarni,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper is not getting started because server folder is not present in the jre/bin for version 1.8.0_181 on 32 bit machine,2018-09-19T12:22:06.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",2.0
,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='security', id='12329414'>]",2018-09-05T15:37:32.000+0000,Grzegorz Grzybek,"I'm in the process of reconfiguring the ensemble to use mutual quorum peer authentication using SASL (ZOOKEEPER-1045).

In order to understand the impact on my code, I've checked _how it works_. Now I'm running & debugging {{org.apache.zookeeper.server.quorum.auth.QuorumDigestAuthTest#testValidCredentials()}} test case.

I have now six threads (3 peers contacting each other):
* ""QuorumConnectionThread-[myid=0]-2@1483"" prio=5 tid=0x2b nid=NA runnable
* ""QuorumConnectionThread-[myid=0]-3@1491"" prio=5 tid=0x36 nid=NA runnable
* ""QuorumConnectionThread-[myid=1]-1@1481"" prio=5 tid=0x2d nid=NA runnable
* ""QuorumConnectionThread-[myid=1]-4@1505"" prio=5 tid=0x3c nid=NA runnable
* ""QuorumConnectionThread-[myid=2]-2@1495"" prio=5 tid=0x37 nid=NA runnable
* ""QuorumConnectionThread-[myid=2]-4@1506"" prio=5 tid=0x3d nid=NA runnable

at this point of invocation:
{noformat}
  java.lang.Thread.State: RUNNABLE
	  at org.apache.zookeeper.server.quorum.auth.SaslQuorumServerCallbackHandler.handleNameCallback(SaslQuorumServerCallbackHandler.java:101)
	  at org.apache.zookeeper.server.quorum.auth.SaslQuorumServerCallbackHandler.handle(SaslQuorumServerCallbackHandler.java:82)
	  at com.sun.security.sasl.digest.DigestMD5Server.validateClientResponse(DigestMD5Server.java:589)
	  at com.sun.security.sasl.digest.DigestMD5Server.evaluateResponse(DigestMD5Server.java:244)
	  at org.apache.zookeeper.server.quorum.auth.SaslQuorumAuthServer.authenticate(SaslQuorumAuthServer.java:100)
	  at org.apache.zookeeper.server.quorum.QuorumCnxManager.handleConnection(QuorumCnxManager.java:467)
	  at org.apache.zookeeper.server.quorum.QuorumCnxManager.receiveConnection(QuorumCnxManager.java:386)
	  at org.apache.zookeeper.server.quorum.QuorumCnxManager$QuorumConnectionReceiverThread.run(QuorumCnxManager.java:422)
	  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	  at java.lang.Thread.run(Thread.java:748)
{noformat}

which is this line:
{code:java}
private void handleNameCallback(NameCallback nc) {
    // check to see if this user is in the user password database.
    if (credentials.get(nc.getDefaultName()) == null) {
        LOG.warn(""User '{}' not found in list of DIGEST-MD5 authenticateable users."",
                nc.getDefaultName());
        return;
    }
    nc.setName(nc.getDefaultName());
    /* >>> */ userName = nc.getDefaultName(); /* <<< */
}
{code}

each pair of threads is operating on single instance of {{org.apache.zookeeper.server.quorum.auth.SaslQuorumServerCallbackHandler#userName}}. In the stack trace we have both shared and local variables/fields:
* {{o.a.z.server.quorum.QuorumCnxManager.QuorumConnectionReceiverThread#sock}} is thread-specific (ok)
* {{o.a.z.server.quorum.QuorumCnxManager#authServer}} is peer-specific (instance of {{o.a.z.server.quorum.auth.SaslQuorumAuthServer}}) but without a state that changes
* {{javax.security.sasl.SaslServer}} is thread-specific (ok) - this instance is created to handle sasl authentication, but is created using peer-specific JAAS subject (which is ok) and peer-specific {{o.a.z.server.quorum.auth.SaslQuorumAuthServer#serverLogin.callbackHadler}} {color:red}which is potentially a problem{color}

Each (out of six) thread handles different connection, but each pair (for given QuorumPeer) calls {{o.a.z.server.quorum.auth.SaslQuorumServerCallbackHandler#handleNameCallback()}} which modifies shared (peer-specific) field - {{userName}}.

I understand that [according to the example from Wiki|https://cwiki.apache.org/confluence/display/ZOOKEEPER/Server-Server+mutual+authentication] all peers may use the same credentials (in simplest case).

But the ""userName"" comes from data sent by each peer, like this:
{noformat}
charset=utf-8,\
username=""test"",\
realm=""zk-quorum-sasl-md5"",\
nonce=""iBqYWtaCrEE013S6Dv6xiOsR9uX2l/qKZcEZ1pm2"",\
nc=00000001,\
cnonce=""LVaL9XYFjNxVBPCjPewXjEBsj9GuwIfBN/RXsKt5"",\
digest-uri=""zookeeper-quorum/zk-quorum-sasl-md5"",\
maxbuf=65536,\
response=dd4e9e2115ec2e304484d5191f3fc771,\
qop=auth,\
authzid=""test""
{noformat}

*And I can imagine such JAAS configuration for DIGEST-MD5 SASL algorithm, that each peer uses own credentials and is able to validate other peers' specific credentials.*:
{noformat}
QuorumServer {
       org.apache.zookeeper.server.auth.DigestLoginModule required
       user_peer1=""peer1"";
       user_peer2=""peer2"";
       user_peer3=""peer3"";
};
QuorumLearner1 {
       org.apache.zookeeper.server.auth.DigestLoginModule required
       username=""peer1""
       password=""peer1"";
};
QuorumLearner2 {
       org.apache.zookeeper.server.auth.DigestLoginModule required
       username=""peer2""
       password=""peer2"";
};
QuorumLearner2 {
       org.apache.zookeeper.server.auth.DigestLoginModule required
       username=""peer3""
       password=""peer3"";
};
{noformat}

Isn't it a race condition? Like this (having 3 peers):
||thread handling peer 2 → peer 1 connection||thread handling peer 3 → peer 1 connection||
|sets o.a.z.s.q.auth.SaslQuorumServerCallbackHandler#userName to ""peer2""| |
| |sets o.a.z.s.q.auth.SaslQuorumServerCallbackHandler#userName to ""peer3""|
|sets PasswordCallback.password to o.a.z.s.q.auth.SaslQuorumServerCallbackHandler#credentials.get(""peer3"")| |
| | continues ...|
|com.sun.security.sasl.digest.DigestMD5Base#generateResponseValue() generates expected response using:
* username: ""peer2""
* password of user ""peer3""| |

Please verify.",[],Bug,ZOOKEEPER-3138,Major,Grzegorz Grzybek,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Potential race condition with Quorum Peer mutual authentication via SASL,2018-09-06T06:36:01.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",4.0
,[],2018-08-30T02:58:18.000+0000,yangkun,"ClientCnxnSocket#updateLastSendAndHeard() method update lastSend、lastHeard to now:

 
{code:java}
void updateLastSendAndHeard() {
    this.lastSend = now;
    this.lastHeard = now;
}

void updateNow() {
    now = Time.currentElapsedTime();
}{code}
In SendThread#run() method, there are some place call updateLastSendAndHeard() method, simplified as follows:

 
{code:java}
public void run() {
    clientCnxnSocket.updateNow();
    // place-1. update lastSend、lastHeard
    clientCnxnSocket.updateLastSendAndHeard();
    while (state.isAlive()) {
        try {
        // ...some operation
        startConnect(serverAddress);
        // place-2. update lastSend、lastHeard
        clientCnxnSocket.updateLastSendAndHeard();
    }
}{code}
 

If so, place-1 and place-2, the lastSend、lastHeard value is equals, However, between place-1 and place-2 has some operation,consume some time,it should actually be unequal.

 

 

 

 ",[],Bug,ZOOKEEPER-3135,Minor,yangkun,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"update lastSend, lastHeard with current timestamp when client reconnects successfully",2020-02-21T06:49:49.000+0000,[],3.0
Fangmin Lv,[],2018-08-28T23:52:19.000+0000,Fangmin Lv,"The outstandingRequests is being decreased when we send the response for sasl request, but it's never increased before we handle it, so it might cause mis-counted outstandingRequests, and might enable receive packets from that socket before it should be.",[],Bug,ZOOKEEPER-3133,Major,Fangmin Lv,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,NIOServerCnxn.outstandingRequests is not updated correctly with sasl request,2022-02-03T08:50:23.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2018-08-28T08:33:01.000+0000,ChaoWang,"In some cases, the variable _watch2Paths_ in _Class WatchManager_ does not remove the entry, even if the associated value ""HashSet"" is empty already. 

The type of key in Map _watch2Paths_ is Watcher, instance of _NettyServerCnxn._ If it is not removed when the associated set of paths is empty, it will cause the memory increases little by little, and OutOfMemoryError triggered finally. 

 

In the following function, the logic should be added to remove the entry.

org.apache.zookeeper.server.WatchManager#removeWatcher(java.lang.String, org.apache.zookeeper.Watcher)

if (paths.isEmpty()) {
 watch2Paths.remove(watcher);
}

For the following function as well:

org.apache.zookeeper.server.WatchManager#triggerWatch(java.lang.String, org.apache.zookeeper.Watcher.Event.EventType, java.util.Set<org.apache.zookeeper.Watcher>)

 

Please confirm this issue?",[],Bug,ZOOKEEPER-3132,Major,ChaoWang,Duplicate,2018-08-28T08:40:44.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,org.apache.zookeeper.server.WatchManager resource leak,2018-08-28T08:40:44.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.5.4', id='12340141'>]",1.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2018-08-28T08:32:41.000+0000,ChaoWang,"In some cases, the variable _watch2Paths_ in _Class WatchManager_ does not remove the entry, even if the associated value ""HashSet"" is empty already. 

The type of key in Map _watch2Paths_ is Watcher, instance of _NettyServerCnxn._ If it is not removed when the associated set of paths is empty, it will cause the memory increases little by little, and OutOfMemoryError triggered finally. 

 

{color:#FF0000}*Possible Solution:*{color}

In the following function, the logic should be added to remove the entry.

org.apache.zookeeper.server.WatchManager#removeWatcher(java.lang.String, org.apache.zookeeper.Watcher)

if (paths.isEmpty())

{ watch2Paths.remove(watcher); }

For the following function as well:

org.apache.zookeeper.server.WatchManager#triggerWatch(java.lang.String, org.apache.zookeeper.Watcher.Event.EventType, java.util.Set<org.apache.zookeeper.Watcher>)

 

Please confirm this issue?","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-3131,Major,ChaoWang,Fixed,2018-09-07T00:35:17.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,org.apache.zookeeper.server.WatchManager resource leak,2019-05-20T17:50:38.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",6.0
Mohammad Arshad,"[<JIRA Component: name='server', id='12312382'>]",2018-08-24T07:02:55.000+0000,Mohammad Arshad,"CLI Get Command display ""org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /b"" when user does not have read access on the znode /b.

Steps to reproduce the bug:
{noformat}
[zk: vm1:2181(CONNECTED) 1] create /b
Created /b
[zk: vm1:2181(CONNECTED) 2] getAcl /b
'world,'anyone
: cdrwa
[zk: vm1:2181(CONNECTED) 3] setAcl /b world:anyone:wa
[zk: vm1:2181(CONNECTED) 4] getAcl /b
'world,'anyone
: wa
[zk: vm1:2181(CONNECTED) 5] get /b
org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /b
[zk: vm1:2181(CONNECTED) 6]
{noformat}

Expected output:
{noformat}
[zk: vm1:2181(CONNECTED) 0] get /b
Insufficient permission : /b
{noformat}
 

 ","[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.8.0', id='12349587'>, <JIRA Version: name='3.7.1', id='12350030'>]",Bug,ZOOKEEPER-3128,Minor,Mohammad Arshad,Fixed,2021-03-28T04:24:42.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Get CLI Command displays Authentication error for Authorization error,2021-03-28T04:29:46.000+0000,[],3.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2018-08-23T06:36:59.000+0000,Fangmin Lv,"Found this issue while checking the code for another issue, this is a relatively rare case which we haven't seen it on prod so far.

Currently, the lastProcessedZxid is updated when applying the first txn of multi-op, if there is a snapshot in progress, it's possible that the zxid associated with the snapshot only include partial of the multi op.

When loading snapshot, it will only load the txns after the zxid associated with snapshot file, which could data inconsistency due to missing sub txns.

To avoid this, we only update the lastProcessedZxid when the whole multi-op txn is applied to DataTree.","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-3127,Critical,Fangmin Lv,Fixed,2018-09-05T20:36:06.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Fixing potential data inconsistency due to update last processed zxid with partial multi-op txn,2019-10-04T14:55:12.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.13', id='12342973'>]",5.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2018-08-21T11:15:43.000+0000,Pritish Kapoor,"Documentation: Overview page [https://zookeeper.apache.org/doc/current/zookeeperOver.html] has missing content marked as _[tbd]_

Refer section ""Nodes and ephemeral nodes"" - Last line:

""Ephemeral nodes are useful when you want to implement _[tbd]_.""

Similarly, many other lines are present with ""_[tbd]_""",[],Bug,ZOOKEEPER-3126,Minor,Pritish Kapoor,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Documentation: Overview page has missing content,2018-08-29T08:19:16.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",3.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2018-08-20T20:13:37.000+0000,Fangmin Lv,"When taking snapshot or syncing snapshot from leader, it's having fuzzy snapshot, which means the parent node might already serialized before the child get deleted, during replay the txn it will skip update the parent pzxid in this case, which will cause inconsistency.","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-3125,Blocker,Fangmin Lv,Fixed,2018-11-08T18:56:26.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Pzxid inconsistent issue when replaying a txn for a deleted node,2019-05-20T17:51:08.000+0000,[],4.0
,[],2018-08-15T08:19:48.000+0000,pacawat k,,[],Bug,ZOOKEEPER-3121,Major,pacawat k,Fixed,2018-08-15T08:20:15.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,DELETE - test,2018-08-21T16:53:13.000+0000,[],2.0
,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='jmx', id='12312451'>, <JIRA Component: name='kerberos', id='12329415'>]",2018-08-14T04:51:51.000+0000,pacawat k,test,[],Bug,ZOOKEEPER-3119,Major,pacawat k,Fixed,2018-08-15T08:12:15.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,DELETE - test,2018-08-21T16:53:34.000+0000,[],3.0
,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='jmx', id='12312451'>]",2018-08-13T10:34:18.000+0000,pacawat k,test,[],Bug,ZOOKEEPER-3118,Major,pacawat k,Invalid,2019-08-27T13:57:46.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,DELETE - test,2019-08-27T13:57:47.000+0000,[],1.0
Fangmin Lv,"[<JIRA Component: name='quorum', id='12312379'>]",2018-08-11T20:58:42.000+0000,Fangmin Lv,"The LeaderBean.followerInfo are returning all the learners, which includes the observers, it's not only followers, correct it to match the name.","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3117,Minor,Fangmin Lv,Fixed,2018-08-17T12:40:21.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0, Correct the LeaderBean.followerInfo to only return the followers list,2018-08-17T13:40:21.000+0000,[],3.0
Andor Molnar,"[<JIRA Component: name='server', id='12312382'>]",2018-08-07T14:58:13.000+0000,Andor Molnar,"EphemeralTypeTest.testServerIds() unit test fails on some systems that System.nanoTime() is smaller than a certain value.

The test generates ephemeralOwner in the old way (pre ZOOKEEPER-2901) without enabling the emulation flag and asserts for exception to be thrown when serverId == 255. This is right. ZooKeeper should fail on this case, because serverId cannot be larger than 254 if extended types are enabled. In this case ephemeralOwner with 0xff in the most significant byte indicates an extended type.

The logic which does the validation is in EphemeralType.get().

It checks 2 things:
 * the extended type byte is set: 0xff,
 * reserved bits (next 2 bytes) corresponds to a valid extended type.

Here is the problem: currently we only have 1 extended type: TTL with value of 0x0000 in the reserved bits.

Logic expects that if we have anything different from it in the reserved bits, the ephemeralOwner is invalid and exception should be thrown. That's what the test asserts for and it works on most systems, because the timestamp part of the sessionId usually have some bits in the reserved bits as well which eventually will be larger than 0, so the value is unsupported.

I think the problem is twofold:
 * Either if we have more extended types, we'll increase the possibility that this logic will accept invalid sessionIds (as long as reserved bits indicate a valid extended type),
 * Or (which happens on some systems) if the currentElapsedTime (timestamp part of sessionId) is small enough and doesn't occupy reserved bits, this logic will accept the invalid sessionId.

Unfortunately I cannot repro the problem yet: it constantly happens on a specific Jenkins slave, but even with the same distro and same JDK version I cannot reproduce the same nanoTime() values.","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-3113,Critical,Andor Molnar,Fixed,2018-10-18T09:19:01.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,EphemeralType.get() fails to verify ephemeralOwner when currentElapsedTime() is small enough,2019-10-04T14:55:12.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",5.0
,"[<JIRA Component: name='java client', id='12312381'>]",2018-08-07T00:46:44.000+0000,Tianzhou Wang,"if connecting domain fail to resolve and lead an UnresolvedAddressException, it would leak the fd.","[<JIRA Version: name='3.5.9', id='12348201'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.2', id='12347809'>]",Bug,ZOOKEEPER-3112,Critical,Tianzhou Wang,Fixed,2020-07-28T08:42:39.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,fd leak due to UnresolvedAddressException on connect.,2020-09-10T10:43:41.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.4.13', id='12342973'>]",6.0
,"[<JIRA Component: name='contrib', id='12312700'>]",2018-07-26T12:53:06.000+0000,yang hao,"when creating a node using python3,  InvalidACLException occurs all the time. it`s caused by imcompatible way of parsing acl passed through python3 api.","[<JIRA Version: name='3.4.15', id='12344988'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.6', id='12345243'>]",Bug,ZOOKEEPER-3105,Major,yang hao,Fixed,2018-07-30T01:22:55.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Character coding problem occur when create a node using python3,2019-10-16T18:59:03.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.14', id='12343587'>]",7.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2018-07-25T16:27:32.000+0000,Fangmin Lv,"Currently, in SNAP sync, the leader will start queuing the proposal/commits and the NEWLEADER packet before sending over the snapshot over wire. So it's possible that the zxid associated with the snapshot might be higher than all the packets queued before NEWLEADER.
 
When the follower received the snapshot, it will apply all the txns queued before NEWLEADER, which may not cover all the txns up to the zxid in the snapshot. After that, it will write the snapshot out to disk with the zxid associated with the snapshot. In case the server crashed after writing this out, when loading the data from disk, it will use zxid of the snapshot file to sync with leader, and it could cause data inconsistent, because we only replayed partial of the historical data during previous syncing.
 
NEWLEADER packet means the learner now has the correct and almost up to data state as leader, so it makes more sense to move the NEWLEADER packet after sending over snapshot, and this is what we did in the fix.
 
Besides this, the socket timeout is changed to use smaller sync timeout after received NEWLEADER ack, in high write traffic ensembles with large snapshot, the follower might be timed out by leader before finishing sending over those queued txns after writing snapshot out, which could cause the follower staying in syncing state forever. Move the NEWLEADER packet after sending over snapshot can avoid this issue as well.","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-3104,Critical,Fangmin Lv,Fixed,2018-08-03T16:53:19.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Potential data inconsistency due to NEWLEADER packet being sent too early during SNAP sync,2019-12-09T21:24:41.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.13', id='12342973'>]",13.0
,"[<JIRA Component: name='server', id='12312382'>]",2018-07-24T06:08:12.000+0000,LuoFucong,"The method 
{code:java}
public void createNode(final String path, byte data[], List<ACL> acl, long ephemeralOwner, int parentCVersion, long zxid, long time, Stat outputStat)
{code}
 

in class DataTree may conceal a potential race condition regarding the session ephemeral nodes map ""Map<Long, HashSet<String>> ephemerals"".

Specifically, the codes start from line 455:

 
{code:java}
} else if (ephemeralOwner != 0) {
    HashSet<String> list = ephemerals.get(ephemeralOwner);
    if (list == null) {
        list = new HashSet<String>();
        ephemerals.put(ephemeralOwner, list);
    }
    synchronized (list) {
        list.add(path);
    }
}{code}
 

When an ephemeral owner tries to create nodes concurrently (under different parent nodes), an empty ""HashSet<String>"" might be created multiple times, and replace each other.

The following unit test reveals the race condition:

 
{code:java}
@Test(timeout = 60000)
public void testSessionEphemeralNodesConcurrentlyCreated()
        throws InterruptedException, NodeExistsException, NoNodeException {
    long session = 0x1234;
    int concurrent = 10;
    Thread[] threads = new Thread[concurrent];
    CountDownLatch latch = new CountDownLatch(1);
    for (int i = 0; i < concurrent; i++) {
        String parent = ""/test"" + i;
        dt.createNode(parent, new byte[0], null, 0, -1, 1, 1);

        Thread thread = new Thread(() -> {
            try {
                latch.await();
            } catch (InterruptedException e) {
                throw new RuntimeException(e);
            }

            String path = parent + ""/0"";
            try {
                dt.createNode(path, new byte[0], null, session, -1, 1, 1);
            } catch (Exception e) {
                throw new IllegalStateException(e);
            }
        });
        thread.start();
        threads[i] = thread;
    }
    latch.countDown();
    for (Thread thread : threads) {
        thread.join();
    }
    int sessionEphemerals = dt.getEphemerals(session).size();
    Assert.assertEquals(concurrent, sessionEphemerals);
}
{code}
The session ""0x1234"" has created 10 ephemeral nodes ""/test\{0~9}/0"" concurrently (in 10 threads), so its ephemeral nodes size retrieved from DataTree should be 10 while doesn't (assertion fail).

 

The fix should be easy:

 
{code:java}
private final ConcurrentMap<Long, HashSet<String>> ephemerals = new ConcurrentHashMap<>();

...

} else if (ephemeralOwner != 0) {
    HashSet<String> list = ephemerals.get(ephemeralOwner);
    if (list == null) {
        list = new HashSet<String>();
        HashSet<String> _list;
        if ((_list = ephemerals.putIfAbsent(ephemeralOwner, list)) != null) {
            list = _list;
        }
    }
    synchronized (list) {
        list.add(path);
    }
}
{code}
 

 

 ",[],Bug,ZOOKEEPER-3102,Minor,LuoFucong,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Potential race condition when create ephemeral nodes,2020-03-28T15:22:56.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",4.0
Andor Molnar,"[<JIRA Component: name='java client', id='12312381'>]",2018-07-23T14:13:21.000+0000,Rajini Sivaram,"The changes to ZooKeeper clients to re-resolve hosts made under ZOOKEEPER-2184 results in delays when only a subset of the addresses that a host resolves to are actually reachable. This can result in connection timeouts on the client.

For example, when running tests with a single ZooKeeper server accepting connections on 127.0.0.1 on a host that has both IPv4 and IPv6, we have seen connection timeouts in tests if client connects using `localhost` rather than `127.0.0.1`. ZooKeeper client resolves `localhost` to both the IPv4 and IPv6 addresses and chooses a random one. If IPv6 was chosen, a fixed one second backoff is applied before retry since there is only one hostname specified. After backoff, 'localhost' is resolved again and a random address chosen, which could also be the unconnectable IPv6 address.

For the list of host names specified for connection, the clients do round-robin without backoffs until connections to all hostnames are attempted. Can we also do the same for addresses that each of the hosts resolves to, so that backoffs are only applied after connection to each address is attempted once and every address is connected to once using round-robin rather than random selection? This will avoid delays in cases where at least one address can be connected to.

 ",[],Bug,ZOOKEEPER-3100,Major,Rajini Sivaram,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper client times out due to random choice of resolved addresses,2019-10-04T14:55:13.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",8.0
,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>]",2018-07-23T05:30:54.000+0000,Jiafu Jiang," 

The default readTimeout timeout of ZooKeeper client is 2/3 * session_time, the default connectTimeout is session_time/hostProvider.size(). If the ZooKeeper cluster has 3 nodes, then connectTimeout is 1/3 * session_time.

 

Supports we have three ZooKeeper servers: zk1, zk2, zk3 deployed. And zk3 is now the leader. Client c1 is now connected to zk2(follower). Then we shutdown the network of zk3(leader), the same time, client c1 begin to write some data to ZooKeeper. After a (syncLimit * tick) timeout, zk2 will disconnect with leader and begin a new election, and zk2 becomes the leader.

 

The write operation will not succeed due to the leader is unavailable. It will take at most readTimeout time for c1 to discover the failure, and client c1 will try to choose another ZooKeeper server. Unfortunately, c1 may choose zk3, which is unreachable now, then c1 will spend connectTimeout to find out that zk3 is unused. Notice that readTimeout + connectTimeout = sesstion_timeout in my case(three-node cluster).

 

Therefore, in this case, the ZooKeeper cluster is unavailable for session timeout time when only one ZooKeeper server is unreachable due to network partition.

 

I have some suggestions:
 # The HostProvider used by ZooKeeper can be specified by an argument.
 # readTimeout can also be specified in any way.

 

 

 ",[],Bug,ZOOKEEPER-3099,Major,Jiafu Jiang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper cluster is unavailable for session_timeout time due to network partition in a three-node environment.   ,2018-10-15T01:52:22.000+0000,"[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.4.12', id='12342040'>, <JIRA Version: name='3.4.13', id='12342973'>]",5.0
Michael Han,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2018-07-20T18:45:55.000+0000,Michael Han,"Currently we don't track LearnerHandler threads in leader; we rely on the socket timeout to raise an exception and use that exception as a signal to let the LearnerHandler thread kills itself. In cases where the learners restarts, if the time between restart beginning to finishing is less than the socket timeout value (currently hardcoded as initLimit * tickTime), then there will be no exception raised and the previous LearnerHandler thread corresponding to this learner will leak.

I have a test case and a proposed fix which I will submit later.",[],Bug,ZOOKEEPER-3096,Major,Michael Han,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Leader should not leak LearnerHandler threads,2018-07-20T18:45:55.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.13', id='12342973'>]",2.0
Kent R. Spillner,"[<JIRA Component: name='c client', id='12312380'>]",2018-07-18T19:52:51.000+0000,Kent R. Spillner,Add missing #define -> string translations to zerror(int),"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-3093,Trivial,Kent R. Spillner,Fixed,2018-07-19T17:32:26.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,sync zerror(int rc) with newest error definitions,2019-05-20T17:51:09.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",4.0
,"[<JIRA Component: name='c client', id='12312380'>]",2018-07-13T05:59:17.000+0000,kevinxw,"crash  when unload libzookeeper_mt.so  by dlclose,     

the tsd keys should be deleted in a destructor in zk_log.c
{code:java}
__attribute__((destructor)) void deleteTSDKeys()
{
    pthread_setspecific(time_now_buffer, NULL);
    pthread_setspecific(format_log_msg_buffer, NULL);
    pthread_key_delete(time_now_buffer);
    pthread_key_delete(format_log_msg_buffer);
}
{code}
 ",[],Bug,ZOOKEEPER-3088,Critical,kevinxw,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zk c client: should delete tsd keys in a destructor ,2018-07-13T05:59:17.000+0000,"[<JIRA Version: name='3.4.12', id='12342040'>]",2.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2018-07-12T19:10:59.000+0000,Ruslan Nigmatullin,"Network outage on leader host can cause `QuorumPeer` thread to stuck for prolonged period of time (2+ hours, depends on tcp keep alive settings). It effectively stalls the whole zookeeper server making it inoperable. We've found it during one of our internal DRTs (Disaster Recovery Test).

The scenario which triggers the behavior (requires relatively high ping-load to the follower):
 # `Follower.processPacket` processes `Leader.PING` message
 # Leader is network partitioned
 # `Learner.ping` makes attempt to write to the leader socket
 # If write socket buffer is full (due to other ping/sync calls) `Learner.ping` blocks
 # As leader is partitioned - `Learner.ping` blocks forever due to lack of write timeout
 # `QuorumPeer` is the only thread reading from the leader socket, effectively meaning that the whole server is stuck and can't recover without manual process restart.

 

Thread dump from the affected server is in attachments.",[],Bug,ZOOKEEPER-3086,Major,Ruslan Nigmatullin,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,[server] Lack of write timeouts causes quorum to stuck,2018-07-20T17:49:16.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.4.12', id='12342040'>]",6.0
Brian Nixon,"[<JIRA Component: name='server', id='12312382'>]",2018-07-05T18:53:01.000+0000,Brian Nixon,"When the ZK server tries to make a snapshot and the machine is out of disk space, the snapshot creation fails and throws an IOException. An empty snapshot file is created, (probably because the server is able to create an entry in the dir) but is not able to write to the file.
 
If snapshot creation fails, the server commits suicide. When it restarts, it will do so from the last known good snapshot. However, when it tries to make a snapshot again, the same thing happens. This results in lots of empty snapshot files being created. If eventually the DataDirCleanupManager garbage collects the good snapshot files then only the empty files remain. At this point, the server is well and truly screwed.","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3082,Minor,Brian Nixon,Fixed,2018-07-30T04:23:22.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Fix server snapshot behavior when out of disk space,2019-10-04T14:55:15.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>, <JIRA Version: name='3.5.5', id='12343268'>]",6.0
,"[<JIRA Component: name='contrib-bindings', id='12312860'>, <JIRA Component: name='contrib-fatjar', id='12312645'>]",2018-07-05T14:40:28.000+0000,pacawat k,this is the test for our project,[],Bug,ZOOKEEPER-3081,Minor,pacawat k,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,DELETE - test hello,2018-08-21T16:53:59.000+0000,[],2.0
Kent R. Spillner,"[<JIRA Component: name='c client', id='12312380'>]",2018-07-03T20:41:18.000+0000,Kent R. Spillner,The function format_endpoint_info in zookeeper.c causes compiler errors when building with GCC 8 due to a potentially unsafe use of sprintf(3).,"[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3079,Minor,Kent R. Spillner,Fixed,2018-07-10T11:04:08.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Fix unsafe use of sprintf(3) for creating IP address strings,2018-07-10T19:55:17.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>]",4.0
,[],2018-07-03T15:14:49.000+0000,Jitkanya Tiawsawat,"Click the 'New Task' icon  in toolbar of the Task List. The 'New Task' wizard will display.
Choose your repository (e.g. 'JAC') from the list of repositories.
Select Next to and choose a project from the list of projects.
Select Finish to open the editor for entering task details.",[],Bug,ZOOKEEPER-3076,Major,Jitkanya Tiawsawat,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,DELETE - test,2018-08-21T16:49:39.000+0000,[],2.0
,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='tests', id='12312427'>]",2018-07-03T08:54:03.000+0000,Enrico Olivelli,"     [exec] zktest_st-TestWatchers.o:(.rodata._ZTIN7CppUnit23TestSuiteBuilderContextI18Zookeeper_watchersEE[_ZTIN7CppUnit23TestSuiteBuilderContextI18Zookeeper_watchersEE]+0x10): undefined reference to `typeinfo for CppUnit::TestSuiteBuilderContextBase'
     [exec] zktest_st-TestWatchers.o:(.rodata._ZTIN7CppUnit10TestCallerI18Zookeeper_watchersEE[_ZTIN7CppUnit10TestCallerI18Zookeeper_watchersEE]+0x10): undefined reference to `typeinfo for CppUnit::TestCase'
     [exec] zktest_st-TestWatchers.o:(.rodata._ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE[_ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE]+0x20): undefined reference to `CppUnit::TestCase::run(CppUnit::TestResult*)'
     [exec] zktest_st-TestWatchers.o:(.rodata._ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE[_ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE]+0x28): undefined reference to `CppUnit::TestLeaf::countTestCases() const'
     [exec] zktest_st-TestWatchers.o:(.rodata._ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE[_ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE]+0x30): undefined reference to `CppUnit::TestLeaf::getChildTestCount() const'
     [exec] zktest_st-TestWatchers.o:(.rodata._ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE[_ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE]+0x38): undefined reference to `CppUnit::Test::getChildTestAt(int) const'
     [exec] zktest_st-TestWatchers.o:(.rodata._ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE[_ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE]+0x40): undefined reference to `CppUnit::TestCase::getName[abi:cxx11]() const'
     [exec] zktest_st-TestWatchers.o:(.rodata._ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE[_ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE]+0x48): undefined reference to `CppUnit::Test::findTestPath(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, CppUnit::TestPath&) const'
     [exec] zktest_st-TestWatchers.o:(.rodata._ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE[_ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE]+0x50): undefined reference to `CppUnit::Test::findTestPath(CppUnit::Test const*, CppUnit::TestPath&) const'
     [exec] zktest_st-TestWatchers.o:(.rodata._ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE[_ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE]+0x58): undefined reference to `CppUnit::Test::findTest(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'
     [exec] zktest_st-TestWatchers.o:(.rodata._ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE[_ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE]+0x60): undefined reference to `CppUnit::Test::resolveTestPath(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'
     [exec] zktest_st-TestWatchers.o:(.rodata._ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE[_ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE]+0x68): undefined reference to `CppUnit::Test::checkIsValidIndex(int) const'
     [exec] zktest_st-TestWatchers.o:(.rodata._ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE[_ZTVN7CppUnit10TestCallerI18Zookeeper_watchersEE]+0x70): undefined reference to `CppUnit::TestLeaf::doGetChildTestAt(int) const'
     [exec] zktest_st-LibCSymTable.o: In function `LibCSymTable::LibCSymTable()':
     [exec] /xxx/zookeeper-3.4.13/src/c/tests/LibCSymTable.cc:36: undefined reference to `dlsym'
     [exec] /xxx/zookeeper-3.4.13/src/c/tests/LibCSymTable.cc:37: undefined reference to `dlsym'
     [exec] /xxx/zookeeper-3.4.13/src/c/tests/LibCSymTable.cc:38: undefined reference to `dlsym'
     [exec] /xxx/zookeeper-3.4.13/src/c/tests/LibCSymTable.cc:39: undefined reference to `dlsym'
     [exec] /xxxzookeeper-3.4.13/src/c/tests/LibCSymTable.cc:40: undefined reference to `dlsym'
     [exec] zktest_st-LibCSymTable.o:/xxxzookeeper-3.4.13/src/c/tests/LibCSymTable.cc:41: more undefined references to `dlsym' follow
     [exec] collect2: error: ld returned 1 exit status
     [exec] make[1]: *** [Makefile:822: zktest-st] Error 1
     [exec] make[1]: uscita dalla directory ""/xxxzookeeper-3.4.13/build/test/test-cppunit""
     [exec] make: *** [Makefile:1718: check-am] Error 2

BUILD FAILED
/xxx/zookeeper-3.4.13/build.xml:1471: The following error occurred while executing this line:
/xxx/zookeeper-3.4.13/build.xml:1481: exec returned: 2",[],Bug,ZOOKEEPER-3075,Critical,Enrico Olivelli,Won't Fix,2020-11-06T12:15:45.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Cannot run cppunit tests on branch 3.4 on Fedora 26,2020-11-06T12:15:45.000+0000,"[<JIRA Version: name='3.4.13', id='12342973'>]",2.0
Botond Hejj,"[<JIRA Component: name='server', id='12312382'>]",2018-06-29T14:00:11.000+0000,Botond Hejj,"There is a race condition in the server throttling code. It is possible that the disableRecv is called after enableRecv.

Basically, the I/O work thread does this in processPacket: [https://github.com/apache/zookeeper/blob/release-3.5.3/src/java/main/org/apache/zookeeper/server/ZooKeeperServer.java#L1102] 

                submitRequest(si);

            }

        }

        cnxn.incrOutstandingRequests(h);

    }

 

incrOutstandingRequests() checks for limit breach, and potentially turns on throttling, [https://github.com/apache/zookeeper/blob/release-3.5.3/src/java/main/org/apache/zookeeper/server/NIOServerCnxn.java#L384]

 

submitRequest() will create a logical request and en-queue it so that Processor thread can pick it up. After being de-queued by Processor thread, it does necessary handling, and then calls this [https://github.com/apache/zookeeper/blob/release-3.5.3/src/java/main/org/apache/zookeeper/server/FinalRequestProcessor.java#L459] :

 

            cnxn.sendResponse(hdr, rsp, ""response"");

 

and in sendResponse(), it first appends to outgoing buffer, and then checks if un-throttle is needed:  [https://github.com/apache/zookeeper/blob/release-3.5.3/src/java/main/org/apache/zookeeper/server/NIOServerCnxn.java#L708]

 

However, if there is a context switch between submitRequest() and cnxn.incrOutstandingRequests(), so that Processor thread completes cnxn.sendResponse() call before I/O thread switches back, then enableRecv() will happen before disableRecv(), and enableRecv() will fail the CAS ops, while disableRecv() will succeed, resulting in a deadlock: un-throttle is needed for letting in requests, and sendResponse is needed to trigger un-throttle, but sendResponse() requires an incoming message. From that point on, ZK server will no longer select the affected client socket for read, leading to the observed client-side failure in the subject.

If you would like to reproduce this than setting the globalOutstandingLimit down to 1 makes this reproducible easier as throttling starts with less requests. 

 ","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3072,Major,Botond Hejj,Fixed,2018-07-28T02:42:01.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Race condition in throttling,2018-11-24T19:58:09.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.5.4', id='12340141'>]",5.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2018-06-25T17:12:57.000+0000,Jan Zerebecki,"The [documentation regarding mutual ZooKeeper server to server authentication with DIGEST-MD5|https://cwiki.apache.org/confluence/display/ZOOKEEPER/Server-Server+mutual+authentication#Server-Servermutualauthentication-DIGEST-MD5basedauthentication] currently doesn't mention whether this is insecure. [DIGEST-MD5 was declared obsolete in 2011 due to security problems.|https://tools.ietf.org/html/rfc6331]

This is in relation to whether this is an effective mitigation for CVE-2018-8012 AKA ZOOKEEPER-1045, as mentioned in [https://lists.apache.org/thread.html/c75147028c1c79bdebd4f8fa5db2b77da85de2b05ecc0d54d708b393@%3Cdev.zookeeper.apache.org%3E].

Would the following be a fitting addition to the documentation?:

DIGEST-MD5 based authentication should not be relied on for authentication as it is insecure, it is only provided for test purposes.

 ",[],Bug,ZOOKEEPER-3069,Minor,Jan Zerebecki,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,document: is mutual auth with DIGEST-MD5 insecure?,2018-06-28T11:28:09.000+0000,[],2.0
Abhishek Singh Chouhan,[],2018-06-11T17:21:37.000+0000,Abhishek Singh Chouhan,In case of an authFailed sasl event we shutdown the send thread however we never close the event thread. Even if the client tries to close the connection it results in a no-op since we check for cnxn.getState().isAlive() which results in negative for auth failed state and we return without cleaning up. For applications that retry in case of auth failed by closing the existing connection and then trying to reconnect(eg. hbase replication) this eventually ends up exhausting the system resources.,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-3059,Critical,Abhishek Singh Chouhan,Fixed,2018-06-25T10:31:11.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,EventThread leak in case of Sasl AuthFailed,2019-10-04T14:55:15.000+0000,"[<JIRA Version: name='3.4.12', id='12342040'>]",5.0
Mohamed Jeelani,"[<JIRA Component: name='other', id='12333125'>]",2018-06-07T00:37:09.000+0000,Mohamed Jeelani,IPv6 literals are not parsed correctly and can lead to potential errors if not be an eye sore. Need to parse and display them correctly.,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.7', id='12346098'>]",Bug,ZOOKEEPER-3057,Minor,Mohamed Jeelani,Resolved,2018-10-13T14:00:17.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Fix IPv6 literal usage,2020-02-14T15:23:37.000+0000,"[<JIRA Version: name='3.4.12', id='12342040'>]",7.0
Michael Han,"[<JIRA Component: name='server', id='12312382'>]",2018-06-05T15:03:09.000+0000,Michael Han,"[An issue|https://lists.apache.org/thread.html/cc17af6ef05d42318f74148f1a704f16934d1253f1472cccc1a93b4b@%3Cdev.zookeeper.apache.org%3E] was reported when a user failed to upgrade from 3.4.10 to 3.5.4 with missing snapshot file.

The code complains about missing snapshot file is [here|https://github.com/apache/zookeeper/blob/release-3.5.4/src/java/main/org/apache/zookeeper/server/persistence/FileTxnSnapLog.java#L206] which is introduced as part of ZOOKEEPER-2325.

With this check, ZK will not load the db without a snapshot file, even the transaction log files are present and valid. This could be a problem for restoring a ZK instance which does not have a snapshot file but have a sound state (e.g. it crashes before being able to take the first snap shot with a large snapCount parameter configured).

 

*how to use this fix*

Add zookeeper.snapshot.trust.empty=true to your server configuration file and start the server.

This property will skip the check.

It is recommended to remove the property once you have a working server, because that check is important to ensure that the system is in good shape","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.6', id='12345243'>]",Bug,ZOOKEEPER-3056,Critical,Michael Han,Fixed,2019-09-03T06:54:03.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Fails to load database with missing snapshot file but valid transaction log file,2020-12-19T07:59:39.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.5.4', id='12340141'>]",20.0
,[],2018-06-02T16:27:56.000+0000,Remil,"hadoopuser@sherin-VirtualBox:~$ sudo su -p - zookeeper -c ""/usr/local/zookeeper/zookeeper-3.4.12/bin/zkServer.sh start"" ZooKeeper JMX enabled by default
ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/zookeeper-3.4.12/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
hadoopuser@sherin-VirtualBox:~$ telnet localhost 2181
Trying 127.0.0.1...
telnet: Unable to connect to remote host: Connection refused
hadoopuser@sherin-VirtualBox:~$

 

hadoopuser@sherin-VirtualBox:~$ telnet localhost 127.0.0.1:2181
telnet: could not resolve localhost/127.0.0.1:2181: Servname not supported for ai_socktype

 

 

 ",[],Bug,ZOOKEEPER-3055,Minor,Remil,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Unable to connect to remote host: Connection refused,2018-06-12T11:56:11.000+0000,[],2.0
,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='tests', id='12312427'>]",2018-05-29T23:40:24.000+0000,Patrick D. Hunt,"When I run the test using (jdk8 tag):
https://hub.docker.com/r/phunt/zk-docker-devenv.ubuntu.14.04/tags/
it fails with an IPV6 failure. afaict the container does not have ipv6 configured, although the kernel has it available as a feature. I believe this to be the real issue - it's a kernel feature but no available in the runtime.",[],Bug,ZOOKEEPER-3054,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ipv6 detection in configure.ac is broken,2018-05-29T23:40:24.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",0.0
Balazs Meszaros,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='tests', id='12312427'>]",2018-05-29T23:37:57.000+0000,Patrick D. Hunt,It would be good to be able to exercise the remove watches functionality from the c client cli. Mostly for testing purposes.,[],Bug,ZOOKEEPER-3053,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,add remove watches capabilities to the c cli,2019-03-19T13:08:57.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",0.0
,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='tests', id='12312427'>]",2018-05-29T23:36:26.000+0000,Patrick D. Hunt,"When running on a slow host (docker ubuntu on mac) the ""sleep(3)"" in tests/zkServer.sh is not sufficient wait for the server to enter RO mode. Recommend adding a ""isro"" 4lw check in the script to wait until the server is in RO mode. If this takes longer than 60 seconds the zkServer.sh should fail.

For more background see the comment here:
https://github.com/apache/zookeeper/pull/522#issuecomment-392980087",[],Bug,ZOOKEEPER-3052,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,testReadOnly fails on slow host,2022-02-03T08:50:28.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",0.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2018-05-21T19:55:19.000+0000,Patrick D. Hunt,The owasp target is complaining about jackson version. We should update to the latest.,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-3051,Blocker,Patrick D. Hunt,Fixed,2018-05-23T03:34:47.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,owasp complaining about jackson version used,2019-05-20T17:50:58.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2018-05-21T19:25:35.000+0000,Patrick D. Hunt,The owasp target highlights that we need to update to new jetty version.,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-3050,Blocker,Patrick D. Hunt,Fixed,2018-05-22T04:37:51.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,owasp ant target is highlighting jetty version needs to be updated,2019-05-20T17:50:47.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2018-05-17T12:43:13.000+0000,Zeynep Arikoglu,After stopping and starting the ZooKeeper stdout of the ZooKeeper is sporadically polluted with EndOfStreamException warnings. As it can be seen from the attached output the warnings are outputted in 0.2 millisecond intervals. This goes on until the ZooKeeper is stopped. If we are dumping the output to a file this fills up the storage immediately.,[],Bug,ZOOKEEPER-3045,Major,Zeynep Arikoglu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,NullPointerException and continuous EndOfStreamException warnings in ZooKeeper stdout after stop and start of ZooKeeper,2018-08-19T12:00:28.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",2.0
Bogdan Kanivets,"[<JIRA Component: name='tests', id='12312427'>]",2018-05-13T08:32:02.000+0000,Bogdan Kanivets,"According to the [dashboard|https://builds.apache.org/job/ZooKeeper-Find-Flaky-Tests/lastSuccessfulBuild/artifact/report.html] testFailedTxnAsPartOfQuorumLoss is flaky. I've looked at some logs and there are multiple causes of flakiness. One of them is in this line after step 5
{code:java}
Assert.assertEquals(1, outstanding.size());
{code}
For example [this|https://builds.apache.org/job/ZooKeeper_branch35_java10/10/artifact/build/test/logs] build of 3.5

I was able to reproduce this particular issue in debug mode and the problem is that 'outstading' map can also have 'closeSession' entries that are expected.

I'll submit a patch to relax this check.","[<JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-3042,Minor,Bogdan Kanivets,Fixed,2018-07-10T10:18:08.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,testFailedTxnAsPartOfQuorumLoss is flaky,2019-05-20T17:50:52.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",3.0
Hugh O'Brien,[],2018-05-13T05:47:51.000+0000,Hugh O'Brien,"simple typo

 

PR here: https://github.com/apache/zookeeper/pull/498/commits/a8cb7f668d31a7bcf12481409328a886231020f6","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.13', id='12342973'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-3041,Trivial,Hugh O'Brien,Fixed,2018-05-16T17:36:03.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Typo in error message, affects log analysis",2018-07-17T04:49:57.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",3.0
Norbert Kalmár,"[<JIRA Component: name='tests', id='12312427'>]",2018-05-10T22:21:41.000+0000,Patrick D. Hunt,"Flakey test EphemeralNodeDeletionTest

{noformat}
java.lang.AssertionError: After session close ephemeral node must be deleted expected null, but was:<4294967302,4294967302,1525988536834,1525988536834,0,0,0,144127862257483776,1,0,4294967302
 {noformat}
",[],Bug,ZOOKEEPER-3040,Major,Patrick D. Hunt,Cannot Reproduce,2018-08-06T09:33:07.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,flaky test EphemeralNodeDeletionTest,2018-08-06T09:33:07.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",4.0
Andor Molnar,[],2018-05-09T23:12:00.000+0000,Andor Molnar,"If more than 1 CRC error is found in the Txn log file, TxnLogToolkit fails to get an answer for the second one, because it has already closed the Scanner which was probably closed the input stream also, so exception is thrown:
{noformat}
ZooKeeper Transactional Log File with dbid 0 txnlog format version 2
CRC ERROR - 4/5/18 5:16:05 AM PDT session 0x16295bafcc40000 cxid 0x1 zxid 0x100000002 closeSession null
Would you like to fix it (Yes/No/Abort) ? y
CRC ERROR - 4/5/18 5:17:34 AM PDT session 0x26295bafcc90000 cxid 0x0 zxid 0x200000001 closeSession null
Would you like to fix it (Yes/No/Abort) ? Exception in thread ""main"" java.util.NoSuchElementException
at java.util.Scanner.throwFor(Scanner.java:862)
at java.util.Scanner.next(Scanner.java:1371)
at org.apache.zookeeper.server.persistence.TxnLogToolkit.askForFix(TxnLogToolkit.java:208)
at org.apache.zookeeper.server.persistence.TxnLogToolkit.dump(TxnLogToolkit.java:175)
at org.apache.zookeeper.server.persistence.TxnLogToolkit.main(TxnLogToolkit.java:101){noformat}","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.13', id='12342973'>]",Bug,ZOOKEEPER-3039,Major,Andor Molnar,Fixed,2018-05-15T16:58:03.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,TxnLogToolkit uses Scanner badly,2018-07-17T04:49:58.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.13', id='12342973'>]",3.0
Andor Molnar,"[<JIRA Component: name='server', id='12312382'>]",2018-05-09T22:39:46.000+0000,Andor Molnar,"A few nitpicks which needs to be cleaned up:

1. Rename OldEphemeralType --> EphemeralTypeEmulate353
 2. Remove unused method: getTTL()
3. Remove unused import from QuorumPeer

 ","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3038,Major,Andor Molnar,Fixed,2018-05-10T04:14:39.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Cleanup some nitpicks in TTL implementation,2019-01-21T14:54:13.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",3.0
,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2018-05-09T10:48:14.000+0000,Oded,"We got an issue with one of the zookeeprs (Leader), causing the entire kafka cluster to fail:

2018-05-09 02:29:01,730 [myid:3] - ERROR [LearnerHandler-/192.168.0.91:42490:LearnerHandler@648] - Unexpected exception causing shutdown while sock still open
java.net.SocketTimeoutException: Read timed out
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
        at java.net.SocketInputStream.read(SocketInputStream.java:171)
        at java.net.SocketInputStream.read(SocketInputStream.java:141)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
        at java.io.DataInputStream.readInt(DataInputStream.java:387)
        at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
        at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:83)
        at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:99)
        at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:559)
2018-05-09 02:29:01,730 [myid:3] - WARN  [LearnerHandler-/192.168.0.91:42490:LearnerHandler@661] - ******* GOODBYE /192.168.0.91:42490 ********

 

We would expect that zookeeper will choose another Leader and the Kafka cluster will continue to work as expected, but that was not the case.

 ",[],Bug,ZOOKEEPER-3036,Critical,Oded,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Unexpected exception in zookeeper,2021-09-30T12:28:16.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",16.0
Balazs Meszaros,"[<JIRA Component: name='build', id='12312383'>]",2018-05-07T09:21:28.000+0000,Namrata Bhave,"Building Zookeeper from source using below steps:

{{git clone git://github.com/apache/zookeeper}}
 {{cd zookeeper}}
 {{git checkout tags/release-3.4.11}}
 {{ant compile}}
 {{cd src/c}}
 {{sudo apt-get install -y libcppunit-dev}}
 {{ACLOCAL=""aclocal -I /usr/share/aclocal"" autoreconf -if}}
 {{./configure && make && sudo make install}}
 {{sudo make distclean}}

 

The 'autoreconf -if' step fails with below error:
 + ACLOCAL='aclocal -I /usr/share/aclocal'
 + autoreconf -if
 configure.ac:37: warning: macro 'AM_PATH_CPPUNIT' not found in library
 libtoolize: putting auxiliary files in '.'.
 libtoolize: copying file './ltmain.sh'
 libtoolize: Consider adding 'AC_CONFIG_MACRO_DIRS([m4])' to configure.ac,
 libtoolize: and rerunning libtoolize and aclocal.
 libtoolize: Consider adding '-I m4' to ACLOCAL_AMFLAGS in Makefile.am.
 configure.ac:37: warning: macro 'AM_PATH_CPPUNIT' not found in library
 configure.ac:37: error: possibly undefined macro: AM_PATH_CPPUNIT
 If this token and others are legitimate, please use m4_pattern_allow.
 See the Autoconf documentation.
 autoreconf: /usr/bin/autoconf failed with exit status: 1
 Build step 'Execute shell' marked build as failure
  

This is happening on Ubuntu 18.04. Can someone please help in resolving this error?","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-3034,Minor,Namrata Bhave,Fixed,2019-02-26T15:39:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Facing issues while building from source,2021-05-25T11:45:26.000+0000,"[<JIRA Version: name='3.4.11', id='12339207'>]",4.0
Andor Molnar,"[<JIRA Component: name='server', id='12312382'>]",2018-04-27T14:31:19.000+0000,Andor Molnar,"In my latest commit regarding TxnLogToolkit there's a refactor to outsource file padding logic from FileTxnLog to a separate class:

[https://github.com/apache/zookeeper/commit/126fb0f22d701cad58bf3123bf7d8f2219e60387#diff-89717124564925d61d29dd817bcdd915]

Unfortunately public static method setPreallocSize(int) has also been moved to the new class, but it's being actively used by hadoop-common project too:

[https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/ClientBaseWithFixes.java#L384]

I'd like to submit a patch to revert the deleted method which is going to call the new one, but will keep backward compatibility with Hadoop.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.13', id='12342973'>]",Bug,ZOOKEEPER-3027,Major,Andor Molnar,Fixed,2018-04-27T18:35:39.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Accidently removed public API of FileTxnLog.setPreallocSize(),2018-07-17T04:50:00.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.13', id='12342973'>]",4.0
Andor Molnar,[],2018-04-24T23:38:08.000+0000,Patrick D. Hunt,"Same issue as ZOOKEEPER-2415

 

Suspend and resume are being called on peers (which are subclasses of Thread):
{quote}// if we don't suspend a peer it will rejoin a quorum
 qu.getPeer(1).peer.suspend();

....

// resume poor fellow
 qu.getPeer(1).peer.resume();
{quote}",[],Bug,ZOOKEEPER-3026,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ReadOnlyModeTest  is using Thread deprecated API.,2018-06-22T04:49:02.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",1.0
Andrew Schwartzmeyer,"[<JIRA Component: name='build', id='12312383'>]",2018-04-23T21:07:23.000+0000,Patrick D. Hunt,"Jenkins build for windows cmake is failing:

started here:

[https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper-trunk-windows-cmake/2717/console]
{noformat}
f:\jenkins\jenkins-slave\workspace\zookeeper-trunk-windows-cmake\src\c\src\hashtable\hashtable.h(6): fatal error C1083: Cannot open include file: 'winconfig.h': No such file or directory [F:\jenkins\jenkins-slave\workspace\ZooKeeper-trunk-windows-cmake\src\c\hashtable.vcxproj]
  hashtable.c{noformat}
 

Looks like one or the other or both of these commits are at issue (jenkins build broken on these two changes being committed)
h2. [#2717 (Apr 16, 2018 4:58:17 AM)|https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper-trunk-windows-cmake/2717/changes]
 # ZOOKEEPER-3017: Link libm in CMake on FreeBSD. — [hanm|https://builds.apache.org/user/hanm/] / [detail|https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper-trunk-windows-cmake/2717/changes#67378512285c4b8dc9be50b90bbd2967068fc24e]
 # ZOOKEEPER-2999: CMake build should use target-level commands — [hanm|https://builds.apache.org/user/hanm/] / [detail|https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper-trunk-windows-cmake/2717/changes#9ba4aeb4f92c1fc3167ff8e2b56e02f3e344d3ba]

 ","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3025,Blocker,Patrick D. Hunt,Fixed,2018-04-24T00:28:57.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,cmake windows build is broken on jenkins,2018-04-24T04:34:54.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",4.0
,[],2018-04-23T06:05:24.000+0000,yijie,"we use  c++ client api:

int zoo_get_children(zhandle_t *zh, const char *path, int watch, struct String_vector *strings)

to list zookeeper dir, zoo_get_children return zok。

then we visit strings, its not right

!image-2018-04-23-14-05-03-534.png!",[],Bug,ZOOKEEPER-3024,Major,yijie,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,C++ Client return sub paths in String_vector illegal after zoo_get_children completed with ZOK,2018-04-23T06:13:43.000+0000,[],1.0
,"[<JIRA Component: name='server', id='12312382'>]",2018-04-04T20:09:26.000+0000,Daniel C,"We have a live Zookeeper environment (quorum size is 2) and observed a strange behavior:
 * Kafka created 2 ephemeral nodes /brokers/ids/822712429 and /brokers/ids/707577499 on 2018-03-12 03:30:36.933
 * The Kafka clients were long gone but as of today (20+ days after), the two ephemeral nodes are still present

 

Troubleshooting:

1) Lists the outstanding sessions and ephemeral nodes

 
{noformat}
$ echo dump | nc $SERVER1 2181
SessionTracker dump:
org.apache.zookeeper.server.quorum.LearnerSessionTracker@6d7fd863
ephemeral nodes dump:
Sessions with Ephemerals (2):
0x162183ea9f70003:
               /brokers/ids/822712429
0x162183ea9f70002:
               /brokers/ids/707577499
               /controller
{noformat}
 

 

2) stat on /brokers/ids/822712429

 
{noformat}
zk> stat /brokers/ids/822712429
czxid: 4294967344
mzxid: 4294967344
pzxid: 4294967344
ctime: 1520825436933 (2018-03-11T20:30:36.933-0700)
mtime: 1520825436933 (2018-03-11T20:30:36.933-0700)
version: 0
cversion: 0
aversion: 0
owner: 99668799174148099
datalen: 102
children: 0
{noformat}
 

 

3) List full connection/session details for all clients connected

 
{noformat}
$ echo cons | nc $SERVER1 2181
 /10.247.114.70:30401[0](queued=0,recved=1,sent=0)
 /10.248.88.235:40430[1](queued=0,recved=345,sent=345,sid=0x162183ea9f70c22,lop=PING,est=1522713395028,to=40000,lcxid=0x12,lzxid=0xffffffffffffffff,lresp=1522717802117,llat=0,minlat=0,avglat=0,maxlat=31)
{noformat}
 

 

 
{noformat}
$ echo cons | nc $SERVER2 2181
 /10.196.18.61:28173[0](queued=0,recved=1,sent=0)
 /10.247.114.69:42679[1](queued=0,recved=73800,sent=73800,sid=0x262183eaa21da96,lop=PING,est=1522651352906,to=9000,lcxid=0xe49f,lzxid=0x10004683d,lresp=1522717854847,llat=0,minlat=0,avglat=0,maxlat=1235)
{noformat}
 

 

4) health

 
{noformat}
$ echo mntr | nc $SERVER1 2181
zk_version           3.4.6-1569965, built on 02/20/2014 09:09 GMT
zk_avg_latency  0
zk_max_latency 443
zk_min_latency  0
zk_packets_received       11158019
zk_packets_sent               11158244
zk_num_alive_connections           2
zk_outstanding_requests              0
zk_server_state follower
zk_znode_count               344
zk_watch_count               0
zk_ephemerals_count     3
zk_approximate_data_size          36654
zk_open_file_descriptor_count   33
zk_max_file_descriptor_count     65536
{noformat}
 

 

5) Server logs with related sessions:
{noformat}
Only found these logs from Server1 related to the sessions (0x162183ea9f70002 and 0x162183ea9f70003):

2018-03-12 03:28:35,127 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.196.18.60:26775

2018-03-12 03:28:35,131 [myid:1] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@822] - Connection request from old client /10.196.18.60:26775; will be dropped if server is in r-o mode

2018-03-12 03:28:35,131 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@868] - Client attempting to establish new session at /10.196.18.60:26775

2018-03-12 03:28:35,137 [myid:1] - INFO  [CommitProcessor:1:ZooKeeperServer@617] - Established session 0x162183ea9f70002 with negotiated timeout 9000 for client /10.196.18.60:26775

 
2018-03-12 03:30:36,415 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.247.114.70:39260

2018-03-12 03:30:36,422 [myid:1] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@822] - Connection request from old client /10.247.114.70:39260; will be dropped if server is in r-o mode

2018-03-12 03:30:36,423 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@868] - Client attempting to establish new session at /10.247.114.70:39260

2018-03-12 03:30:36,428 [myid:1] - INFO  [CommitProcessor:1:ZooKeeperServer@617] - Established session 0x162183ea9f70003 with negotiated timeout 9000 for client /10.247.114.70:39260

 
2018-03-31 01:29:58,865 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1007] - Closed socket connection for client /10.247.114.70:39260 which had sessionid 0x162183ea9f70003{noformat}
6) Txn logs on the two ephemeral nodes /brokers/ids/707577499 and /brokers/ids/822712429:
{noformat}
3/11/18 8:28:35 PM PDT session 0x162183ea9f70002 cxid 0x6 zxid 0x10000001b create '/brokers/ids,,v{s{31,s{'world,'anyone}}},F,1

3/11/18 8:28:35 PM PDT session 0x162183ea9f70002 cxid 0x2c zxid 0x100000028 create '/brokers/ids/707577499,#7b226a6d785f706f7274223a31303130332c2274696d657374616d70223a2231353230383235333135363931222c22686f7374223a22736c6331336e79692e75732e6f7261636c652e636f6d222c2276657273696f6e223a312c22706f7274223a393039327d,v{s{31,s{'world,'anyone}}},T,1

3/11/18 8:30:36 PM PDT session 0x162183ea9f70003 cxid 0x14 zxid 0x100000030 create '/brokers/ids/822712429,#7b226a6d785f706f7274223a31303130332c2274696d657374616d70223a2231353230383235343336393139222c22686f7374223a22736c6331336e796a2e75732e6f7261636c652e636f6d222c2276657273696f6e223a312c22706f7274223a393039327d,v{s{31,s{'world,'anyone}}},T,2{noformat}
 

7) Additional questions from [~andorm]
{noformat}
1) Why is the session closed, the client closed it or the cluster expired it?

[Daniel Chan] in this case, the client got killed and we expect the session would be expired by the cluster

 
2) which server was the session attached to - the first (44sec max

lat) or one of the others? Which server was the leader?

[Daniel Chan] The sessions creating the ephemeral nodes were attached to Server1 (443 max latency) while Server2 is the leader

 
3) the znode exists on all 4 servers, is that right?

[Daniel Chan] The cluster has 2 members not 4, and the ephemeral nodes are present on both servers

 {noformat}",[],Bug,ZOOKEEPER-3018,Major,Daniel C,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Ephemeral node not deleted after session is gone,2019-10-04T14:55:16.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",7.0
,[],2018-04-03T05:20:57.000+0000,sumit agrawal,"While accepting connection from client, and message is incorrect, this causes NegativeArraySizeException while creating byte array of negative size.

 

~2018-03-02 23:51:21 [UTC:20180302T235121+0100]|INFO ||/xx.xx.xx.xx:3888hread|Coordination > Received connection request /yy.yy.yy.yy:18320 (QuorumCnxManager.java:511)~

~2018-03-02 23:51:21 [UTC:20180302T235121+0100]|ERROR||/xx.xx.xx.xx:3888hread|Coordination > Thread Thread[/xx.xx.xx.xx:3888,5,main] died (NIOServerCnxnFactory.java:44)~
 ~java.lang.NegativeArraySizeException~
 ~at org.apache.zookeeper.server.quorum.QuorumCnxManager.receiveConnection(QuorumCnxManager.java:242)~
 ~at org.apache.zookeeper.server.quorum.QuorumCnxManager$Listener.run(QuorumCnxManager.java:513)~

 

Below is code reference having the issue.

int num_remaining_bytes = din.readInt();
 byte[] b = new byte[num_remaining_bytes];

 

This makes other node in quorum unable to connect to this node. Here client is security scan app.

 

Check for invalid input must be present to avoid Node crashing and security.

 

 ","[<JIRA Version: name='3.4.7', id='12325149'>]",Bug,ZOOKEEPER-3016,Major,sumit agrawal,Fixed,2018-04-09T10:12:29.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Follower QuorumCnxManager$Listener thread died due to incorrect client packet,2018-04-09T10:12:45.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2018-03-28T08:48:23.000+0000,CHQ,"We have client A which create a znode ,and its path is /zk/lock/100000.  Another client B thread is acquiring for the lock, so it calls the exist command with watch periodically to check if it is available. Then Client A has finished this work, and  delete this znode. Client b still calls exist command with watch. Because the code doesn't check node existence, when the  Watch add operation comes , it will add to non-exist node path.

This problem may be cause by the follow code. 
{code:java}
public Stat statNode(String path, Watcher watcher)
throws KeeperException.NoNodeException {
Stat stat = new Stat();
DataNode n = nodes.get(path);
if (watcher != null) {
dataWatches.addWatch(path, watcher);
}
if (n == null) {
throw new KeeperException.NoNodeException();
}
synchronized (n) {
n.copyStat(stat);
return stat;
}
}
{code}
The zk version we use is 3.4.5. We meet a problem that is the zk client try to reestablish to zk cluster failed after disconnect for some reason.We find it causes by ZOOKEEPER-706. But we try to know why there are so many watches. Then we find this problem.

 

 

 ","[<JIRA Version: name='3.4.12', id='12342040'>]",Bug,ZOOKEEPER-3014,Major,CHQ,Not A Problem,2018-03-29T16:09:22.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,watch can be added to non-existed path by exist command,2019-10-04T14:55:12.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.4.6', id='12323310'>]",2.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2018-03-27T21:21:36.000+0000,Sanjay Pillai,Fix a minor typo in zookeeperProgrammers.html doc,[],Bug,ZOOKEEPER-3013,Trivial,Sanjay Pillai,Invalid,2018-03-27T21:30:47.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Fix Typo in doc,2018-03-27T21:30:47.000+0000,[],1.0
,[],2018-03-26T08:18:08.000+0000,lujie,"Inspired by ZK-3006 , I develop a simple static analysis tool to find other Potential NPE like ZK-3006.Due to that  i am a newbie here, some of them i am not sure whether they will truly cause NPE, anyway I still list them in here(format:caller,callee):
 # StaticHostProvider#updateServerList,StaticHostProvider#getServerAtCurrentIndex
 # DataTree#getACL,ReferenceCountedACLCache#convertLong
 # ConnectionBean#toString,ConnectionBean#getSourceIP
 # Leader#propose,SerializeUtils#serializeRequest

Hopefully someone can confirm them and help improve this tool",[],Bug,ZOOKEEPER-3011,Major,lujie,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Some NPEs, maybe",2018-03-26T09:29:53.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",2.0
,[],2018-03-26T08:09:03.000+0000,lujie,"Inspired by ZK-3006 , I develop a simple static analysis tool to find other Potential NPE like ZK-3006. This bug is found by this tool ,and I have carefully studied it.  But i am a newbie at here so i may be wrong, hope someone could confirm it and help me improve this tool.
h2. Bug description:

callee Learner#findLeader will return null and callee developer check it but just log:
{code:java}
// code placeholder
if (leaderServer == null) {
   LOG.warn(""Couldn't find the leader with id = "" + current.getId());
}
return leaderServer;
{code}
caller  Observer#observeLeader and Follower#followLeader will directly use return value w/o null check:
{code:java}
//Follower#followLeader
QuorumServer leaderServer = findLeader();
try {
    connectToLeader(leaderServer.addr, leaderServer.hostname);
    ..........
}
//Observer#observeLeader
QuorumServer leaderServer = findLeader();
LOG.info(""Observing "" + leaderServer.addr);
try {
    connectToLeader(leaderServer.addr, leaderServer.hostname);
}{code}",[],Bug,ZOOKEEPER-3010,Major,lujie,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Potential NPE in Observer#observeLeader and Follower#followLeader,2018-04-22T07:23:26.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",3.0
lujie,[],2018-03-26T07:46:57.000+0000,lujie,"Inspired by ZK-3006 , I develop a simple static analysis tool to find other Potential NPE like ZK-3006. This bug is found by this tool ,and I have carefully studied it.  But i am a newbie at here so i may be wrong, hope someone could confirm it and help me improve this tool.
h2. Bug description:

 class NIOServerCnxn has three method :getSocketAddress,getRemoteSocketAddress can return null just like :
{code:java}
// code placeholder
if (sock.isOpen() == false) {
  return null;
}
{code}
some of their caller give null checker, some(total 3 list in below) are not. 
{code:java}
// ServerCnxn#getConnectionInfo
Map<String, Object> info = new LinkedHashMap<String, Object>();
info.put(""remote_socket_address"", getRemoteSocketAddress());// Map.put will throw NPE if parameter is null

//IPAuthenticationProvider#handleAuthentication
tring id = cnxn.getRemoteSocketAddress().getAddress().getHostAddress();
cnxn.addAuthInfo(new Id(getScheme(), id));// finally call Set.add(it will throw NPE if parameter is null )

//NIOServerCnxnFactory#addCnxn
InetAddress addr = cnxn.getSocketAddress();
Set<NIOServerCnxn> set = ipMap.get(addr);// Map.get will throw NPE if parameter is null{code}
I think we should add null check in above three caller .

 ","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.4.14', id='12343587'>]",Bug,ZOOKEEPER-3009,Major,lujie,Fixed,2018-06-15T06:03:26.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Potential NPE in NIOServerCnxnFactory,2019-10-04T14:55:09.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",5.0
,[],2018-03-26T07:42:11.000+0000,lujie,"Inspired by ZK-3006 , I develop a simple static analysis tool to find other Potential NPE like ZK-3006. This bug is found by this tool ,and I have carefully studied it.  But i am a newbie at here so i may be wrong, hope someone could confirm it and help me improve this tool.
h2. Bug description:

callee :SecurityUtils#createSaslClient will return null while encounter exception
{code:java}
// code placeholder
catch (Exception e) {
  LOG.error(""Exception while trying to create SASL client"", e);
  return null;
}
{code}
but its caller has no null check just like:
{code:java}
// code placeholder
sc = SecurityUtils.createSaslClient();
if (sc.hasInitialResponse()) {
   responseToken = createSaslToken(new byte[0], sc, learnerLogin);
}
{code}
I think we should add null check in caller while callee return null",[],Bug,ZOOKEEPER-3008,Major,lujie,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Potential NPE in SaslQuorumAuthLearner#authenticate and SaslQuorumAuthServer#authenticate,2018-08-12T03:40:04.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",3.0
lujie,[],2018-03-26T07:30:58.000+0000,lujie,"Inspired by ZK-3006 , I develop a simple static analysis tool to find other Potential NPE like ZK-3006. This bug is found by this tool ,and I have carefully studied it.  But i am a newbie at here so i may be wrong, hope someone could confirm it and help me improve this tool.
h3. Bug describtion:

callee BinaryInputArchive#startVector will return null:
{code:java}
// code placeholder
public Index startVector(String tag) throws IOException {
    int len = readInt(tag);
     if (len == -1) {
     return null;
}
{code}
and caller ReferenceCountedACLCache#deserialize  call it without null check
{code:java}
// code placeholder
Index j = ia.startVector(""acls"");
while (!j.done()) {
  ACL acl = new ACL();
  acl.deserialize(ia, ""acl"");
}{code}
but all the other 14 caller of BinaryInputArchive#startVector performs null checker like:
{code:java}
// code placeholder
Index vidx1 = a_.startVector(""acl"");
  if (vidx1!= null)
     for (; !vidx1.done(); vidx1.incr()){
     .....
    }
   }
}
{code}
so i think we also need add null check in caller ReferenceCountedACLCache#deserialize  just like other 14 caller

 ","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.13', id='12342973'>]",Bug,ZOOKEEPER-3007,Major,lujie,Fixed,2018-04-26T22:24:44.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Potential NPE in ReferenceCountedACLCache#deserialize ,2018-07-17T04:50:00.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",5.0
Edward Ribeiro,[],2018-03-24T08:16:21.000+0000,lujie,"I have found a potential NPE in ZKDatabase#calculateTxnLogSizeLimit:

 
{code:java}
//ZKDatabase
public long calculateTxnLogSizeLimit() {
long snapSize = 0;
try {
snapSize = snapLog.findMostRecentSnapshot().length();
} catch (IOException e) {
LOG.error(""Unable to get size of most recent snapshot"");
}
return (long) (snapSize * snapshotSizeFactor);
}
{code}
 in FileTxnSnapLog#findMostRecentSnapshot(), it will return the result of  FileSnap#findMostRecentSnapshot:
{code:java}
// called by FileTxnSnapLog#findMostRecentSnapshot()
public File findMostRecentSnapshot() throws IOException {
List<File> files = findNValidSnapshots(1);
if (files.size() == 0) {
return null;
}
return files.get(0);
}
{code}
So it will return null when the files sizes is 0, but ZKDatabase#calculateTxnLogSizeLimit has no null checker

 

 ","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3006,Major,lujie,Fixed,2018-04-06T04:04:40.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Potential NPE in ZKDatabase#calculateTxnLogSizeLimit,2018-04-06T04:31:58.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",6.0
,[],2018-03-22T19:15:27.000+0000,Abraham Fine,3.4 currently supports Java 6. While working on the release of 3.4.12 I noticed a minor issue while using java 6 to build zookeeper (see the linked issue). We should have a jenkins job that continuously tests 3.4 and pull requests targeting 3.4 against this older jdk.,[],Bug,ZOOKEEPER-3004,Major,Abraham Fine,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,create jenkins jobs to test java 6 for branch 3.4,2018-03-22T19:16:16.000+0000,"[<JIRA Version: name='3.4.11', id='12339207'>]",1.0
,[],2018-03-22T19:12:10.000+0000,Abraham Fine,"While working on the release of 3.4.12 and testing under jdk 6 I noticed that our javadoc task currently fails due to the yetus api compatability annotations we have. The yetus annotations target jdk 7.

While I don't think this is too much of a problem since it should not impact ZooKeeper operation under jdk 6 we should definitely avoid silent failures.",[],Bug,ZOOKEEPER-3003,Major,Abraham Fine,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ant package does not fail if javadoc generation fails,2018-03-22T19:16:16.000+0000,"[<JIRA Version: name='3.4.11', id='12339207'>]",1.0
selfish finch,"[<JIRA Component: name='server', id='12312382'>]",2018-03-18T14:13:23.000+0000,selfish finch,"The log message when trying to delete a container node is not proper, missing
*_String.format_*","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-3001,Trivial,selfish finch,Fixed,2018-03-26T01:49:42.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Incorrect log message when try to delete container node,2018-03-26T02:39:54.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",4.0
Andrew Schwartzmeyer,[],2018-03-09T21:05:59.000+0000,Andrew Schwartzmeyer,The \{{CMakeLists.txt}} file in the master branch declares version 3.5.3 instead of 3.6.0.,"[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2998,Minor,Andrew Schwartzmeyer,Fixed,2018-03-26T02:17:16.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,CMake declares incorrect ZooKeeper version,2018-03-26T03:37:57.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",4.0
Andrew Schwartzmeyer,[],2018-03-09T21:00:23.000+0000,Andrew Schwartzmeyer,"When writing the CMake build, I erroneously forced ZooKeeper to link to the Windows CRT statically. Instead of setting this, we should rely on CMake's defaults, and let users override it if they choose to by configuring with  setting {{CMAKE_CXX_ARGS}}.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2997,Major,Andrew Schwartzmeyer,Fixed,2018-03-26T02:19:48.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,CMake should not force static CRT linking,2018-03-26T06:02:50.000+0000,[],4.0
,[],2018-03-09T05:28:51.000+0000,prashantkumar,"I see below core with 3.5.1-alpha

{code:java}

#0  __GI_raise (sig=sig@entry=6) at /usr/src/debug/glibc/2.24-r0/git/sysdeps/unix/sysv/linux/raise.c:58
58	/usr/src/debug/glibc/2.24-r0/git/sysdeps/unix/sysv/linux/raise.c: No such file or directory.
[Current thread is 1 (LWP 20486)]
(gdb) bt
#0  __GI_raise (sig=sig@entry=6) at /usr/src/debug/glibc/2.24-r0/git/sysdeps/unix/sysv/linux/raise.c:58
#1  0x00007f39f9f439a1 in __GI_abort () at /usr/src/debug/glibc/2.24-r0/git/stdlib/abort.c:89
#2  0x00007f39f9f81ac0 in __libc_message (do_abort=do_abort@entry=1, fmt=fmt@entry=0x7f39fa078959 ""%s"") at /usr/src/debug/glibc/2.24-r0/git/sysdeps/posix/libc_fatal.c:175
#3  0x00007f39f9f81b0a in __GI___libc_fatal (message=0x7f39e68c3350 ""Unexpected error 9 on netlink descriptor 20"") at /usr/src/debug/glibc/2.24-r0/git/sysdeps/posix/libc_fatal.c:185
#4  0x00007f39fa019315 in __GI___netlink_assert_response (fd=fd@entry=20, result=<optimized out>) at /usr/src/debug/glibc/2.24-r0/git/sysdeps/unix/sysv/linux/netlink_assert_response.c:103
#5  0x00007f39fa0189f2 in make_request (pid=<optimized out>, fd=<optimized out>) at /usr/src/debug/glibc/2.24-r0/git/sysdeps/unix/sysv/linux/check_pf.c:171
#6  __check_pf (seen_ipv4=seen_ipv4@entry=0x7f39e68c4642, seen_ipv6=seen_ipv6@entry=0x7f39e68c4643, in6ai=in6ai@entry=0x7f39e68c4650, in6ailen=in6ailen@entry=0x7f39e68c4658) at /usr/src/debug/glibc/2.24-r0/git/sysdeps/unix/sysv/linux/check_pf.c:329
#7  0x00007f39f9fe9679 in __GI_getaddrinfo (name=<optimized out>, name@entry=0x7f39e560d2a0 ""128.0.0.4"", service=service@entry=0x7f39e560d2aa ""2181"", hints=hints@entry=0x7f39e68c4b60, pai=pai@entry=0x7f39e68c4b38) at /usr/src/debug/glibc/2.24-r0/git/sysdeps/posix/getaddrinfo.c:2338
#8  0x00007f39f5d33ca5 in resolve_hosts (avec=0x7f39e68c4b40, hosts_in=0x7f39e560d250 ""128.0.0.4:2181"", zh=0x7f39e8756000) at /usr/src/debug/zookeeper/3.5.1-alpha-r0/zookeeper-3.5.1-alpha/src/c/src/zookeeper.c:723
#9  update_addrs (zh=zh@entry=0x7f39e8756000) at /usr/src/debug/zookeeper/3.5.1-alpha-r0/zookeeper-3.5.1-alpha/src/c/src/zookeeper.c:862
#10 0x00007f39f5d36611 in zookeeper_interest (zh=zh@entry=0x7f39e8756000, fd=fd@entry=0x7f39e68c4ce8, interest=interest@entry=0x7f39e68c4cec, tv=tv@entry=0x7f39e68c4d00) at /usr/src/debug/zookeeper/3.5.1-alpha-r0/zookeeper-3.5.1-alpha/src/c/src/zookeeper.c:2167
#11 0x00007f39f5d42ca8 in do_io (v=0x7f39e8756000) at /usr/src/debug/zookeeper/3.5.1-alpha-r0/zookeeper-3.5.1-alpha/src/c/src/mt_adaptor.c:380
#12 0x00007f3a00967490 in start_thread (arg=0x7f39e68eb700) at /usr/src/debug/glibc/2.24-r0/git/nptl/pthread_create.c:456
#13 0x00007f39f9ffc41f in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:105

{code}

",[],Bug,ZOOKEEPER-2996,Major,prashantkumar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"core in netlink with ""Unexpected error 9 on netlink descriptor 20"" error",2018-03-09T05:30:48.000+0000,[],1.0
,[],2018-03-08T05:31:36.000+0000,Abraham Fine,"When attempting to compile the documentation (with JAVA_HOME set to 1.7) I see output like this:
{code}
$ ant clean 
docs -Dforrest.home=$(brew info apache-forrest | grep /Cellar | awk '{print $1;}') -d
Apache Ant(TM) version 1.9.7 compiled on April 9 2016
Trying the default build file: build.xml
Buildfile: REDACTED/zookeeper/build.xml
Adding reference: ant.PropertyHelper
Detected Java version: 1.7 in: /Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home/jre

OTHER STUFF

docs:
Class org.apache.tools.ant.taskdefs.condition.Os loaded from parent loader (parentFirst)
Condition false; setting forrest.exec to forrest
Setting project property: forrest.exec -> forrest
     [exec] Current OS is Mac OS X
     [exec] Executing '/usr/local/Cellar/apache-forrest/0.9/bin/forrest'
     [exec] The ' characters around the executable and arguments are
     [exec] not part of the command.
Execute:Java13CommandLauncher: Executing '/usr/local/Cellar/apache-forrest/0.9/bin/forrest'
The ' characters around the executable and arguments are
not part of the command.
     [exec] Apache Forrest.  Run 'forrest -projecthelp' to list options
     [exec]
     [exec] Buildfile: /usr/local/Cellar/apache-forrest/0.9/libexec/main/forrest.build.xml
     [exec]
     [exec] check-java-version:
     [exec] This is apache-forrest-0.9
     [exec] Using Java 1.6 from /Library/Java/JavaVirtualMachines/jdk-9.0.1.jdk/Contents/Home

MORE STUFF

     [exec]
     [exec] BUILD FAILED
     [exec] /usr/local/Cellar/apache-forrest/0.9/libexec/main/targets/site.xml:180: Warning: Could not find file REDACTED/zookeeper/src/docs/build/tmp/brokenlinks.xml to copy.
     [exec]
     [exec] Total time: 3 seconds
     [exec] -Djava.endorsed.dirs=/usr/local/Cellar/apache-forrest/0.9/libexec/lib/endorsed:${java.endorsed.dirs} is not supported. Endorsed standards and standalone APIs
     [exec] Error: Could not create the Java Virtual Machine.
     [exec] in modular form will be supported via the concept of upgradeable modules.
     [exec] Error: A fatal exception has occurred. Program will exit.
     [exec]
     [exec]   Copying broken links file to site root.
     [exec]

BUILD FAILED
REDACTED/zookeeper/build.xml:501: exec returned: 1
	at org.apache.tools.ant.taskdefs.ExecTask.runExecute(ExecTask.java:644)
	at org.apache.tools.ant.taskdefs.ExecTask.runExec(ExecTask.java:670)
	at org.apache.tools.ant.taskdefs.ExecTask.execute(ExecTask.java:496)
	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:293)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
	at org.apache.tools.ant.Task.perform(Task.java:348)
	at org.apache.tools.ant.Target.execute(Target.java:435)
	at org.apache.tools.ant.Target.performTasks(Target.java:456)
	at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1405)
	at org.apache.tools.ant.Project.executeTarget(Project.java:1376)
	at org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)
	at org.apache.tools.ant.Project.executeTargets(Project.java:1260)
	at org.apache.tools.ant.Main.runBuild(Main.java:854)
	at org.apache.tools.ant.Main.startAnt(Main.java:236)
	at org.apache.tools.ant.launch.Launcher.run(Launcher.java:285)
	at org.apache.tools.ant.launch.Launcher.main(Launcher.java:112)
{code}

The build succeeds when I uninstall java 9.
",[],Bug,ZOOKEEPER-2995,Major,Abraham Fine,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ant docs fails when Java 1.9 is present on my system,2018-03-08T05:32:41.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.6.0', id='12326518'>]",1.0
jason wang,"[<JIRA Component: name='build', id='12312383'>]",2018-03-05T19:48:26.000+0000,jason wang,"There are Rcc.java and other required files under the src/java/main/org/apache/jute/compiler/generated directory.

However, when I tried to add the source distribution to our own git repo, the .gitignore file has ""generated"" as a key word in line 55 - which prevents the dir and files under that dir to be added to the repo.  The compilation later fails due to the missing dir and files.

*compile_jute*
 :*19:02:54* [mkdir] Created dir: /home/jenkins/workspace/3PA/PMODS/zookeeper-pgdi-patch-in-maven-repo/src/java/generated*

19:02:54* [mkdir] Created dir: /home/jenkins/workspace/3PA/PMODS/zookeeper-pgdi-patch-in-maven-repo/src/c/generated*

19:02:54* [java] Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF8

*19:02:54* [java] Error: Could not find or load main class org.apache.jute.compiler.generated.Rcc*

19:02:54* [java] Java Result: 1*19:02:54* [java] Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF8

*19:02:54* [java] Error: Could not find or load main class org.apache.jute.compiler.generated.Rcc*

19:02:54* [java] Java Result: 1*19:02:54* [touch] Creating /home/jenkins/workspace/3PA/PMODS/zookeeper-pgdi-patch-in-maven-repo/src/java/generated/.generated*

 

Fix is to remove or comment out the generated key word in line 55.

#
 #generated
 #

 ","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.13', id='12342973'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-2993,Minor,jason wang,Fixed,2018-05-23T03:44:16.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,.ignore file prevents adding src/java/main/org/apache/jute/compiler/generated dir to git repo,2018-07-17T04:49:58.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",5.0
Shawn Heisey,"[<JIRA Component: name='build', id='12312383'>]",2018-03-04T16:51:45.000+0000,Shawn Heisey,"The eclipse build target downloads a component from sourceforge.  It does this download with http, but sourceforge now requires https downloads.  The sourceforge page redirects to https, but ant is refusing to follow the redirect because it changes protocol.

The download in build.xml just needs to be changed to https and it will work.
","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",Bug,ZOOKEEPER-2992,Major,Shawn Heisey,Fixed,2018-03-05T02:52:15.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,The eclipse build target fails due to protocol redirection: http->https,2018-03-05T08:34:47.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.4.11', id='12339207'>]",4.0
,"[<JIRA Component: name='server', id='12312382'>]",2018-03-02T10:45:33.000+0000,Paul Millar,"Commit d497aac4 introduced the ZooKeeperServer#registerServerShutdownHandler method and corresponding ZooKeeperServerShutdownHandler class.  Both the method and class are package-protected, resulting in the expectation that non-ZK code should not use either.

However, if registerServerShutdownHandler is *not* called, then ZK will log an error:
{quote}ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes
{quote}
There are several problems here.  In order of importance (for me, at least!)

First, (most important) this certainly should not be logged as an error.  Depending on usage, there may be no need for a shutdown handler.  Always logging an error (with no opportunity to silence it) is therefore wrong.

Second, the ability to learn of state changes may be of general interest (monitoring, etc); however, this is not possible if the method is protected.

Third, the method accepts a concrete class that is designed to use a CountDownLatch. This is not appropriate in all cases.  The method should be updated to accept an interface.",[],Bug,ZOOKEEPER-2991,Major,Paul Millar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Server logs error on shutdown,2018-05-07T07:57:28.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.4.11', id='12339207'>]",1.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2018-03-01T20:34:50.000+0000,Rick Trudeau,"We're using ZK 3.5.3-beta.

When using literal IPv6 addresses in the zoo.cfg.dynamic file, ZK fails to come up with the connection to the peer ZKs keeps getting reset.

zookeeper.log indicates a badly formed address is the cause.
{noformat}
<2018.03.01 15:14:30 163 -0500><E><sdn3></2001:db8:0:0:0:0:0:4:3888><org.apache.zookeeper.server.quorum.QuorumCnxManager> org.apache.zookeeper.server.quorum.QuorumCnxManager$InitialMessage$InitialMessageException: Badly formed address: 2001:db8:0:0:0:0:0:2:3888{noformat}
Our zoo.cfg.dynamic uses literal IPv6 addresses which according to ZOOKEEPER-1460 is supported.
{noformat}
server.1=[2001:db8::2]:2888:3888
server.2=[2001:db8::3]:2888:3888
server.3=[2001:db8::4]:2888:3888{noformat}
 

Digging into QuorumCnxManager.java, InitialMessage.parse attemps to seperate the host portion from the port portion using "":"" as a delimeter, which is a problem for IPv6 IPs.  And there's this comment:
{code:java}
// FIXME: IPv6 is not supported. Using something like Guava's HostAndPort
// parser would be good.{code}
So it looks like peers address:port is failing to be parsed if they are specified as literal IPv6 addresses.  To confirm a workaround, I replaced my zoo.cfg.dynamic with hostnames instead, and everything worked as expected.

 

 

 

 ",[],Bug,ZOOKEEPER-2989,Major,Rick Trudeau,Duplicate,2019-08-28T03:41:59.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,IPv6 literal address causes problems for Quorum members,2020-03-28T15:17:11.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",2.0
Brian Nixon,"[<JIRA Component: name='leaderElection', id='12312378'>]",2018-03-01T20:18:13.000+0000,Brian Nixon,"We've observed the following behavior in elections when a node is lagging behind the quorum in its view of the ensemble topology.

- Node A is operating with node B in its voting view, but without view of node C.

- B votes for C.

- A then switches its vote to C, but throws a NPE when attempting to connect.

This causes the QuorumPeer to spin up a Follower only to immediately have it shutdown by the exception.

Ideally, A would not advertise a vote for a server that it will not follow.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.13', id='12342973'>]",Bug,ZOOKEEPER-2988,Minor,Brian Nixon,Fixed,2018-04-30T04:35:29.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,NPE triggered if server receives a vote for a server id not in their voting view,2018-07-17T04:49:57.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.4.12', id='12342040'>]",4.0
,[],2018-02-28T22:22:40.000+0000,Maciej Lopacinski,"The order of params passed to the PurgeTxnLog via the zkCleanup.sh script is invalid.

 

See PR for details.",[],Bug,ZOOKEEPER-2987,Major,Maciej Lopacinski,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Invalid PurgeTxnLog params order in the zkCleanup.sh,2018-02-28T22:23:25.000+0000,[],2.0
,[],2018-02-22T19:25:57.000+0000,Mohammad Etemad,"Zookeeper 3.5.3-beta is throwing the following error. I am facing the issue of  ""My id 1 not in the peer list"". If I use the alpha version (3.5.2) and then upgrade to the 3.5.3 beta version, the problem goes away. But if I implement the 3.5.3 version directly, the clustering never  happens and I get the error. To give you a bit more overview of the implementation:
  
 The pods use a persistent volume claim on a gluster volume. Each pod is assigned its own volume on the gluster file system. I run zookeeper as a stateful set with 3 pods. 
  
 In my cfg file I have:
  
{code:java}
standaloneEnabled=false 
tickTime=2000 
initLimit=10 
syncLimit=5 
#snapshot file dir 
dataDir=/data 
#tran log dir 
dataLogDir=/dataLog 
#zk log dir 
logDir=/logs 
4lw.commands.whitelist=* 
dynamicConfigFile=/opt/zookeeper/conf/zoo_replicated1.cfg.dynamic{code}
  
 and in my cfg.dynamic file I have:
   
{code:java}
server.0=zookeeper-0:2888:3888 
server.1=zookeeper-1:2888:3888 
server.2=zookeeper-2:2888:3888{code}
  
 Has there been any change on the clustering side of things that makes the new version not work?
 Sample logs:
{code:java}
2018-02-22 19:21:18,078 [myid:1] - ERROR [main:QuorumPeerMain@98] - Unexpected exception, exiting abnormally
 java.lang.RuntimeException: My id 1 not in the peer list
 at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:770)
 at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:185)
 at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:120)
 at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:79){code}",[],Bug,ZOOKEEPER-2986,Major,Mohammad Etemad,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,My id not in the peer list,2019-08-29T02:25:03.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",2.0
,[],2018-02-22T16:59:02.000+0000,Chris Thunes,"We recently observed an inconsistency in our Kafka cluster which we tracked down to ZooKeeper sessions expiring and then re-appearing after a ZooKeeper leadership failover. The Kafka nodes received session ""Expired"" events, leading to them starting new sessions and attempting to re-create some ephemeral nodes (broker ID nodes in kafka/brokers/ids specifically). However, between receiving the session Expired event and establishing a new session a leadership failover occurred within the ZooKeeper cluster which resulted in the expired session re-appearing. When Kafka attempted to re-create the ephemeral nodes mentioned above it (unexpectedly) received NODEEXISTS errors.

This behavior is a result of how session expiration is handled by the leader. Specifically, the expired session is marked as ""closing"" immediately upon expiration (in SessionTrackerImpl) and _before_ the corresponding ""closeSession"" entry is committed. A client can therefore receive a session Expired event before its session is fully closed. A leadership failover which results in the loss of the (uncommitted) closeSession entry thus leads to the sessions' ephemeral nodes ""re-appearing"" until another expiration of the old session on the new leader takes place.

I'm not certain if this should be considered a bug or an edge case that client are expected to handle. If it is the latter then I think it would be good to include this in the Programmer's Guide in the documentation.

If it's helpful I have code to reproduce this on an in-process cluster running 3.4.11 or 3.5.3-beta.",[],Bug,ZOOKEEPER-2985,Major,Chris Thunes,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Expired session may unexpired after leader failover,2019-07-26T12:13:02.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.4.11', id='12339207'>]",24.0
,[],2018-02-21T08:21:47.000+0000,Yayan Sinchan,h2.  ,[],Bug,ZOOKEEPER-2984,Major,Yayan Sinchan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Master,2019-08-27T14:07:09.000+0000,[],3.0
Flavio Paiva Junqueira,"[<JIRA Component: name='server', id='12312382'>]",2018-02-19T19:28:06.000+0000,Eron Wright,"ZOOKEEPER-1506 fixed a DNS resolution issue in 3.4.  Some portions of the fix haven't yet been ported to 3.5.

To recap the outstanding problem in 3.5, if a given ZK server is started before all peer addresses are resolvable, that server may cache a negative lookup result and forever fail to resolve the address.    For example, deploying ZK 3.5 to Kubernetes using a StatefulSet plus a Service (headless) may fail because the DNS records are created lazily.

{code}
2018-02-18 09:11:22,583 [myid:0] - WARN  [QuorumPeer[myid=0](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Follower@95] - Exception when following the leader
java.net.UnknownHostException: zk-2.zk.default.svc.cluster.local
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:589)
        at org.apache.zookeeper.server.quorum.Learner.sockConnect(Learner.java:227)
        at org.apache.zookeeper.server.quorum.Learner.connectToLeader(Learner.java:256)
        at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:76)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1133)
{code}

In the above example, the address `zk-2.zk.default.svc.cluster.local` was not resolvable when the server started, but became resolvable shortly thereafter.    The server should eventually succeed but doesn't.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2982,Blocker,Eron Wright,Fixed,2018-05-08T22:57:31.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Re-try DNS hostname -> IP resolution,2019-12-31T19:46:56.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.3', id='12335444'>]",8.0
,"[<JIRA Component: name='java client', id='12312381'>]",2018-02-12T13:57:11.000+0000,achimbab,"At line 518, 'existWatches.remove(clientPath)' is null because watches for clientPath is already removed.

https://github.com/apache/zookeeper/pull/461/commits/a6044af23ae1096a8c5305633320fa139cf730b2

 ","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",Bug,ZOOKEEPER-2978,Trivial,achimbab,Fixed,2018-02-20T22:38:57.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,fix potential null pointer exception when deleting node,2018-02-20T23:35:21.000+0000,"[<JIRA Version: name='3.4.11', id='12339207'>]",4.0
sumit agrawal,"[<JIRA Component: name='quorum', id='12312379'>]",2018-02-12T03:40:47.000+0000,sumit agrawal,"When client performs multiple times addAuth with different credential at follower concurrently, the communication between follower gets corrupt. This causes shutdown of Follower due to the failure.

Analysis:

In org.apache.zookeeper.server.quorum.QuorumPacket.serialize method,
 * call a_.startVector(authinfo,""authinfo""); which write the length of authinfo to packet (suppose it writes length 1)
 * get length of authinfo to write all details in loop (here gets length as 2)

<-- Here in concurrency scenario, buffer gets corrupt having extra bytes in channel for additional authinfo.

 

So When Leader reads next quorum packet, it reads previous extra bytes (incorrect) and possibly identify greater size of message (as corrupt byte pattern) causes exception...

Coordination > Unexpected exception causing shutdown while sock still open (LearnerHandler.java:633)
 java.io.IOException: Unreasonable length = 1885430131

 

 

ServerCnxn.getAuthInfo returns Unmodifiable list, but while addAuthInfo, there is no check. So this causes concurrency issue.

 

 

 

 ",[],Bug,ZOOKEEPER-2977,Critical,sumit agrawal,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Concurrency for addAuth corrupts quorum packets,2020-03-31T10:43:21.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",5.0
,[],2018-02-03T18:01:26.000+0000,ilovezfs,"https://www.apache.org/dyn/closer.cgi?path=zookeeper/stable

 

shows 3.4.10. But 3.4.11 is shown on http://zookeeper.apache.org/",[],Bug,ZOOKEEPER-2976,Major,ilovezfs,Done,2018-02-21T12:25:21.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,What is the latest stable version?,2018-02-21T12:25:21.000+0000,[],2.0
,"[<JIRA Component: name='server', id='12312382'>]",2018-02-02T10:27:46.000+0000,Marimuthu PMS Dhavamani,"1)      Create session from zk Client (Self client)
2)      Stop the zkServer where still the zk Client connected
3)      Wait for the socket to be cleared in server side
èServer side TCP session should be removed from TIME_WAIT status.
4)      Start the zkServer
5)      Now, re connection from ZK Client is denied ..Please analyse... Re connection should happen without any problem... ",[],Bug,ZOOKEEPER-2975,Major,Marimuthu PMS Dhavamani,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Re connection should happen without any problem After zk server restarted,2018-02-02T10:43:02.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",1.0
,[],2018-02-02T00:42:01.000+0000,Ethan Wang,Curator users are assumed to know ZooKeeper. A good place to start is here: [http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html],[],Bug,ZOOKEEPER-2974,Major,Ethan Wang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Link invalid, please update http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html",2018-02-02T00:42:01.000+0000,[],1.0
,[],2018-01-31T08:45:14.000+0000,wanggang_123,"I am running a three node ZooKeeper cluster. At 2018-01-28 17:56:30,leader node has error log:

2018-01-28 17:56:30 [UTC:20180128T175630+0800]|ERROR||LearnerHandler-/118.123.180.23:44836hread|Coordination > Unexpected exception causing shutdown while sock still open (LearnerHandler.java:633)
java.io.IOException: Unreasonable length = 1885430131
 at org.apache.jute.BinaryInputArchive.readBuffer(BinaryInputArchive.java:95)
 at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:85)
 at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:103)
 at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:546)
2018-01-28 17:56:30 [UTC:20180128T175630+0800]|WARN ||LearnerHandler-/118.123.180.23:44836hread|Coordination > ******* GOODBYE /118.123.180.23:44836 ******** (LearnerHandler.java:646)
2018-01-28 17:56:30 [UTC:20180128T175630+0800]|INFO ||ProcessThread(sid:2 cport:-1):hread|Coordination > Got user-level KeeperException when processing sessionid:0x16138593ad43cf9 type:delete cxid:0x5 zxid:0xc104b59e9 txntype:-1 reqpath:n/a Error Path:/VSP/Leader/syncScore-0/_c_9101a3d6-f431-4792-b71d-a493e938895d-latch-0000093037 Error:KeeperErrorCode = NoNode for /VSP/Leader/syncScore-0/_c_9101a3d6-f431-4792-b71d-a493e938895d-latch-0000093037 (PrepRequestProcessor.java:645)",[],Bug,ZOOKEEPER-2973,Minor,wanggang_123,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"""Unreasonable length"" exception ",2021-01-06T10:38:54.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",7.0
,"[<JIRA Component: name='recipes', id='12313246'>]",2018-01-29T08:16:49.000+0000,wuyiyun,"When use SSL on zookeeper server, counts of watches may increase more than forty thousands and lead zoookeeper process outofmemroy error after zookeeper server started one day.

check command:

echo wchs | nc localhost 2181

check result:

[zookeeper@localhost bin]$ echo wchs | nc localhost 2181
44412 connections watching 1 paths
Total watches:44412",[],Bug,ZOOKEEPER-2972,Major,wuyiyun,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"When use SSL on zookeeper server, counts of watches may increase more than forty thousands and lead zoookeeper process outofmemory error",2018-02-01T01:09:03.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",1.0
,[],2018-01-19T23:38:40.000+0000,Travis Gockel,"I have two zhandle_ts connected to two different ZK ensembles. Differentiating between log messages of the two is quite difficult, as the callback only gives you the message, with no reasonable way to grab connection that created it (the address of the handle is in the log message, but parsing this value seems rather error-prone). It would be nice if the log callback gave me the handle.
  

I attached a patch for a potential fix...it adds a few functions without breaking backwards compatibility:

 

{{typedef void (*log_callback_ext_fn)(const zhandle_t *zh,}}
 {{    const void *log_context, ZooLogLevel level, const char *message);}}

{{ZOOAPI void zoo_get_log_callback_ext(const zhandle_t *zh,}}
 {{    log_callback_ext_fn *callback, const void **context);}}

{{ZOOAPI void zoo_set_log_callback_ext(zhandle_t *zh,}}
 {{    log_callback_ext_fn callback, const void *context);}}

{{ZOOAPI zhandle_t *zookeeper_init3(const char *host, watcher_fn fn,}}
 {{  int recv_timeout, const clientid_t *clientid, void *context, int flags,}}
 {{  log_callback_ext_fn log_callback, const void *log_callback_context);}}

 

The fallback ordering is changed to: log_callback_ext_fn -> log_callback_fn -> global stream.

Let me know if this is completely crazy.",[],Bug,ZOOKEEPER-2969,Minor,Travis Gockel,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,C API Log Callback Lacks Context,2018-01-20T00:02:51.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2018-01-08T06:51:01.000+0000,Qihong Xu,"I foung a bug that ""conf"" command would return dataDir and dataLogDir opposingly.

This bug only exists in versions newer than 3.5. I only found dumpConf in [ZookeeperServer.java|https://github.com/apache/zookeeper/blob/master/src/java/main/org/apache/zookeeper/server/ZooKeeperServer.java#L188] prints these two paths opposingly. Unlike ZOOKEEPER-2960, the actual paths are not affected and server function is ok.

I made a small patch to fix this bug. Any review is appreciated.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2964,Minor,Qihong Xu,Fixed,2018-01-19T00:13:44.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"""Conf"" command returns dataDir and dataLogDir opposingly",2019-10-04T14:55:10.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",7.0
Ling Mao,[],2018-01-04T09:28:10.000+0000,wu xiaoxue,"Today is China New Year's Day.I am still a single dog.
 When reading this line code annotation, I burst into tear.
 My New Year's Resolution is girlfriend(s)!!!!!!!!!!!!!!!!!!!!!!!",[],Bug,ZOOKEEPER-2963,Major,wu xiaoxue,Invalid,2018-01-18T00:01:24.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,standalone,2018-12-31T16:39:35.000+0000,[],3.0
Abraham Fine,[],2017-12-22T22:10:24.000+0000,Abraham Fine,This test relies on hooking into our logging system and creates a new appender using a PatternLayout object shared with the CONSOLE appender. PatternLayout has some synchronization issues (https://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/PatternLayout.html) so we should create a new instance of it. ,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",Bug,ZOOKEEPER-2961,Major,Abraham Fine,Fixed,2018-01-19T00:05:35.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Fix testElectionFraud Flakyness,2018-01-19T00:33:38.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.6.0', id='12326518'>]",4.0
Andor Molnar,"[<JIRA Component: name='server', id='12312382'>]",2017-12-22T17:17:47.000+0000,Dan Milon,"_emphasized text_After upgrading from zookeeper 3.4.5, to 3.4.11, without editing {{zoo.cfg}}, the new version of the server tries to use the {{dataDir}} as the {{dataLogDir}}, and the {{dataLogDir}} as the {{dataDir}}. Or at least some parts of the server.

Configuration file has:
{noformat}
$ grep -i data /etc/zookeeper/zoo.cfg 
dataLogDir=/var/lib/zookeeper/datalog
dataDir=/var/lib/zookeeper/data
{noformat}

But runtime configuration has:
{noformat}
$ echo conf | nc localhost 2181 | grep -i data
dataDir=/var/lib/zookeeper/datalog/version-2
dataLogDir=/var/lib/zookeeper/data/version-2
{noformat}

Also, I got this in the debug logs, so clearly some parts of the server confuse things.

{noformat}
[PurgeTask:FileTxnSnapLog@79] - Opening datadir:/var/lib/zookeeper/datalog snapDir:/var/lib/zookeeper/data
[main:FileTxnSnapLog@79] - Opening datadir:/var/lib/zookeeper/data snapDir:/var/lib/zookeeper/datalog
{noformat}

I tried to look in the code for wrong uses of the directories. I only found [ZookeeperServer.java|https://github.com/apache/zookeeper/blob/master/src/java/main/org/apache/zookeeper/server/ZooKeeperServer.java#L227] is passing the arguments to {{FileTxnSnapLog}} in the wrong order, but the code comment says that this is legacy only for tests, so I assume it isn't the cause for my case.","[<JIRA Version: name='3.4.12', id='12342040'>]",Bug,ZOOKEEPER-2960,Critical,Dan Milon,Fixed,2018-01-08T09:40:34.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,The dataDir and dataLogDir are used opposingly,2019-10-04T14:55:15.000+0000,"[<JIRA Version: name='3.4.11', id='12339207'>]",9.0
Bogdan Kanivets,[],2017-12-20T06:46:38.000+0000,xiangyq000,"Once the ZooKeeper cluster finishes the election for new leader, all learners report their accepted epoch to the leader for the computation of new cluster epoch.

org.apache.zookeeper.server.quorum.Leader#getEpochToPropose
{code:java}
    private final HashSet<Long> connectingFollowers = new HashSet<Long>();
    public long getEpochToPropose(long sid, long lastAcceptedEpoch) throws InterruptedException, IOException {
        synchronized(connectingFollowers) {
            if (!waitingForNewEpoch) {
                return epoch;
            }
            if (lastAcceptedEpoch >= epoch) {
                epoch = lastAcceptedEpoch+1;
            }
            connectingFollowers.add(sid);
            QuorumVerifier verifier = self.getQuorumVerifier();
            if (connectingFollowers.contains(self.getId()) &&
                                            verifier.containsQuorum(connectingFollowers)) {
                waitingForNewEpoch = false;
                self.setAcceptedEpoch(epoch);
                connectingFollowers.notifyAll();
            } else {
                long start = Time.currentElapsedTime();
                long cur = start;
                long end = start + self.getInitLimit()*self.getTickTime();
                while(waitingForNewEpoch && cur < end) {
                    connectingFollowers.wait(end - cur);
                    cur = Time.currentElapsedTime();
                }
                if (waitingForNewEpoch) {
                    throw new InterruptedException(""Timeout while waiting for epoch from quorum"");
                }
            }
            return epoch;
        }
    }
{code}

The computation will get an outcome once :
# The leader has call method ""getEpochToPropose""
# The number of all reporters is greater than half of participants.

The problem is, an observer server will also send its accepted epoch to the leader, while this procedure treat observers as participants.

Supposed that the cluster consists of 1 leader, 2 followers and 1 observer, and now the leader and the observer have reported their accepted epochs while neither of the followers has. Thus, the connectingFollowers set consists of two elements, resulting in a size of 2, which is greater than half quorum, namely, 2. Then QuorumVerifier#containsQuorum will return true, because it does not check whether the elements of the parameter are participants.

The same flaw exists in org.apache.zookeeper.server.quorum.Leader#waitForEpochAck","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.13', id='12342973'>]",Bug,ZOOKEEPER-2959,Blocker,xiangyq000,Fixed,2018-05-10T04:03:32.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ignore accepted epoch and LEADERINFO ack from observers when a newly elected leader computes new epoch,2019-03-20T00:26:47.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>]",7.0
Benedict Jin,"[<JIRA Component: name='server', id='12312382'>]",2017-12-15T08:43:55.000+0000,Benedict Jin,"We cannot delete znode that owns too many child znodes by `rmr` command, due to the list of child znodes could be 172 MB, which is too huge for the default value of `jute.maxbuffer` (1MB). In fact, we shouldn't be effected by the number of child znodes when we want to delete some special znodes recursively.",[],Bug,ZOOKEEPER-2956,Major,Benedict Jin,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Cannot delete znode that owns too many child znodes by `rmr` command,2019-12-19T15:28:44.000+0000,[],6.0
,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='jute', id='12312385'>]",2017-12-13T10:26:07.000+0000,Aditya Pawaskar,"when I run Apache Zookeeper 3.4.11 using OpenJDK-8 and clone source code from git.
At 'ant compile_jute' command I get following error message :

{noformat}
Buildfile: /root/zookeeper/build.xml

init:

jute:
    [javac] Compiling 39 source files to /root/zookeeper/build/classes
    [javac] warning: [options] bootstrap class path not set in conjunction with -source 1.6
    [javac] /root/zookeeper/src/java/main/org/apache/jute/Record.java:21: error: package org.apache.yetus.audience does not exist
    [javac] import org.apache.yetus.audience.InterfaceAudience;
    [javac]                                 ^
    [javac] /root/zookeeper/src/java/main/org/apache/jute/Record.java:29: error: package InterfaceAudience does not exist
    [javac] @InterfaceAudience.Public
    [javac]                   ^
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 2 errors
    [javac] 1 warning

BUILD FAILED
/root/zookeeper/build.xml:315: Compile failed; see the compiler error output for details.
{noformat}

According to error, ant is unable to get InterfaceAudience which is part of audience-annotations-0.5.0.jar mentioned in build.xml
when I search for this jar file, I could not find it in source code.	

Thanks and Regards,
Aditya",[],Bug,ZOOKEEPER-2954,Major,Aditya Pawaskar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ant compile_jute is failing for zookeeper 3.4.11,2017-12-18T05:14:26.000+0000,"[<JIRA Version: name='3.4.11', id='12339207'>]",3.0
Abraham Fine,[],2017-12-13T00:24:44.000+0000,Abraham Fine,"testNoLogBeforeLeaderEstablishment has been flaky on 3.4, 3.5, and master for quite awhile. My understanding is that the purpose of the test is to make sure that a server receives support from the quorum before changing the epoch and acting as leader. 

There are a couple issues with the test in its current state. First, the assertions the test makes are not always true. It is possible, if the zookeeper database is not cleared, for a follower to be ahead of a leader when the quorum is shutdown. That follower will then likely become leader when the quorum is restarted. This is the cause of the flaky behavior. Second, the test does not appear to create the conditions it wants to test for. Since, ZOOKEEPER-335 (specifically the ZOOKEEPER-1081 subtask) we take the epoch into consideration in {{FastLeaderElection}} so the test no longer ""believes it is the leader once it recovers"".

After discussing the issue offline with [~phunt] we decided it would still be valuable to test the situation where a server is elected leader without the support of the quorum. So I removed {{testNoLogBeforeLeaderEstablishment}} and created a new test called {{testElectionFraud}}.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",Bug,ZOOKEEPER-2953,Major,Abraham Fine,Fixed,2017-12-16T00:49:05.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Flaky Test: testNoLogBeforeLeaderEstablishment,2017-12-16T02:05:46.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.6.0', id='12326518'>]",4.0
,"[<JIRA Component: name='server', id='12312382'>]",2017-12-11T08:51:09.000+0000,Jorg Heymans,"this is the output i get (apologies for the cut-off line endings)

{noformat}
C:\RC\Tools\zookeeper-3.4.11\bin>zkServer.cmd

call ""c:\RC\jdk\jdk1.8.0_121\""\bin\java ""-Dzookeeper.log.dir=C:\RC\Tools\zookeeper-3.4.11\bin\.."" ""-Dzookeeper.root.logger=INFO,CONSOLE"" -cp ""C:\RC\Tools\zookeeper-3.4.11\bin\..\build\classes;C:\RC\Tools\zookeeper-3.4.11\bin\..\build\lib\*;C:\RC\Tools\zookeeper-3.4.11\bin\..\*;C:\RC\Tools\zookeeper-
3.4.11\bin\..\lib\*;C:\RC\Tools\zookeeper-3.4.11\bin\..\conf"" org.apache.zookeeper.server.quorum.QuorumPeerMain ""C:\RC\Tools\zookeeper-3.4.11\bin\..\c
onf\zoo.cfg""
Usage: java [-options] class [args...]
           (to execute a class)
   or  java [-options] -jar jarfile [args...]
           (to execute a jar file)
where options include:
    -d32          use a 32-bit data model if available
    -d64          use a 64-bit data model if available
    -server       to select the ""server"" VM
                  The default VM is server.

    -cp <class search path of directories and zip/jar files>
    -classpath <class search path of directories and zip/jar files>
                  A ; separated list of directories, JAR archives,
                  and ZIP archives to search for class files.
    -D<name>=<value>
                  set a system property
    -verbose:[class|gc|jni]
                  enable verbose output
    -version      print product version and exit
    -version:<value>
                  Warning: this feature is deprecated and will be removed
                  in a future release.
                  require the specified version to run
    -showversion  print product version and continue
    -jre-restrict-search | -no-jre-restrict-search
                  Warning: this feature is deprecated and will be removed
                  in a future release.
                  include/exclude user private JREs in the version search
    -? -help      print this help message
    -X            print help on non-standard options
    -ea[:<packagename>...|:<classname>]
    -enableassertions[:<packagename>...|:<classname>]
                  enable assertions with specified granularity
    -da[:<packagename>...|:<classname>]
    -disableassertions[:<packagename>...|:<classname>]
                  disable assertions with specified granularity
    -esa | -enablesystemassertions
                  enable system assertions
    -dsa | -disablesystemassertions
                  disable system assertions
    -agentlib:<libname>[=<options>]
                  load native agent library <libname>, e.g. -agentlib:hprof
                  see also, -agentlib:jdwp=help and -agentlib:hprof=help
    -agentpath:<pathname>[=<options>]
                  load native agent library by full pathname
    -javaagent:<jarpath>[=<options>]
                  load Java programming language agent, see java.lang.instrument
    -splash:<imagepath>
                  show splash screen with specified image
See http://www.oracle.com/technetwork/java/javase/documentation/index.html for more details.

endlocal

{noformat}","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",Bug,ZOOKEEPER-2951,Major,Jorg Heymans,Fixed,2017-12-12T18:40:38.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zkServer.cmd does not start when JAVA_HOME ends with a \,2017-12-12T19:31:47.000+0000,"[<JIRA Version: name='3.4.11', id='12339207'>]",5.0
,"[<JIRA Component: name='java client', id='12312381'>]",2017-11-27T03:26:22.000+0000,Feng Shaobao,"In our environment, the zk clusters are all behind a proxy, the proxy decide to transfer the request from client based on the ""ServerName"" field in SSL Hello packet(the proxy served on SSL only). but the Hello packets that zk client sended do proxy do not contain the ""ServerName"" field in it. after inspect the codes, we have found that it is because that zk client did not specify the peerHost when initializing the SSLContext.

In the method initSSL of class ZKClientPipelineFactory, it initialize the SSLEngine like below:

sslEngine = sslContext.createSSLEngine();

Actually the sslContext provide another factory method that receives the hostName and port parameter.

public final SSLEngine createSSLEngine(String hostName, int port)

If we call this method to create the SSLEngine, then the proxy will know which zk cluster it really want to access.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2949,Major,Feng Shaobao,Fixed,2018-01-30T20:58:04.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"SSL ServerName not set when using hostname, some proxies may failed to proxy the request.",2018-01-30T21:26:48.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",4.0
Abraham Fine,[],2017-11-21T20:44:48.000+0000,Abraham Fine,"Looks like someone is creating our test files outside of jenkins. I modified the job to output our id and look at the perms on those files:

----
[ZooKeeper-trunk] $ /bin/bash /tmp/jenkins291402182647699851.sh
uid=910(jenkins) gid=910(jenkins) groups=910(jenkins),999(docker)

drwxr-xr-x 3 10025 12036 4096 Nov 10 01:39 /tmp/zkdata
-rw-r--r-- 1 10025 12036    2 Nov 10 01:39 /tmp/zkdata/myid

/tmp/zkdata/version-2:
total 20
drwxr-xr-x 2 10025 12036 4096 Oct 22 23:35 .
drwxr-xr-x 3 10025 12036 4096 Nov 10 01:39 ..
-rw-r--r-- 1 10025 12036    1 Oct 22 23:35 acceptedEpoch
-rw-r--r-- 1 10025 12036    1 Oct 22 23:35 currentEpoch
-rw-r--r-- 1 10025 12036  562 Oct 22 23:35 snapshot.0
----

Notice that it's not jenkins.
","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",Bug,ZOOKEEPER-2948,Major,Abraham Fine,Fixed,2017-11-22T17:46:01.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Failing c unit tests on apache jenkins,2018-03-26T18:25:42.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",5.0
,[],2017-11-21T19:48:14.000+0000,Abraham Fine,"This is a follow on for ZOOKEEPER-2944. We should raise some sort of error when the counter for sequential nodes overflows, rather than silently overflowing and creating nodes with a negative counter. ",[],Bug,ZOOKEEPER-2947,Major,Abraham Fine,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Find a reasonable way to handle sequential node counter overflowing,2017-11-21T20:03:32.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.6.0', id='12326518'>]",1.0
,[],2017-11-21T10:06:58.000+0000,anaud,"The truncate() function in FileTxnLog.java may fail to properly remove the uncommitted write. This happens when the follower that has the uncommitted writes tries to resync with the leader after a few epochs have past. The failure results in data inconsistency in the in-memory data tree across nodes. Here is one procedure to reproduce the inconsistency. 

Initially:
# Start the ensemble with three nodes: node 0, 1 and 2 (the node 2 is the leader)
# Create 5 znodes with initial values as follow (key = value)
{noformat}
/testDivergenceResync0 = 0
/testDivergenceResync1 = 1
/testDivergenceResync2 = 2
/testDivergenceResync3 = 3
/testDivergenceResync4 = 4
{noformat}

To Reproduce:
# Diverge the node 2
a. Shutdown the node 0 and 1
b. Async setData to the node 2 writing 1000 to the key ‘/testDivergenceResync0’
c. Shutdown the node 2
# Restart the node 0 and 1 (let them finish with resync)
# Diverge the node 1
a. Shutdown the node 0
b. Async setData to the node 1 writing 1001 to the key ‘/testDivergenceResync1’
c. Shutdown the node 1
# Restart the node 0, 1 and 2 (let them finish with resync)
# Diverge the node 2
a. Shutdown the node 0 and 1
b. Async setData to the node 2 writing 1002 to the key ‘/testDivergenceResync2’
c. Shutdown the node 2
# Restart the node 0 and 2 (let them finish with resync)
# Diverge the node 2
a. Shutdown the node 0
b. Async setData to the node 2 writing 1003 to the key ‘/testDivergenceResync3’
c. Shutdown the node 2
# Restart the node 0 and 1 (let them finish with resync)
# Diverge the node 1
a. Shutdown the node 0
b. Async setData to the node 1 writing 1004 to the key ‘/testDivergenceResync4’
c. Shutdown the node 1
# Restart the node 0 and 2 (let them finish with resync)
# Restart the node 1 (let it finish with resync)

Reading each key from each node directly will give us the output:
{noformat}
/testDivergenceResync0 on the node 0 = 0
/testDivergenceResync0 on the node 1 = 0
/testDivergenceResync0 on the node 2 = 0
/testDivergenceResync1 on the node 0 = 1001
/testDivergenceResync1 on the node 1 = 1001
/testDivergenceResync1 on the node 2 = 1001
/testDivergenceResync2 on the node 0 = 2
/testDivergenceResync2 on the node 1 = 2
/testDivergenceResync2 on the node 2 = 2
/testDivergenceResync3 on the node 0 = 3
/testDivergenceResync3 on the node 1 = 3
/testDivergenceResync3 on the node 2 = 1003
/testDivergenceResync4 on the node 0 = 1004
/testDivergenceResync4 on the node 1 = 1004
/testDivergenceResync4 on the node 2 = 1004
{noformat}
Thus, the value of key /testDivergenceResync3 is inconsistent across nodes.

What seems to happen:
# At the step 7, setData (at zxid 0x400000001) writing value 1003 is committed on the node 2.
{panel:title=Log from the node 2:}
...
2017-11-16 03:08:14,123 [myid:2] - DEBUG [ProcessThread(sid:2 cport:-1)::CommitProcessor@174] - Processing request:: sessionid:0x2000117327c0000 type:setData cxid:0x4 zxid:0x400000001 txntype:5 reqpath:n/a
2017-11-16 03:08:14,124 [myid:2] - DEBUG [ProcessThread(sid:2 cport:-1)::Leader@787] - Proposing:: sessionid:0x2000117327c0000 type:setData cxid:0x4 zxid:0x400000001 txntype:5 reqpath:n/a
2017-11-16 03:08:14,124 [myid:2] - INFO  [SyncThread:2:FileTxnLog@209] - Creating new log file: log.400000001
2017-11-16 03:08:14,188 [myid:2] - DEBUG [SyncThread:2:Leader@600] - Count for zxid: 0x400000001 is 1
2017-11-16 03:08:15,752 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:Leader@512] - Shutting down
2017-11-16 03:08:15,753 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:Leader@518] - Shutdown called
java.lang.Exception: shutdown Leader! reason: Not sufficient followers synced, only synced with sids: [ 2 ]
...
{panel}
# At the step 10, the node 2 is restarted and supposed to be properly resync’ed with the node 0 which is the leader.
    a. The node 0 sends TRUNC msg so that the node 2 can truncate the setData at zxid 0x400000001.. 
    b. On the other hand, the node 2 tries to truncate log to get in sync with the leader 0x200000001. However, the node 2 failed to properly truncate the setData at zxid 0x400000001. So, even if resync was finished, the value 1003 is still remained intact on the node 2 while other nodes have value 3 for the same key.
    c. It seems on the node 2, there is only log.100000001 and log.400000001 but no log.200000001. This seems to cause failing to delete log.400000001 during truncate(). It looks like we will be considering log.400000001 by the time returning from the init() of FileTxnLog.java so that we will never execute ‘itr.logFile.delete()’ for the log.400000001. 
   d. Then, after returning from the truncate(), loadDatabase() will be invoked and log.400000001 will be read and the setData at zxid 0x400000001 gets loaded into the in-memory data tree.
{panel:title=Log from the node 2:}
...
2017-11-16 03:08:59,051 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:Follower@65] - FOLLOWING - LEADER ELECTION TOOK - 215
2017-11-16 03:08:59,052 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:QuorumPeer$QuorumServer@184] - Resolved hostname: 127.0.0.1 to address: /127.0.0.1
2017-11-16 03:08:59,125 [myid:2] - WARN  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:Learner@349] - Truncating log to get in sync with the leader 0x200000001
2017-11-16 03:08:59,125 [myid:2] - DEBUG [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:FileTxnLog$FileTxnIterator@606] - Created new input stream /home/ben/project/strata/test-5-3-ZooKeeper-3.4.11-strata-0.1/data/2/version-2/log.100000001
2017-11-16 03:08:59,125 [myid:2] - DEBUG [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:FileTxnLog$FileTxnIterator@609] - Created new input archive /home/ben/project/strata/test-5-3-ZooKeeper-3.4.11-strata-0.1/data/2/version-2/log.100000001
2017-11-16 03:08:59,126 [myid:2] - DEBUG [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:FileTxnLog$FileTxnIterator@647] - EOF excepton java.io.EOFException
2017-11-16 03:08:59,126 [myid:2] - DEBUG [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:FileTxnLog$FileTxnIterator@606] - Created new input stream /home/ben/project/strata/test-5-3-ZooKeeper-3.4.11-strata-0.1/data/2/version-2/log.400000001
2017-11-16 03:08:59,126 [myid:2] - DEBUG [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:FileTxnLog$FileTxnIterator@609] - Created new input archive /home/ben/project/strata/test-5-3-ZooKeeper-3.4.11-strata-0.1/data/2/version-2/log.400000001
2017-11-16 03:08:59,126 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:FileSnap@83] - Reading snapshot /home/ben/project/strata/test-5-3-ZooKeeper-3.4.11-strata-0.1/data/2/version-2/snapshot.200000001
2017-11-16 03:08:59,127 [myid:2] - DEBUG [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:FileTxnLog$FileTxnIterator@606] - Created new input stream /home/ben/project/strata/test-5-3-ZooKeeper-3.4.11-strata-0.1/data/2/version-2/log.100000001
2017-11-16 03:08:59,127 [myid:2] - DEBUG [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:FileTxnLog$FileTxnIterator@609] - Created new input archive /home/ben/project/strata/test-5-3-ZooKeeper-3.4.11-strata-0.1/data/2/version-2/log.100000001
2017-11-16 03:08:59,128 [myid:2] - DEBUG [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:FileTxnLog$FileTxnIterator@647] - EOF excepton java.io.EOFException
2017-11-16 03:08:59,128 [myid:2] - DEBUG [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:FileTxnLog$FileTxnIterator@606] - Created new input stream /home/ben/project/strata/test-5-3-ZooKeeper-3.4.11-strata-0.1/data/2/version-2/log.400000001
2017-11-16 03:08:59,128 [myid:2] - DEBUG [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:FileTxnLog$FileTxnIterator@609] - Created new input archive /home/ben/project/strata/test-5-3-ZooKeeper-3.4.11-strata-0.1/data/2/version-2/log.400000001
2017-11-16 03:08:59,128 [myid:2] - DEBUG [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:FileTxnLog$FileTxnIterator@647] - EOF excepton java.io.EOFException
2017-11-16 03:08:59,131 [myid:2] - WARN  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:Learner@387] - Got zxid 0x500000001 expected 0x1
2017-11-16 03:08:59,132 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:FileTxnSnapLog@248] - Snapshotting: 0x500000004 to /home/ben/project/strata/test-5-3-ZooKeeper-3.4.11-strata-0.1/data/2/version-2/snapshot.500000004
...
{panel}
{panel:title=Log from the node 0:}
...
2017-11-16 03:08:59,050 [myid:0] - INFO  [QuorumPeer[myid=0]/0:0:0:0:0:0:0:0:11221:Leader@372] - LEADING - LEADER ELECTION TOOK - 222
2017-11-16 03:08:59,055 [myid:0] - INFO  [LearnerHandler-/127.0.0.1:54482:LearnerHandler@346] - Follower sid: 2 : info : org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@266e422
2017-11-16 03:08:59,124 [myid:0] - INFO  [LearnerHandler-/127.0.0.1:54482:LearnerHandler@401] - Synchronizing with Follower sid: 2 maxCommittedLog=0x500000004 minCommittedLog=0x100000001 peerLastZxid=0x400000001
2017-11-16 03:08:59,124 [myid:0] - DEBUG [LearnerHandler-/127.0.0.1:54482:LearnerHandler@415] - proposal size is 14
2017-11-16 03:08:59,124 [myid:0] - DEBUG [LearnerHandler-/127.0.0.1:54482:LearnerHandler@418] - Sending proposals to follower
2017-11-16 03:08:59,124 [myid:0] - INFO  [LearnerHandler-/127.0.0.1:54482:LearnerHandler@475] - Sending TRUNC
2017-11-16 03:08:59,147 [myid:0] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11221:NIOServerCnxnFactory@215] - Accepted socket connection from /127.0.0.1:55118
2017-11-16 03:08:59,184 [myid:0] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11221:NIOServerCnxn@383] - Exception causing close of session 0x0: ZooKeeperServer not running
2017-11-16 03:08:59,184 [myid:0] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11221:NIOServerCnxn@386] - IOException stack trace
java.io.IOException: ZooKeeperServer not running
        at org.apache.zookeeper.server.NIOServerCnxn.readLength(NIOServerCnxn.java:977)
        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:257)
        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:226)
        at java.lang.Thread.run(Thread.java:745)
2017-11-16 03:08:59,184 [myid:0] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11221:NIOServerCnxn@1040] - Closed socket connection for client /127.0.0.1:55118 (no session established for client)
2017-11-16 03:08:59,224 [myid:0] - INFO  [LearnerHandler-/127.0.0.1:54482:LearnerHandler@535] - Received NEWLEADER-ACK message from 2
2017-11-16 03:08:59,224 [myid:0] - INFO  [QuorumPeer[myid=0]/0:0:0:0:0:0:0:0:11221:Leader@962] - Have quorum of supporters, sids: [ 0,2 ]; starting up and setting last processed zxid: 0x600000000
...
{panel}",[],Bug,ZOOKEEPER-2946,Major,anaud,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,The truncate() function in FileTxnLog.java may fail to properly remove an uncommitted write resulting in data inconsistency,2017-11-27T19:29:08.000+0000,"[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.4.12', id='12342040'>]",2.0
,[],2017-11-20T08:23:49.000+0000,anaud,"Synchronization code in the syncWithLeader function of Learner.java doesn't seems to truncate uncommitted writes properly when the follower receives SNAP msg from the leader. This results in data inconsistency in the in-memory data tree across nodes. Here is one procedure to reproduce the inconsistency. (Actually, this seems similar to my previous report on ZOOKEEPER-2832, but it was for 3.4.10 and this one is for 3.4.11 and later)

Initially:
# Start the ensemble with three nodes: node 0, 1 and 2 (the node 2 is the leader)
# Create 5 znodes with initial values as follow (key = value)
{noformat}
/testDivergenceResync0 = 0
/testDivergenceResync1 = 1
/testDivergenceResync2 = 2
/testDivergenceResync3 = 3
/testDivergenceResync4 = 4
{noformat}

To Reproduce:
# Diverge the node 2
a. Shutdown the node 0 and 1
b. Async setData to the node 2 writing 1000 to the key ‘/testDivergenceResync0’
c. Shutdown the node 2
# Restart the node 0 and 1 (let them finish with resync)
# Diverge the node 1
a. Shutdown the node 0
b. Async setData to the node 1 writing 1001 to the key ‘/testDivergenceResync1’
c. Shutdown the node 1
# Restart the node 0 and 1 (let them finish with resync)
# Diverge the node 1
a. Shutdown the node 0
b. Async setData to the node 1 writing 1002 to the key ‘/testDivergenceResync2’
c. Shutdown the node 1
# Restart the node 0 and 2 (let them finish with resync)
# Diverge the node 0
a. Shutdown the node 2
b. Async setData to the node 0 writing 1003 to the key ‘/testDivergenceResync3’
c. Shutdown the node 0
# Restart the node 1 and 2 (let them finish with resync)
# Diverge the node 2
a. Shutdown the node 1
b. Async setData to the node 2 writing 1004 to the key ‘/testDivergenceResync4’
c. Shutdown the node 2
# Restart the node 1 and 2 (let them finish with resync)
# Restart the node 0 (let it finish with resync)

Reading each key from each node directly will give us the output:
{noformat}
/testDivergenceResync0 on the node 0 = 0
/testDivergenceResync0 on the node 1 = 0
/testDivergenceResync0 on the node 2 = 0
/testDivergenceResync1 on the node 0 = 1001
/testDivergenceResync1 on the node 1 = 1001
/testDivergenceResync1 on the node 2 = 1001
/testDivergenceResync2 on the node 0 = 2
/testDivergenceResync2 on the node 1 = 1002
/testDivergenceResync2 on the node 2 = 2
/testDivergenceResync3 on the node 0 = 3
/testDivergenceResync3 on the node 1 = 3
/testDivergenceResync3 on the node 2 = 3
/testDivergenceResync4 on the node 0 = 1004
/testDivergenceResync4 on the node 1 = 1004
/testDivergenceResync4 on the node 2 = 1004
{noformat}
The value of key /testDivergenceResync2 is inconsistent across nodes -- node 1 has a new value that will never be replicated to the other nodes.

What seems to happen:
# At the step 5, setData (at zxid 0x300000001) writing the value 1002 is committed on the node 1.
{panel:title=Log from the node 1:}
...
2017-11-16 03:02:19,964 [myid:1] - DEBUG [ProcessThread(sid:1 cport:-1)::CommitProcessor@174] - Processing request:: sessionid:0x100011108080000 type:setData cxid:0x4 zxid:0x300000001 txntype:5 reqpath:n/a
2017-11-16 03:02:19,964 [myid:1] - DEBUG [ProcessThread(sid:1 cport:-1)::Leader@787] - Proposing:: sessionid:0x100011108080000 type:setData cxid:0x4 zxid:0x300000001 txntype:5 reqpath:n/a
2017-11-16 03:02:19,965 [myid:1] - INFO  [SyncThread:1:FileTxnLog@209] - Creating new log file: log.300000001
2017-11-16 03:02:20,016 [myid:1] - DEBUG [SyncThread:1:Leader@600] - Count for zxid: 0x300000001 is 1
2017-11-16 03:02:21,173 [myid:1] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:Leader@512] - Shutting down
2017-11-16 03:02:21,173 [myid:1] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:Leader@518] - Shutdown called
java.lang.Exception: shutdown Leader! reason: Not sufficient followers synced, only synced with sids: [ 1 ]
...
{panel}
# At the step 8, the node 1 is restarted and supposed to be properly resync’ed with the node 2 which is the leader.
    a. The node 2 sends SNAP msg so that the node 1 can restore its in-memory data tree from the snapshot of the in-memory data tree on the node 2. 
    b. On the other hand, the node 1 will clear its in-memory data tree and restore it with the snapshot from the node 2. Then, it takes its own snapshot at zxid 0x200000001. 
    c. However, this does not remove the setData at zxid 0x300000001 from the transaction log on the node 1.
{panel:title=Log from the node 2:}
...
2017-11-16 03:02:37,470 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:Leader@372] - LEADING - LEADER ELECTION TOOK - 232
2017-11-16 03:02:37,479 [myid:2] - INFO  [LearnerHandler-/127.0.0.1:46899:LearnerHandler@346] - Follower sid: 1 : info : org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@46cc2846
2017-11-16 03:02:37,626 [myid:2] - INFO  [LearnerHandler-/127.0.0.1:46899:LearnerHandler@401] - Synchronizing with Follower sid: 1 maxCommittedLog=0x0 minCommittedLog=0x0 peerLastZxid=0x300000001
2017-11-16 03:02:37,626 [myid:2] - DEBUG [LearnerHandler-/127.0.0.1:46899:LearnerHandler@472] - proposals is empty
2017-11-16 03:02:37,626 [myid:2] - INFO  [LearnerHandler-/127.0.0.1:46899:LearnerHandler@475] - Sending SNAP
2017-11-16 03:02:37,626 [myid:2] - INFO  [LearnerHandler-/127.0.0.1:46899:LearnerHandler@499] - Sending snapshot last zxid of peer is 0x300000001  zxid of leader is 0x500000000sent zxid of db as 0x200000001
2017-11-16 03:02:37,701 [myid:2] - INFO  [LearnerHandler-/127.0.0.1:46899:LearnerHandler@535] - Received NEWLEADER-ACK message from 1
2017-11-16 03:02:37,702 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:Leader@962] - Have quorum of supporters, sids: [ 1,2 ]; starting up and setting last processed zxid: 0x500000000
...
{panel}
{panel:title=Log from the node 1:}
...
2017-11-16 03:02:37,473 [myid:1] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:Follower@65] - FOLLOWING - LEADER ELECTION TOOK - 218
2017-11-16 03:02:37,475 [myid:1] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:QuorumPeer$QuorumServer@184] - Resolved hostname: 
127.0.0.1 to address: /127.0.0.1
2017-11-16 03:02:37,593 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11224:NIOServerCnxnFactory@215] - Accepted socket connection from /127.0.0.1:57338
2017-11-16 03:02:37,626 [myid:1] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:Learner@336] - Getting a snapshot from leader 0x200000001
2017-11-16 03:02:37,627 [myid:1] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11224:NIOServerCnxn@383] - Exception causing close of 
session 0x0: ZooKeeperServer not running
2017-11-16 03:02:37,627 [myid:1] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11224:NIOServerCnxn@386] - IOException stack trace
java.io.IOException: ZooKeeperServer not running
        at org.apache.zookeeper.server.NIOServerCnxn.readLength(NIOServerCnxn.java:977)
        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:257)
        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:226)
        at java.lang.Thread.run(Thread.java:745)
2017-11-16 03:02:37,627 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11224:NIOServerCnxn@1040] - Closed socket connection f
or client /127.0.0.1:57338 (no session established for client)
2017-11-16 03:02:37,629 [myid:1] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:FileTxnSnapLog@248] - Snapshotting: 0x200000001 to /home/ben/project/strata/test-5-3-ZooKeeper-3.4.11-strata-0.1/data/1/version-2/snapshot.200000001
...
{panel}
# At the step 10, the node 1 is restarted again and supposed to be properly resync’ed with the node 2 which is the leader again.
a. When the node 1 is restarted, it restores its in-memory data tree from the snapshot at zxid 0x200000001 and replay setData at zxid 0x300000001 (which actually needed to be truncated)
b. However, the node 2 just sends DIFF containing setData written at 9th path, and no truncation will be occurred.
c. As a result, the node 1 still has the value 1002 while other nodes will have the value 2 for the same key
{panel}
{panel:title=Log from the node 2:}
…
2017-11-16 03:03:21,033 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:Leader@372] - LEADING - LEADER ELECTION TOOK - 217
2017-11-16 03:03:21,038 [myid:2] - INFO  [LearnerHandler-/127.0.0.1:46967:LearnerHandler@346] - Follower sid: 1 : info : org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@1e1cf18c
2017-11-16 03:03:21,103 [myid:2] - INFO  [LearnerHandler-/127.0.0.1:46967:LearnerHandler@401] - Synchronizing with Follower sid: 1 maxCommittedLog=0x500000004 minCommittedLog=0x500000001 peerLastZxid=0x500000003
2017-11-16 03:03:21,103 [myid:2] - DEBUG [LearnerHandler-/127.0.0.1:46967:LearnerHandler@415] - proposal size is 4
2017-11-16 03:03:21,103 [myid:2] - DEBUG [LearnerHandler-/127.0.0.1:46967:LearnerHandler@418] - Sending proposals to follower
2017-11-16 03:03:21,103 [myid:2] - INFO  [LearnerHandler-/127.0.0.1:46967:LearnerHandler@475] - Sending DIFF
2017-11-16 03:03:21,156 [myid:2] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11227:NIOServerCnxnFactory@215] - Accepted socket connection from /127.0.0.1:49611
2017-11-16 03:03:21,178 [myid:2] - INFO  [LearnerHandler-/127.0.0.1:46967:LearnerHandler@535] - Received NEWLEADER-ACK message from 1
2017-11-16 03:03:21,178 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11227:Leader@962] - Have quorum of supporters, sids: [ 1,2 ]; starting up and setting last processed zxid: 0x600000000
2017-11-16 03:03:21,196 [myid:2] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11227:NIOServerCnxn@383] - Exception causing close of session 0x0: ZooKeeperServer not running
2017-11-16 03:03:21,196 [myid:2] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11227:NIOServerCnxn@386] - IOException stack trace
java.io.IOException: ZooKeeperServer not running
        at org.apache.zookeeper.server.NIOServerCnxn.readLength(NIOServerCnxn.java:977)
        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:257)
        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:226)
        at java.lang.Thread.run(Thread.java:745)
2017-11-16 03:03:21,196 [myid:2] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11227:NIOServerCnxn@1040] - Closed socket connection for client /127.0.0.1:49611 (no session established for client)
2017-11-16 03:03:21,237 [myid:2] - DEBUG [LearnerHandler-/127.0.0.1:46967:Leader@579] - outstanding is 0
...
{panel}
{panel}
{panel:title=Log from the node 1:}
…
2017-11-16 03:03:21,034 [myid:1] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:Follower@65] - FOLLOWING - LEADER ELECTION TOOK - 222
2017-11-16 03:03:21,035 [myid:1] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:QuorumPeer$QuorumServer@184] - Resolved hostname: 127.0.0.1 to address: /127.0.0.1
2017-11-16 03:03:21,104 [myid:1] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:Learner@332] - Getting a diff from the leader 0x500000004
2017-11-16 03:03:21,105 [myid:1] - WARN  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:Learner@387] - Got zxid 0x500000004 expected 0x1
2017-11-16 03:03:21,189 [myid:1] - INFO  [SyncThread:1:FileTxnLog@209] - Creating new log file: log.500000004
2017-11-16 03:03:21,189 [myid:1] - DEBUG [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:CommitProcessor@164] - Committing request:: sessionid:0x2000111082b0000 type:setData cxid:0x4 zxid:0x500000004 txntype:5 reqpath:n/a
2017-11-16 03:03:21,189 [myid:1] - DEBUG [CommitProcessor:1:FinalRequestProcessor@89] - Processing request:: sessionid:0x2000111082b0000 type:setData cxid:0x4 zxid:0x500000004 txntype:5 reqpath:n/a
...
{panel}",[],Bug,ZOOKEEPER-2945,Major,anaud,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Synchronization code on the follower does not properly truncate uncommitted write resulting in data inconsistency,2017-11-27T19:23:45.000+0000,"[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.4.12', id='12342040'>]",4.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2017-11-17T18:46:45.000+0000,Chris Donati,"When a sequence counter exceeds 2147483647, the next value is -2147483648.

https://zookeeper.apache.org/doc/trunk/zookeeperProgrammers.html#Sequence+Nodes+--+Unique+Naming","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",Bug,ZOOKEEPER-2944,Trivial,Chris Donati,Fixed,2017-11-21T17:50:02.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Specify correct overflow value,2017-11-21T19:49:02.000+0000,[],4.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2017-11-15T21:47:55.000+0000,Jack Foy,"http://zookeeper.apache.org/doc/r3.4.11 and links under that location all return 404, including the Release Notes link on http://zookeeper.apache.org/releases.html under ""9 November, 2017: release 3.4.11 available"".",[],Bug,ZOOKEEPER-2942,Major,Jack Foy,Cannot Reproduce,2018-01-05T23:08:53.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,All 3.4.11 documentation links on site return 404,2018-01-05T23:08:53.000+0000,"[<JIRA Version: name='3.4.11', id='12339207'>]",0.0
,[],2017-11-09T00:31:57.000+0000,Abhay Bothra,"We see the following logs in the node with {{myid: 1}}
{code}
2017-11-08 15:06:28,375 [myid:1] - INFO  [WorkerSender[myid=1]:QuorumCnxManager@193] - Have smaller server identifier, so dropping the connection: (2, 1)
2017-11-08 15:06:28,375 [myid:1] - INFO  [WorkerSender[myid=1]:QuorumCnxManager@193] - Have smaller server identifier, so dropping the connection: (3, 1)
2017-11-08 15:07:28,375 [myid:1] - INFO  [WorkerReceiver[myid=1]:FastLeaderElection@597] - Notification: 1 (message format version), 1 (n.leader), 0x28e000a8750 (n.zxid), 0x1 (n.round), LOOKING (n.state), 1 (n.sid), 0x28e (n.peerEpoch) LOOKING (my state)
2017-11-08 15:07:28,375 [myid:1] - INFO  [WorkerSender[myid=1]:QuorumCnxManager@193] - Have smaller server identifier, so dropping the connection: (2, 1)
2017-11-08 15:07:28,376 [myid:1] - INFO  [WorkerSender[myid=1]:QuorumCnxManager@193] - Have smaller server identifier, so dropping the connection: (3, 1)
2017-11-08 15:08:28,375 [myid:1] - INFO  [WorkerReceiver[myid=1]:FastLeaderElection@597] - Notification: 1 (message format version), 1 (n.leader), 0x28e000a8750 (n.zxid), 0x1 (n.round), LOOKING (n.state), 1 (n.sid), 0x28e (n.peerEpoch) LOOKING (my state)
2017-11-08 15:08:28,376 [myid:1] - INFO  [WorkerSender[myid=1]:QuorumCnxManager@193] - Have smaller server identifier, so dropping the connection: (2, 1)
2017-11-08 15:08:28,376 [myid:1] - INFO  [WorkerSender[myid=1]:QuorumCnxManager@193] - Have smaller server identifier, so dropping the connection: (3, 1)
2017-11-08 15:09:28,376 [myid:1] - INFO  [WorkerReceiver[myid=1]:FastLeaderElection@597] - Notification: 1 (message format version), 1 (n.leader), 0x28e000a8750 (n.zxid), 0x1 (n.round), LOOKING (n.state), 1 (n.sid), 0x28e (n.peerEpoch) LOOKING (my state)
2017-11-08 15:09:28,376 [myid:1] - INFO  [WorkerSender[myid=1]:QuorumCnxManager@193] - Have smaller server identifier, so dropping the connection: (2, 1)
2017-11-08 15:09:28,376 [myid:1] - INFO  [WorkerSender[myid=1]:QuorumCnxManager@193] - Have smaller server identifier, so dropping the connection: (3, 1)
2017-11-08 15:10:28,376 [myid:1] - INFO  [WorkerReceiver[myid=1]:FastLeaderElection@597] - Notification: 1 (message format version), 1 (n.leader), 0x28e000a8750 (n.zxid), 0x1 (n.round), LOOKING (n.state), 1 (n.sid), 0x28e (n.peerEpoch) LOOKING (my state)
2017-11-08 15:10:28,376 [myid:1] - INFO  [WorkerSender[myid=1]:QuorumCnxManager@193] - Have smaller server identifier, so dropping the connection: (2, 1)
2017-11-08 15:10:28,377 [myid:1] - INFO  [WorkerSender[myid=1]:QuorumCnxManager@193] - Have smaller server identifier, so dropping the connection: (3, 1)
{code}

On the nodes with {{myid: 2}} and {{myid: 3}}, we see connection broken events for {{myid: 1}}
{code}
2017-11-07 02:54:32,135 [myid:2] - WARN  [RecvWorker:1:QuorumCnxManager$RecvWorker@780] - Connection broken for id 1, my id = 2, error =
java.net.SocketException: Connection reset
        at java.net.SocketInputStream.read(SocketInputStream.java:209)
        at java.net.SocketInputStream.read(SocketInputStream.java:141)
        at java.net.SocketInputStream.read(SocketInputStream.java:223)
        at java.io.DataInputStream.readInt(DataInputStream.java:387)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:765)
2017-11-07 02:54:32,135 [myid:2] - WARN  [RecvWorker:1:QuorumCnxManager$RecvWorker@783] - Interrupting SendWorker
2017-11-07 02:54:32,135 [myid:2] - WARN  [SendWorker:1:QuorumCnxManager$SendWorker@697] - Interrupted while waiting for message on queue
java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2088)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.pollSendQueue(QuorumCnxManager.java:849)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.access$500(QuorumCnxManager.java:64)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:685)
2017-11-07 02:54:32,135 [myid:2] - WARN  [SendWorker:1:QuorumCnxManager$SendWorker@706] - Send worker leaving thread
{code}

From the reported occurrences, it looks like this is a problem only when the node with the smallest {{myid}} loses connection.",[],Bug,ZOOKEEPER-2938,Major,Abhay Bothra,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Server is unable to join quorum after connection broken to other peers,2022-01-24T16:00:14.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.4.14', id='12343587'>]",40.0
,"[<JIRA Component: name='server', id='12312382'>]",2017-11-08T16:30:58.000+0000,Sriram Chandramouli,"we have created an authentication provider plugin that can authenticate clients based on the cert that client is presenting. our zookeeper instance has been configured (and started) to authenticate and allow only certain appid's. this works as intended when clients (ours are c-clients) send an auth message via yca_add_auth containing the cert *and* the authentication provider is configured to allow it.

however, if the clients do *not* present one (i.e. do not send an auth packet), and if the authentication provider allows only certain appid's, this connection still goes through - i.e. clients are able to connect, create/watch nodes etc.! this is unexpected and does *not* allow us to prevent certain clients from connecting to a zookeeper quorum (as they can still connect without present any credentials). 

it looks like zookeeper will only invoke the auth providers if it receives an auth packet from the client.

none of this block - https://github.com/sriramch/zookeeper/blob/master/src/java/main/org/apache/zookeeper/server/ZooKeeperServer.java#L1060

ever gets executed, and it directly jumps to this 

https://github.com/sriramch/zookeeper/blob/master/src/java/main/org/apache/zookeeper/server/ZooKeeperServer.java#L1108

we have a usecase where we only want clients that can present valid credentials to connect to zookeeper (zk). 

i was hoping to expose an interface where different auth providers (when they are loaded)  would let zk know if they need to authenticate a client before processing other data packets. the default ones (kerberos/ip/digest etc.) would say no to maintain compatibility. our auth provider can be configured to say yes/no (default no) depending on use-case. zk before processing a data packet can look at the auth info in the server connection to see the schemes that requires authentication and have successfully authenticated. connection will succeed if all schemes that require authentication have successfully authenticated; else, we disable receive.

can someone please look into this issue and evaluate the proposal? i can work on creating a pr for this.","[<JIRA Version: name='3.4.6', id='12323310'>]",Bug,ZOOKEEPER-2937,Major,Sriram Chandramouli,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,zookeeper issues with handling authentication...,2021-07-16T09:24:10.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",3.0
,"[<JIRA Component: name='contrib', id='12312700'>, <JIRA Component: name='other', id='12333125'>]",2017-11-08T15:51:51.000+0000,Hari Sekhon,"Recent versions of ZooKeeper have introduced the following duplicate keys in to the contrib log4j.properties files.

In this file:
{code}
./zookeeper-3.4.8/contrib/rest/conf/log4j.properties
{code}
and this file:
{code}
./zookeeper-3.4.8/src/contrib/rest/conf/log4j.properties
{code}

the following duplicate keys are found:

{code}
log4j.appender.ROLLINGFILE.layout=org.apache.log4j.PatternLayout
log4j.appender.ROLLINGFILE.layout.ConversionPattern=%d{ISO8601} - %-5p [%t:%C{1}@%L] - %m%n
{code}

This was discovered because I've written file validators for most major formats which recurse all my github repos and this was failing my integration tests when pulling ZooKeeper source code. I actually added --exclude and --ignore-duplicate-keys switches to {code}validate_ini.py{code} to work around this and fix my builds for now but just remembered to raise this to you guys.

The validator tools if you're interested can be found at:

https://github.com/harisekhon/pytools

Cheers

Hari","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",Bug,ZOOKEEPER-2936,Trivial,Hari Sekhon,Fixed,2018-03-02T22:13:48.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Duplicate Keys in log4j.properties config files,2018-03-02T22:35:00.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",5.0
Andor Molnar,"[<JIRA Component: name='recipes', id='12313246'>]",2017-11-06T20:44:47.000+0000,Abraham Fine,"I see errors like: 
{code}
/var/zookeeper/src/recipes/queue/src/c/../../../../../src/c/include/zookeeper_log.h:39:74: error: expected expression before ')' token
     log_message(_cb, ZOO_LOG_LEVEL_DEBUG, __LINE__, __func__, __VA_ARGS__)
                                                                          ^
{code}","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2934,Major,Abraham Fine,Fixed,2017-11-15T22:19:08.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,c versions of election and queue recipes do not compile,2017-11-15T22:32:24.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>]",5.0
,[],2017-11-05T18:56:51.000+0000,Javier Cacheiro,"When the nodes are sorted in WriteLock.java using a TreeSet the whole znode path is taken into account and not just the sequence number.

This causes an issue when the sessionId is included in the znode path because a znode with a lower sessionId will appear as lower than other znode with a higher sessionId even if its sequence number is bigger. 

In specific situations this ended with two clients holding the lock at the same time.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",Bug,ZOOKEEPER-2931,Major,Javier Cacheiro,Fixed,2017-11-15T23:36:03.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,WriteLock recipe: incorrect znode ordering when the sessionId is part of the znode name,2017-11-16T00:27:19.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>]",4.0
,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2017-11-03T08:16:20.000+0000,Jiafu Jiang,"I deploy a cluster of ZooKeeper with three nodes:

ofs_zk1:20.10.11.101, 30.10.11.101
ofs_zk2:20.10.11.102, 30.10.11.102
ofs_zk3:20.10.11.103, 30.10.11.103

I shutdown the network interfaces of ofs_zk2 using ""ifdown eth0 eth1"" command.

It is supposed that the new Leader should be elected in some seconds, but the fact is, ofs_zk1 and ofs_zk3 just keep electing again and again, but none of them can become the new Leader.

I change the log level to DEBUG (the default is INFO), and restart zookeeper servers on ofs_zk1 and ofs_zk2 again, but it can not fix the problem.

I read the log and the ZooKeeper source code, and I think I find the reason.

When the potential leader(says ofs_zk3) begins the election(FastLeaderElection.lookForLeader()), it will send notifications to all the servers. 
When it fails to receive any notification during a timeout, it will resend the notifications, and double the timeout. This process will repeat until any notification is received or the timeout reaches a max value.
The FastLeaderElection.sendNotifications() just put the notification message into a queue and return. The WorkerSender is responsable to send the notifications.

The WorkerSender just process the notifications one by one by passing the notifications to QuorumCnxManager. Here comes the problem, the QuorumCnxManager.toSend() blocks for a long time when the notification is send to ofs_zk2(whose network is down) and some notifications (which belongs to ofs_zk1) will thus be blocked for a long time. The repeated notifications by FastLeaderElection.sendNotifications() just make things worse.

Here is the related source code:

{code:java}
    public void toSend(Long sid, ByteBuffer b) {
        /*
         * If sending message to myself, then simply enqueue it (loopback).
         */
        if (this.mySid == sid) {
             b.position(0);
             addToRecvQueue(new Message(b.duplicate(), sid));
            /*
             * Otherwise send to the corresponding thread to send.
             */
        } else {
             /*
              * Start a new connection if doesn't have one already.
              */
             ArrayBlockingQueue<ByteBuffer> bq = new ArrayBlockingQueue<ByteBuffer>(SEND_CAPACITY);
             ArrayBlockingQueue<ByteBuffer> bqExisting = queueSendMap.putIfAbsent(sid, bq);
             if (bqExisting != null) {
                 addToSendQueue(bqExisting, b);
             } else {
                 addToSendQueue(bq, b);
             }
             
             // This may block!!!
             connectOne(sid);
                
        }
    }
{code}

Therefore, when ofs_zk3 believes that it is the leader, it begins to wait the epoch ack, but in fact the ofs_zk1 does not receive the notification(which says the leader is ofs_zk3) because the ofs_zk3 has not sent the notification(which may still exist in the sendqueue of WorkerSender). At last, the potential leader ofs_zk3 fails to receive the epoch ack in timeout, so it quits the leader and begins a new election. 

The log files of ofs_zk1 and ofs_zk3 are attached.",[],Bug,ZOOKEEPER-2930,Critical,Jiafu Jiang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Leader cannot be elected due to network timeout of some members.,2020-04-03T20:16:00.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.4.12', id='12342040'>]",10.0
,"[<JIRA Component: name='java client', id='12312381'>]",2017-11-02T14:20:11.000+0000,Pavan,"1. Zookeeper server deployed as a docker container in Kubernetes
2. In the Java Client configured zookeeper 'domainname' for the server address 
3. Once we restart the Zookeeper 'POD', the Zookeeper container starting with new IP
4. During this time the Zookeeper client able to resolve the new ip and making the connection But it is also keep trying to connect to old IP also. The connection status in netstat is coming as 
        SYNC_SENT and Connection getting closed 

Note: Already applied https://issues.apache.org/jira/browse/ZOOKEEPER-2184 patch 

",[],Bug,ZOOKEEPER-2929,Major,Pavan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,[DNS Support] Zookeeper client still trying to establish the connection with old IP even after Zookeeper Server restarted with new IP when domain name configured at the client side,2019-10-04T14:55:10.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.3', id='12335444'>]",2.0
,"[<JIRA Component: name='c client', id='12312380'>]",2017-10-30T07:59:42.000+0000,xiaomingzhongguo,"when call zookeeper_close 
thread hang at pthread_join ,  do_io thread not exist , and do_completion not exit 

#0  0x00002b8e38b6b725 in pthread_join () from /lib64/libpthread.so.0
#1  0x0000000000cc6b86 in adaptor_finish (zh=0x2aaaaae05240) at src/mt_adaptor.c:285
#2  0x0000000000cc21f3 in zookeeper_close (zh=0x2aaaaae05240) at src/zookeeper.c:2493
#3  0x00000000008eeb04 in ZkAPI::ZkClose ()
#4  0x00000000009270b1 in AgentInfo::zkCloseConnection ()
#5  0x0000000000929e02 in AgentInfo::timeSyncHandler ()
#6  0x00000000010f0585 in event_base_loop (base=0x1679d00, flags=0) at event.c:1350
#7  0x0000000000924f31 in AgentInfo::run ()
#8  0x00000000008998bf in gseThread::run_helper ()
#9  0x0000000000922956 in tos::util_thread_start ()
#10 0x00002b8e38b6a193 in start_thread () from /lib64/libpthread.so.0
#11 0x00002b8e3929ff0d in clone () from /lib64/libc.so.6

#0  0x00002b8e38b6e326 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x0000000000cc70be in do_completion (v=0x2aaaaae05240) at src/mt_adaptor.c:463
#2  0x00002b8e38b6a193 in start_thread () from /lib64/libpthread.so.0
#3  0x00002b8e3929ff0d in clone () from /lib64/libc.so.6
#4  0x0000000000000000 in ?? ()
",[],Bug,ZOOKEEPER-2928,Critical,xiaomingzhongguo,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,pthread_join  hang at zookeeper_close,2018-04-02T21:11:52.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",2.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2017-10-27T23:23:08.000+0000,Fangmin Lv,"The local session upgrading feature will upgrade the session locally before receving a quorum commit of creating global session. It's possible that the server shutdown before the creating session request being sent to leader, if we retained the ZKDatabase or there is Snapshot happened just before shutdown, then only this server will have the global session. 

If that server didn't become leader, then it will have more global sessions than others, and those global sessions won't expire as the leader doesn't know it's existence. If the server became leader, it will accept the client renew session request and the client is allowed to create ephemeral nodes, which means other servers only have ephemeral nodes but not that global session. If there is follower going to have SNAP sync with it, then it will also have the global session. If the server without that global session becomes new leader, it will check and delete those dangling ephemeral node before serving traffic. These could introduce the issues that the ephemeral node being exist on some servers but not others. 

There is dangling global session issue even without local session feature, because on leader it will update the ZKDatabase when processing ConnectionRequest and in the PrepRequestProcessor before it's quorum committed, which also has this risk.","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2926,Critical,Fangmin Lv,Fixed,2018-08-08T04:21:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Data inconsistency issue due to the flaw in the session management,2018-08-08T07:22:15.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",7.0
,"[<JIRA Component: name='other', id='12333125'>]",2017-10-27T22:16:03.000+0000,Robert P. Thille,"Due to two threads trying to create the dataDir and snapDir, and the java.io.File.mkdirs() call returning false both for errors and for the directory already existing, sometimes ZooKeeper will fail to start with the following stack trace:

{noformat}
2017-10-25 22:30:40,069 [myid:] - INFO  [main:ZooKeeperServerMain@95] - Starting server
2017-10-25 22:30:40,075 [myid:] - INFO  [main:Environment@100] - Server environment:zookeeper.version=3.4.6-mdavis8efb625--1, built on 10/25/2017 01:12 GMT

[ More 'Server environment:blah blah blah' messages trimmed]

2017-10-25 22:30:40,077 [myid:] - INFO  [main:Environment@100] - Server environment:user.dir=/
2017-10-25 22:30:40,081 [myid:] - ERROR [main:ZooKeeperServerMain@63] - Unexpected exception, exiting abnormally
java.io.IOException: Unable to create data directory /bp2/data/version-2
    at org.apache.zookeeper.server.persistence.FileTxnSnapLog.<init>(FileTxnSnapLog.java:85)
    at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:104)
    at org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:86)
    at org.apache.zookeeper.server.ZooKeeperServerMain.main(ZooKeeperServerMain.java:52)
    at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:116)
    at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)
2017-10-25 22:30:40,085 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
{noformat}

this is caused by the QuorumPeerMain thread and the PurgeTask thread both competing to create the directories.","[<JIRA Version: name='3.4.10', id='12338036'>]",Bug,ZOOKEEPER-2925,Major,Robert P. Thille,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,ZooKeeper server fails to start on first-startup due to race to create dataDir & snapDir,2019-10-04T14:55:15.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",8.0
Andor Molnar,"[<JIRA Component: name='server', id='12312382'>, <JIRA Component: name='tests', id='12312427'>]",2017-10-26T14:31:54.000+0000,Andor Molnar,"From https://builds.apache.org/job/ZooKeeper_branch34_openjdk7/1682/

Same issue happens in jdk8 and jdk9 builds as well.

Issue has already been fixed by https://issues.apache.org/jira/browse/ZOOKEEPER-2484 , but I believe that the root cause here is that test startup / cleanup code is included in the tests instead of using try-finally block or Before-After methods.

As a consequence, when exception happens during test execution, ZK test server doesn't get shutdown properly and still listening on the port bound to the test class.

As mentioned above there could be 2 approaches to address this:
#1 Wrap cleanup code block with finally
#2 Use JUnit's Before-After methods for initialization and cleanup

Test where original issue happens:

{noformat}
...   
     [junit] 2017-10-12 15:05:20,135 [myid:] - INFO  [ProcessThread(sid:0 cport:11221)::PrepRequestProcessor@653] - Got user-level KeeperException when processing sessionid:0x104cd7b190c0000 type:create cxid:0x8c zxid:0x8d txntype:-1 req$
     [junit] 2017-10-12 15:05:20,137 [myid:] - INFO  [ProcessThread(sid:0 cport:11221)::PrepRequestProcessor@653] - Got user-level KeeperException when processing sessionid:0x104cd7b190c0000 type:create cxid:0x8d zxid:0x8e txntype:-1 req$
     [junit] 2017-10-12 15:05:20,139 [myid:] - INFO  [ProcessThread(sid:0 cport:11221)::PrepRequestProcessor@653] - Got user-level KeeperException when processing sessionid:0x104cd7b190c0000 type:create cxid:0x8e zxid:0x8f txntype:-1 req$
     [junit] 2017-10-12 15:05:20,142 [myid:] - INFO  [ProcessThread(sid:0 cport:11221)::PrepRequestProcessor@653] - Got user-level KeeperException when processing sessionid:0x104cd7b190c0000 type:create cxid:0x8f zxid:0x90 txntype:-1 req$
     [junit] 2017-10-12 15:05:20,144 [myid:] - INFO  [ProcessThread(sid:0 cport:11221)::PrepRequestProcessor@653] - Got user-level KeeperException when processing sessionid:0x104cd7b190c0000 type:create cxid:0x90 zxid:0x91 txntype:-1 req$
     [junit] 2017-10-12 15:05:30,479 [myid:] - INFO  [SessionTracker:ZooKeeperServer@354] - Expiring session 0x104cd7b190c0000, timeout of 6000ms exceeded
     [junit] 2017-10-12 15:05:32,996 [myid:] - INFO  [ProcessThread(sid:0 cport:11221)::PrepRequestProcessor@653] - Got user-level KeeperException when processing sessionid:0x104cd7b190c0000 type:ping cxid:0xfffffffffffffffe zxid:0xfffff$
     [junit] 2017-10-12 15:05:24,147 [myid:] - WARN  [main-SendThread(127.0.0.1:11221):ClientCnxn$SendThread@1111] - Client session timed out, have not heard from server in 4002ms for sessionid 0x104cd7b190c0000
     [junit] 2017-10-12 15:05:32,996 [myid:] - INFO  [main-SendThread(127.0.0.1:11221):ClientCnxn$SendThread@1159] - Client session timed out, have not heard from server in 4002ms for sessionid 0x104cd7b190c0000, closing socket connectio$
     [junit] 2017-10-12 15:05:21,479 [myid:] - INFO  [SessionTracker:SessionTrackerImpl@163] - SessionTrackerImpl exited loop!
     [junit] 2017-10-12 15:05:32,998 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11221:NIOServerCnxn@376] - Unable to read additional data from client sessionid 0x104cd7b190c0000, likely client has closed socket
     [junit] 2017-10-12 15:05:33,067 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11221:NIOServerCnxn@1040] - Closed socket connection for client /127.0.0.1:45735 which had sessionid 0x104cd7b190c0000
     [junit] 2017-10-12 15:05:32,996 [myid:] - INFO  [ProcessThread(sid:0 cport:11221)::PrepRequestProcessor@487] - Processed session termination for sessionid: 0x104cd7b190c0000
     [junit] 2017-10-12 15:05:33,889 [myid:] - INFO  [main:ZooKeeper@687] - Session: 0x104cd7b190c0000 closed
     [junit] 2017-10-12 15:05:33,890 [myid:] - INFO  [main-EventThread:ClientCnxn$EventThread@520] - EventThread shut down for session: 0x104cd7b190c0000
     [junit] 2017-10-12 15:05:33,891 [myid:] - INFO  [main:JUnit4ZKTestRunner$LoggedInvokeMethod@74] - TEST METHOD FAILED testRestoreWithTransactionErrors
     [junit] org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /invaliddir/test-
     [junit]     at org.apache.zookeeper.KeeperException.create(KeeperException.java:102)
     [junit]     at org.apache.zookeeper.KeeperException.create(KeeperException.java:54)
     [junit]     at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:786)
     [junit]     at org.apache.zookeeper.test.LoadFromLogTest.testRestoreWithTransactionErrors(LoadFromLogTest.java:368)
     [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
     [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
     [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
     [junit]     at java.lang.reflect.Method.invoke(Method.java:606)
     [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
     [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
     [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
     [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
     [junit]     at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:55)
     [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
     [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
     [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
     [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
     [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
     [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
     [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
     [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
     [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
     [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:535)
     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1182)
     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:1033)
{noformat}

Test #2 where port is already in use:

{noformat}
     [junit] 2017-10-12 15:05:33,899 [myid:] - INFO  [main:ZKTestCase$1@59] - STARTING testReloadSnapshotWithMissingParent
     [junit] 2017-10-12 15:05:33,899 [myid:] - INFO  [main:JUnit4ZKTestRunner$LoggedInvokeMethod@53] - RUNNING TEST METHOD testReloadSnapshotWithMissingParent
     [junit] 2017-10-12 15:05:33,900 [myid:] - INFO  [main:ZooKeeperServer@173] - Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 datadir /home/jenkins/jenkins-slave/workspace/ZooKeeper_branch34_openjdk7/$
     [junit] 2017-10-12 15:05:33,900 [myid:] - INFO  [main:ServerCnxnFactory@117] - Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory
     [junit] 2017-10-12 15:05:33,900 [myid:] - INFO  [main:NIOServerCnxnFactory@89] - binding to port 0.0.0.0/0.0.0.0:11221
     [junit] 2017-10-12 15:05:33,901 [myid:] - INFO  [main:JUnit4ZKTestRunner$LoggedInvokeMethod@74] - TEST METHOD FAILED testReloadSnapshotWithMissingParent
     [junit] java.net.BindException: Address already in use
     [junit]     at sun.nio.ch.Net.bind0(Native Method)
     [junit]     at sun.nio.ch.Net.bind(Net.java:463)
     [junit]     at sun.nio.ch.Net.bind(Net.java:455)
     [junit]     at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
     [junit]     at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
     [junit]     at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:67)
     [junit]     at org.apache.zookeeper.server.NIOServerCnxnFactory.configure(NIOServerCnxnFactory.java:90)
     [junit]     at org.apache.zookeeper.server.ServerCnxnFactory.createFactory(ServerCnxnFactory.java:137)
     [junit]     at org.apache.zookeeper.server.ServerCnxnFactory.createFactory(ServerCnxnFactory.java:130)
     [junit]     at org.apache.zookeeper.test.LoadFromLogTest.testReloadSnapshotWithMissingParent(LoadFromLogTest.java:412)
     [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
     [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
     [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
     [junit]     at java.lang.reflect.Method.invoke(Method.java:606)
     [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
     [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
     [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
     [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
     [junit]     at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:55)
     [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
     [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
     [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
     [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
     [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
     [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
     [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
     [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
     [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
     [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:535)
     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1182)
     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:1033)
{noformat}
","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",Bug,ZOOKEEPER-2924,Major,Andor Molnar,Fixed,2017-12-12T18:01:24.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Flaky Test: org.apache.zookeeper.test.LoadFromLogTest.testRestoreWithTransactionErrors,2017-12-12T18:16:10.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",5.0
Jiafu Jiang,"[<JIRA Component: name='quorum', id='12312379'>]",2017-10-23T02:25:09.000+0000,Jiafu Jiang,"The comment of the  variable matchSyncs in class CommitProcessor says:


{code:java}
    /**
     * This flag indicates whether we need to wait for a response to come back from the
     * leader or we just let the sync operation flow through like a read. The flag will
     * be true if the CommitProcessor is in a Leader pipeline.
     */
    boolean matchSyncs;
{code}

I search the source code and find that matchSyncs will be false if  the CommitProcessor is in a Leader pipeline, and it will be true if the CommitProcessor  is in a Follower pipeline.
Therefore I think the comment should be modified to match the code.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",Bug,ZOOKEEPER-2923,Minor,Jiafu Jiang,Fixed,2017-11-15T21:48:13.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,The comment of the variable matchSyncs in class CommitProcessor has a mistake.,2017-11-15T22:32:22.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>]",4.0
Patrick D. Hunt,"[<JIRA Component: name='build', id='12312383'>]",2017-10-17T21:49:16.000+0000,Abraham Fine,,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.13', id='12342973'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-2920,Major,Abraham Fine,Fixed,2018-05-31T03:58:07.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Upgrade OWASP Dependency Check to 3.2.1,2020-01-23T18:17:01.000+0000,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",6.0
Michael Han,[],2017-10-16T22:58:47.000+0000,Jun Rao,"We found the following issue when using ZK. A client (a Kafka broker) registered an ephemeral node in ZK. The client then received a session expiration event and created the new session. The client tried to create the same ephemeral node in ZK in the new session but received a NodeExistException. The following are the details.

From Kafka broker 1:
Broker 1 received the expiration of session 55bcff0f02d0002 at 13:33:26.

{code:java}
[2017-07-29 13:33:26,706] INFO Unable to reconnect to ZooKeeper service, session 0x55bcff0f02d0002 has expired, closing socket connection (org.apache.zookeeper.ClientCnxn)
{code}

It then established a new session 55d8f690ca20038 at 13:33:33.

{code:java}
[2017-07-29 13:33:33,405] INFO Session establishment complete on server rdalnydbbdqs10/10.122.104.12:2181, sessionid = 0x55d8f690ca20038, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
{code}

However, the re-registration of the broker id fails.

{code:java}
[2017-07-29 13:33:33,408] INFO Result of znode creation is: NODEEXISTS (kafka.utils.ZKCheckedEphemeral)
[2017-07-29 13:33:33,408] ERROR Error handling event ZkEvent[New session event sent to kafka.server.KafkaHealthcheck$SessionExpireListener@74ad6d14] (org.I0Itec.zkclient.ZkEvent
Thread)
java.lang.RuntimeException: A broker is already registered on the path /brokers/ids/1. This probably indicates that you either have configured a brokerid that is already in use, or else you have shutdown this broker and restarted it faster than the zookeeper timeout so it appears to be re-registering.
        at kafka.utils.ZkUtils.registerBrokerInZk(ZkUtils.scala:375)
        at kafka.utils.ZkUtils.registerBrokerInZk(ZkUtils.scala:361)
        at kafka.server.KafkaHealthcheck.register(KafkaHealthcheck.scala:71)
        at kafka.server.KafkaHealthcheck$SessionExpireListener.handleNewSession(KafkaHealthcheck.scala:105)
        at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:736)
        at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:72)
{code}

From ZK server (my id 4) :
It expired the old session 55bcff0f02d0002 correctly before broker received the session expiration. It then went to ZK leader election soon after.

{code:java}
[2017-07-29 13:33:26,000] INFO Expiring session 0x55bcff0f02d0002, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2017-07-29 13:33:26,019] INFO Processed session termination for sessionid: 0x55bcff0f02d0002 (org.apache.zookeeper.server.PrepRequestProcessor)
[2017-07-29 13:33:33,582] INFO Shutting down (org.apache.zookeeper.server.quorum.CommitProcessor)
[2017-07-29 13:33:34,344] INFO New election. My id =  4, proposed zxid=0x5830d1163b (org.apache.zookeeper.server.quorum.FastLeaderElection)
[2017-07-29 13:34:22,499] INFO FOLLOWING - LEADER ELECTION TOOK - 48915 (org.apache.zookeeper.server.quorum.Learner)
{code}

From ZK server (my id 5):
It lost the connection to the old session 55bcff0f02d0002 before the session got expired. It then went into ZK leader election and became the leader. However, it didn't think the old session 55bcff0f02d0002 was expired after becoming the leader. Therefore, the new session 55d8f690ca20038 failed to create /brokers/ids/1. Only after that, it eventually expired the old session 55bcff0f02d0002.

{code:java}
[2017-07-29 13:33:24,216] WARN caught end of stream exception (org.apache.zookeeper.server.NIOServerCnxn)
EndOfStreamException: Unable to read additional data from client sessionid 0x55bcff0f02d0002, likely client has closed socket
        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)
        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:203)
        at java.lang.Thread.run(Thread.java:745)
2017-07-29 13:33:24,216] INFO Closed socket connection for client /10.122.73.147:59615 which had sessionid 0x55bcff0f02d0002 (org.apache.zookeeper.server.NIOServerCnxn)
[2017-07-29 13:33:30,921] INFO New election. My id =  5, proposed zxid=0x5830d1113f (org.apache.zookeeper.server.quorum.FastLeaderElection)
[2017-07-29 13:33:31,126] INFO LEADING - LEADER ELECTION TOOK - 1122 (org.apache.zookeeper.server.quorum.Leader)
[2017-07-29 13:33:33,405] INFO Established session 0x55d8f690ca20038 with negotiated timeout 6000 for client /10.122.73.147:47106 (org.apache.zookeeper.server.ZooKeeperServer)
[2017-07-29 13:33:33,407] INFO Got user-level KeeperException when processing sessionid:0x55d8f690ca20038 type:create cxid:0x5 zxid:0x5900000352 txntype:-1 reqpath:n/a Error Path:/brokers/ids/1 Error:KeeperErrorCode = NodeExists for /brokers/ids/1 (org.apache.zookeeper.server.PrepRequestProcessor)
[2017-07-29 13:33:40,002] INFO Expiring session 0x55bcff0f02d0002, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2017-07-29 13:33:40,074] INFO Processed session termination for sessionid: 0x55bcff0f02d0002 (org.apache.zookeeper.server.PrepRequestProcessor)
{code}

According to http://mail-archives.apache.org/mod_mbox/zookeeper-user/201701.mbox/%3CB512F6DE-C0BF-45CE-8102-6F242988268E%40apache.org%3E from [~fpj], a ZK client in a new session shouldn't see the ephemeral node created in its previous session. So, could this be a potential bug in ZK during ZK leader transition?
",[],Bug,ZOOKEEPER-2919,Major,Jun Rao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,expired ephemeral node reappears after ZK leader change,2019-09-03T18:37:17.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",9.0
,[],2017-10-08T20:12:20.000+0000,Steven Raspudic,"Basically seeing the same issue as documtented in 

https://github.com/zk-ruby/zookeeper/pull/54#issuecomment-28764204

e.g.

[myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@349] - caught end of stream exception
EndOfStreamException: Unable to read additional data from client sessionid 0x14252332fe501c9, likely client has closed socket
        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:220)
        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
        at java.lang.Thread.run(Thread.java:724)
2013-11-19 01:57:53,625 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@476] - Processed session termination for sessionid: 0x14252332fe501c9
2013-11-19 01:57:53,626 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1001] - Closed socket connection for client /127.0.0.1:58253 which had sessionid 0x
14252332fe501c9

",[],Bug,ZOOKEEPER-2917,Major,Steven Raspudic,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"c client doesn't wait for server response upon closing handle, generates EndOfStreamException and CancelledKeyExceptions warnings",2017-10-08T20:12:20.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>]",2.0
Andor Molnar,"[<JIRA Component: name='build', id='12312383'>]",2017-10-04T17:41:49.000+0000,Patrick D. Hunt,"There are a number of warnings that crop up on branch 3.4/3.5/trunk when compiling ""ant clean compile-test"" using java 9.

Perhaps someone can verify/fix any warning across the jdks that we support (jdk6/7/8/9) and 9 in particular since it's just reached GA.","[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2914,Minor,Patrick D. Hunt,Fixed,2017-10-07T19:25:12.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,compiler warning using java 9,2017-10-08T12:24:40.000+0000,"[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",3.0
Ling Mao,"[<JIRA Component: name='tests', id='12312427'>]",2017-10-02T14:57:48.000+0000,Patrick D. Hunt,"testEphemeralNodeDeletion is showing up as flakey across a number of jobs.

1.https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper-Find-Flaky-Tests/lastSuccessfulBuild/artifact/report.html
2.https://builds.apache.org/job/ZooKeeper_branch34_java9/305/testReport/junit/org.apache.zookeeper.server.quorum/EphemeralNodeDeletionTest/testEphemeralNodeDeletion/

After session close ephemeral node must be deleted","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-2913,Major,Patrick D. Hunt,Fixed,2018-09-05T14:16:35.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,testEphemeralNodeDeletion is flaky,2019-05-20T17:50:29.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.6.0', id='12326518'>]",5.0
,"[<JIRA Component: name='build-infrastructure', id='12333105'>]",2017-10-01T15:56:08.000+0000,Nikhil Bhide,"error: some local refs could not be updated; try running
 'git remote prune git://github.com/apache/zookeeper.git' to remove any old, conflicting branches

	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandIn(CliGitAPIImpl.java:1924)
	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandWithCredentials(CliGitAPIImpl.java:1643)
	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.access$300(CliGitAPIImpl.java:71)
	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl$1.execute(CliGitAPIImpl.java:352)
	at org.jenkinsci.plugins.gitclient.RemoteGitImpl$CommandInvocationHandler$1.call(RemoteGitImpl.java:153)
	at org.jenkinsci.plugins.gitclient.RemoteGitImpl$CommandInvocationHandler$1.call(RemoteGitImpl.java:146)
	at hudson.remoting.UserRequest.perform(UserRequest.java:181)
	at hudson.remoting.UserRequest.perform(UserRequest.java:52)
	at hudson.remoting.Request$2.run(Request.java:336)
	at hudson.remoting.InterceptingExecutorService$1.call(InterceptingExecutorService.java:68)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
	at ......remote call to H0(Native Method)
	at hudson.remoting.Channel.attachCallSiteStackTrace(Channel.java:1554)
	at hudson.remoting.UserResponse.retrieve(UserRequest.java:281)
	at hudson.remoting.Channel.call(Channel.java:839)
	at org.jenkinsci.plugins.gitclient.RemoteGitImpl$CommandInvocationHandler.execute(RemoteGitImpl.java:146)
	at sun.reflect.GeneratedMethodAccessor748.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.jenkinsci.plugins.gitclient.RemoteGitImpl$CommandInvocationHandler.invoke(RemoteGitImpl.java:132)
	at com.sun.proxy.$Proxy109.execute(Unknown Source)
	at hudson.plugins.git.GitSCM.fetchFrom(GitSCM.java:815)
	... 11 more
ERROR: Error fetching remote repo 'origin'


Refer to [https://builds.apache.org/job/PreCommit-ZOOKEEPER-github-pr-build/1074/console] for complete log",[],Bug,ZOOKEEPER-2911,Major,Nikhil Bhide,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Precommit checks are failing because of issuer pertaining to repo,2017-10-01T15:56:08.000+0000,[],1.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2017-09-30T09:11:44.000+0000,ken huang,"when zookeeper start, it will listen on three port:
    2181 for client connnect
    3888 for leader election
    random for what ?
three are three port config in zoo.cfg, 2181、2888、3888, but no 2888 listened on. ",[],Bug,ZOOKEEPER-2910,Major,ken huang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zookeeper listening on unknown random port,2017-10-22T02:11:45.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>]",1.0
Abraham Fine,[],2017-09-29T18:14:35.000+0000,Abraham Fine,,"[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2909,Major,Abraham Fine,Fixed,2017-10-09T17:58:48.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Create ant task to generate ivy dependency reports,2017-10-09T18:35:02.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",4.0
Mark Fenes,"[<JIRA Component: name='security', id='12329414'>, <JIRA Component: name='tests', id='12312427'>]",2017-09-29T17:00:49.000+0000,Patrick D. Hunt,"quorum.auth.MiniKdcTest.testKerberosLogin is failing with an NPE on Java 9.

I recently setup jenkins jobs for java 9 on branch 3.4 and 3.5 and the test is failing as follows.

{noformat}
javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input(s)
	at java.base/java.util.Objects.requireNonNull(Objects.java:246)
	at java.base/javax.security.auth.Subject$SecureSet.remove(Subject.java:1172)
	at java.base/java.util.Collections$SynchronizedCollection.remove(Collections.java:2039)
	at jdk.security.auth/com.sun.security.auth.module.Krb5LoginModule.logout(Krb5LoginModule.java:1193)
	at java.base/javax.security.auth.login.LoginContext.invoke(LoginContext.java:732)
	at java.base/javax.security.auth.login.LoginContext.access$000(LoginContext.java:194)
	at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:665)
	at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:663)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:663)
	at java.base/javax.security.auth.login.LoginContext.logout(LoginContext.java:613)
	at org.apache.zookeeper.server.quorum.auth.MiniKdcTest.testKerberosLogin(MiniKdcTest.java:179)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:55)

	at java.base/javax.security.auth.login.LoginContext.invoke(LoginContext.java:821)
	at java.base/javax.security.auth.login.LoginContext.access$000(LoginContext.java:194)
	at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:665)
	at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:663)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:663)
	at java.base/javax.security.auth.login.LoginContext.logout(LoginContext.java:613)
	at org.apache.zookeeper.server.quorum.auth.MiniKdcTest.testKerberosLogin(MiniKdcTest.java:179)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:55)
{noformat}

https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper_branch34_java9/1/testReport/junit/org.apache.zookeeper.server.quorum.auth/MiniKdcTest/testKerberosLogin/","[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>]",Bug,ZOOKEEPER-2908,Blocker,Patrick D. Hunt,Fixed,2017-10-05T15:20:32.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,quorum.auth.MiniKdcTest.testKerberosLogin failing with NPE on java 9,2018-03-06T13:54:13.000+0000,"[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>]",2.0
Abraham Fine,[],2017-09-28T07:29:26.000+0000,Abraham Fine,The owasp dependency-check-ant jar that we use contains a SLF4J binding that can break logging. We should move it into a separate classpath. ,"[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2906,Major,Abraham Fine,Fixed,2017-09-29T22:56:48.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,The OWASP dependency check jar should not be included in the default classpath,2017-10-02T18:58:13.000+0000,"[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",4.0
Andrew Schwartzmeyer,[],2017-09-25T22:41:49.000+0000,Andrew Schwartzmeyer,"In ZOOKEEPER-2841 I fixed the inclusion of project-specific porting changes that were included in the public headers, which then broke upstream projects (in my case, Mesos).

Unfortunately, I inadvertently created the exact same problem for Linux (or really any system that uses Autotools), and it wasn't evident until the build was coupled with another project with the same problem. More specifically, when including ZooKeeper (with my changes) in Mesos, and including Google's Glog in Mesos, and building both with Autotools (which we also support), both packages define the pre-processor macro {{PACKAGE_VERSION}}, and so so publicly. This is defined in {{config.h}} by Autotools, and is not a problem _unless included publicly_.

When refactoring, I saw two includes in {{zookeeper.h}} that instead of being guarded by e.g. {{#ifdef HAVE_SYS_SOCKET_H}} were guarded by {{#ifndef WIN32}}. Without realizing that I would create the exact same problem I was elsewhere fixing, I erroneously added {{#include ""config.h""}} and guarded the includes ""properly."" But there is _very good reasons_ not to do this (explained above).

The patch to fix this is simple:

{noformat}
diff --git a/src/c/include/zookeeper.h b/src/c/include/zookeeper.h
index d20e70af4..b0bb09e3f 100644
--- a/src/c/include/zookeeper.h
+++ b/src/c/include/zookeeper.h
@@ -21,13 +21,9 @@

 #include <stdlib.h>

-#include ""config.h""
-
-#ifdef HAVE_SYS_SOCKET_H
+/* we must not include config.h as a public header */
+#ifndef WIN32
 #include <sys/socket.h>
-#endif
-
-#ifdef HAVE_SYS_TIME_H
 #include <sys/time.h>
 #endif

diff --git a/src/c/src/zookeeper.c b/src/c/src/zookeeper.c
index 220c57dc4..9b837f227 100644
--- a/src/c/src/zookeeper.c
+++ b/src/c/src/zookeeper.c
@@ -24,6 +24,7 @@
 #define USE_IPV6
 #endif

+#include ""config.h""
 #include <zookeeper.h>
 #include <zookeeper.jute.h>
 #include <proto.h>
{noformat}

I am opening pull requests in a few minutes to have this applied to branch 3.4 and 3.5.

I'm sorry!","[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2905,Major,Andrew Schwartzmeyer,Fixed,2017-09-27T22:19:17.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Don't include `config.h` in `zookeeper.h`,2017-09-27T23:34:47.000+0000,[],4.0
Jordan Zimmerman,"[<JIRA Component: name='server', id='12312382'>]",2017-09-20T18:54:16.000+0000,Mark Johnson,"In the code that determines the EphemeralType it is looking at the owner (which is the client ID or connection ID):

EphemeralType.java:

   public static EphemeralType get(long ephemeralOwner) {
       if (ephemeralOwner == CONTAINER_EPHEMERAL_OWNER) {
           return CONTAINER;
       }
       if (ephemeralOwner < 0) {
           return TTL;
       }
       return (ephemeralOwner == 0) ? VOID : NORMAL;
   }

However my connection ID is:

header.getClientId(): -720548323429908480

This causes the code to think this is a TTL Ephemeral node instead of a
NORMAL Ephemeral node.

This also explains why this is random - if my client ID is non-negative
then the node gets added correctly.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2901,Blocker,Mark Johnson,Fixed,2018-05-09T22:12:54.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Session ID that is negative causes mis-calculation of Ephemeral Type,2019-01-21T14:52:39.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",9.0
,"[<JIRA Component: name='leaderElection', id='12312378'>]",2017-09-15T01:00:19.000+0000,Yicheng Fang,"ZK was used with Kafka (version 0.10.0) for coordination. We had a lot of Kafka consumers writing  consumption offsets to ZK.

We observed the issue two times within the last year. Each time after ZXID overflowed, ZK was not receiving packets even though leader election looked successful from the logs, and ZK servers were up. As a result, the whole Kafka system came to a halt.

As an attempt to reproduce (and hopefully fixing) the issue, I set up test ZK and Kafka clusters and feed them with like-production test traffic. Though not really able to reproduce the issue, I did see that the Kafka consumers, which used ZK clients, essentially DOSed the ensemble, filling up the `submittedRequests` in `PrepRequestProcessor`, causing even 100ms+ read latencies.

More details are included in the comments.",[],Bug,ZOOKEEPER-2899,Major,Yicheng Fang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper not receiving packets after ZXID overflows,2017-10-05T18:18:59.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",2.0
,"[<JIRA Component: name='build', id='12312383'>]",2017-09-14T05:36:36.000+0000,Nikhil Bhide,"Jenkins precommit Zookeeper builds are failing for no reason.
I opened PR for ZOOKEEPER-2896 and changes are pretty simple. 
Changes are done in  org.apache.zookeeper.test.CreateTest.java, and changes should not break anything, still build is failing.
Test results are showing issues with other tests.

https://github.com/apache/zookeeper/pull/374
https://builds.apache.org/job/PreCommit-ZOOKEEPER-github-pr-build/1029/",[],Bug,ZOOKEEPER-2897,Critical,Nikhil Bhide,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Jenkins precommit Zookeeper builds are failing for no reason,2017-09-14T05:38:34.000+0000,[],1.0
Neha Bathra,"[<JIRA Component: name='other', id='12333125'>]",2017-09-12T03:51:59.000+0000,huangyun," I  deploy ZooKeeper in a cluster of three nodes on three different linux machines.
All is going well,but I got the output of "" Error contacting service. It is probably not running "" when I executed ""zkServer.sh status"" to check  if the node of cluster was running.
Finally, I found a problem in zkServer.sh 
it got ""clientPort=2181"" when executing ""zkServer.sh status"" ,but the correct String is only ""2181"".So, It is a solution to the problem that removing ""clientPort="" from  ""clientPort=2181"" .

I hope you understand me. My poor English.",[],Bug,ZOOKEEPER-2895,Major,huangyun,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,a bug in zkServer.sh ,2017-09-13T09:08:05.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",3.0
Alexander A. Strelets,"[<JIRA Component: name='c client', id='12312380'>]",2017-09-08T11:26:29.000+0000,Alexander A. Strelets,"ZooKeeper C Client *+single thread+* build

*The problem:*

First of all, ZooKeeper C Client design allows calling _zookeeper_close()_ in two ways:
a) from a ZooKeeper callback handler (completion or watcher) which in turn is called through _zookeeper_process()_
b) and from other places -- i.e., when the call-stack does not pass through any of zookeeper mechanics prior to enter into mentioned _zookeeper_close()_

The issue described here below is +specific only to the case (b)+. So, it's Ok with the case (a).

When _zookeeper_close()_ is called in the (b) way, the following happens:
1. +If there are requests waiting for responses in _zh.sent_requests_ queue+, they all are removed from this queue and each of them is ""completed"" with personal fake response having status ZCLOSING. Such fake responses are put into _zh.completions_to_process_ queue. It's Ok
2. But then, _zh.completions_to_process_ queue is left unhandled. *+Neither completion callbacks are called, nor dynamic memory allocated for fake responses is freed+*
3. Different structures within _zh_ are dismissed and finally _zh_ is freed

This is illustrated on the screenshot attached to this ticket: you may see that the next instruction to execute will be _free(zh)_ while _zh.completions_to_process_ queue is not empty (see the ""Variables"" tab to the right).

Alternatively, the same situation but in the case (a) is handled properly -- i.e., all completion callback handlers are truly called with ZCLOSING and the memory is freed, both for subcases (a.1) when there is a failure like connection-timeout, connection-closed, etc., or (a.2) there is not failure. The reason is that any callback handler (completion or watcher) in the case (a) is called from the _process_completions()_ function which runs in the loop until _zh.completions_to_process_ queue gets empty. So, this function guarantees this queue to be completely processed even if new completions occur during reaction on previously queued completions.

*Consequently:*
1. At least there is definitely the +memory leak+ in the case (b) -- all the fake responses put into _zh.completions_to_process_ queue are lost after _free(zh)_
2. And it looks like a great misbehavior not to call completions on sent requests in the case (b) while they are called with ZCLOSING in the case (a) -- so, I think it's not ""by design"" but a +completions leak+

+To reproduce the case (b) do the following:+
- open ZooKeeper session, connect to a server, receive and process connected-watch, etc.
- then somewhere +from the main events loop+ call for example _zoo_acreate()_ with valid arguments -- it shall return ZOK
- then, +immediately after it returned+, call _zookeeper_close()_
- note that completion callback handler for _zoo_acreate()_ *will not be called*

+To reproduce the case (a) do the following:+
- the same as above, open ZooKeeper session, connect to a server, receive and process connected-watch, etc.
- the same as above, somewhere from the main events loop call for example _zoo_acreate()_ with valid arguments -- it shall return ZOK
- but now don't call _zookeeper_close()_ immediately -- wait for completion callback on the commenced request
- when _zoo_acreate()_ completes, +from within its completion callback handler+, call another _zoo_acreate()_ and immediately after it returned call _zookeeper_close()_
- note that completion callback handler for the second _zoo_acreate()_ *will be called with ZCLOSING, unlike the case (b) described above*

*To fix this I propose:*
Just call _process_completions()_ from _destroy(zhandle_t *zh)_ as it is done in _handle_error(zhandle_t *zh,int rc)_.

This is a proposed fix: https://github.com/apache/zookeeper/pull/1000
// Previously proposed fix: https://github.com/apache/zookeeper/pull/363

[upd]
There are another tickets with about the same problem: ZOOKEEPER-1493, ZOOKEEPER-2073 (the ""same"" just according to their titles).
However, as I can see, the corresponding patches were applied on the branch 3.4.10, but the effect still persists -- so, this ticket does not duplicate the mentioned two.","[<JIRA Version: name='3.4.15', id='12344988'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.6', id='12345243'>]",Bug,ZOOKEEPER-2894,Critical,Alexander A. Strelets,Fixed,2019-07-09T00:22:36.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Memory and completions leak on zookeeper_close,2019-10-16T18:59:08.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",7.0
Andor Molnar,"[<JIRA Component: name='java client', id='12312381'>]",2017-09-07T09:50:36.000+0000,Paul Millar,"We are using ZooKeeper in our project and have received reports that, when suffering a networking problem, log files become flooded with messages like:
{quote}
07 Sep 2017 08:22:00 (System) [] Session 0x45d3151be3600a9 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.NoRouteToHostException: No route to host
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[na:1.8.0_131]
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[na:1.8.0_131]
        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
{quote}

Looking at the code that logs this message ({{ClientCnxn}}), there seems to be quite a few problems here:
# the code logs a stack-trace, even though there is no bug here.  In our project, we treat all logged stack-traces as bugs,
# if the networking issue is not fixed promptly, the log files is flooded with these message,
# The message is built using {{ClientCnxnSocket#getRemoteSocketAddress}}, yet in this case, this does not provide the expected information (yielding {{null}}),
# The log message fails to include a description of what actually went wrong.

(Additionally, the code uses string concatenation rather than templating when building the message; however, this is an optimisation issue)

My suggestion is that this log entry is updated so that it doesn't log a stack-trace, but does include some indication why the connection failed.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",Bug,ZOOKEEPER-2893,Major,Paul Millar,Fixed,2017-12-19T19:05:05.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,very poor choice of logging if client fails to connect to server,2019-10-04T14:55:13.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",7.0
Alexander A. Strelets,"[<JIRA Component: name='c client', id='12312380'>]",2017-09-05T12:41:41.000+0000,Alexander A. Strelets,"ZooKeeper C Client *+single thread+* build

When I call _zookeeper_close()_ while there is a pending _multi_ request, I expect the request completes with _ZCLOSING_ (-116) status.

But with the existing code I actually get the following:
- the program exits with _SIGABRT_ from _assert(entry)_ in _deserialize_multi()_
- and even if I remove this assertion and just break the enclosing loop, the returned status is _ZOK_ but not _ZCLOSING_

So, there are two defects with processing calls to _zookeeper_close()_ for pending _multi_ requests: improper assertion in implementation and invalid status in confirmation.

*+I propose two changes in the code:+*

1. Screen _assert(entry)_ in _deserialize_multi()_ when it's called due to _zookeeper_close()_ (note that _entry_ may normally become equal to _NULL_ in this case), and as soon as _entry == NULL_, break the loop. To provide this, _deserialize_multi()_ must be informed by the caller weather it's currently the ""normal"" or the ""special"" case.

I propose adding a new parameter _rc_hint_ (return code hint) to _deserialize_multi()_. When _deserialize_multi()_ is called in ""normal"" case, _rc_hint_ is preset with _ZOK_ (0), and the behavior is absolutely the same as with the existing code. But when it's called due to _zookeeper_close()_, the _rc_hint_ is automatically preset with _ZCLOSING_ (-116) by the caller, and this changes the behavior of _deserialize_multi()_ as described above.

How it works:
Let _zookeeper_close()_ is called while there is a pending _multi_ request. Then function _deserialize_multi()_ is called for the so-called ""Fake response"" on _multi_ request which is fabricated by the function _free_completions()_. Such fake response includes only the header but zero bytes for the body. Due to this _deserialize_MultiHeader(ia, ""multiheader"", &mhdr)_, which is called repeatedly for each _completion_list_t *entry = dequeue_completion(clist)_, does not assign the _mhdr_ and keeps _mhdr.done == 0_ as it was originally initialized. Consequently the _while (!mhdr.done)_ loop does not ever end, and finally falls into the _assert(entry)_ with _entry == NULL_ when all fake sub-requests are ""completed"".
But if, as I propose, the caller made a hint to _deserialize_multi()_ that it's actually the ""special"" case (that it processes the fake response indeed, for example), with the proposed changes it would omit improper assertion and break the loop on the first _entry == NULL_. Now at least _deserialize_multi()_ exits and does not emit _SIGABRT_.

2. Passthrough the ""return code hint"" _rc_hint_, as it was initially specified by the caller, to the _deserialize_multi()_ return code, if the hint is not _ZOK_ (0).

How it works:
With the existing code _deserialize_multi()_ returns unsuccessful _rc_-code only if there is an error in processing some of subrequests. And if there are no errors, it returns _ZOK_ (0) which is assigned as the default value to _rc_ at the very beginning of the function. Indeed, in the case of fake multi-response there are no errors in subresponses (because they are empty and fake). So, _deserialize_multi()_ returns _ZOK_ (0). Then, with _rc = deserialize_multi(xid, cptr, ia)_ in _deserialize_response()_ it overrides the true _ZCLOSING_ status.
But if the true status (for example, _ZCLOSING_) is initially hinted to _deserialize_multi()_, as I propose, _deserialize_multi()_ would reproduce it back instead of irrelevant _ZOK_ (0). And consequently _deserialize_response()_ would finally report the true status (particularly _ZCLOSING_).

This is a proposed fix: https://github.com/apache/zookeeper/pull/999
// Previously proposed fix: https://github.com/apache/zookeeper/pull/360

[upd]
It looks like about the same problem is described in ZOOKEEPER-1636
However, the patch proposed in this ticket also remedies the second linked problem: reporting _ZCLOSING_ status (as required) to the multi-request completion handler.","[<JIRA Version: name='3.4.15', id='12344988'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.6', id='12345243'>]",Bug,ZOOKEEPER-2891,Critical,Alexander A. Strelets,Fixed,2019-08-06T00:00:25.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Invalid processing of zookeeper_close for mutli-request,2019-10-16T18:59:17.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",5.0
Alexander A. Strelets,"[<JIRA Component: name='c client', id='12312380'>]",2017-09-05T10:53:16.000+0000,Alexander A. Strelets,"ZooKeeper C Client *+single thread+* build

Function *_deserialize_response()_*, in _case COMPLETION_STRING_, uses local automatic variable *_struct CreateResponse res_* which is +left uninitialized+ and passed to the function _deserialize_GetACLResponse()_ and then to _deallocate_GetACLResponse()_.

The _deserialize_ function, which is called the first, is expected to assign the _res_ variable with a value from the parsed _struct iarchive *ia_. But, if _ia_ contains for example insufficient amount of bytes the _deserialize_String()_ function refuses of assigning a value to _res_, and _res_ stays uninitialized (the true case is described below). Then, the _deallocate_ function calls _deallocate_String()_ passing uninitialized _res_ with arguments. If incidentally the memory region in the program stack under the _res_ was not equal to NULL, the last call +leads to _free()_ by invalid address+.

The true case: this happens when an active _multi_ request with _create_ sub-request is completed on call to _zookeeper_close()_ with the so called ""Fake response"" which is fabricated by the function _free_completions()_. Such response includes only the header but +zero bytes for the body+. The significant condition is that the _create_ request is not a stand-alone one, but namely a sub-request within the _multi_ request. In this case the _deserialize_response()_ is called recursively (for each sub-request), and when it is called for the _create_ subrequest (from the nested _deserialize_multi()_) the _failed_ parameter is assigned with false (0), so the _if (failed)_ condition branches to the _else_ part. Note that in the stand-alone create-request case this does not occur.

*I suspect this may happen not only due to call to _zookeeper_close()_ but on reception of a true multi-response from the server* containing insufficient number of bytes (I'm not sure if it can be a proper response from the server with an error overall status and empty or insufficient payload).

This is a proposed fix: https://github.com/apache/zookeeper/pull/359","[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2890,Critical,Alexander A. Strelets,Fixed,2017-10-10T18:51:08.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Local automatic variable is left uninitialized and then freed.,2019-02-07T15:03:41.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",7.0
,[],2017-09-02T08:58:21.000+0000,Karl Wright,"Starting Zookeeper in the following way causes ""ClassNotFoundException"" errors, and aborts, in a log4j 2.x environment:

{code}
""%JAVA_HOME%\bin\java"" %JAVAOPTIONS% org.apache.zookeeper.server.quorum.QuorumPeerMain zookeeper.cfg
{code}

The log4j 2.x jars in the classpath are:

{code}
log4j-1.2-api
log4j-core
log4j-api
{code}

It appears that the Zookeeper QuorumPeerMain class is incompatible with the limited log4j 1.2 API that log4j 2.x includes.  Zookeeper 3.4.8 works fine with log4j 2.x except when you start it as a service in this way.

",[],Bug,ZOOKEEPER-2889,Major,Karl Wright,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper standalone instance startup references logging classes incompatible with log4j-1.2-api,2017-09-06T12:18:01.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>]",2.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2017-09-01T19:52:27.000+0000,Cesar Stuardo,"When we run our Distributed system Model Checking (DMCK) in ZooKeeper v3.5.3 by following the workload (complete details attached):

1. start a 5 node cluster (all nodes know each other).
2. wait for the cluster to reach a steady state.
3. issue reconfig command which does not add or remove nodes but changes all the ports of the existing cluster (no role change either). 

We observer that in some situations, one of the followers my end up isolated since the other nodes change their ports and end up setting up new connections. The consequence is similar to the one at [ZK-2865|https://issues.apache.org/jira/browse/ZOOKEEPER-2865?jql=] but the scenario is different.

We provide further details in the attached document.

",[],Bug,ZOOKEEPER-2888,Major,Cesar Stuardo,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Reconfig Command Isolates One of the Nodes when All Ports Change,2017-09-01T19:59:18.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",2.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2017-08-30T18:39:44.000+0000,Fangmin Lv,"If there are slow followers, it's possible that the leader and the client disagree on where the client is connecting to, therefore the client keeps getting ""Session Moved"" error. Partial of the issue fixed in Jira: ZOOKEEPER-710, but leaves the issue in multi-op only connection. ","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2886,Major,Fangmin Lv,Fixed,2018-07-10T11:03:05.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Permanent session moved error in multi-op only connections,2018-07-10T19:27:40.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",6.0
Michael Han,[],2017-08-30T00:02:01.000+0000,Gabriel,"I downloaded the zookeeper-3.5.3-beta.tar.gz file from several mirror sites and all of them are corrupted.

{quote}$ wget http://www-us.apache.org/dist/zookeeper/zookeeper-3.5.3-beta/zookeeper-3.5.3-beta.tar.gz
$:~/dockerfiles$ tar -xzvf zookeeper-3.5.3-beta.tar.gz

gzip: stdin: not in gzip format
tar: Child returned status 1
tar: Error is not recoverable: exiting now{quote}

If this is my mistake, please could you explain me what I did wrong?

","[<JIRA Version: name='3.5.3', id='12335444'>]",Bug,ZOOKEEPER-2885,Critical,Gabriel,Fixed,2017-08-30T10:44:01.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zookeeper-3.5.3-beta.tar.gz file in mirror site is corrupted,2018-06-22T22:37:09.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",6.0
guoxiang niu,"[<JIRA Component: name='c client', id='12312380'>]",2017-08-23T15:03:13.000+0000,guoxiang niu,"1、in check_events() function, no null check for the pointer returned by allocate_buffer, the pointer will be passed to recv_buffer(), then the curr_offset member of pointer will be accessed directly.

2、in queue_session_event(), curr_offset also be accessed directly without null check.",[],Bug,ZOOKEEPER-2883,Minor,guoxiang niu,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,no null check for the pointer which returned by allocate_buffer() function,2017-08-24T04:46:17.000+0000,[],2.0
guoxiang niu,"[<JIRA Component: name='c client', id='12312380'>]",2017-08-23T14:55:08.000+0000,guoxiang niu,"when default branch is executed in switch(op->type) , alloced memory for oa variable will leak, so, close_buffer_oarchive(&oa, 1); should be called before returning in default branch.",[],Bug,ZOOKEEPER-2882,Minor,guoxiang niu,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,memory leak in zoo_amulti() function,2017-09-11T22:39:22.000+0000,[],2.0
,"[<JIRA Component: name='c client', id='12312380'>]",2017-08-23T14:46:25.000+0000,guoxiang niu,"in create_buffer_iarchive() function, null check of ia and return should be done before allocing memory for buff, otherwise, memory of buff might be leak.

same issue is existing in create_buffer_oarchive() function.",[],Bug,ZOOKEEPER-2881,Minor,guoxiang niu,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,memory leak in create_buffer_iarchive() and create_buffer_oarchive(),2017-08-23T15:17:55.000+0000,[],1.0
,"[<JIRA Component: name='recipes', id='12313246'>]",2017-08-21T06:33:15.000+0000,H Y,"There are three issues in the c code of lock.
1, It not multi-thread safe, because  pmutex is the local mutex of zkr_lock_mutex_t, if there are more than one thread is calling zkr_lock_lock, it may watch children node fail(retry_zoowexists may return NOT ZOK, 'unable to watch my predecessor' will output. I suggest that changing the pmutex to global.
2,child_floor function is not correct, it should compare the sequence of the node.(zoo_lock.c, line145 should be 'if (strcmp((sorted_data[i] + 19), (element + 19)) < 0)'
3, Logic mistaking in zkr_lock_operation of zoo_lock.c at line 256. mutex->id should be allocated by getName function. So, I think that we should delete from line 249 to 257.
",[],Bug,ZOOKEEPER-2878,Minor,H Y,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,some issues in c code of lock of recipes ,2017-08-21T06:33:15.000+0000,[],3.0
,"[<JIRA Component: name='build-infrastructure', id='12333105'>, <JIRA Component: name='tests', id='12312427'>]",2017-08-16T21:52:48.000+0000,Michael Han,"The github pull request test script (which is invoked as part of pre-commit workflow) should output -1 on a patch which does not include any tests, unless the patch is a documentation only patch.

We had this expected behavior before when we use the old PATCH approach:
{noformat}
    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.
{noformat}

A quick look on the [script|https://github.com/apache/zookeeper/blob/master/src/java/test/bin/test-github-pr.sh#L224] indicates that we do not set up the $PATCH/jira directory in the github test pull script, so it always thinks incoming pull request is a documentation only patch. This should be fixed so we get the old behavior and enforce that any new pull request must have tests unless explicitly justified not have to.",[],Bug,ZOOKEEPER-2876,Major,Michael Han,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Github pull request test script should output -1 when there is no tests provided in patch, unless the subject under test is a documentation JIRA",2017-08-16T21:52:48.000+0000,[],2.0
Andrew Schwartzmeyer,[],2017-08-11T22:02:53.000+0000,Andrew Schwartzmeyer,"While not apparent when building ZooKeeper stand-alone, further testing when linking with Mesos revealed it was ZooKeeper that was causing the warning:

{noformat}
LIBCMTD.lib(initializers.obj) : warning LNK4098: defaultlib 'libcmt.lib' conflicts with use of other libs; use /NODEFAULTLIB:library [C:\Users\andschwa\src\mesos\build\src\slave\mesos-agent.vcxproj]
{noformat}

As Mesos is linking with {{/MTd}} in Debug configuration (which is the most common practice).

Once I found the source of the warning, the fix is trivial and I am posting a patch.","[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2874,Major,Andrew Schwartzmeyer,Fixed,2017-08-18T04:21:30.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Windows Debug builds don't link with `/MTd`,2017-08-18T05:35:24.000+0000,[],5.0
Brian Nixon,"[<JIRA Component: name='server', id='12312382'>]",2017-08-10T18:07:29.000+0000,Brian Nixon,"There is a way for observers to permanently lose data from their local data tree while remaining members of good standing with the ensemble and continuing to serve client traffic when the following chain of events occurs.

1. The observer dies in epoch N from machine failure.
2. The observer comes back up in epoch N+1 and requests a snapshot sync to catch up.
3. The machine powers off before the snapshot is synced to disc and after some txn's have been logged (depending on the OS, this can happen!).
4. The observer comes back a second time and replays its most recent snapshot (epoch <= N) as well as the txn logs (epoch N+1). 
5. A diff sync is requested from the leader and the observer broadcasts availability.

In this scenario, any commits from epoch N that the observer did not receive before it died the first time will never be exposed to the observer and no part of the ensemble will complain. 

This situation is not unique to observers and can happen to any learner. As a simple fix, fsync-ing the snapshots received from the leader will avoid the case of missing snapshots causing data loss.","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2872,Major,Brian Nixon,Fixed,2021-02-11T19:19:12.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Interrupted snapshot sync causes data loss,2022-01-03T04:49:16.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",7.0
Michael Han,[],2017-08-04T23:58:52.000+0000,Jun Rao,"Not sure if this is a real bug, but I found an instance when a ZK client seems to be able to renew a session already expired by the ZK server.

From ZK server log, session 25cd1e82c110001 was expired at 22:04:39.

{code:java}
June 27th 2017, 22:04:39.000	INFO	org.apache.zookeeper.server.ZooKeeperServer	Expiring session 0x25cd1e82c110001, timeout of 12000ms exceeded
June 27th 2017, 22:04:39.001	DEBUG	org.apache.zookeeper.server.quorum.Leader	Proposing:: sessionid:0x25cd1e82c110001 type:closeSession cxid:0x0 zxid:0x200000fc4 txntype:-11 reqpath:n/a
June 27th 2017, 22:04:39.001	INFO	org.apache.zookeeper.server.PrepRequestProcessor	Processed session termination for sessionid: 0x25cd1e82c110001
June 27th 2017, 22:04:39.001	DEBUG	org.apache.zookeeper.server.quorum.CommitProcessor	Processing request:: sessionid:0x25cd1e82c110001 type:closeSession cxid:0x0 zxid:0x200000fc4 txntype:-11 reqpath:n/a
June 27th 2017, 22:05:20.324	INFO	org.apache.zookeeper.server.quorum.Learner	Revalidating client: 0x25cd1e82c110001
June 27th 2017, 22:05:20.324	INFO	org.apache.zookeeper.server.ZooKeeperServer	Client attempting to renew session 0x25cd1e82c110001 at /100.96.5.6:47618
June 27th 2017, 22:05:20.325	INFO	org.apache.zookeeper.server.ZooKeeperServer	Established session 0x25cd1e82c110001 with negotiated timeout 12000 for client /100.96.5.6:47618
{code}

From ZK client's log, it was able to renew the expired session on 22:05:20.

{code:java}
June 27th 2017, 22:05:18.590	INFO	org.apache.zookeeper.ClientCnxn	Client session timed out, have not heard from server in 4485ms for sessionid 0x25cd1e82c110001, closing socket connection and attempting reconnect	0
June 27th 2017, 22:05:18.590	WARN	org.apache.zookeeper.ClientCnxn	Client session timed out, have not heard from server in 4485ms for sessionid 0x25cd1e82c110001	0
June 27th 2017, 22:05:19.325	WARN	org.apache.zookeeper.ClientCnxn	SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/opt/confluent/etc/kafka/server_jaas.conf'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it.	0
June 27th 2017, 22:05:19.326	INFO	org.apache.zookeeper.ClientCnxn	Opening socket connection to server 100.65.188.168/100.65.188.168:2181	0
June 27th 2017, 22:05:20.324	INFO	org.apache.zookeeper.ClientCnxn	Socket connection established to 100.65.188.168/100.65.188.168:2181, initiating session	0
June 27th 2017, 22:05:20.327	INFO	org.apache.zookeeper.ClientCnxn	Session establishment complete on server 100.65.188.168/100.65.188.168:2181, sessionid = 0x25cd1e82c110001, negotiated timeout = 12000	0

{code}
",[],Bug,ZOOKEEPER-2867,Major,Jun Rao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,an expired ZK session can be re-established,2018-05-31T00:13:57.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",4.0
Alexander Shraer,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2017-08-04T21:31:14.000+0000,Jeffrey F. Lukman,"When we run our Distributed system Model Checking (DMCK) in ZooKeeper v3.5.3
by following the workload in ZK-2778:
* initially start 2 ZooKeeper nodes
* start 3 new nodes and let them join the cluster
* do a reconfiguration where the newly joined will be PARTICIPANTS, 
while the previous 2 nodes change to be OBSERVERS

We think our DMCK found this following bug:
* one of the newly joined node crashes due to 
it receives an *unexpected* PROPOSAL message
from the new leader in the cluster.

For complete information of the bug, please see the document that is attached.",[],Bug,ZOOKEEPER-2866,Major,Jeffrey F. Lukman,Not A Problem,2017-08-10T03:16:36.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Reconfig Causes Newly Joined Node to Crash,2017-08-10T03:17:02.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",3.0
,[],2017-08-04T16:31:17.000+0000,Isabelle Giguere,"When downloading a config (ex: a Solr config) from Zookeeper 3.4.10, if a file is empty, it is downloaded as a folder (on Windows, at least).

A Zookeeper browser (Eclipse: Zookeeper Explorer) shows the file as a file, however, in ZK.

Noticed because we keep an empty synonyms.txt file in the Solr config provided with our product, in case a client would want to use it.

The workaround is simple, if the file allows comments: just add a comment, so it is not empty.",[],Bug,ZOOKEEPER-2863,Minor,Isabelle Giguere,Fixed,2017-08-04T17:53:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,downconfig downloads empty file as folder,2017-08-04T17:53:56.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",1.0
Michael Han,"[<JIRA Component: name='documentation', id='12312422'>, <JIRA Component: name='java client', id='12312381'>]",2017-08-03T18:01:37.000+0000,Michael Han,"{{StaticHostProvider#updateServerList}} uses wrong syntax to embed https link in the java doc. Previously this issue was not visible because StaticHostProvider was not public of public java doc; after ZOOKEEPER-2829 {{StaticHostProvider}} is now part of API doc and incorrect syntax leads to java doc warnings, which creates noises in Jenkins pre-commit jobs.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2862,Major,Michael Han,Fixed,2017-08-03T18:35:46.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Incorrect javadoc syntax for web links in StaticHostProvider.java,2017-08-03T18:35:56.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",3.0
Yaniv Kunda,"[<JIRA Component: name='build', id='12312383'>]",2017-08-01T12:23:21.000+0000,Yaniv Kunda,"ZOOKEEPER-82 (fixed since 3.0.0) had QuorumPeerMain extracted from QuorumPeer, but the Main-Class attribute in the JAR manifest was not changed accordingly.
Fixing this will enable to start the server using ""java -jar"" instead of specifying the class name.","[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2861,Minor,Yaniv Kunda,Fixed,2017-08-18T22:30:49.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Main-Class JAR manifest attribute is incorrect,2017-08-18T23:41:29.000+0000,[],5.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2017-07-27T09:14:20.000+0000,Andrey,"Currently sample server jaas configuration for kerberos contains:
{code}
principal=""zookeeper/yourzkhostname""
{code}

Background on why ""princinpal=SPN"" and ""isInitiator=true"" won't work is here:
https://dmdaa.wordpress.com/2010/03/27/the-impact-of-isinitiator-on-jaas-login-configuration-and-the-role-if-spn/

Expected:
{code}
       isInitiator=false
       principal=""zookeeper/yourzkhostname"";
{code}

",[],Bug,ZOOKEEPER-2860,Major,Andrey,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Update sample server jaas config for kerberos auth,2017-08-21T11:19:38.000+0000,[],3.0
Andrew Schwartzmeyer,[],2017-07-26T21:25:47.000+0000,Andrew Schwartzmeyer,"Couple problems:

libm, librt, and libdl are all Linux specific, and provided ""for free"" on OS X

CppUnit (at least on OS X) needs `-std=c++11`

clang's ld doesn't understand --wrap

I can post an easy patch that at least lets you build the client (but not the tests). The tests use that `--wrap` and it's non trivial to fix that on OS X.","[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2859,Major,Andrew Schwartzmeyer,Fixed,2017-08-13T05:22:04.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,CMake build doesn't support OS X,2017-08-13T06:35:04.000+0000,[],5.0
,"[<JIRA Component: name='server', id='12312382'>]",2017-07-26T07:57:06.000+0000,Bo Hu,"NIOServerCnxn.java
private boolean readLength(SelectionKey k) throws IOException 
if (!initialized && checkFourLetterWord(sk, len)) {
            return false;
}

I think this is a problem. when initialized is true, it also need execute checkFourLetterWord, but it don't execute.
",[],Bug,ZOOKEEPER-2857,Major,Bo Hu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Server deal command has problem,2017-07-31T05:20:09.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",3.0
Alexander Shraer,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2017-07-24T20:36:23.000+0000,Jeffrey F. Lukman,"We are testing our distributed system model checking (DMCK)
 by directing our DMCK to reproduce the ZooKepeer-2172 bug in the ZooKeeper v3.5.3.

After some exploration, our DMCK found that the ZOOKEEPER-2172 still linger in the reported fixed version, ZooKeeper v.3.5.3.

Here we attached the complete bug scenario to reproduce the bug.
 We have communicated this bug to [~shralex] and he has confirmed that this bug exists.",[],Bug,ZOOKEEPER-2855,Major,Jeffrey F. Lukman,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Rebooting a Joined Node Failed Due The Joined Node Previously Failed to Update Its Configuration Correctly,2018-05-18T19:29:31.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",5.0
,"[<JIRA Component: name='contrib-fatjar', id='12312645'>]",2017-07-24T13:08:12.000+0000,fragfutter,"The fatjar in 3.4.10 contains signature parts (META-INF/BCKEY.DSA and META-INF/BCKEY.SF). As a result it is not runnable

Exception in thread ""main"" java.lang.SecurityException: Invalid signature file digest for Manifest main attributes

deleting these from the jar solves the issue.
{{zip -d contrib/fatjar/*-fatjar.jar 'META-INF/*.SF' 'META-INF/*.DSA'}}

As far as i know a jar is signed all or nothing.
",[],Bug,ZOOKEEPER-2854,Minor,fragfutter,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"fatjar invalid, contains incomplete signature",2018-05-20T01:08:22.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",4.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2017-07-24T01:33:29.000+0000,Fangmin Lv,"There is log in FileTxnLog#append to track the txn with smaller zxid than the last seen, but the lastZxidSeen is never being assigned, so that log is not printed when it's happened.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2853,Minor,Fangmin Lv,Fixed,2017-08-03T18:41:59.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,The lastZxidSeen in FileTxnLog.java is never being assigned,2017-08-03T21:04:52.000+0000,[],5.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2017-07-24T01:10:25.000+0000,Fangmin Lv,"There is data inconsistency issue found when leader is using on disk txn files to sync with learner: ZOOKEEPER-2846, tried to disable this feature by setting zookeeper.snapshotSizeFactor system property, but found this system property is not being read anywhere.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2852,Major,Fangmin Lv,Fixed,2017-08-03T18:28:51.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Snapshot size factor is not read from system property,2017-08-03T18:31:01.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",4.0
Michael Han,"[<JIRA Component: name='server', id='12312382'>]",2017-07-19T13:48:53.000+0000,prashantkumar,"Hi
I am seeing below error in ZK logs :
 ""Unexpected exception causing shutdown while sock still open java.io.EOFException""
and then  ZK server shuts down with ""shutdown Leader! reason: Not sufficient followers synced, only synced with sids: "" error.

I am using zookeeper-3.5.1-alpha version.
it is ensemble of 2 servers
Could you please help me resolve this issue
Thanks
 
config
{code:java}

initLimit=10
syncLimit=5
maxClientCnxns=0
tickTime=2000
quorumListenOnAllIPs=true
dataDir=/var/run/zookeeper/conf/default
admin.enableServer=false
standaloneEnabled=false

{code}

zookeeper server logs 
{code:java}

114829 2017-06-22 11:24:18,182 [myid:2147483652] - INFO  [ProcessThread(sid:2147483652 cport:-1)::PrepRequestProcessor@649] - Processed session termination for sessionid:                  0x40000007cef003d
114830 2017-06-22 11:24:18,182 [myid:2147483652] - INFO  [NIOWorkerThread-8:MBeanRegistry@119] - Unregister MBean [org.apache.ZooKeeperService:name0=ReplicatedServer_id2147483652,         name1=replica.2147483652,name2=Leader,name3=Connections,name4=128.0.0.5,name5=0x40000007cef003d]
114831 2017-06-22 11:24:18,183 [myid:2147483652] - INFO  [NIOWorkerThread-8:NIOServerCnxn@606] - Closed socket connection for client /128.0.0.5:34651 which had sessionid                   0x40000007cef003d
114832 2017-06-22 11:24:18,421 [myid:2147483652] - ERROR [LearnerHandler-/128.0.0.5:33610:LearnerHandler@604] - Unexpected exception causing shutdown while sock still open
114833 java.io.EOFException
114834     at java.io.DataInputStream.readInt(DataInputStream.java:403)
114835     at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
114836     at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:83)
114837     at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:99)
114838     at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:515)
114839 2017-06-22 11:24:18,422 [myid:2147483652] - WARN  [LearnerHandler-/128.0.0.5:33610:LearnerHandler@619] - ******* GOODBYE /128.0.0.5:33610 ********
114840 2017-06-22 11:24:18,422 [myid:2147483652] - INFO  [NIOServerCxnFactory.AcceptThread:/0.0.0.0:61808:NIOServerCnxnFactory$AcceptThread@296] - Accepted socket connection from /        128.0.0.4:42854
114841 2017-06-22 11:24:18,422 [myid:2147483652] - INFO  [NIOWorkerThread-4:ZooKeeperServer@969] - Client attempting to renew session 0x40000007cef0001 at /128.0.0.4:42854
114842 2017-06-22 11:24:18,422 [myid:2147483652] - INFO  [NIOWorkerThread-4:ZooKeeperServer@678] - Established session 0x40000007cef0001 with negotiated timeout 20000 for client /         128.0.0.4:42854
114843 2017-06-22 11:24:18,423 [myid:2147483652] - INFO  [NIOServerCxnFactory.AcceptThread:/0.0.0.0:61808:NIOServerCnxnFactory$AcceptThread@296] - Accepted socket connection from /        128.0.0.4:42862
114844 2017-06-22 11:24:18,423 [myid:2147483652] - INFO  [NIOServerCxnFactory.AcceptThread:/0.0.0.0:61808:NIOSe
 
{code}

 
 After some time ..

{code:java}
 
114851 2017-06-22 11:24:18,423 [myid:2147483652] - INFO  [NIOWorkerThread-12:ZooKeeperServer@678] - Established session 0x40000007cef0003 with negotiated timeout 20000 for client /        128.0.0.4:42866
114852 2017-06-22 11:24:19,001 [myid:2147483652] - INFO  [NIOServerCxnFactory.AcceptThread:/0.0.0.0:61808:NIOServerCnxnFactory$AcceptThread@296] - Accepted socket connection from /        128.0.0.4:42892
114853 2017-06-22 11:24:19,001 [myid:2147483652] - INFO  [NIOWorkerThread-13:ZooKeeperServer@964] - Client attempting to establish new session at /128.0.0.4:42892
114854 2017-06-22 11:24:19,211 [myid:2147483652] - INFO  [SessionTracker:ZooKeeperServer@384] - Expiring session 0x40000007cef016c, timeout of 20000ms exceeded
114855 2017-06-22 11:24:19,211 [myid:2147483652] - INFO  [SessionTracker:QuorumZooKeeperServer@132] - Submitting global closeSession request for session 0x40000007cef016c
114856 2017-06-22 11:24:19,211 [myid:2147483652] - INFO  [SessionTracker:ZooKeeperServer@384] - Expiring session 0x40000007cef016d, timeout of 20000ms exceeded
114857 2017-06-22 11:24:19,211 [myid:2147483652] - INFO  [ProcessThread(sid:2147483652 cport:-1)::PrepRequestProcessor@649] - Processed session termination for sessionid:                  0x40000007cef016c
114858 2017-06-22 11:24:19,211 [myid:2147483652] - INFO  [SessionTracker:QuorumZooKeeperServer@132] - Submitting global closeSession request for session 0x40000007cef016d
114859 2017-06-22 11:24:19,211 [myid:2147483652] - INFO  [ProcessThread(sid:2147483652 cport:-1)::PrepRequestProcessor@649] - Processed session termination for sessionid:                  0x40000007cef016d
114860 2017-06-22 11:24:19,579 [myid:2147483652] - INFO  [QuorumPeer[myid=2147483652](plain=/0:0:0:0:0:0:0:0:61808)(secure=disabled):Leader@613] - Shutting down
114861 2017-06-22 11:24:19,579 [myid:2147483652] - INFO  [QuorumPeer[myid=2147483652](plain=/0:0:0:0:0:0:0:0:61808)(secure=disabled):Leader@619] - Shutdown called
114862 java.lang.Exception: shutdown Leader! reason: Not sufficient followers synced, only synced with sids: [ [2147483652] ]
114863     at org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:619)
114864     at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:590)
114865     at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1077)
114866 2017-06-22 11:24:19,579 [myid:2147483652] - INFO  [QuorumPeer[myid=2147483652](plain=/0:0:0:0:0:0:0:0:61808)(secure=disabled):MBeanRegistry@119] - Unregister MBean [org.            apache.ZooKeeperService:name0=ReplicatedServer_id2147483652,name1=replica.2147483652,name2=Leader,name3=Connections,name4=128.0.0.4,name5=0x40000007cef006b]
114867 2017-06-22 11:24:19,579 [myid:2147483652] - INFO  [LearnerCnxAcceptor-0.0.0.0/0.0.0.0:61809:Leader$LearnerCnxAcceptor@373] - exception while shutting down acceptor: java.net.       SocketException: Socket closed
114868 2017-06-22 11:24:19,581 [myid:2147483652] - INFO  [QuorumPeer[myid=2147483652](plain=/0:0:0:0:0:0:0:0:61808)(secure=disabled):NIOServerCnxn@606] - Closed socket connection          for client /128.0.0.4:41674 which had sessionid 0x40000007cef006b

{code}
",[],Bug,ZOOKEEPER-2848,Minor,prashantkumar,Not A Bug,2018-06-01T22:03:50.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"Getting this error in ZK server logs : shutdown Leader! reason: Not sufficient followers synced, only synced with sids",2018-06-01T22:03:50.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",3.0
Yisong Yue,"[<JIRA Component: name='server', id='12312382'>]",2017-07-19T06:14:54.000+0000,Fangmin Lv,"When started the ensemble with old static config that the server string doesn't have client port, dynamically remove and add the same server from the ensemble will cause that server cannot bind to client port, and the ZooKeeper server cannot serve client requests anymore.

From the code, we'll set the clientAddr to null when start up with old static config, and dynamic config forces to have <client port> part, which will trigger the following rebind code in QuorumPeer#processReconfig, and cause the address already in used issue.

    public boolean processReconfig(QuorumVerifier qv, Long suggestedLeaderId, Long zxid, boolean restartLE) {
        ...
        if (myNewQS != null && myNewQS.clientAddr != null
                && !myNewQS.clientAddr.equals(oldClientAddr)) {
            cnxnFactory.reconfigure(myNewQS.clientAddr);
            updateThreadName();
        }
        ...
    }","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2847,Major,Fangmin Lv,Fixed,2018-10-01T04:11:33.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Cannot bind to client port when reconfig based on old static config,2019-10-04T14:55:16.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",7.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2017-07-18T19:34:46.000+0000,Fangmin Lv,"On disk txn sync could cause data inconsistency if the current leader just had a snap sync before it became leader, and then having diff sync with its followers may synced the txns gap on disk. Here is scenario: 

Let's say S0 - S3 are followers, and S4 is leader at the beginning:

1. Stop S2 and send one more request
2. Stop S3 and send more requests to the quorum to let S3 have a snap sync with S4 when it started up
3. Stop S4 and S3 became the new leader
4. Start S2 and had a diff sync with S3, now there are gaps in S2

Attached the test case to verify the issue. Currently, there is no efficient way to check the gap in txn files is a real gap or due to Epoch change. We need to add that support, but before that, it would be safer to disable the on disk txn leader-follower sync.

Another two scenarios which could cause the same issue:

(Scenario 1) Server A, B, C, A is leader, the others are followers:

  1). A synced to disk, but the other 2 restarted before receiving the proposal
  2). B and C formed quorum, B is leader, and committed some requests
  3). A looking again, and sync with B, B won't able to trunc A but send snap instead, and leaves the extra txn in A's txn file
  4). A became new leader, and someone else has a diff sync with A it will have the extra txn 

(Scenario 2) Diff sync with committed txn, will only apply to data tree but not on disk txn file, which will also leave hole in it and lead to data inconsistency issue when syncing with learners.",[],Bug,ZOOKEEPER-2846,Critical,Fangmin Lv,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Leader follower sync with on disk txns can possibly leads to data inconsistency,2022-02-01T07:58:29.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",6.0
Robert Joseph Evans,"[<JIRA Component: name='quorum', id='12312379'>]",2017-07-14T23:20:00.000+0000,Fangmin Lv,"In ZOOKEEPER-2678, the ZKDatabase is retained to reduce the unavailable time during leader election. In ZooKeeper ensemble, it's possible that the snapshot is ahead of txn file (due to slow disk on the server, etc), or the txn file is ahead of snapshot due to no commit message being received yet. 

If snapshot is ahead of txn file, since the SyncRequestProcessor queue will be drained during shutdown, the snapshot and txn file will keep consistent before leader election happening, so this is not an issue.

But if txn is ahead of snapshot, it's possible that the ensemble will have data inconsistent issue, here is the simplified scenario to show the issue:

Let's say we have a 3 servers in the ensemble, server A and B are followers, and C is leader, and all the snapshot and txn are up to T0:
1. A new request reached to leader C to create Node N, and it's converted to txn T1 
2. Txn T1 was synced to disk in C, but just before the proposal reaching out to the followers, A and B restarted, so the T1 didn't exist in A and B
3. A and B formed a new quorum after restart, let's say B is the leader
4. C changed to looking state due to no enough followers, it will sync with leader B with last Zxid T0, which will have an empty diff sync
5. Before C take snapshot it restarted, it replayed the txns on disk which includes T1, now it will have Node N, but A and B doesn't have it.

Also I included the a test case to reproduce this issue consistently. 

We have a totally different RetainDB version which will avoid this issue by doing consensus between snapshot and txn files before leader election, will submit for review.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",Bug,ZOOKEEPER-2845,Critical,Fangmin Lv,Fixed,2018-02-23T23:20:00.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Data inconsistency issue due to retain database in leader election,2018-09-14T00:19:07.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",10.0
,[],2017-07-13T08:34:08.000+0000,Avi Steiner,"I'm using Zookeeper 3.4.6
    
The ZK log data folder keeps growing with transaction logs files (log.*).
    
I set the following in zoo.cfg:
autopurge.purgeInterval=1
autopurge.snapRetainCount=3
dataDir=..\\data
    
Per ZK log, it reads those parameters:
    
2017-07-13 10:36:21,266 [myid:] - INFO  [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3
2017-07-13 10:36:21,266 [myid:] - INFO  [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 1
    
It also says that cleanup process is running:
    
2017-07-13 10:36:21,266 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
2017-07-13 10:36:21,297 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
    
But actually nothing is deleted.
Every service restart, a new file is created.
    
The only parameter I managed to change is preAllocSize, which means the minimum size per file. The default is 64MB. I changed it to 10KB only for testing, and I swa the effect as expected: new files were created with 10KB.

I also tried to create a batch file that will run the following:

java -cp zookeeper-3.4.6.jar;lib/slf4j-api-1.6.1.jar;lib/slf4j-log4j12-1.6.1.jar;lib/log4j-1.2.16.jar;conf org.apache.zookeeper.server.PurgeTxnLog .\data -n 3

But it still doesn't do the job.

Please advise.",[],Bug,ZOOKEEPER-2844,Major,Avi Steiner,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper auto purge process does not purge files,2020-06-05T13:20:22.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",6.0
Andrew Schwartzmeyer,"[<JIRA Component: name='c client', id='12312380'>]",2017-07-07T22:48:30.000+0000,Andrew Schwartzmeyer,"The fundamental problem is that the port of the C client to Windows is now close to six years old, with very few updates. This port leaks a lot of changes that should be internal to ZooKeeper, and many of those changes are simply no longer relevant. The correct thing to do is attempt to refactor the Windows port for new versions of ZooKeeper, removing dead/unneeded porting code, and moving dangerous porting code to C files instead of public headers.

Two primary examples of this problem are [ZOOKEEPER-2491|https://issues.apache.org/jira/browse/ZOOKEEPER-2491] and [MESOS-7541|https://issues.apache.org/jira/browse/MESOS-7541].

The first issue stems from this ancient porting code:
{noformat}
#define snprintf _snprintf
{noformat}
 in [winconfig.h|https://github.com/apache/zookeeper/blob/ddf0364903bf7ac7cd25b2e1927f0d9d3c7203c4/src/c/include/winconfig.h#L179]. Newer versions of Windows C libraries define {{snprintf}} as a function, and so it cannot be redefined.

The second issue comes from this undocumented change:

{noformat}
#undef AF_INET6
{noformat}

again in [winconfig.h|https://github.com/apache/zookeeper/blob/ddf0364903bf7ac7cd25b2e1927f0d9d3c7203c4/src/c/include/winconfig.h#L169] which breaks any library that uses IPv6 and {{winsock2.h}}.

Furthermore, the inclusion of the following defines and headers causes terrible problems for consuming libraries, as they leak into ZooKeeper's public headers:

{noformat}
#define _CRT_SECURE_NO_WARNINGS
#define WIN32_LEAN_AND_MEAN
#include <Windows.h>
#include <Winsock2.h>
#include <winstdint.h>
#include <process.h>
#include <ws2tcpip.h>
{noformat}

Depending on the order that a project includes or compiles files, this may or may not cause {{WIN32_LEAN_AND_MEAN}} to become unexpectedly defined, and {{windows.h}} to be unexpectedly included. This problem is exacberated by the fact that the {{winsock2.h}} and {{windows.h}} headers are order-dependent (if you read up on this, you'll see that defining {{WIN32_LEAN_AND_MEAN}} was meant to work-around this).

Going forward, porting changes should live next to where they are used, preferably in source files, not header files, so they remain contained.","[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2841,Major,Andrew Schwartzmeyer,Fixed,2017-08-01T15:48:53.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZooKeeper public include files leak porting changes,2017-08-01T17:34:19.000+0000,[],4.0
Benedict Jin,"[<JIRA Component: name='java client', id='12312381'>]",2017-07-05T12:17:17.000+0000,Benedict Jin,"Should using `System.nanoTime() ^ this.hashCode()` for StaticHostProvider instead of `System.currentTimeMillis()`. Because if we have three Zookeeper server nodes and set the `zookeeper.leaderServes` as `no`, then those connections from client will always connect with the first Zookeeper server node. Due to...

```java
    @Test
    public void testShuffle() throws Exception {
        LinkedList<InetSocketAddress> inetSocketAddressesList = new LinkedList<>();
        inetSocketAddressesList.add(new InetSocketAddress(0));
        inetSocketAddressesList.add(new InetSocketAddress(1));
        inetSocketAddressesList.add(new InetSocketAddress(2));
        /*
        1442045361
        currentTime: 1499253530044, currentTime ^ hashCode: 1500143845389, Result: 1 2 0
        currentTime: 1499253530044, currentTime ^ hashCode: 1500143845389, Result: 2 0 1
        currentTime: 1499253530045, currentTime ^ hashCode: 1500143845388, Result: 0 1 2
        currentTime: 1499253530045, currentTime ^ hashCode: 1500143845388, Result: 1 2 0
        currentTime: 1499253530046, currentTime ^ hashCode: 1500143845391, Result: 1 2 0
        currentTime: 1499253530046, currentTime ^ hashCode: 1500143845391, Result: 1 2 0
        currentTime: 1499253530046, currentTime ^ hashCode: 1500143845391, Result: 1 2 0
        currentTime: 1499253530046, currentTime ^ hashCode: 1500143845391, Result: 1 2 0
        currentTime: 1499253530047, currentTime ^ hashCode: 1500143845390, Result: 1 2 0
        currentTime: 1499253530047, currentTime ^ hashCode: 1500143845390, Result: 1 2 0
         */
        internalShuffleMillis(inetSocketAddressesList);
        /*
        146611050
        currentTime: 22618159623770, currentTime ^ hashCode: 22618302559536, Result: 2 1 0
        currentTime: 22618159800738, currentTime ^ hashCode: 22618302085832, Result: 0 1 2
        currentTime: 22618159967442, currentTime ^ hashCode: 22618302248888, Result: 1 0 2
        currentTime: 22618160135080, currentTime ^ hashCode: 22618302013634, Result: 2 1 0
        currentTime: 22618160302095, currentTime ^ hashCode: 22618301535077, Result: 2 1 0
        currentTime: 22618160490260, currentTime ^ hashCode: 22618301725822, Result: 1 0 2
        currentTime: 22618161566373, currentTime ^ hashCode: 22618300303823, Result: 1 0 2
        currentTime: 22618161745518, currentTime ^ hashCode: 22618300355844, Result: 2 1 0
        currentTime: 22618161910357, currentTime ^ hashCode: 22618291603775, Result: 2 1 0
        currentTime: 22618162079549, currentTime ^ hashCode: 22618291387479, Result: 0 1 2
         */
        internalShuffleNano(inetSocketAddressesList);

        inetSocketAddressesList.clear();
        inetSocketAddressesList.add(new InetSocketAddress(0));
        inetSocketAddressesList.add(new InetSocketAddress(1));

        /*
        415138788
        currentTime: 1499253530050, currentTime ^ hashCode: 1499124456998, Result: 0 1
        currentTime: 1499253530050, currentTime ^ hashCode: 1499124456998, Result: 0 1
        currentTime: 1499253530050, currentTime ^ hashCode: 1499124456998, Result: 0 1
        currentTime: 1499253530050, currentTime ^ hashCode: 1499124456998, Result: 0 1
        currentTime: 1499253530050, currentTime ^ hashCode: 1499124456998, Result: 0 1
        currentTime: 1499253530050, currentTime ^ hashCode: 1499124456998, Result: 0 1
        currentTime: 1499253530053, currentTime ^ hashCode: 1499124456993, Result: 0 1
        currentTime: 1499253530055, currentTime ^ hashCode: 1499124456995, Result: 0 1
        currentTime: 1499253530055, currentTime ^ hashCode: 1499124456995, Result: 0 1
        currentTime: 1499253530055, currentTime ^ hashCode: 1499124456995, Result: 0 1
         */
        internalShuffleMillis(inetSocketAddressesList);
        /*
        13326370
        currentTime: 22618168292396, currentTime ^ hashCode: 22618156149774, Result: 1 0
        currentTime: 22618168416181, currentTime ^ hashCode: 22618156535703, Result: 1 0
        currentTime: 22618168534056, currentTime ^ hashCode: 22618156432394, Result: 0 1
        currentTime: 22618168666548, currentTime ^ hashCode: 22618155774358, Result: 0 1
        currentTime: 22618168818946, currentTime ^ hashCode: 22618155623712, Result: 0 1
        currentTime: 22618168936821, currentTime ^ hashCode: 22618156011863, Result: 1 0
        currentTime: 22618169056251, currentTime ^ hashCode: 22618155893721, Result: 1 0
        currentTime: 22618169611103, currentTime ^ hashCode: 22618157370237, Result: 1 0
        currentTime: 22618169744528, currentTime ^ hashCode: 22618156713138, Result: 1 0
        currentTime: 22618171273170, currentTime ^ hashCode: 22618184562672, Result: 1 0
         */
        internalShuffleNano(inetSocketAddressesList);
    }

    private void internalShuffleMillis(LinkedList<InetSocketAddress> inetSocketAddressesList) throws Exception {
        int hashCode = new StaticHostProvider(inetSocketAddressesList).hashCode();
        System.out.println(hashCode);
        int count = 10;
        Random r;
        while (count > 0) {
            long currentTime = System.currentTimeMillis();
            r = new Random(currentTime ^ hashCode);
            System.out.print(String.format(""currentTime: %s, currentTime ^ hashCode: %s, Result: "",
                    currentTime, currentTime ^ hashCode));
            Collections.shuffle(inetSocketAddressesList, r);
            for (InetSocketAddress inetSocketAddress : inetSocketAddressesList) {
                System.out.print(String.format(""%s "", inetSocketAddress.getPort()));
            }
            System.out.println();
            count--;
        }
    }

    private void internalShuffleNano(LinkedList<InetSocketAddress> inetSocketAddressesList) throws Exception {
        int hashCode = new StaticHostProvider(inetSocketAddressesList).hashCode();
        System.out.println(hashCode);
        int count = 10;
        Random r;
        while (count > 0) {
            long currentTime = System.nanoTime();
            r = new Random(currentTime ^ hashCode);
            System.out.print(String.format(""currentTime: %s, currentTime ^ hashCode: %s, Result: "",
                    currentTime, currentTime ^ hashCode));
            Collections.shuffle(inetSocketAddressesList, r);
            for (InetSocketAddress inetSocketAddress : inetSocketAddressesList) {
                System.out.print(String.format(""%s "", inetSocketAddress.getPort()));
            }
            System.out.println();
            count--;
        }
    }
```","[<JIRA Version: name='3.5.10', id='12349434'>]",Bug,ZOOKEEPER-2840,Major,Benedict Jin,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Should using `System.nanoTime() ^ this.hashCode()` for StaticHostProvider,2020-11-26T14:24:34.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",4.0
,[],2017-07-05T12:14:23.000+0000,Bhupendra Kumar Jain,"Race condition between AcceptThread and SelectorThread may allow connections beyond the max client connection limit 

As per current code in NIOServerCnxnFactory
1. AcceptThread checks for max connection limit , accept the connection and add to acceptedQueue.
2. Later selector thread will poll the accepted connection , adds the new connection to the connection map.

So if too many concurrent connection happening at same time from same client and Selector thread has not yet processed the already accepted connections from acceptedQueue, then AcceptThread will accept more connections beyond the limit as it still gets the less current connection count  ",[],Bug,ZOOKEEPER-2839,Major,Bhupendra Kumar Jain,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Race condition between AcceptThread and SelectorThread may allow connections beyond the max client connection limit,2017-07-05T12:14:23.000+0000,[],2.0
,"[<JIRA Component: name='server', id='12312382'>]",2017-07-05T08:18:46.000+0000,Paul Millar,"The problem stems from closing the ServerSocketChannel before stopping the thread(s) working with the corresponding Selector.  Closing the ServerSocketChannel will invalidate any SelectionKey objects that have been declared.  This is equivalent to calling cancel on the SelectionKey.  Therefore, after the ServerSocketChannel's close method is called, it is possible that any thread working with a SelectionKey will experience CancelledKeyException.

I noticed the problem with ZooKeeper v3.4.6, which resulted in the following stack-trace:

{quote}
04 Jul 2017 15:54:15 (zookeeper) [] Ignoring unexpected runtime exception
java.nio.channels.CancelledKeyException: null
	at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:73) ~[na:1.8.0_131]
	at sun.nio.ch.SelectionKeyImpl.readyOps(SelectionKeyImpl.java:87) ~[na:1.8.0_131]
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:187) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
{quote}

From manually inspecting the source code, I see the problem is present in all currently released versions of ZooKeeper.",[],Bug,ZOOKEEPER-2838,Minor,Paul Millar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,race-condition when shutting down NIOServerCnxnFactory yields CancelledKeyException,2017-07-05T08:18:46.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.5.3', id='12335444'>]",2.0
Benedict Jin,"[<JIRA Component: name='server', id='12312382'>]",2017-07-04T11:55:31.000+0000,Benedict Jin,"Add a special START_SERVER_JVMFLAGS option only for `start` command to distinguish JVMFLAGS and SERVER_JVMFLAGS.

If we use the normal way to add JVM options with `JVMFLAGS` in `conf/java.env`, then it will effect almost all shell scripts under `bin` directory. Even if using `SERVER_JVMFLAGS` will effect some commands like `zkServer.sh status`, include four-letters commands.

For example, if the JVMFLAGS is 
```bash
export JVMFLAGS=""-Xms3G -Xmx3G -Xmn1G -XX:+AlwaysPreTouch -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:-PrintGCTimeStamps -Xloggc:/home/zookeeper/logs/zookeeper_`date '+%Y%m%d%H%M%S'`.gc -XX:-UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=64M""
```
then we will get too many GC log files due to using the `mntr` four-letters command regularly  in some monitor situation.
```bash
$ ls ~/logs
zookeeper_20170704175942.gc
zookeeper_20170704180101.gc
zookeeper_20170704180201.gc
zookeeper_20170704180301.gc
zookeeper_20170704180401.gc
...
```",[],Bug,ZOOKEEPER-2837,Major,Benedict Jin,Won't Fix,2019-01-30T14:51:26.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Add a special START_SERVER_JVMFLAGS option only for `start` command to distinguish JVMFLAGS and SERVER_JVMFLAGS,2019-01-30T14:51:26.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",3.0
gaoshu,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='quorum', id='12312379'>]",2017-07-04T11:50:53.000+0000,Amarjeet Singh,"QuorumCnxManager Listener thread blocks SocketServer on accept but we are getting SocketTimeoutException  on our boxes after 49days 17 hours . As per current code there is a 3 times retry and after that it says ""_As I'm leaving the listener thread, I won't be able to participate in leader election any longer: $<hostname>/$<ip>:3888__"" , Once server nodes reache this state and we restart or add a new node ,it fails to join cluster and logs 'WARN  QuorumPeer<myid=1>/0:0:0:0:0:0:0:0:2181:QuorumCnxManager@383 - Cannot open channel to 3 at election address $<hostname>/$<ip>:3888' .


        As there is no timeout specified for ServerSocket it should never timeout but there are some already discussed issues where people have seen this issue and added checks for SocketTimeoutException explicitly like https://issues.apache.org/jira/browse/KARAF-3325 . 

        I think we need to handle SocketTimeoutException on similar lines for zookeeper as well ","[<JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-2836,Critical,Amarjeet Singh,Fixed,2020-03-31T10:28:24.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,QuorumCnxManager.Listener Thread Better handling of SocketTimeoutException,2021-03-28T08:54:27.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",9.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2017-07-02T19:37:37.000+0000,anaud,"Synchronization code may fail to truncate an uncommitted transaction in the follower’s transaction log. Here is a scenario:
 
Initial condition:
Start the ensemble with three nodes A, B and C with C being the leader
The current epoch is 1
For simplicity of the example, let’s say zxid is a two digit number, with epoch being the first digit
Create two znodes ‘key0’ and ‘key1’ whose value is ‘0’ and ‘1’, respectively
The zxid is 12 -- 11 for creating key0 and 12 for creating key1. (For simplicity of the example, the zxid gets increased only by transactions directly changing the data of znodes.)
All the nodes have seen the change 12 and have persistently logged it
Shut down all
 
Step 1
Start Node A and B. Epoch becomes 2. Then, a request, setData(key0, 1000), with zxid 21 is issued. The leader B writes it to the log but Node A is shutdown before writing it to the log. Then, the leader B is also shut down. The change 21 is applied only to B but not to A or C.
 
Step 2
Start Node A and C. Epoch becomes 3. Node A has the higher zxid than Node C (i.e. 20 > 12). So, Node A becomes the leader. Yet, the last processed zxid is 12 for both Node A and C. So, they are in sync already. Node A sends an empty DIFF to Node C. Node C takes a snapshot and creates snapshot.12. Then, A and C are shut down. Now, C has the higher zxid than Node B.
 
Step 3
Start Node B and C. Epoch becomes 4. Node C has the higher zxid than Node B (i.e. 30 > 21). So, Node C becomes the leader. Node B and C has the different last processed zxid (i.e. 21 vs 12), and the LinkedList object ‘proposals’ is empty. Thus, Node C sends SNAP to Node B. Node B takes a clean snapshot and creates snapshot.12 as the zxid 12 is the last processed zxid of the leader C. (Note the newly created snapshot on B is assigned the lower zxid then the change 21 in the log). Then, the request, setData(key1, 1001), with zxid 41 is issued. Both B and C apply the change 41 into their logs. (Note that now B and C have the same last processed zxid) Then, B and C are shut down.
 
Step 4
Start Node B and C. Epoch becomes 5.  Node B and C use their local log and snapshot files to restore their in-memory data tree. Node B has 1000 as the value of key0, because it’s latest valid snapshot is snapshot.12 and there was a later transaction with zxid 21 in its log. Yet, Node C has 0 as the value of key0, because the change 21 was never written on C. Node C is the leader. Node B and C have the same last processed zxid, i.e. 41. So, they are considered to be in sync already, and Node C sends an empty DIFF to Node B. So, the synchronization completes with the initially restored in-memory data tree on B and C.
 
Problem
The value of key0 on B is 1000, while the value of the key0 on Node C is 0. The LearnerHandler.run on C at Step 3, 	never sends TRUNC but just SNAP. So, the change 21 was never truncated on B. Also, at step 4, since B uses the snapshot of the lower zxid to restore its in-memory data tree, the change 21 could get into the data tree. Then, the leader C at the step 4 did not send SNAP, because the change 41 made to both B and C makes the leader C think the B and C are already in sync. Thus, data inconsistency occurs.
 
The attached test case can deterministically reproduce the bug.
","[<JIRA Version: name='3.4.10', id='12338036'>]",Bug,ZOOKEEPER-2832,Major,anaud,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Data Inconsistency occurs if follower has uncommitted transaction in the log while synchronizing with the leader that has the lower last processed zxid,2020-09-25T02:45:14.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",8.0
Benedict Jin,"[<JIRA Component: name='jmx', id='12312451'>]",2017-06-27T09:40:10.000+0000,Benedict Jin,"The wrong `ObjectName` about `MBeanServer` in JMX module, should `log4j:hierarchy=default` rather than `log4j:hiearchy=default`.","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-2822,Minor,Benedict Jin,Fixed,2018-11-27T08:59:04.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Wrong `ObjectName` about `MBeanServer` in JMX module,2019-05-20T17:51:07.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",4.0
Michael Han,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2017-06-23T16:06:01.000+0000,Michael Han,"In 3.5.x there is no easy way of changing the membership config using rolling restarts because of the introduction of dynamic reconfig feature in ZOOKEEPER-107, which automatically manages membership configuration parameters.

ZOOKEEPER-2014 introduced a reconfigEnabled flag to turn on / off the reconfig feature. We can use same flag and when it sets to false, it should disable both in memory and on disk updates of membership configuration information, besides disabling the reconfig commands on CLI which ZOOKEEPER-2014 already did, so users can continue using rolling restarts if needed. 

We should also document explicitly the support of membership changes via rolling restarts will be deprecated at what release time frame and promote reconfig as the replacement.

The problem was raised at user mailing list by Guillermo Vega-Toro, reference thread:
http://zookeeper-user.578899.n2.nabble.com/How-to-add-nodes-to-a-Zookeeper-3-5-3-beta-ensemble-with-reconfigEnabled-false-td7583138.html","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2819,Critical,Michael Han,Fixed,2017-07-06T16:49:16.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Changing membership configuration via rolling restart does not work on 3.5.x.,2020-07-21T10:49:41.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.5.3', id='12335444'>]",7.0
Brahma Reddy Battula,[],2017-06-23T03:41:43.000+0000,Brahma Reddy Battula,"As per discussion in [mailinglist|http://mail-archives.apache.org/mod_mbox/zookeeper-user/201706.mbox/browser],It's better improve Java doc or argument which might not mislead for new users.
","[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2818,Major,Brahma Reddy Battula,Fixed,2017-07-04T05:56:59.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Improve the ZooKeeper#setACL  java doc,2017-07-05T08:42:19.000+0000,[],5.0
Nikhil Bhide,[],2017-06-21T09:00:18.000+0000,Viliam Durina,"I'm using the following connection string:

{{10.0.0.179:2181,<space>10.0.0.176:2181}}

However, I get:

{{java.net.UnknownHostException: 10.0.0.176: Name or service not known
at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928)
at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323)
at java.net.InetAddress.getAllByName0(InetAddress.java:1276)
at java.net.InetAddress.getAllByName(InetAddress.java:1192)
at java.net.InetAddress.getAllByName(InetAddress.java:1126)
at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
at ...
...}}

The problem was the space after the comma. I suggest to either make the space optional or produce error on it, as this is a real pain to spot. Using the space also makes the connect string more readable. Spaces are not allowed in domain names.
",[],Bug,ZOOKEEPER-2814,Minor,Viliam Durina,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Ignore space after comma in connection string,2019-01-30T16:23:10.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",7.0
,"[<JIRA Component: name='server', id='12312382'>]",2017-06-19T08:54:40.000+0000,Paul Millar,"A failure during accepting an incoming connection results in the acceptor thread being caught in a tight-loop.  For example:

{noformat}
13 Jun 2017 15:31:39 (zookeeper) [] Ignoring unexpected runtime exception
java.lang.NullPointerException: null
	at org.apache.zookeeper.server.ZooKeeperServer.processConnectRequest(ZooKeeperServer.java:864) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at org.apache.zookeeper.server.NIOServerCnxn.readConnectRequest(NIOServerCnxn.java:418) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at org.apache.zookeeper.server.NIOServerCnxn.readPayload(NIOServerCnxn.java:198) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:244) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:203) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
13 Jun 2017 15:31:39 (zookeeper) [] Ignoring unexpected runtime exception
java.lang.NullPointerException: null
	at org.apache.zookeeper.server.ZooKeeperServer.createSession(ZooKeeperServer.java:569) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at org.apache.zookeeper.server.ZooKeeperServer.processConnectRequest(ZooKeeperServer.java:902) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at org.apache.zookeeper.server.NIOServerCnxn.readConnectRequest(NIOServerCnxn.java:418) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at org.apache.zookeeper.server.NIOServerCnxn.readPayload(NIOServerCnxn.java:198) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:244) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:203) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
13 Jun 2017 15:31:40 (zookeeper) [] Ignoring unexpected runtime exception
java.lang.NullPointerException: null
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:185) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
13 Jun 2017 15:31:40 (zookeeper) [] Ignoring unexpected runtime exception
java.lang.NullPointerException: null
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:185) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
13 Jun 2017 15:31:40 (zookeeper) [] Ignoring unexpected runtime exception
java.lang.NullPointerException: null
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:185) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
{noformat}

The first stack-trace is due to ZOOKEEPER-2810, the second is due to ZOOKEEPER-2812. 

The other stack-traces (NPE from NIOServerCnxnFactory.java:185) are never-ending, as the service has been caught in a tight-loop.

The reason is that the NIOServerCnxnFactory class fails to guarantee that `selected` variable is clearer, so the SelectionKey that triggered the bugs remains ""live"".  However, since there are no incoming connections, the call to `accept()` returns null, triggering the NPE.

It appears this problem is fixed with 3.5.0 (with commit 6302d7a7).   If back-porting this patch is too invasive, another solution might be to place the `selected.clear()` statement inside the finally-clause of the try-statement.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-2813,Minor,Paul Millar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Failure tight loop in acceptor,2017-06-19T08:54:40.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2017-06-19T07:51:38.000+0000,Paul Millar,"As with ZOOKEEPER-2810, NIOServerCnxnFactory#startup current starts the acceptor thread before initialising the ZooKeeperServer object.  This leads to a race-condition between any incoming connection and the thread initialising the ZooKeeperServer.

If the incoming connection wins the race then the thread processing this connection will see an uninitialised SessionTracker object, resulting in the following NPE being thrown:

{noformat}
java.lang.NullPointerException: null
	at org.apache.zookeeper.server.ZooKeeperServer.createSession(ZooKeeperServer.java:569) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at org.apache.zookeeper.server.ZooKeeperServer.processConnectRequest(ZooKeeperServer.java:902) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at org.apache.zookeeper.server.NIOServerCnxn.readConnectRequest(NIOServerCnxn.java:418) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at org.apache.zookeeper.server.NIOServerCnxn.readPayload(NIOServerCnxn.java:198) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:244) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:203) ~[zookeeper-3.4.8.jar:3.4.8--1]
{noformat}

Again, as with ZOOKEEPER-2810, the naive fix (starting the acceptor thread last in NIOServerCnxnFactory#startup method) may fix this issue.",[],Bug,ZOOKEEPER-2812,Minor,Paul Millar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Racy implicit SessionTracker creation,2017-06-19T07:51:38.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>]",2.0
Michael Han,"[<JIRA Component: name='documentation', id='12312422'>]",2017-06-18T22:36:44.000+0000,Michael Han,"The Java doc of PurgeTxnLog#validateAndGetFile is missing the value of its return tag, which causes -1 in the JavaDoc category of pre-commit build:

{noformat}
  [javadoc] /home/jenkins/jenkins-slave/workspace/PreCommit-ZOOKEEPER-github-pr-build/src/java/main/org/apache/zookeeper/server/PurgeTxnLog.java:214: warning - @return tag has no arguments.
{noformat}","[<JIRA Version: name='3.4.11', id='12339207'>]",Bug,ZOOKEEPER-2811,Minor,Michael Han,Fixed,2017-07-06T16:53:36.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,PurgeTxnLog#validateAndGetFile: return tag has no arguments.,2017-07-13T02:59:20.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2017-06-16T13:24:18.000+0000,Paul Millar,"The NIOServerCnxnFactory#startup method first starts the acceptor thread and then initialises the ZooKeeperServer instance.  In particular, the call to ZooKeeperServer#startdata method creates the ZKDatabase if it does not already exist.

This creates a race-condition: if the acceptor thread accepts an incoming connection before the ZKDatabase is established then there is a NullPointerException:

{noformat}
java.lang.NullPointerException: null
	at org.apache.zookeeper.server.ZooKeeperServer.processConnectRequest(ZooKeeperServer.java:864) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at org.apache.zookeeper.server.NIOServerCnxn.readConnectRequest(NIOServerCnxn.java:418) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at org.apache.zookeeper.server.NIOServerCnxn.readPayload(NIOServerCnxn.java:198) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:244) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:203) ~[zookeeper-3.4.8.jar:3.4.8--1]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
{noformat}

The same problem appears to be present in release-3.5 and master branches.

The naive fix would be to start the acceptor thread last in NIOServerCnxnFactory#startup, but I can't say whether this would cause any other problems.",[],Bug,ZOOKEEPER-2810,Minor,Paul Millar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Racy implicit ZKDatabase creation,2017-06-16T13:24:18.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>]",2.0
Mark Fenes,"[<JIRA Component: name='server', id='12312382'>]",2017-06-16T11:07:34.000+0000,Paul Millar,"In ZK 3.4.x, if the client disconnects unexpectedly then the server logs this with a stack-trace (see src/java/main/org/apache/zookeeper/server/NIOServerCnxn.java:356).

This is unfortunate as we are using an embedded ZK server in our project (in a test environment) and we consider all stack-traces as bugs.

I noticed that ZK 3.5 and later no longer log a stack-trace.  This change is due to commit 6206b495 (in branch-3.5), which adds ZOOKEEPER-1504 and seems to fix this issue almost as a side-effect; a similar change in master has the same effect.

I was wondering if the change in how EndOfStreamException is logged (i.e., logging the message without a stack-trace) could be back-ported to 3.4 branch, so could be included in the next 3.4 release.","[<JIRA Version: name='3.4.11', id='12339207'>]",Bug,ZOOKEEPER-2809,Minor,Paul Millar,Fixed,2017-09-11T22:08:10.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Unnecessary stack-trace in server when the client disconnect unexpectedly,2017-10-04T21:51:24.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>]",5.0
Fangmin Lv,"[<JIRA Component: name='server', id='12312382'>]",2017-06-15T17:46:22.000+0000,Fangmin Lv,"When Zeus start up, it will create DataTree instance, in which the empty config znode is created with READ_UNSAFE acl, the acl will be stored in a map with index 1. Then it's going to load the snapshot from disk, the nodes and acl map will be cleared, but the reconfig znode is still reference to acl index 1. The reconfig znode will be reused, so actually it may reference to a different ACL stored in the snasphot. After leader-follower syncing, the reconfig znode will be added back again (if it doesn't exist), which will remove the previous reference to ACL index 1, if the index 1 has 0 reference it will be removed from the ACL map, which could cause that ACL un-usable, and that znode will not be readable.

Error logs related:
-----------------------------
2017-06-12 12:02:21,443 [myid:2] - ERROR [CommitProcWorkThread-14:DataTree@249] - ERROR: ACL not available for long 1
2017-06-12 12:02:21,444 [myid:2] - ERROR [CommitProcWorkThread-14:FinalRequestProcessor@567] - Failed to process sessionid:0x201035cc882002d type:getChildren cxid:0x1 zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
java.lang.RuntimeException: Failed to fetch acls for 1
        at org.apache.zookeeper.server.DataTree.convertLong(DataTree.java:250)
        at org.apache.zookeeper.server.DataTree.getACL(DataTree.java:799)
        at org.apache.zookeeper.server.ZKDatabase.getACL(ZKDatabase.java:574)
        at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:463)
        at org.apache.zookeeper.server.quorum.CommitProcessor$CommitWorkRequest.doWork(CommitProcessor.java:439)
        at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:151)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2808,Critical,Fangmin Lv,Fixed,2017-06-18T17:23:34.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ACL with index 1 might be removed if it's only being used once,2017-06-19T04:02:56.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",6.0
Abraham Fine,[],2017-06-13T18:42:42.000+0000,Abraham Fine,,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",Bug,ZOOKEEPER-2806,Major,Abraham Fine,Fixed,2018-03-26T18:27:05.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Flaky test: org.apache.zookeeper.server.quorum.FLEBackwardElectionRoundTest.testBackwardElectionRound,2018-03-26T18:27:05.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",6.0
,[],2017-06-09T16:50:09.000+0000,Abdullah Alamoudi,Stacktrace not available but it is on close call of the LSMBtreeSearchCursor.close() call,[],Bug,ZOOKEEPER-2805,Major,Abdullah Alamoudi,Invalid,2017-06-12T23:29:14.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,NullPointerException when using no merge merge policy and too many disk components,2017-06-12T23:29:14.000+0000,[],2.0
Bhupendra Kumar Jain,[],2017-06-09T10:20:21.000+0000,Bhupendra Kumar Jain,"If null ACLs are passed then zk node creation or set ACL fails with NPE
{code}
java.lang.NullPointerException
	at org.apache.zookeeper.server.PrepRequestProcessor.removeDuplicates(PrepRequestProcessor.java:1301)
	at org.apache.zookeeper.server.PrepRequestProcessor.fixupACL(PrepRequestProcessor.java:1341)
	at org.apache.zookeeper.server.PrepRequestProcessor.pRequest2Txn(PrepRequestProcessor.java:519)
	at org.apache.zookeeper.server.PrepRequestProcessor.pRequest(PrepRequestProcessor.java:1126)
	at org.apache.zookeeper.server.PrepRequestProcessor.run(PrepRequestProcessor.java:178)
{code}

Expected to handle null in server and return proper error code to client","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2804,Major,Bhupendra Kumar Jain,Fixed,2017-08-18T21:39:57.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Node creation fails with NPE if ACLs are null,2017-08-18T23:41:29.000+0000,[],5.0
Abraham Fine,[],2017-06-08T22:08:44.000+0000,Abraham Fine,"Please ignore. I tested against the wrong version of ZooKeeper and this was resolved by ZOOKEEPER-1653

-We have noticed on internal executions of the integration tests rare failures of org.apache.zookeeper.test.CnxManagerTest.testWorkerThreads.-

{code}
java.lang.RuntimeException: Unable to run quorum server 
	at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:565)
	at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:520)
	at org.apache.zookeeper.test.CnxManagerTest.testWorkerThreads(CnxManagerTest.java:328)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
Caused by: java.io.IOException: The current epoch, 0, is older than the last zxid, 4294967296
	at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:546)
{code}

-along with this strange stack trace in the logs:-
{code}
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:380)
	at org.apache.zookeeper.common.AtomicFileOutputStream.close(AtomicFileOutputStream.java:71)
	at org.apache.zookeeper.server.quorum.QuorumPeer.writeLongToFile(QuorumPeer.java:1232)
	at org.apache.zookeeper.server.quorum.QuorumPeer.setCurrentEpoch(QuorumPeer.java:1253)
	at org.apache.zookeeper.server.quorum.Learner.syncWithLeader(Learner.java:412)
	at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:83)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:851)
{code}

-It appears that this failure is related to the usage of {{((FileOutputStream) out).getChannel().force(true)}} in {{AtomicFileOutputStream}}. {{FileChannel#force}} appears to be interruptible, which is not desirable behavior when writing the epoch file. The interrupt may be triggered by the repeated starting and shutting down of quorum peers in {{testWorkerThreads}}. Branch 3.5 uses {{FileDescriptor#sync}} which is not interruptible and does not appear to have the same problem.-

-I was able to find another JIRA ticket describing a similar issue here:' https://issues.apache.org/jira/browse/DERBY-4963-

-There is also interesting discussion in ZOOKEEPER-1835 (where the change was made for 3.5) although these discussions appear to be Windows centric (we noticed the issue on Linux)-https://issues.apache.org/jira/browse/ZOOKEEPER-1835

-The failure appears to have popped up on ""ZOOKEEPER-2297 PreCommit Build #3241"" but jenkins cleared out the logs (I only still have the test report from the mailing list).-

-In addition, {{testWorkerThreads}} appears to be failing every few months on Solaris on Apache Jenkins (for 3.4 ZooKeeper_branch34_solaris - Build # 1430  and 3.5 ZooKeeper_branch35_solaris - Build # 387), but at the time I wrote this Jenkins had cleaned out the logs from the latest failed run so I have no way of determining if the cause is the same.-",[],Bug,ZOOKEEPER-2803,Major,Abraham Fine,Not A Problem,2017-06-09T21:23:44.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Flaky test: org.apache.zookeeper.test.CnxManagerTest.testWorkerThreads,2017-06-09T21:33:26.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",2.0
,"[<JIRA Component: name='c client', id='12312380'>]",2017-06-07T23:05:22.000+0000,yihao yang,"I was using zookeeper 3.4.6 c client to access one zookeeper server in a VM. The VM environment is not stable and I get a lot of EXPIRED_SESSION_STATE events. I will create another session to ZK when I get an expired event. I also have a read/write lock to protect session read (get/list/... on zk) and write(connect, close, reconnect zhandle).
The problem is the session got an EXPIRED_SESSION_STATE event and when it tried to hold the write lock and  reconnect the session, it found there is a thread was holding the read lock (which was operating sync list on zk). See the stack below:

GDBStack:
Thread 7 (Thread 0x7f838a43a700 (LWP 62845)):
#0 pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1 0x0000000000636033 in  wait_sync_completion (sc=sc@entry=0x7f8344000af0) at src/mt_adaptor.c:85
#2 0x0000000000633248 in zoo_wget_children2_ (zh=<optimized out>, path=0x7f83440677a8 ""/dict/objects/__services/RLS-GSE/_static_nodes"", watcher=0x0, watcherCtx=0x13e6310, strings=0x7f838a4397b0, stat=0x7f838a4398d0) at src/zookeeper.c:3630
#3 0x000000000045e6ff in ZooKeeperContext::getChildren (this=0x13e6310, path=..., children=children@entry=0x7f838a439890, stat=stat@entry=0x7f838a4398d0) at zookeeper_context.cpp:xxx

This sync list didn't return a ZINVALIDSTAT but hung. Anyone know the problem?",[],Bug,ZOOKEEPER-2802,Critical,yihao yang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper C client hang @wait_sync_completion,2017-09-04T00:40:51.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",4.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2017-06-06T04:39:57.000+0000,Jiafu Jiang,"I deploy a cluster of ZooKeeper with three nodes:

ofs_zk1:30.0.0.72
ofs_zk2:30.0.0.73
ofs_zk3:30.0.0.99

On 2017-06-02, use the c zk client to create some ephemeral sequential nodes,:
/adm_election/rolemgr/rolemgr0000000008,
/adm_election/rolemgr/rolemgr0000000011,
/adm_election/rolemgr/rolemgr0000000012,

with sesstion timeout 20000 ms.

Then  I restart ofs_zk1 and ofs_zk2.


On 2017-06-05, I found that, these ephemeral  nodes still exist on ofs_zk1.
I can check the nodes by zkCli.sh get command on ofs_zk1.
But these nodes doesn't not exist on ofs_zk2 and ofs_zk3.
Is it odd?


I have upload the whole deploy directory of three nodes to:
https://pan.baidu.com/s/1miohiCo ,
The log is printed in log/zookeeper.out

log of ofs_zk3 is too large, so I only show the head 1000 lines.

Since I find this PR a little late, some snapshot and log may be deleted.
I hope anyone can help find the reason.
",[],Bug,ZOOKEEPER-2800,Critical,Jiafu Jiang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zookeeper ephemeral node not deleted after server restart and consistency is not hold,2019-11-06T06:31:09.000+0000,"[<JIRA Version: name='3.4.11', id='12339207'>]",5.0
Abraham Fine,[],2017-06-02T20:24:58.000+0000,Abraham Fine,"We often run our tests in parallel (for example, our GitHub hook), this means the tests are often difficult to debug since all of the tests are logging to the console and become interwoven. We should have each test log to its own file.",[],Bug,ZOOKEEPER-2799,Major,Abraham Fine,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Separate logs generated by tests in jenkins jobs when tests are run in parallel,2017-06-06T19:59:17.000+0000,[],2.0
Abraham Fine,[],2017-06-01T19:06:10.000+0000,Abraham Fine,"This test appears to be failing intermitently on both 3.4 and 3.5. Here are a couple of example failing jobs.

3.4: https://builds.apache.org/job/ZooKeeper_branch34_jdk7/1404/

3.5: https://builds.apache.org/job/ZooKeeper_branch35_jdk8/459/","[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2798,Major,Abraham Fine,Fixed,2017-06-08T15:54:27.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Fix flaky test: org.apache.zookeeper.test.ReadOnlyModeTest.testConnectionEvents,2017-06-08T17:26:54.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>]",6.0
Patrick White,"[<JIRA Component: name='security', id='12329414'>, <JIRA Component: name='server', id='12312382'>]",2017-05-30T23:32:20.000+0000,Patrick White,"I was adding container and TTL support to kazoo, and managed to screw something up which set the TTL to a negative value. This invalid TTL blew up the commit processor, and got written to the log, preventing the zookeepers from starting back up.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2797,Major,Patrick White,Fixed,2017-06-01T03:46:18.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Invalid TTL from misbehaving client nukes zookeeper,2022-04-05T03:38:37.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",7.0
Abraham Fine,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='quorum', id='12312379'>]",2017-05-24T15:03:48.000+0000,Mike Heffner,"When zxid rolls over the ensemble is unable to recover without manually restarting the cluster. The leader enters shutdown() state when zxid rolls over, but the remaining four nodes in the ensemble are not able to re-elect a new leader. This state has persisted for at least 15 minutes before an operator manually restarted the cluster and the ensemble recovered.

Config:
--------
tickTime=2000
initLimit=10
syncLimit=5
dataDir=/raid0/zookeeper
clientPort=2181
maxClientCnxns=100
autopurge.snapRetainCount=14
autopurge.purgeInterval=24
leaderServes: True
server.7=172.26.134.88:2888:3888
server.6=172.26.136.143:2888:3888
server.5=172.26.135.103:2888:3888
server.4=172.26.134.16:2888:3888
server.9=172.26.135.19:2888:3888

Logs:

https://gist.github.com/mheffner/d615d358d4a360ae56a0d0a280040640
",[],Bug,ZOOKEEPER-2791,Major,Mike Heffner,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Quorum doesn't recover after zxid rollover,2017-05-26T18:41:59.000+0000,"[<JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.4.8', id='12326517'>]",5.0
Benedict Jin,"[<JIRA Component: name='quorum', id='12312379'>]",2017-05-23T01:45:02.000+0000,Benedict Jin,"If it is `1k/s` ops, then as long as $2^32 / (86400 * 1000) \approx 49.7$ days ZXID will exhausted. But, if we reassign the `ZXID` into 16bit for `epoch` and 48bit for `counter`, then the problem will not occur until after  $Math.min(2^16 / 365, 2^48 / (86400 * 1000 * 365)) \approx Math.min(179.6, 8925.5) = 179.6$ years.

However, i thought the ZXID is `long` type, reading and writing the long type (and `double` type the same) in JVM, is divided into high 32bit and low 32bit part of the operation, and because the `ZXID` variable is not  modified with `volatile` and is not boxed for the corresponding reference type (`Long` / `Double`), so it belongs to [non-atomic operation] (https://docs.oracle.com/javase/specs/jls/se8 /html/jls-17.html#jls-17.7). Thus, if the lower 32 bits of the upper 32 bits are divided into the entire 32 bits of the `long`, there may be a concurrent problem.",[],Bug,ZOOKEEPER-2789,Major,Benedict Jin,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Reassign `ZXID` for solving 32bit overflow problem,2022-02-03T08:50:16.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",10.0
Edward Ribeiro,"[<JIRA Component: name='scripts', id='12312384'>]",2017-05-20T23:07:10.000+0000,Mostafa Shahdadi,"In the ZK 3.4.3 release's version of zkCli.sh, the last command that was executed is *re*-executed when you {{ctrl+d}} out of the shell. In the snippet below, {{ls}} is executed, and then {{ctrl+d}} is triggered (inserted below to illustrate), the output from {{ls}} appears again, due to the command being re-run. 
{noformat}
[zk: zookeeper.example.com:2181(CONNECTED) 0] ls /blah
[foo]
[zk: zookeeper.example.com:2181(CONNECTED) 1] <ctrl+d> [foo]
$
{noformat}","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-2787,Major,Mostafa Shahdadi,Won't Fix,2019-11-06T18:52:02.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,CLONE - ZK Shell/Cli re-executes last command on exit,2019-11-06T18:52:02.000+0000,[],2.0
Abraham Fine,[],2017-05-19T18:33:01.000+0000,Abraham Fine,"This test is broken on 3.4 and 3.5, but is broken in ""different"" ways. Please see the individual pull requests for detailed descriptions for the issues faced in both branches.

","[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2786,Major,Abraham Fine,Fixed,2017-08-09T18:31:48.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Flaky test: org.apache.zookeeper.test.ClientTest.testNonExistingOpCode,2017-08-09T19:26:47.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>]",5.0
Abhishek Singh Chouhan,"[<JIRA Component: name='server', id='12312382'>]",2017-05-17T07:45:13.000+0000,Abhishek Singh Chouhan,"When a zk server is running close to its outstanding requests limit, the server incorrectly throttles the sasl request. This leads to the client waiting for the final sasl packet (session is already established) and deferring all non priming packets till then which also includes the ping packets. The client then waits for the final packet but never gets it and times out saying haven't heard from server. This is fatal for services such as HBase which retry for finite attempts and exit post these attempts.

Issue being that in ZooKeeperServer.processPacket(..) incase of sasl we send the response and incorrectly also call cnxn.incrOutstandingRequests(h), which throttles the connection if we're running over outstandingrequests limit, which results in the server not processing the subsequent packet from the client. Also we donot have any pending request to send for the connection and hence never call enableRecv(). We should return after sending response to the sasl request.","[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2785,Critical,Abhishek Singh Chouhan,Fixed,2017-05-18T21:15:23.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Server inappropriately throttles connections under load before SASL completes,2017-05-24T02:34:59.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",12.0
Ben Sherman,"[<JIRA Component: name='leaderElection', id='12312378'>]",2017-05-13T00:43:26.000+0000,Ben Sherman,"We have a 5 node cluster running 3.4.10 we saw this in .8 and .9 as well), and sometimes, a node gets a read timeout, drops all the connections and tries to re-establish itself to the quorum.  It can usually do this in a few seconds, but last night it took almost 15 minutes to reconnect.

These are 5 servers in AWS, and we've tried tuning the timeouts, but the are exceeding any reasonable timeout and still failing.

In the attached logs, 5 is a follower, 3 is the leader.  5 loses connectivity at 11:21:34.  3 sees the disconnect at the same moment.

5 tries to re-establish the quorum, but cannot do it until the connections to the other servers expire at 11:37:02.  After the connections are re-established, 5 connects immediately.

At 11:41:08, the operator restarted the server, and it reconnected normally.

I suspect there is a problem with stale connections to the rest of the quorum - the other services on this box were fine (monitoring, puppet) and able to establish new connections with no problems.

I posed this problem to the zookeeper-users list and was asked to open a ticket.","[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>]",Bug,ZOOKEEPER-2783,Major,Ben Sherman,Duplicate,2017-06-11T18:59:46.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,follower disconnects and cannot reconnect,2017-06-11T18:59:46.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",3.0
Michael K. Edwards,"[<JIRA Component: name='quorum', id='12312379'>]",2017-05-06T05:08:32.000+0000,Michael Han,"It's possible to have a deadlock during recovery phase. 
Found this issue by analyzing thread dumps of ""flaky"" ReconfigRecoveryTest [1]. . Here is a sample thread dump that illustrates the state of the execution:

{noformat}
    [junit]  java.lang.Thread.State: BLOCKED
    [junit]         at  org.apache.zookeeper.server.quorum.QuorumPeer.getElectionAddress(QuorumPeer.java:686)
    [junit]         at  org.apache.zookeeper.server.quorum.QuorumCnxManager.initiateConnection(QuorumCnxManager.java:265)
    [junit]         at  org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:445)
    [junit]         at  org.apache.zookeeper.server.quorum.QuorumCnxManager.receiveConnection(QuorumCnxManager.java:369)
    [junit]         at  org.apache.zookeeper.server.quorum.QuorumCnxManager$Listener.run(QuorumCnxManager.java:642)
    [junit] 


    [junit]  java.lang.Thread.State: BLOCKED
    [junit]         at  org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:472)
    [junit]         at  org.apache.zookeeper.server.quorum.QuorumPeer.connectNewPeers(QuorumPeer.java:1438)
    [junit]         at  org.apache.zookeeper.server.quorum.QuorumPeer.setLastSeenQuorumVerifier(QuorumPeer.java:1471)
    [junit]         at  org.apache.zookeeper.server.quorum.Learner.syncWithLeader(Learner.java:520)
    [junit]         at  org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:88)
    [junit]         at  org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1133)
{noformat}

The dead lock happens between the quorum peer thread which running the follower that doing sync with leader work, and the listener of the qcm of the same quorum peer that doing the receiving connection work. Basically to finish sync with leader, the follower needs to synchronize on both QV_LOCK and the qmc object it owns; while in the receiver thread to finish setup an incoming connection the thread needs to synchronize on both the qcm object the quorum peer owns, and the same QV_LOCK. It's easy to see the problem here is the order of acquiring two locks are different, thus depends on timing / actual execution order, two threads might end up acquiring one lock while holding another.

[1] org.apache.zookeeper.server.quorum.ReconfigRecoveryTest.testCurrentServersAreObserversInNextConfig","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-2778,Blocker,Michael Han,Fixed,2018-12-07T12:12:08.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Potential server deadlock between follower sync with leader and follower receiving external connection requests.,2019-10-04T14:55:14.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",8.0
Nikhil Bhide,"[<JIRA Component: name='contrib', id='12312700'>]",2017-05-05T15:09:43.000+0000,Frederic Leger,"While trying to create an RPM from zookeeper 3.4.10, I got an error when it tried to compile the file :

zookeeper/contrib/zkpython/src/python/zk.py"", line 55
    """"""Pretty print(a zookeeper tree, starting at root"""""")
                                                         ^
SyntaxError: invalid syntax
","[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2777,Major,Frederic Leger,Fixed,2017-09-11T21:29:38.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,There is a typo in zk.py which prevents from using/compiling it.,2017-09-11T22:32:54.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",8.0
,"[<JIRA Component: name='leaderElection', id='12312378'>]",2017-05-05T08:30:48.000+0000,chenbo,"We found a bug of zookeeper election when used in our environment. It could be simply reproduced in 3 nodes cluster with default settings.
# Assume zookeeper services down on all nodes and node 3 has bigger zxid than node1. this makes node 3 a potential leader.
# Make node 2 down (or drop all incoming packages by firewall).
# Start zookeeper services on node 1 and node 3.

Zookeeper cluster cannot be successfully established in such a case. The following logs could be found and verified:
# Notifications to node 2 always times out.
# node 3 is always leading but always failed because (Timeout while waiting for epoch from quorum). It rarely get Follower during the period.
# node 1 is always following but always failed to connect Leader. it gives up after tried for 5 times and then another round election started again and again.
# the time node 3 decided to be a leader is 1s after node 1 giving up contacting it.
# node 3 always receive Notification packages 5s after node 1.

Then we analyzed source code of zookeeper-3.4.6 and found:
# In election, Zookeeper send leader election message sequentially and has connection timeout 5s by default. This makes a 5s recv delay for nodes after (by id) the down node. Those nodes will get the same election notification 5s after those nodes which have smaller id than the down node. 

In the case mentioned above, node 3 realized the situation and jumped into LEADING status 5s after node 1 decided to follow it. For follower node 1, it tried to connect leader 5 attempts with 1s interval (hard-coded). This means all followers give up connecting leader after 4s. At the time when follower gave up, the node 3 has not even become the leader.

-- So, Is there any solution to configure or bypass this problem?",[],Bug,ZOOKEEPER-2776,Major,chenbo,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Election Failed when (medium id) node down,2017-05-05T08:30:48.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",3.0
Mohammad Arshad,"[<JIRA Component: name='java client', id='12312381'>]",2017-05-05T06:12:03.000+0000,Bhupendra Kumar Jain,"
During Network unreachable scenario in one of the cluster, we observed Xid out of order and Nothing in the queue error continously. And ZK client it finally not able to connect successully to ZK server. 

*Logs:*

unexpected error, closing socket connection and attempting reconnect | org.apache.zookeeper.ClientCnxn (ClientCnxn.java:1447) 
java.io.IOException: Xid out of order. Got Xid 52 with err 0 expected Xid 53 for a packet with details: clientPath:null serverPath:null finished:false header:: 53,101  replyHeader:: 0,0,-4  request:: 12885502275,v{'/app1/controller,'/app1/config/changes},v{},v{'/app1/config/changes}  response:: null
	at org.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:996)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:101)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:370)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1426)

unexpected error, closing socket connection and attempting reconnect 
java.io.IOException: Nothing in the queue, but got 1
	at org.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:983)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:101)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:370)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1426)
	
*Analysis:* 
1) First time Client fails to do SASL login due to network unreachable problem.
2017-03-29 10:03:59,377 | WARN  | [main-SendThread(192.168.130.8:24002)] | SASL configuration failed: javax.security.auth.login.LoginException: Network is unreachable (sendto failed) Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it. | org.apache.zookeeper.ClientCnxn (ClientCnxn.java:1307) 
	Here the boolean saslLoginFailed becomes true.

2) After some time network connection is recovered and client is successully able to login but still the boolean saslLoginFailed is not reset to false. 

3) Now SASL negotiation between client and server start happening and during this time no user request will be sent. ( As the socket channel will be closed for write till sasl negotiation complets)
4) Now response from server for SASL packet will be processed by the client and client assumes that tunnelAuthInProgress() is finished ( method checks for saslLoginFailed boolean Since the boolean is true it assumes its done.) and tries to process the packet as a other packet and will result in above errors. 

*Solution:*  Reset the saslLoginFailed boolean every time before client login
","[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2775,Critical,Bhupendra Kumar Jain,Fixed,2017-06-12T23:18:39.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZK Client not able to connect with Xid out of order error ,2018-01-25T22:07:58.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",8.0
Jiafu Jiang,"[<JIRA Component: name='server', id='12312382'>]",2017-05-04T02:28:33.000+0000,Jiafu Jiang,"1. Deploy a ZooKeeper cluster with one node.
2. Create a Ephemeral znode.
3. Change the system time of the ZooKeeper node to a earlier point.
4. Disconnect the client with the ZooKeeper server.

Then the ephemeral znode will exist for a long time even when session timeout.

I have read the ZooKeeper source code and I find the code int SessionTrackerImpl.java，
{code:title=SessionTrackerImpl.java|borderStyle=solid}
    @Override
    synchronized public void run() {
        try {
            while (running) {
                currentTime = System.currentTimeMillis();
                if (nextExpirationTime > currentTime) {
                    this.wait(nextExpirationTime - currentTime);
                    continue;
                }
                SessionSet set;
                set = sessionSets.remove(nextExpirationTime);
                if (set != null) {
                    for (SessionImpl s : set.sessions) {
                        setSessionClosing(s.sessionId);
                        expirer.expire(s);
                    }
                }
                nextExpirationTime += expirationInterval;
            }
        } catch (InterruptedException e) {
            handleException(this.getName(), e);
        }
        LOG.info(""SessionTrackerImpl exited loop!"");
    }
{code}

I think it may be better to use System.nanoTime(), not System.currentTimeMillis, because the later can be changed manually or automatically by a NTP client. ","[<JIRA Version: name='3.4.11', id='12339207'>]",Bug,ZOOKEEPER-2774,Major,Jiafu Jiang,Fixed,2017-05-18T21:31:31.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"Ephemeral znode will not be removed when sesstion timeout, if the system time of ZooKeeper node changes unexpectedly.",2017-07-13T02:59:36.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.4.10', id='12338036'>]",5.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2017-05-03T04:35:03.000+0000,Ashwath,"Hi
I run zookeeper in 3 Linux Machines. 

1.I downloaded zookeeper-3.4.10.jar file and extracted that.
2.I copy zoo_sample to zoo.cfg and edited datadir and added 3 ip address.
3.I created a new file called myid and insert numbers into that.
Now I am running zookeeper cluster successfully..but

When I am trying to run it as a service I am getting following error

zookeeper.service - Apache ZooKeeper
  Loaded: loaded (/lib/systemd/system/zookeeper.service; disabled; vendor preset: enabled)
  Active: activating (auto-restart) (Result: exit-code) since Wed 2017-05-03 09:56:28 IST; 1s ago
 Process: 678 ExecStart=/home/melon/software/ZooKeeper/zk/bin/zkServer.sh start-foreground (code=exited
Main PID: 678 (code=exited, status=127)

May 03 09:56:28 deds14 systemd[1]: zookeeper.service: Unit entered failed state.
May 03 09:56:28 deds14 systemd[1]: zookeeper.service: Failed with result 'exit-code'.

Here the code I added

Unit]
Description=Apache ZooKeeper
After=network.target
ConditionPathExists=/home/melon/software/ZooKeeper/zookeeper-3.4.10-beta/conf/zoo.cfg
ConditionPathExists=/home/melon/software/ZooKeeper/zookeeper-3.4.10-beta/conf/log4j.properties

[Service]
Environment=""ZOOCFGDIR=/home/melon/software/ZooKeeper/zookeeper-3.4.10-beta/conf""
SyslogIdentifier=zookeeper
WorkingDirectory=/home/melon/software/ZooKeeper
ExecStart=/home/melon/software/ZooKeeper/zookeeper-3.4.10-beta/bin/zkServer.sh start-foreground
Restart=on-failure
RestartSec=20
User=root
Group=root

Thank you
","[<JIRA Version: name='3.4.10', id='12338036'>]",Bug,ZOOKEEPER-2773,Major,Ashwath,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zookeeper-service,2018-01-18T08:54:41.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",6.0
,"[<JIRA Component: name='security', id='12329414'>]",2017-05-02T21:19:42.000+0000,joe smith,"I set the acl to not be able to delete a node - but was able to delete regardless.

I am not familiar with the code, but a reply from Martin in the user@ mailing list seems to confirm the issue.  I will paste his response below - sorry for the long listing.

Martin's reply are inline prefixed with: MG>

----------
From: joe smith <water4u99@yahoo.com.INVALID>
Sent: Tuesday, May 2, 2017 8:40 AM
To: user@zookeeper.apache.org
Subject: Acl block detete not working

Hi,
I'm using 3.4.10 and setting custom aol to block deletion of a znode.  However, I'm able to delete the node even after I've set acl from cdrwa to cra.

Can anyone point out if I missed some step.

Thanks for the help

Here is the trace:
[zk: localhost:2181(CONNECTED) 0] ls /
[zookeeper]

[zk: localhost:2181(CONNECTED) 1] create /test ""data""
Created /test

[zk: localhost:2181(CONNECTED) 2] ls /
[zookeeper, test]

[zk: localhost:2181(CONNECTED) 3] addauth myfqdn localhost
[zk: localhost:2181(CONNECTED) 4] setAcl /test myfqdn:localhost:cra
cZxid = 0x2
ctime = Tue May 02 08:28:42 EDT 2017
mZxid = 0x2
mtime = Tue May 02 08:28:42 EDT 2017
pZxid = 0x2
cversion = 0
dataVersion = 0
aclVersion = 1
ephemeralOwner = 0x0
dataLength = 4
numChildren = 0

MG>in SetAclCommand you can see the acl being parsed and acl being set by setAcl into zk object

    List<ACL> acl = AclParser.parse(aclStr);
        int version;
        if (cl.hasOption(""v"")) {
            version = Integer.parseInt(cl.getOptionValue(""v""));
        } else {
            version = -1;
        }
        try {
            Stat stat = zk.setACL(path, acl, version);

MG>later on in DeleteCommand there is no check for aforementioned acl parameter
  public boolean exec() throws KeeperException, InterruptedException {
        String path = args[1];
        int version;
        if (cl.hasOption(""v"")) {
            version = Integer.parseInt(cl.getOptionValue(""v""));
        } else {
            version = -1;
        }

        try {
        zk.delete(path, version);
        } catch(KeeperException.BadVersionException ex) {
            err.println(ex.getMessage());
        }
        return false;

MG>as seen here the testCase works properly saving the Zookeeper object
    LsCommand entity = new LsCommand();
        entity.setZk(zk);


MG>but setACL does not save the zookeeper object anywhere but instead seems to discard zookeeper object with accompanying ACLs

MG>can you report this bug to Zookeeper?
https://issues.apache.org/jira/browse/ZOOKEEPER/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel

ZooKeeper - ASF JIRA - issues.apache.org<https://issues.apache.org/jira/browse/ZOOKEEPER/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel>
issues.apache.org
Apache ZooKeeper is a service for coordinating processes of distributed applications. Versions: Unreleased. Name Release date; Unreleased 3.2.3 : Unreleased 3.3.7

MG>Thanks Joe!

[zk: localhost:2181(CONNECTED) 5] getAcl /test
'myfqdn,'localhost
: cra

[zk: localhost:2181(CONNECTED) 6] get /testdata
cZxid = 0x2
ctime = Tue May 02 08:28:42 EDT 2017
mZxid = 0x2
mtime = Tue May 02 08:28:42 EDT 2017
pZxid = 0x2
cversion = 0
dataVersion = 0
aclVersion = 1
ephemeralOwner = 0x0
dataLength = 4
numChildren = 0

[zk: localhost:2181(CONNECTED) 7] set /test ""testwrite""
Authentication is not valid : /test

[zk: localhost:2181(CONNECTED) 8] delete /test
[zk: localhost:2181(CONNECTED) 9] ls /
[zookeeper]

[zk: localhost:2181(CONNECTED) 10]
The auth provider imple is here: http://s000.tinyupload.com/?file_id=42827186839577179157
",[],Bug,ZOOKEEPER-2772,Major,joe smith,Not A Bug,2017-05-17T14:32:49.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Delete node command does not honor Acl policy,2017-05-17T14:32:49.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.4.10', id='12338036'>]",3.0
,[],2017-05-02T08:15:58.000+0000,masoud rezai,,[],Bug,ZOOKEEPER-2771,Major,masoud rezai,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,all resolved bug,2017-05-02T08:15:58.000+0000,[],2.0
Andor Molnar,[],2017-04-28T19:44:50.000+0000,Abraham Fine,"We have xml files that compile into our documentation in src/docs/ and precompiled documentation docs/

We should remove them and only have uncompiled documentation under source control",[],Bug,ZOOKEEPER-2769,Major,Abraham Fine,Not A Bug,2017-10-13T22:58:01.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Compiled documentation should not be under source control,2017-10-14T04:42:38.000+0000,[],3.0
,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='quorum', id='12312379'>]",2017-04-25T10:18:42.000+0000,Patrick Kleindienst,"When I start a ZooKeeper ensemble comprising 3 nodes, I'm currently facing the following behavior:
Two nodes (let's say node 2 and 3) out of the three start their own quorum, and finally one of them is elected the new leader (node 3), while the other one becomes the follower (node 2). Node 1 seems to be able to establish a connection to node 3 (elected leader) in my case, but this seems to fail for node 2. 
Node 1 shows the following in its logs:

 2017-04-25 09:24:02,806 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):QuorumPeer@1055] - LOOKING
2017-04-25 09:24:02,808 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):FastLeaderElection@894] - New election. My id =  1, proposed zxid=0x0
2017-04-25 09:24:02,811 [myid:1] - INFO  [WorkerReceiver[myid=1]:FastLeaderElection@688] - Notification: 2 (message format version), 1 (n.leader), 0x0 (n.zxid), 0x1 (n.round), LOOKING (n.state), 1 (n.sid), 0x0 (
n.peerEPoch), LOOKING (my state)0 (n.config version)
2017-04-25 09:24:02,817 [myid:1] - WARN  [WorkerSender[myid=1]:QuorumCnxManager@457] - Cannot open channel to 2 at election address /9.152.171.98:3888
java.net.ConnectException: Connection refused (Connection refused)
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:589)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:443)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:486)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.toSend(QuorumCnxManager.java:421)
        at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.process(FastLeaderElection.java:486)
        at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.run(FastLeaderElection.java:465)
        at java.lang.Thread.run(Thread.java:745)
2017-04-25 09:24:02,822 [myid:1] - INFO  [WorkerSender[myid=1]:QuorumCnxManager@278] - Have smaller server identifier, so dropping the connection: (3, 1)
2017-04-25 09:24:03,025 [myid:1] - WARN  [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):QuorumCnxManager@457] - Cannot open channel to 2 at election address /9.152.171.98:3888

However, that's not all, since the quorum consisting of node 2 and 3 does not work properly. The nodes' logs tell that leader election between these two works fine.
Here's what node 3 (leader) says:

 2017-04-25 09:09:33,842 [myid:3] - INFO  [WorkerReceiver[myid=3]:FastLeaderElection@688] - Notification: 2 (message format version), 3 (n.leader), 0x0 (n.zxid), 0x1 (n.round), LOOKING (n.state), 3 (n.sid), 0x0 (n.peerEPoch), LOOKING (my state)0 (n.config version)
2017-04-25 09:09:33,844 [myid:3] - INFO  [WorkerReceiver[myid=3]:FastLeaderElection@688] - Notification: 2 (message format version), 2 (n.leader), 0x0 (n.zxid), 0x1 (n.round), LOOKING (n.state), 2 (n.sid), 0x0 (n.peerEPoch), LOOKING (my state)0 (n.config version)
2017-04-25 09:09:33,851 [myid:3] - INFO  [WorkerReceiver[myid=3]:FastLeaderElection@688] - Notification: 2 (message format version), 3 (n.leader), 0x0 (n.zxid), 0x1 (n.round), LOOKING (n.state), 2 (n.sid), 0x0 (n.peerEPoch), LOOKING (my state)0 (n.config version)
2017-04-25 09:09:34,051 [myid:3] - INFO  [QuorumPeer[myid=3](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):MBeanRegistry@128] - Unregister MBean [org.apache.ZooKeeperService:name0=ReplicatedServer_id3,name1=replica.3,name2=LeaderElection]
2017-04-25 09:09:34,052 [myid:3] - INFO  [QuorumPeer[myid=3](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):QuorumPeer@1143] - LEADING
2017-04-25 09:09:34,055 [myid:3] - INFO  [QuorumPeer[myid=3](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Leader@63] - TCP NoDelay set to: true
2017-04-25 09:09:34,055 [myid:3] - INFO  [QuorumPeer[myid=3](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Leader@83] - zookeeper.leader.maxConcurrentSnapshots = 10
2017-04-25 09:09:34,056 [myid:3] - INFO  [QuorumPeer[myid=3](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Leader@85] - zookeeper.leader.maxConcurrentSnapshotTimeout = 5


And here's the output node 2 (follower) provides:

2017-04-25 09:09:31,875 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection@688] - Notification: 2 (message format version), 3 (n.leader), 0x0 (n.zxid), 0x1 (n.round), LOOKING (n.state), 2 (n.sid), 0x0 (
n.peerEPoch), LOOKING (my state)0 (n.config version)
2017-04-25 09:09:32,077 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):MBeanRegistry@128] - Unregister MBean [org.apache.ZooKeeperService:name0=ReplicatedServer_id2,name1=repl
ica.2,name2=LeaderElection]
2017-04-25 09:09:32,077 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):QuorumPeer@1131] - FOLLOWING
2017-04-25 09:09:32,082 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Learner@88] - TCP NoDelay set to: true

So far, so good. But seconds later the connection between node 2 and 3 seems to get lost, causing leader node 3 to report an EOFExeption. If I understand the logs correctly, node 2 (follower) properly closes the connection (sending ""Goodbye""), whilst node 3 says that the socket is still open. 

2017-04-25 09:09:34,190 [myid:3] - INFO  [QuorumPeer[myid=3](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Leader@414] - LEADING - LEADER ELECTION TOOK - 138 MS
2017-04-25 09:09:34,197 [myid:3] - INFO  [QuorumPeer[myid=3](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):FileTxnSnapLog@320] - Snapshotting: 0x0 to /data/version-2/snapshot.0
2017-04-25 09:09:35,076 [myid:3] - INFO  [LearnerHandler-/9.152.171.98:51328:LearnerHandler@382] - Follower sid: 2 : info : 9.152.171.98:2888:3888:participant;0.0.0.0:2181
2017-04-25 09:09:35,113 [myid:3] - INFO  [LearnerHandler-/9.152.171.98:51328:LearnerHandler@683] - Synchronizing with Follower sid: 2 maxCommittedLog=0x0 minCommittedLog=0x0 lastProcessedZxid=0x0 peerLastZxid=0x
0
2017-04-25 09:09:35,114 [myid:3] - INFO  [LearnerHandler-/9.152.171.98:51328:LearnerHandler@727] - Sending DIFF zxid=0x0 for peer sid: 2
2017-04-25 09:09:35,133 [myid:3] - INFO  [QuorumPeer[myid=3](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Leader@1258] - Have quorum of supporters, sids: [ [2, 3],[2, 3] ]; starting up and setting last processed zxid: 0x100000000
2017-04-25 09:09:35,169 [myid:3] - INFO  [QuorumPeer[myid=3](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):CommitProcessor@255] - Configuring CommitProcessor with 2 worker threads.
2017-04-25 09:09:35,179 [myid:3] - INFO  [QuorumPeer[myid=3](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):ContainerManager@64] - Using checkIntervalMs=60000 maxPerMinute=10000
2017-04-25 09:09:35,196 [myid:3] - WARN  [RecvWorker:2:QuorumCnxManager$RecvWorker@919] - Connection broken for id 2, my id = 3, error = 
java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:392)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:904)
2017-04-25 09:09:35,196 [myid:3] - WARN  [RecvWorker:2:QuorumCnxManager$RecvWorker@922] - Interrupting SendWorker
2017-04-25 09:09:35,197 [myid:3] - WARN  [SendWorker:2:QuorumCnxManager$SendWorker@836] - Interrupted while waiting for message on queue
java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2088)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.pollSendQueue(QuorumCnxManager.java:986)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.access$500(QuorumCnxManager.java:65)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:824)
2017-04-25 09:09:35,197 [myid:3] - WARN  [SendWorker:2:QuorumCnxManager$SendWorker@845] - Send worker leaving thread  id 2 my id = 3
2017-04-25 09:09:35,204 [myid:3] - ERROR [LearnerHandler-/9.152.171.98:51328:LearnerHandler@604] - Unexpected exception causing shutdown while sock still open
java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:392)
        at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
        at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:83)
        at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:99)
        at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:515)
2017-04-25 09:09:35,204 [myid:3] - WARN  [LearnerHandler-/9.152.171.98:51328:LearnerHandler@619] - ******* GOODBYE /9.152.171.98:51328 ********
2017-04-25 09:09:37,181 [myid:3] - INFO  [QuorumPeer[myid=3](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Leader@626] - Shutting down
2017-04-25 09:09:37,182 [myid:3] - INFO  [QuorumPeer[myid=3](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Leader@632] - Shutdown called
java.lang.Exception: shutdown Leader! reason: Not sufficient followers synced, only synced with sids: [ [3] ]
        at org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:632)
        at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:612)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1146)

Unfortunately, node 2 does not provide any additional information on what exactly is going on. After leader election, the only thing it reports is this:

2017-04-25 09:09:32,091 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Follower@68] - FOLLOWING - LEADER ELECTION TOOK - 13 MS
2017-04-25 09:09:32,094 [myid:2] - WARN  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Learner@273] - Unexpected exception, tries=0, remaining init limit=9999, connecting to /9.152.171.12:288
8
java.net.ConnectException: Connection refused (Connection refused)
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:589)
        at org.apache.zookeeper.server.quorum.Learner.sockConnect(Learner.java:227)
        at org.apache.zookeeper.server.quorum.Learner.connectToLeader(Learner.java:256)
        at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:76)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1133)
2017-04-25 09:09:33,142 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Learner@369] - Getting a diff from the leader 0x0
2017-04-25 09:09:33,146 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Learner@516] - Learner received NEWLEADER message
2017-04-25 09:09:33,207 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Learner@499] - Learner received UPTODATE message
2017-04-25 09:09:33,220 [myid:2] - WARN  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):QuorumPeer@1446] - Restarting Leader Election
2017-04-25 09:09:33,221 [myid:2] - INFO  [/0.0.0.0:3888:QuorumCnxManager$Listener@665] - Leaving listener
2017-04-25 09:09:33,222 [myid:2] - WARN  [SendWorker:3:QuorumCnxManager$SendWorker@836] - Interrupted while waiting for message on queue
java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2088)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.pollSendQueue(QuorumCnxManager.java:986)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.access$500(QuorumCnxManager.java:65)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:824)
2017-04-25 09:09:33,222 [myid:2] - WARN  [RecvWorker:3:QuorumCnxManager$RecvWorker@919] - Connection broken for id 3, my id = 2, error = 
java.net.SocketException: Socket closed
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
        at java.net.SocketInputStream.read(SocketInputStream.java:171)
        at java.net.SocketInputStream.read(SocketInputStream.java:141)
        at java.net.SocketInputStream.read(SocketInputStream.java:224)
        at java.io.DataInputStream.readInt(DataInputStream.java:387)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:904)

As far as I get that, node 2 wants to start a new leader election, but fails to establish a connection to the other nodes. It tries over and over, finally ending up in a timeout. Unfortunately, this doesn't give me any hint on what exactly breaks up the connection between the follower node (node 2) and the leader node (node 3) and why it can be re-established. 

It might also be relevant that I'm running ZooKeeper in Docker containers, using the host network option.",[],Bug,ZOOKEEPER-2766,Major,Patrick Kleindienst,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Quorum fails with java.io.EOFException,2017-07-19T14:39:09.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",4.0
,[],2017-04-19T09:54:16.000+0000,Arne Bachmann,"Using the same Vagrant provisioning script as for 3.5.2-alpha, suddenly all monitoring tools told me that the ZK instance was unavailable or had an error. Investigating further, the instance was fine as a follower, but the response to telnet ""ruok"" was actually ""ruok ... is not in the whitelist"".
Is this a new default not reflected in the documentation yet? It says since 3.4.10 there's a whitelist option, but all commands are by default on it (same as 4lw.commands.whitelist=*).","[<JIRA Version: name='3.5.3', id='12335444'>]",Bug,ZOOKEEPER-2764,Minor,Arne Bachmann,Not A Bug,2017-04-20T08:10:21.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"By default, only srvr four-letter word is on the whitelist, while documentation says all are",2017-05-18T03:43:54.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",1.0
Alburt Hoffman,"[<JIRA Component: name='jute', id='12312385'>]",2017-04-19T06:34:02.000+0000,Brandon Berg,"org.apache.jute.Utils.toCsvBuffer(), which converts a byte array to a string containing the hex representation of that byte array, omits the leading zero for any byte less than 0x10, due to its use of Integer.toHexString, which has the same behavior.

https://github.com/apache/zookeeper/blob/master/src/java/main/org/apache/jute/Utils.java#L234

One consequence of this is that the hex strings printed by ClientCnxn.Packet.toString(), used in the debug logging for ClientCnxn.readResponse(), cannot be parsed to determine the result of a Zookeeper request from client debug logs.

Utils.toXmlBuffer() appears to have the same issue.
{code}",[],Bug,ZOOKEEPER-2763,Minor,Brandon Berg,Won't Fix,2019-08-08T17:13:30.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Utils.toCsvBuffer() omits leading 0 for bytes < 0x10,2019-08-08T17:13:30.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",6.0
,"[<JIRA Component: name='build', id='12312383'>]",2017-04-18T11:31:19.000+0000,Arne Bachmann,"This breaks my build scripts. Did work fine with 3.5.2-alpha

Using 7-Zip on Windows I got a warning, but the archive was extracted fine.

On Linux, tar -xzf exits with an error code, as it pipes through gunzip, which encounters an invalid file (seems to be a pure tar archive).

Hence the huge file size (?)",[],Bug,ZOOKEEPER-2761,Major,Arne Bachmann,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Build is packaged as uncompressed tar archive but file name ends with .gz,2020-01-23T18:16:11.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",5.0
,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='c client', id='12312380'>]",2017-04-18T07:49:07.000+0000,Yuqi Gu,"Zookeeper-3.4.10 is integrated into Apache Bigtop : 
https://github.com/apache/bigtop/commit/b00ac093634437e749561c8837179d13d95fda91

But compiling error occurred when we build the bigtop zookeeper component on AArch64 :

""[exec] libtool: compile: gcc -DHAVE_CONFIG_H -I. -I/ws/output/zookeeper/zookeeper-3.4.10/src/c -I/ws/output/zookeeper/zookeeper-3.4.10/src/c/include -I/ws/output/zookeeper/zookeeper-3.4.10/src/c/tests -I/ws/output/zookeeper/zookeeper-3.4.10/src/c/generated -Wdate-time -D_FORTIFY_SOURCE=2 -DTHREADED -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -MT libzkmt_la-mt_adaptor.lo -MD -MP -MF .deps/libzkmt_la-mt_adaptor.Tpo -c /ws/output/zookeeper/zookeeper-3.4.10/src/c/src/mt_adaptor.c -fPIC -DPIC -o .libs/libzkmt_la-mt_adaptor.o
[exec] Makefile:946: recipe for target 'libzkmt_la-mt_adaptor.lo' failed
[exec] make[2]: Leaving directory '/ws/output/zookeeper/zookeeper-3.4.10/build/c'
[exec] /tmp/cc4YHZ73.s: Assembler messages:
[exec] /tmp/cc4YHZ73.s:1713: Error: unknown mnemonic lock' --lock xaddl x1,[x0]'
[exec] make[2]: *** [libzkmt_la-mt_adaptor.lo] Error 1
""
","[<JIRA Version: name='3.4.10', id='12338036'>]",Bug,ZOOKEEPER-2760,Blocker,Yuqi Gu,Duplicate,2018-09-07T07:13:00.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"AAch64 build error: Error: unknown mnemonic `lock' -- `lock xaddl x1,[x0]'",2018-09-07T07:13:00.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",2.0
Abraham Fine,[],2017-04-17T22:45:41.000+0000,Abraham Fine,,"[<JIRA Version: name='3.4.11', id='12339207'>]",Bug,ZOOKEEPER-2759,Major,Abraham Fine,Fixed,2017-04-27T21:15:27.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Flaky test: org.apache.zookeeper.server.quorum.QuorumCnxManagerTest.testNoAuthLearnerConnectToAuthRequiredServerWithHigherSid,2017-07-21T15:32:05.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",4.0
Jeff Widman,[],2017-04-15T06:30:58.000+0000,Jeff Widman,Typo in src/docs/src/documentation/content/xdocs/zookeeperAdmin.xml,"[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2758,Trivial,Jeff Widman,Fixed,2017-04-24T17:10:34.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Typo: transasction --> transaction,2017-07-13T03:03:35.000+0000,[],5.0
Abraham Fine,[],2017-04-14T21:58:40.000+0000,Flavio Paiva Junqueira,"If I try {{delete test}} without the leading /, then the CLI crashes with this exception:

{noformat}
Exception in thread ""main"" java.lang.IllegalArgumentException: Path must start with / character
	at org.apache.zookeeper.common.PathUtils.validatePath(PathUtils.java:51)
	at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:1659)
	at org.apache.zookeeper.cli.DeleteCommand.exec(DeleteCommand.java:83)
	at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:655)
	at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:586)
	at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:370)
	at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:330)
	at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:292)
{noformat}

It should really fail the operation rather than crash the CLI.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2757,Minor,Flavio Paiva Junqueira,Fixed,2017-05-26T22:52:57.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Incorrect path crashes zkCli,2017-05-26T23:44:14.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",5.0
Abraham Fine,[],2017-04-06T22:01:58.000+0000,Abraham Fine,"Currently when compiling ZooKeeper we se a compilation warning:
{code}
    [javac] /zookeeper/src/java/main/org/apache/zookeeper/admin/ZooKeeperAdmin.java:43: warning: [try] auto-closeable resource ZooKeeperAdmin has a member method close() that could throw InterruptedException
    [javac] public class ZooKeeperAdmin extends ZooKeeper {
    [javac]        ^
    [javac] 2 warnings
{code}

This is due to the implementation of AutoCloseable in the ZooKeeper superclass. That class has a warning suppression and explanation, we should copy it to the ZooKeeperAdmin class.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2747,Major,Abraham Fine,Fixed,2017-04-17T23:58:20.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Fix ZooKeeperAdmin Compilation Warning,2017-04-18T00:42:44.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",4.0
,"[<JIRA Component: name='server', id='12312382'>]",2017-04-04T21:55:38.000+0000,Abhay Bothra,"If disk is full on 1 zookeeper node in a 3 node ensemble, it is able to join the quorum with partial data.

Setup:
--------
- Running a 3 node zookeeper ensemble on Ubuntu 12.04 as upstart services. Let's call the nodes: A, B and C.

Observation:
-----------------
- Connecting to 2 (Node A and B) of the 3 nodes and doing an `ls` in zookeeper data directory was giving:
/foo
/bar
/baz
But an `ls` on node C was giving:
/baz
- On node C, the zookeeper data directory had the following files:
log.1001
log.1600
snapshot.1000 -> size 200
snapshot.1200 -> size 269
snapshot.1300 -> size 300
- Snapshot sizes on node A and B were in the vicinity of 500KB

RCA
-------
- Disk was full on node C prior to the creation time of the small snapshot
  files.
- Looking at zookeeper server logs, we observed that zookeeper had crashed and restarted a few times after the first instance of disk full. Everytime time zookeeper starts, it does 3 things:
  1. Run the purge task to cleanup old snapshot and txn logs. Our
  autopurge.snapRetainCount is set to 3.
  2. Restore from the most recent valid snapshot and the txn logs that follow.
  3. Take part in a leader election - realize it has missed something - become the follower - get diff of missed txns from the current leader - create a new snapshot of its current state.
- We confirmed that a valid snapshot of the system had existed prior to, and
  immediately after the crash. Let's call this snapshot snapshot.800.
- Over the next 3 restarts, zookeeper did the following:
  - Purged older snapshots
  - Restored from snapshot.800 + txn logs
  - Synced up with master, tried to write its updated state to a new snapshot. Crashed due to disk full. The snapshot file, even though invalid, had been created.
- *Note*: This is the first source of the bug. It might be more appropriate to first write the snapshot to a temporary file, and then rename it
snapshot.<txn_id>. That would gives us more confidence in the validity of snapshots in the data dir. 
- Let's say the snapshot files created above were snapshot.850, snapshot.920 and snapshot.950
- On the 4th restart, the purge task retained the 3 recent snapshots - snapshot.850, snapshot.920, and snapshot.950, and proceeded to purge snapshot.800 and associated txn logs assuming that they were no longer needed.
- *Note*: This is the second source of the bug. Instead of retaining the 3 most recent *valid* snapshots, the server just retains 3 most recent snapshots, regardless of their validity.
- When restoring, zookeeper doesn't find any valid snapshot logs to restore from. So it tries to reload its state from txn logs starting at zxid 0. However, those transaction logs would have long ago been garbage collected. It reloads from whatever txn logs are present. Let's say the only txn log file present (log.951) contains logs for zxid 951 to 998.  It reloads from that log file, syncs with master - gets txns 999 and 1000, and writes the snapshot log snapshot.1000 to disk. Now that we have deleted snapshot.800, we have enough free disk space to write snapshot.1000. From this state onwards, zookeeper will always assume it has the state till txn id 1000, even though it only has state from txn id 951 to 1000.",[],Bug,ZOOKEEPER-2745,Critical,Abhay Bothra,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Node loses data after disk-full event, but successfully joins Quorum",2018-11-22T01:46:09.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",5.0
Michael Han,"[<JIRA Component: name='server', id='12312382'>]",2017-04-01T20:30:47.000+0000,Michael Han,"This is a tricky issue found while debugging failure of ""flaky"" watcher test (ZOOKEEPER-2686). When closing a Netty connection, depend on timing the connection bean registered when the connection was provisioned might not get unregistered, leading to leaked Java beans. 

The race happens at the time when the client is in the process of finalizing the session. As part of session finalization, a connection bean will be registered [1]. But right before the registering bean, the connection might gets closed, in cases for example the server that the client is connecting to is shutdown. As part of connection close, the bean will be un-registered, as expected [2], however the problem is when we execute at [2], the connection bean might not finish registering at [1], so the unregister of bean is a NOP. What's worse, as part of connection close, we remove this connection from connection factory [3], so future connection close call will get short circuited and directly return; in other words the bean unregister code in connection close call will only get executed once. Depends on luck, the bean might not get unregistered, as previously illustrated.


[1] https://github.com/apache/zookeeper/blob/master/src/java/main/org/apache/zookeeper/server/ZooKeeperServer.java#L700
[2] https://github.com/apache/zookeeper/blob/master/src/java/main/org/apache/zookeeper/server/NettyServerCnxn.java#L114
[3] 
https://github.com/apache/zookeeper/blob/master/src/java/main/org/apache/zookeeper/server/NettyServerCnxn.java#L96","[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2743,Major,Michael Han,Fixed,2017-04-10T17:20:14.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Netty connection leaks JMX connection bean upon connection close in certain race conditions.,2017-04-17T23:44:45.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.2', id='12331981'>]",4.0
Michael Han,"[<JIRA Component: name='server', id='12312382'>]",2017-03-27T17:35:36.000+0000,Michael Han,"ZOOKEEPER-2737 fix is pending to branch-3.4 because we are in middle of release. This fix should get in after 3.4.10 gets out.

cc [~rakeshr].","[<JIRA Version: name='3.4.11', id='12339207'>]",Bug,ZOOKEEPER-2740,Critical,Michael Han,Fixed,2017-07-20T17:42:48.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Port ZOOKEEPER-2737 to branch-3.4,2017-07-20T17:42:48.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.4.10', id='12338036'>]",2.0
,[],2017-03-25T01:22:16.000+0000,Vincent Poon,"The maxClientCnxns field isn't being used in NettyServerCnxnFactory, and therefore the connection limit isn't observed.

See attached test",[],Bug,ZOOKEEPER-2739,Major,Vincent Poon,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,maxClientCnxns not working in NettyServerCnxnFactory,2017-03-25T04:34:20.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.6.0', id='12326518'>]",4.0
,[],2017-03-25T01:12:18.000+0000,Vincent Poon,"The test MaxCnxnsTest is incorrect as it only creates up the maxCnxns number of threads, whereas it should create more.  See attached patch

When the test is fixed, it fails on master and 3.5, where ZOOKEEPER-1504 removed some synchronization.
",[],Bug,ZOOKEEPER-2738,Major,Vincent Poon,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,maxClientCnxns not limiting concurrent connections properly,2019-01-30T13:33:02.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",4.0
Michael Han,"[<JIRA Component: name='server', id='12312382'>]",2017-03-24T17:16:17.000+0000,Michael Han,"Found this while debugging occasionally failed unit tests. Currently we do this if exception occurs during writing to a channel with Netty:

{code}
@Override
        public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent e)
            throws Exception
        {
            LOG.warn(""Exception caught "" + e, e.getCause());
            NettyServerCnxn cnxn = (NettyServerCnxn) ctx.getAttachment();
            if (cnxn != null) {
                if (LOG.isDebugEnabled()) {
                    LOG.debug(""Closing "" + cnxn);
                    cnxn.close();
                }
            }
        }
{code}

So the connection is only closed when debug mode is enabled. This is problematic as lots of clean up code is abstracted inside the close and without proper close the connection we are leaking resources.

[Commit log|https://github.com/apache/zookeeper/blob/master/src/java/main/org/apache/zookeeper/server/NettyServerCnxnFactory.java#L147] indicates the issue exists since day 1 with ZOOKEEPER-733. Note the original patch uploaded to ZOOKEEPER-733 has this close call in right place, and the call gets moved around during iteration of the patches w/o gets noticed.","[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2737,Critical,Michael Han,Fixed,2017-03-27T17:32:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,NettyServerCnxFactory leaks connection if exception happens while writing to a channel.,2017-05-18T03:43:58.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",4.0
Woojin Joe,"[<JIRA Component: name='scripts', id='12312384'>]",2017-03-23T01:27:20.000+0000,Woojin Joe,"There are same typos in shell files follows:
* https://github.com/apache/zookeeper/blob/master/bin/zkCleanup.sh#L28
* https://github.com/apache/zookeeper/blob/master/bin/zkCli.sh#L28
* https://github.com/apache/zookeeper/blob/master/bin/zkServer.sh#L25

TO-BE
need to be changed from *POSTIX* to *POSIX*","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2735,Trivial,Woojin Joe,Fixed,2017-03-23T17:22:40.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Typo fixes in some scripts,2017-07-13T03:10:35.000+0000,[],5.0
Rakesh Radhakrishnan,[],2017-03-20T05:13:06.000+0000,Rakesh Radhakrishnan,"This jira to cleanup findbug warnings reported in branch-3.4

[Branch3.4 FindbugsWarnings.html|https://builds.apache.org/job/PreCommit-ZOOKEEPER-github-pr-build/444/artifact/build/test/findbugs/newPatchFindbugsWarnings.html]","[<JIRA Version: name='3.4.11', id='12339207'>]",Bug,ZOOKEEPER-2728,Major,Rakesh Radhakrishnan,Fixed,2017-06-19T10:36:45.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Clean up findbug warnings in branch-3.4,2017-07-13T03:04:54.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",3.0
Mark Fenes,[],2017-03-17T15:48:40.000+0000,Andrey,"Steps to reproduce:
* setup zookeeper
* setup TCP load balancer. This balancer should check zookeeper clientPort liveness(healthcheck) by opening and closing TCP connection to clientPort. See https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ or https://www.digitalocean.com/community/tutorials/how-to-create-your-first-digitalocean-load-balancer#step-2-—-creating-the-load-balancer for details. 
* in logs:
{code}
2017-03-17 15:41:19,843 [myid:1] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@357] - caught end of stream exception
EndOfStreamException: Unable to read additional data from client sessionid 0x0, likely client has closed socket
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:203)
	at java.lang.Thread.run(Thread.java:745)
{code}

Issue is here:
https://github.com/apache/zookeeper/blob/5fe68506f217246c7ebd96803f9c78e13ec2f11a/src/java/main/org/apache/zookeeper/server/NIOServerCnxn.java#L322

-1 is a normal socket termination. 

Expected:
* reduce log level to INFO
* do not log stacktrace.",[],Bug,ZOOKEEPER-2727,Major,Andrey,Won't Fix,2018-03-23T14:43:11.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,WARN and stacktrace for normally closed socket,2018-03-23T14:43:11.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",4.0
Kyle Nusbaum,[],2017-03-16T19:15:05.000+0000,Kyle Nusbaum,"We noticed when porting the patch, that isEnabled is not thread-safe. 
Synchronizing it and resetWhitelist should solve the issue.","[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2726,Major,Kyle Nusbaum,Fixed,2017-03-16T20:01:13.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Patch for ZOOKEEPER-2693 introduces potential race condition,2017-03-31T09:01:12.000+0000,[],5.0
Brian Nixon,"[<JIRA Component: name='server', id='12312382'>]",2017-03-15T22:34:55.000+0000,Brian Nixon,"On an ensemble with local sessions enabled, when a client with a local session requests the creation of an ephemeral node within a multi-op, the client gets a session expired message.  The same multi-op works if the session is already global. This breaks the client's expectation of seamless promotion from local session to global session server-side. ","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2725,Major,Brian Nixon,Fixed,2017-03-21T16:50:14.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Upgrading to a global session fails with a multiop,2017-07-13T03:05:51.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",5.0
Vishal Khandelwal,[],2017-03-15T11:11:43.000+0000,Vishal Khandelwal,"f2017-03-14 07:10:26,247 INFO [main] zookeeper.ZooKeeper - Initiating client connection, connectString=x1-1-was.ops.sfdc.net:2181,x2-1-was.ops.sfdc.net:2181,x3-1-was.ops.sfdc.net:2181,x4-1-was.ops.sfdc.net:2181,x5-1-was.ops.sfdc.net:2181:/hbase sessionTimeout=60000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@6e16b8b5 2017-03-14 07:10:26,250 ERROR [main] client.StaticHostProvider - Unable to connect to server: x5-1-was.ops.sfdc.net:2181:2181 java.net.UnknownHostException: x5-1-was.ops.sfdc.net:2181: Name or service not known at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method) at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928) at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323) at java.net.InetAddress.getAllByName0(InetAddress.java:1276) at java.net.InetAddress.getAllByName(InetAddress.java:1192) at java.net.InetAddress.getAllByName(InetAddress.java:1126) at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:60) at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:446) at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380) at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.checkZk(RecoverableZooKeeper.java:141) at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.<init>(RecoverableZooKeeper.java:128) at org.apache.hadoop.hbase.zookeeper.ZKUtil.connect(ZKUtil.java:135) at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.<init>(ZooKeeperWatcher.java:173) at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.<init>(ZooKeeperWatcher.java:147) at org.apache.hadoop.hbase.client.ZooKeeperKeepAliveConnection.<init>(ZooKeeperKeepAliveConnection.java:43) at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getKeepAliveZooKeeperWatcher(HConnectionManager.java:1875) at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:82) at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.retrieveClusterId(HConnectionManager.java:929) at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.<init>(HConnectionManager.java:714) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.hadoop.hbase.client.HConnectionManager.createConnection(HConnectionManager.java:466) at org.apache.hadoop.hbase.client.HConnectionManager.createConnection(HConnectionManager.java:445) at org.apache.hadoop.hbase.client.HConnectionManager.getConnection(HConnectionManager.java:326)",[],Bug,ZOOKEEPER-2723,Major,Vishal Khandelwal,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ConnectStringParser does not parse correctly if quorum string has znode path,2017-04-18T20:45:43.000+0000,[],3.0
Michael Han,"[<JIRA Component: name='tests', id='12312427'>]",2017-03-14T23:16:11.000+0000,Michael Han,"{noformat}
Error Message

KeeperErrorCode = ConnectionLoss for /test
Stacktrace

org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /test
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1423)
	at org.apache.zookeeper.test.ReadOnlyModeTest.testSessionEstablishment(ReadOnlyModeTest.java:238)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:79)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

Looks like we should retry before giving up.","[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2722,Major,Michael Han,Fixed,2017-04-17T23:38:07.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Flaky Test: org.apache.zookeeper.test.ReadOnlyModeTest.testSessionEstablishment,2017-04-24T16:55:29.000+0000,[],5.0
,"[<JIRA Component: name='c client', id='12312380'>]",2017-03-09T07:12:55.000+0000,Tharindu Kumara,"Recently, carried out a test to to find the behavior of clients when a  zookeeper server is isolated from the zookeeper leader.

Here used a ensemble of 3 zookeeper servers called A, B and C. And quorum was set up like below. 

A - Follower 
B - Leader 
C - Follower​ 

A  <==> B <==> C 
 I____________I

And 3 clients are connected to ensemble like below. 

C1 is connected A. Both C1 and A are in the same machine.
C2 is connected B. Both C2 and B are in the same machine.
C3 is connected C. Both C3 and C are in the same machine.

To remove the network link between B and C iptables utility is used. 

command used: 
iptables -I INPUT -s Server_B_IP -j DROP 
iptables -I INPUT -s Server_C_IP -j DROP 

After removing the link connections looks like below. 

A  <===> B             C 
 I________I

Simply there is no way to send any packets from zookeeper server  B to zookeeper server C and vice versa. But the connection exists between between B and C. And also there is no way to send any packets from  B to C3 and vice versa. But the connection exists between between B and C3. 

Here What I noticed is that the client connected to Zookeeper Server ""C"", could not connect to the ensemble, resulting a session expiration timeout. 

For this experiment I used tickTime of 3000ms and client session expiration timeout of 45000ms. And tested with different combinations also. ",[],Bug,ZOOKEEPER-2715,Major,Tharindu Kumara,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Sessions Expire due to Network partitioning in Zookeeper ,2017-03-10T05:38:37.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2017-03-08T23:17:54.000+0000,Daniel C,"We've a standalone ZK setup. Upon restart, it failed to serve requests. 

Here are the logs:
------------------
2017-03-05 17:33:58,888 [myid:] - INFO  [main:QuorumPeerConfig@103] - Reading configuration from: /zookeeper/zookeeper-3.4.6/conf/zoo.1.cfg
2017-03-05 17:33:58,898 [myid:] - WARN  [main:QuorumPeerConfig@293] - No server failure will be tolerated. You need at least 3 servers.
2017-03-05 17:33:58,898 [myid:] - INFO  [main:QuorumPeerConfig@340] - Defaulting to majority quorums
2017-03-05 17:33:58,909 [myid:1] - INFO  [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 10
2017-03-05 17:33:58,910 [myid:1] - INFO  [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 5
2017-03-05 17:33:58,911 [myid:1] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
2017-03-05 17:33:58,946 [myid:1] - INFO  [main:QuorumPeerMain@127] - Starting quorum peer
2017-03-05 17:33:58,966 [myid:1] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
2017-03-05 17:33:58,991 [myid:1] - INFO  [main:NIOServerCnxnFactory@94] - binding to port 0.0.0.0/0.0.0.0:2181
2017-03-05 17:33:59,016 [myid:1] - INFO  [main:QuorumPeer@959] - tickTime set to 2000
2017-03-05 17:33:59,016 [myid:1] - INFO  [main:QuorumPeer@979] - minSessionTimeout set to -1
2017-03-05 17:33:59,016 [myid:1] - INFO  [main:QuorumPeer@990] - maxSessionTimeout set to -1
2017-03-05 17:33:59,016 [myid:1] - INFO  [main:QuorumPeer@1005] - initLimit set to 20
2017-03-05 17:34:01,328 [myid:1] - INFO  [main:QuorumPeer@473] - currentEpoch not found! Creating with a reasonable default of 0. This should only happen when you are upgrading your installation
2017-03-05 17:34:01,332 [myid:1] - INFO  [main:QuorumPeer@488] - acceptedEpoch not found! Creating with a reasonable default of 0. This should only happen when you are upgrading your installation
2017-03-05 17:34:01,335 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.245.66.147:48198
2017-03-05 17:34:01,339 [myid:1] - INFO  [Thread-4:QuorumCnxManager$Listener@504] - My election bind port: server001-internal/10.245.66.137:3888
2017-03-05 17:34:01,346 [myid:1] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@362] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
2017-03-05 17:34:01,346 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1007] - Closed socket connection for client /10.245.66.147:48198 (no session established for client)
2017-03-05 17:34:01,346 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.245.66.147:48199
2017-03-05 17:34:01,347 [myid:1] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@362] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
2017-03-05 17:34:01,347 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1007] - Closed socket connection for client /10.245.66.147:48199 (no session established for client)
2017-03-05 17:34:01,347 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.245.66.147:48200
2017-03-05 17:34:01,347 [myid:1] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@362] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
2017-03-05 17:34:01,348 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1007] - Closed socket connection for client /10.245.66.147:48200 (no session established for client)
2017-03-05 17:34:01,348 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.245.66.147:48201
2017-03-05 17:34:01,348 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.245.66.137:46628
2017-03-05 17:34:01,348 [myid:1] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@362] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
------------------

Is it a race issue during startup? 
2017-03-05 17:34:01,346 [myid:1] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@362] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running

",[],Bug,ZOOKEEPER-2714,Blocker,Daniel C,Cannot Reproduce,2017-03-09T16:48:18.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper (standalone) failed to start up,2017-03-09T16:48:18.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",3.0
Rakesh Radhakrishnan,"[<JIRA Component: name='tests', id='12312427'>]",2017-03-08T15:04:48.000+0000,Rakesh Radhakrishnan,"MiniKdc test cases are intermittently failing due to not finding the principal. Below is the failure stacktrace.
{code}
2017-03-08 13:21:10,843 [myid:] - ERROR [NioProcessor-1:AuthenticationService@187] - Error while searching for client learner@EXAMPLE.COM : Client not found in Kerberos database
2017-03-08 13:21:10,843 [myid:] - WARN  [NioProcessor-2:KerberosProtocolHandler@241] - Server not found in Kerberos database (7)
2017-03-08 13:21:10,845 [myid:] - WARN  [NioProcessor-2:KerberosProtocolHandler@242] - Server not found in Kerberos database (7)
2017-03-08 13:21:10,844 [myid:] - WARN  [NioProcessor-1:KerberosProtocolHandler@241] - Client not found in Kerberos database (6)
2017-03-08 13:21:10,845 [myid:] - WARN  [NioProcessor-1:KerberosProtocolHandler@242] - Client not found in Kerberos database (6)
{code}

Will attach the detailed log to jira.","[<JIRA Version: name='3.4.10', id='12338036'>]",Bug,ZOOKEEPER-2712,Critical,Rakesh Radhakrishnan,Fixed,2017-03-22T01:52:05.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,MiniKdc test case intermittently failing due to principal not found in Kerberos database,2017-07-19T12:56:40.000+0000,[],4.0
Josh Elser,[],2017-03-07T18:19:04.000+0000,Josh Elser,"Observed the following issue in some $dayjob testing environments. Line numbers are a little off compared to master/branch-3.5, but I did confirm the same issue exists there.

With the NettyServerCnxnFactory, before a request is dispatched, the code synchronizes on the {{NettyServerCnxn}} object. However, with some 4LW commands (like {{stat}}), each {{ServerCnxn}} object is also synchronized to (safely) iterate over the internal contents of the object to generate the necessary debug message. As such, multiple concurrent {{stat}} commands can both lock their own {{NettyServerCnxn}} objects, and then be blocked waiting to lock each others' {{ServerCnxn}} in the {{StatCommand}}, deadlocked.
{noformat}
""New I/O worker #55"":
	at org.apache.zookeeper.server.ServerCnxn.dumpConnectionInfo(ServerCnxn.java:407)
	- waiting to lock <0x00000000fabc01b8> (a org.apache.zookeeper.server.NettyServerCnxn)
	at org.apache.zookeeper.server.NettyServerCnxn$StatCommand.commandRun(NettyServerCnxn.java:478)
	at org.apache.zookeeper.server.NettyServerCnxn$CommandThread.run(NettyServerCnxn.java:311)
	at org.apache.zookeeper.server.NettyServerCnxn$CommandThread.start(NettyServerCnxn.java:306)
	at org.apache.zookeeper.server.NettyServerCnxn.checkFourLetterWord(NettyServerCnxn.java:677)
	at org.apache.zookeeper.server.NettyServerCnxn.receiveMessage(NettyServerCnxn.java:790)
	at org.apache.zookeeper.server.NettyServerCnxnFactory$CnxnChannelHandler.processMessage(NettyServerCnxnFactory.java:211)
	at org.apache.zookeeper.server.NettyServerCnxnFactory$CnxnChannelHandler.messageReceived(NettyServerCnxnFactory.java:135)
	- locked <0x00000000fab68178> (a org.apache.zookeeper.server.NettyServerCnxn)
	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
""New I/O worker #51"":
	at org.apache.zookeeper.server.ServerCnxn.dumpConnectionInfo(ServerCnxn.java:407)
	- waiting to lock <0x00000000fab68178> (a org.apache.zookeeper.server.NettyServerCnxn)
	at org.apache.zookeeper.server.NettyServerCnxn$StatCommand.commandRun(NettyServerCnxn.java:478)
	at org.apache.zookeeper.server.NettyServerCnxn$CommandThread.run(NettyServerCnxn.java:311)
	at org.apache.zookeeper.server.NettyServerCnxn$CommandThread.start(NettyServerCnxn.java:306)
	at org.apache.zookeeper.server.NettyServerCnxn.checkFourLetterWord(NettyServerCnxn.java:677)
	at org.apache.zookeeper.server.NettyServerCnxn.receiveMessage(NettyServerCnxn.java:790)
	at org.apache.zookeeper.server.NettyServerCnxnFactory$CnxnChannelHandler.processMessage(NettyServerCnxnFactory.java:211)
	at org.apache.zookeeper.server.NettyServerCnxnFactory$CnxnChannelHandler.messageReceived(NettyServerCnxnFactory.java:135)
	- locked <0x00000000fabc01b8> (a org.apache.zookeeper.server.NettyServerCnxn)
	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{noformat}
It would appear that the synchronization on the {{NettyServerCnxn}} in {{NettyServerCnxnFactory}} is to blame (and I can see why it was done originally). I think we can just use a different Object (and monitor) to provide mutual exclusion at Netty layer (and avoid synchronization issues at the ""application"" layer).

 ",[],Bug,ZOOKEEPER-2711,Critical,Josh Elser,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Deadlock between concurrent 4LW commands that iterate over connections with Netty server,2020-03-29T18:35:54.000+0000,[],5.0
Rakesh Radhakrishnan,"[<JIRA Component: name='documentation', id='12312422'>]",2017-03-07T17:06:26.000+0000,Rakesh Radhakrishnan,This jira can be used to regenerate the documentation as some of the recent commits didn't regenerated the doc section.,"[<JIRA Version: name='3.4.10', id='12338036'>]",Bug,ZOOKEEPER-2710,Major,Rakesh Radhakrishnan,Fixed,2017-03-07T17:34:39.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Regenerate documentation for branch-3.4 release,2017-03-31T09:01:16.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",2.0
,[],2017-03-03T19:59:30.000+0000,Angelo Esquivel,"We are configuring Zookeeper with log4j to create a tracelog file separated from the zookeeper.log.

We have test using the following java properties:

call %JAVA% ""-DrequestTraceFile"" ""-Dzookeeper.log.dir=%ZOO_LOG_DIR%"" ""-Dzookeeper.root.logger=%ZOO_LOG4J_PROP%"" -cp ""%CLASSPATH%"" %ZOOMAIN% ""%ZOOCFG%"" %*


Is there a way to set this in a separate file?
If not, can this be included in the zookeeper.log content?

Please let us know if there is a way.
",[],Bug,ZOOKEEPER-2708,Minor,Angelo Esquivel,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,TracelogFile not being created.,2017-03-03T19:59:30.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",2.0
Abraham Fine,[],2017-02-23T19:54:26.000+0000,Abraham Fine,"While working on ZOOKEEPER-2696, [~rakeshr] and I noticed that checkstyle is failing to execute on branch-3.4 with the following error:

{code}
BUILD FAILED
/Users/abefine/cloudera_code/zookeeper/build.xml:1595: Unable to create a Checker: cannot initialize module PackageHtml - Unable to instantiate PackageHtml
{code}

This should essentially be a backport of ZOOKEEPER-412","[<JIRA Version: name='3.4.10', id='12338036'>]",Bug,ZOOKEEPER-2706,Major,Abraham Fine,Fixed,2017-03-23T02:14:56.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,checkstyle broken on branch-3.4,2017-03-31T09:01:16.000+0000,[],4.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2017-02-23T12:24:39.000+0000,Steve Fitzgerald,"Zookeeper version: 3.5.1-alpha
Curator Framework version: 3.2.0

We have a 5 node cluster. When we register a service instance everything is created within zookeeper successfully, e.g. for a service names ""fake-test-service"" I can see the following created:

1. /api/enablement/fake-test-service
2. /api/enablement/fake-test-service/bb831396-5c55-4456-a7c0-5950ba294fd5

When I abnormally kill (kill -9) the process that the service is registered from I expect both of the above to get removed by zookeeper when it expires the session. But only /api/enablement/fake-test-service/bb831396-5c55-4456-a7c0-5950ba294fd5 gets removed successfully.

Here is a snippet of the log file:

{noformat}
2017-02-23 05:50:00,977 [myid:5] - TRACE [SessionTracker:SessionTrackerImpl@208][] - Session closing: 0x502dbce4df60000
2017-02-23 05:50:00,977 [myid:5] - INFO  [SessionTracker:ZooKeeperServer@384][] - Expiring session 0x502dbce4df60000, timeout of 40000ms exceeded
2017-02-23 05:50:00,977 [myid:5] - INFO  [SessionTracker:QuorumZooKeeperServer@132][] - Submitting global closeSession request for session 0x502dbce4df60000
2017-02-23 05:50:00,977 [myid:5] - TRACE [ProcessThread(sid:5 cport:-1)::ZooTrace@90][] - :Psessionid:0x502dbce4df60000 type:closeSession cxid:0x0 zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2017-02-23 05:50:00,978 [myid:5] - TRACE [ProcessThread(sid:5 cport:-1)::SessionTrackerImpl@208][] - Session closing: 0x502dbce4df60000
2017-02-23 05:50:00,978 [myid:5] - INFO  [ProcessThread(sid:5 cport:-1)::PrepRequestProcessor@649][] - Processed session termination for sessionid: 0x502dbce4df60000
2017-02-23 05:50:00,978 [myid:5] - DEBUG [ProcessThread(sid:5 cport:-1)::CommitProcessor@340][] - Processing request:: sessionid:0x502dbce4df60000 type:closeSession cxid:0x0 zxid:0x1d00000003 txntype:-11 reqpath:n/a
2017-02-23 05:50:00,978 [myid:5] - DEBUG [ProcessThread(sid:5 cport:-1)::Leader@1066][] - Proposing:: sessionid:0x502dbce4df60000 type:closeSession cxid:0x0 zxid:0x1d00000003 txntype:-11 reqpath:n/a
2017-02-23 05:50:00,981 [myid:5] - TRACE [SyncThread:5:Leader@787][] - Ack zxid: 0x1d00000003
2017-02-23 05:50:00,981 [myid:5] - TRACE [SyncThread:5:Leader@790][] - outstanding proposal: 0x1d00000003
2017-02-23 05:50:00,981 [myid:5] - TRACE [SyncThread:5:Leader@793][] - outstanding proposals all
2017-02-23 05:50:00,982 [myid:5] - TRACE [LearnerHandler-/10.24.128.164:38716:Leader@787][] - Ack zxid: 0x1d00000003
2017-02-23 05:50:00,982 [myid:5] - TRACE [LearnerHandler-/10.24.128.164:38716:Leader@790][] - outstanding proposal: 0x1d00000003
2017-02-23 05:50:00,982 [myid:5] - TRACE [LearnerHandler-/10.24.128.164:38716:Leader@793][] - outstanding proposals all
2017-02-23 05:50:00,982 [myid:5] - TRACE [LearnerHandler-/10.24.128.161:55588:Leader@787][] - Ack zxid: 0x1d00000003
2017-02-23 05:50:00,982 [myid:5] - TRACE [LearnerHandler-/10.24.128.161:55588:Leader@790][] - outstanding proposal: 0x1d00000003
2017-02-23 05:50:00,982 [myid:5] - TRACE [LearnerHandler-/10.24.128.161:55588:Leader@793][] - outstanding proposals all
2017-02-23 05:50:00,982 [myid:5] - DEBUG [LearnerHandler-/10.24.128.161:55588:CommitProcessor@327][] - Committing request:: sessionid:0x502dbce4df60000 type:closeSession cxid:0x0 zxid:0x1d00000003 txntype:-11 reqpath:n/a
2017-02-23 05:50:00,982 [myid:5] - TRACE [LearnerHandler-/10.24.128.162:47580:Leader@787][] - Ack zxid: 0x1d00000003
2017-02-23 05:50:00,982 [myid:5] - TRACE [LearnerHandler-/10.24.128.162:47580:Leader@793][] - outstanding proposals all
2017-02-23 05:50:00,983 [myid:5] - DEBUG [LearnerHandler-/10.24.128.162:47580:Leader@808][] - outstanding is 0
2017-02-23 05:50:00,983 [myid:5] - TRACE [LearnerHandler-/10.24.128.160:41119:Leader@787][] - Ack zxid: 0x1d00000003
2017-02-23 05:50:00,983 [myid:5] - TRACE [LearnerHandler-/10.24.128.160:41119:Leader@793][] - outstanding proposals all
2017-02-23 05:50:00,983 [myid:5] - DEBUG [LearnerHandler-/10.24.128.160:41119:Leader@808][] - outstanding is 0
2017-02-23 05:50:00,983 [myid:5] - DEBUG [CommitProcWorkThread-1:FinalRequestProcessor@91][] - Processing request:: sessionid:0x502dbce4df60000 type:closeSession cxid:0x0 zxid:0x1d00000003 txntype:-11 reqpath:n/a
2017-02-23 05:50:00,983 [myid:5] - TRACE [CommitProcWorkThread-1:ZooTrace@90][] - :Esessionid:0x502dbce4df60000 type:closeSession cxid:0x0 zxid:0x1d00000003 txntype:-11 reqpath:n/a
2017-02-23 05:50:00,983 [myid:5] - DEBUG [CommitProcWorkThread-1:DataTree@1034][] - Deleting ephemeral node /api/enablement/fake-test-service/bb831396-5c55-4456-a7c0-5950ba294fd5 for session 0x502dbce4df60000
2017-02-23 05:50:00,983 [myid:5] - DEBUG [CommitProcWorkThread-1:SessionTrackerImpl@218][] - Removing session 0x502dbce4df60000
2017-02-23 05:50:00,983 [myid:5] - TRACE [CommitProcWorkThread-1:ZooTrace@71][] - SessionTrackerImpl --- Removing session 0x502dbce4df60000
2017-02-23 05:50:00,984 [myid:5] - DEBUG [CommitProcWorkThread-1:NettyServerCnxnFactory@411][] - closeSession sessionid:0x361092599260774400
2017-02-23 05:50:00,984 [myid:5] - DEBUG [CommitProcWorkThread-1:NettyServerCnxnFactory@411][] - closeSession sessionid:0x361092599260774400
2017-02-23 05:50:03,525 [myid:5] - TRACE [New I/O worker #5:NettyServerCnxnFactory$CnxnChannelHandler@156][] - message received called BigEndianHeapChannelBuffer(ridx=0, widx=12, cap=12)
2017-02-23 05:50:03,527 [myid:5] - DEBUG [New I/O worker #5:NettyServerCnxnFactory$CnxnChannelHandler@160][] - New message [id: 0xd28589b8, /10.24.128.113:41935 => /10.24.128.165:2281] RECEIVED: BigEndianHeapChannelBuffer(ridx=0, widx=12, cap=12) from [id: 0xd28589b8, /10.24.128.113:41935 => /10.24.128.165:2281]
2017-02-23 05:50:03,527 [myid:5] - DEBUG [New I/O worker #5:NettyServerCnxnFactory$CnxnChannelHandler@175][] - 502d2842d930004 queuedBuffer: null
2017-02-23 05:50:03,527 [myid:5] - TRACE [New I/O worker #5:NettyServerCnxnFactory$CnxnChannelHandler@202][] - 502d2842d930004 buf 0x00000008fffffffe0000000b
2017-02-23 05:50:03,527 [myid:5] - DEBUG [New I/O worker #5:NettyServerCnxnFactory$CnxnChannelHandler@221][] - not throttled
2017-02-23 05:50:03,527 [myid:5] - TRACE [New I/O worker #5:NettyServerCnxn@355][] - message readable 12 bblenrem 4
2017-02-23 05:50:03,528 [myid:5] - TRACE [New I/O worker #5:NettyServerCnxn@360][] - 502d2842d930004 bbLen 0x
2017-02-23 05:50:03,528 [myid:5] - TRACE [New I/O worker #5:NettyServerCnxn@375][] - 502d2842d930004 bbLen 0x00000008
2017-02-23 05:50:03,528 [myid:5] - TRACE [New I/O worker #5:NettyServerCnxn@382][] - 502d2842d930004 bbLen len is 8
2017-02-23 05:50:03,528 [myid:5] - TRACE [New I/O worker #5:NettyServerCnxn@302][] - message readable 8 bb len 8 java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]
2017-02-23 05:50:03,529 [myid:5] - TRACE [New I/O worker #5:NettyServerCnxn@306][] - 502d2842d930004 bb 0x
2017-02-23 05:50:03,529 [myid:5] - TRACE [New I/O worker #5:NettyServerCnxn@320][] - after readBytes message readable 0 bb len 0 java.nio.HeapByteBuffer[pos=8 lim=8 cap=8]
2017-02-23 05:50:03,529 [myid:5] - TRACE [New I/O worker #5:NettyServerCnxn@325][] - after readbytes 502d2842d930004 bb 0xfffffffe0000000b
2017-02-23 05:50:03,530 [myid:5] - DEBUG [ProcessThread(sid:5 cport:-1)::SessionTrackerImpl@291][] - Checking session 0x502d2842d930004
2017-02-23 05:50:03,530 [myid:5] - DEBUG [ProcessThread(sid:5 cport:-1)::CommitProcessor@340][] - Processing request:: sessionid:0x502d2842d930004 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2017-02-23 05:50:03,530 [myid:5] - DEBUG [CommitProcWorkThread-1:FinalRequestProcessor@91][] - Processing request:: sessionid:0x502d2842d930004 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2017-02-23 05:50:03,530 [myid:5] - DEBUG [CommitProcWorkThread-1:FinalRequestProcessor@178][] - sessionid:0x502d2842d930004 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
2017-02-23 05:50:03,531 [myid:5] - TRACE [New I/O worker #5:NettyServerCnxnFactory$CnxnChannelHandler@267][] - write complete [id: 0xd28589b8, /10.24.128.113:41935 => /10.24.128.165:2281] WRITTEN_AMOUNT: 85
2017-02-23 05:50:04,275 [myid:5] - ERROR [ContainerManagerTask:ContainerManager$1@84][] - Error checking containers
java.lang.NullPointerException
	at org.apache.zookeeper.server.ContainerManager.getCandidates(ContainerManager.java:151)
	at org.apache.zookeeper.server.ContainerManager.checkContainers(ContainerManager.java:111)
	at org.apache.zookeeper.server.ContainerManager$1.run(ContainerManager.java:78)
	at java.util.TimerThread.mainLoop(Timer.java:555)
	at java.util.TimerThread.run(Timer.java:505)
2017-02-23 05:50:11,569 [myid:5] - TRACE [New I/O worker #2:NettyServerCnxnFactory$CnxnChannelHandler@156][] - message received called BigEndianHeapChannelBuffer(ridx=0, widx=12, cap=12)
2017-02-23 05:50:11,569 [myid:5] - DEBUG [New I/O worker #2:NettyServerCnxnFactory$CnxnChannelHandler@160][] - New message [id: 0x677c2a25, /10.157.130.185:60591 => /10.24.128.165:2181] RECEIVED: BigEndianHeapChannelBuffer(ridx=0, widx=12, cap=12) from [id: 0x677c2a25, /10.157.130.185:60591 => /10.24.128.165:2181]
2017-02-23 05:50:11,570 [myid:5] - DEBUG [New I/O worker #2:NettyServerCnxnFactory$CnxnChannelHandler@175][] - 10145a3f4f803e5 queuedBuffer: null
2017-02-23 05:50:11,570 [myid:5] - TRACE [New I/O worker #2:NettyServerCnxnFactory$CnxnChannelHandler@202][] - 10145a3f4f803e5 buf 0x00000008fffffffe0000000b
{noformat}

I believe the NullPointerException in the log above is what makes it fail to remove the remaining /api/enablement/fake-test-service directory.

Could someone shed some light on why this might be happening?",[],Bug,ZOOKEEPER-2705,Major,Steve Fitzgerald,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Container node remains indefinitely after session has long expired!,2017-02-24T16:43:55.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",3.0
,[],2017-02-22T12:42:00.000+0000,gopalakrishna,"Zookeeper version : 3.4.9

OS version is ubuntu 14.04(trusty)

Default configuration of zoo.cfg 
tickTime=2000
initLimit=10
syncLimit=5

I have setup the zookeeper ensemble with three servers zk1.com, zk2.com, zk3.com.

Initial State:

ZK1(FOLLOWER)---ZK2(LEADER)-------ZK3(FOLLOWER)


This morning, ZK2(LEADER) went down and it became a FOLLOWER with in fraction of seconds. It took 20 minutes for new LEADER to be decided for the ensemble. ZK3 was the new LEADER.

New State:
ZK(FOLLOWER)----ZK2(FOLLOWER)-----ZK3(LEADER) (after 20 minutes).


Can somone help me to debug what happened? 

Zookeeper is managing the solr cloud 2shards, 4 nodes. ",[],Bug,ZOOKEEPER-2702,Major,gopalakrishna,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zookeeper ensemble took 20 minutes to come back up after leader failed,2017-02-22T12:52:12.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",4.0
,[],2017-02-20T06:22:56.000+0000,Jiafu Jiang,"Environment:
 I deploy ZooKeeper in a cluster of three nodes. Each node has three network interfaces(eth0, eth1, eth2).

Hostname is used instead of IP address in zoo.cfg, and quorumListenOnAllIPs=true

Probleam:
 I start three ZooKeeper servers( node A, node B, and node C) one by one, 
 when the leader election finishes, node B is the leader. 
 Then I shutdown one network interface of node A by command ""ifdown eth0"". The ZooKeeper server on node A will lost connection to node B and node C. In my test, I will take about 20 minites that the ZooKeepr server of node A realizes the event and try to call the QuorumServer.recreateSocketAddress the resolve the hostname.

I try to read the source code, and I find the code in
{code:java|title=QuorumCnxManager.java:|borderStyle=solid}
    class RecvWorker extends ZooKeeperThread {
        Long sid;
        Socket sock;
        volatile boolean running = true;
        final DataInputStream din;
        final SendWorker sw;

        RecvWorker(Socket sock, DataInputStream din, Long sid, SendWorker sw) {
            super(""RecvWorker:"" + sid);
            this.sid = sid;
            this.sock = sock;
            this.sw = sw;
            this.din = din;
            try {
                // OK to wait until socket disconnects while reading.
                sock.setSoTimeout(0);
            } catch (IOException e) {
                LOG.error(""Error while accessing socket for "" + sid, e);
                closeSocket(sock);
                running = false;
            }
        }
       ...
     }
{code}
I notice that the soTime is set to 0 in RecvWorker constructor. I think this is reasonable when the IP address of a ZooKeeper server never change, but considering that the IP address of each ZooKeeper server may change, maybe we should better set a timeout here.

I think this is a problem.",[],Bug,ZOOKEEPER-2701,Major,Jiafu Jiang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Timeout for RecvWorker is too long,2018-07-16T07:56:40.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.4.11', id='12339207'>]",6.0
Mohammad Arshad,"[<JIRA Component: name='security', id='12329414'>, <JIRA Component: name='server', id='12312382'>]",2017-02-17T07:54:17.000+0000,Mohammad Arshad,"Currently 4lw commands are executed without authentication and can be accessed from any IP which has access to ZooKeeper server. ZOOKEEPER-2693 attempts to limit the 4lw commands which are enabled by default or enabled by configuration.

In addition to ZOOKEEPER-2693 we should also restrict 4lw commands based on client IP as well. It is required for following scenario
# User wants to enable all the 4lw commands
# User wants to limit the access of the commands which are considered to be safe by default.
 
*Implementation:*
we can introduce new property 4lw.commands.host.whitelist
# By default we allow all the hosts, but off course only on the 4lw exposed commands as per the ZOOKEEPER-2693
# It can be configured to allow individual IPs(192.168.1.2,192.168.1.3 etc.)
# It can also be configured to allow group of IPs like 192.168.1.*",[],Bug,ZOOKEEPER-2699,Major,Mohammad Arshad,Won't Fix,2017-02-22T15:11:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Restrict 4lw commands based on client IP,2017-04-28T10:49:22.000+0000,[],3.0
Abraham Fine,"[<JIRA Component: name='build', id='12312383'>]",2017-02-15T00:01:26.000+0000,Abraham Fine,Following the changes made in ZOOKEEPER-2689 IDE's using the .classpath file generated by the eclipse ant task (I tested both idea and eclipse) cannot compile the tests.,"[<JIRA Version: name='3.4.10', id='12338036'>]",Bug,ZOOKEEPER-2696,Major,Abraham Fine,Fixed,2017-02-27T06:13:32.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Eclipse ant task no longer determines correct classpath for tests after ZOOKEEPER-2689,2017-03-31T09:01:08.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",4.0
Mohammad Arshad,"[<JIRA Component: name='java client', id='12312381'>]",2017-02-14T13:38:49.000+0000,Mohammad Arshad,"In Zookeeper rolling upgrade scenario where server is new but client is old, when sever sends error code which is not understood by the client, client throws NullPointerException. 
KeeperException.SystemErrorException should be thrown for all unknown error code.

",[],Bug,ZOOKEEPER-2695,Major,Mohammad Arshad,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Handle unknown error for rolling upgrade old client new server scenario,2021-06-24T09:00:41.000+0000,[],3.0
Ling Mao,"[<JIRA Component: name='java client', id='12312381'>]",2017-02-14T13:12:20.000+0000,Mohammad Arshad,"sync CLI command does not wait for result from server. It returns immediately after invoking the sync's asynchronous API.
Executing bellow command does not give the expected result
 {{<zkServer>/bin/zkCli.sh -server host:port sync /}}","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.6', id='12345243'>]",Bug,ZOOKEEPER-2694,Major,Mohammad Arshad,Fixed,2019-05-23T16:43:28.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,sync CLI command does not wait for result from server,2019-10-16T18:59:00.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",3.0
Michael Han,"[<JIRA Component: name='security', id='12329414'>, <JIRA Component: name='server', id='12312382'>]",2017-02-14T03:33:13.000+0000,Patrick D. Hunt,"The wchp/wchc four letter words can be exploited in a DOS attack on the ZK client port - typically 2181. The following POC attack was recently published on the web:

https://vulners.com/exploitdb/EDB-ID:41277

The most straightforward way to block this attack is to not allow access to the client port to non-trusted clients - i.e. firewall the ZooKeeper service and only allow access to trusted applications using it for coordination.","[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2693,Blocker,Patrick D. Hunt,Fixed,2017-03-07T06:26:52.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,DOS attack on wchp/wchc four letter words (4lw),2021-03-06T21:36:13.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>]",8.0
Jiafu Jiang,[],2017-02-10T11:08:41.000+0000,Jiafu Jiang,"The QuorumPeer$QuorumServer.recreateSocketAddress()  is used to resolved the hostname to a new IP address(InetAddress) when any exception happens to the socket. It will be very useful when a hostname can be resolved to more than one IP address.
But the problem is Java API InetAddress.getByName(String hostname) will always return the first IP address when the hostname can be resolved to more than one IP address, and the first IP address may be unreachable forever. For example, if a machine has two network interfaces: eth0, eth1, say eth0 has ip1, eth1 has ip2, the relationship between hostname and the IP addresses is set in /etc/hosts. When I ""close"" the eth0 by command ""ifdown eth0"", the InetAddress.getByName(String hostname)  will still return ip1, which is unreachable forever.

So I think it will be better to check the IP address by InetAddress.isReachable(long) and choose the reachable IP address. 


I have modified the ZooKeeper source code, and test the new code in my own environment, and it can work very well when I turn down some network interfaces using ""ifdown"" command.

The original code is:
{code:title=QuorumPeer.java|borderStyle=solid}
        public void recreateSocketAddresses() {
            InetAddress address = null;
            try {
                address = InetAddress.getByName(this.hostname);
                LOG.info(""Resolved hostname: {} to address: {}"", this.hostname, address);
                this.addr = new InetSocketAddress(address, this.port);
                if (this.electionPort > 0){
                    this.electionAddr = new InetSocketAddress(address, this.electionPort);
                }
            } catch (UnknownHostException ex) {
                LOG.warn(""Failed to resolve address: {}"", this.hostname, ex);
                // Have we succeeded in the past?
                if (this.addr != null) {
                    // Yes, previously the lookup succeeded. Leave things as they are
                    return;
                }
                // The hostname has never resolved. Create our InetSocketAddress(es) as unresolved
                this.addr = InetSocketAddress.createUnresolved(this.hostname, this.port);
                if (this.electionPort > 0){
                    this.electionAddr = InetSocketAddress.createUnresolved(this.hostname,
                                                                           this.electionPort);
                }
            }
        }
{code}

After my modification:
{code:title=QuorumPeer.java|borderStyle=solid}
        public void recreateSocketAddresses() {
            InetAddress address = null;
            try {
                address = getReachableAddress(this.hostname);
                LOG.info(""Resolved hostname: {} to address: {}"", this.hostname, address);
                this.addr = new InetSocketAddress(address, this.port);
                if (this.electionPort > 0){
                    this.electionAddr = new InetSocketAddress(address, this.electionPort);
                }
            } catch (UnknownHostException ex) {
                LOG.warn(""Failed to resolve address: {}"", this.hostname, ex);
                // Have we succeeded in the past?
                if (this.addr != null) {
                    // Yes, previously the lookup succeeded. Leave things as they are
                    return;
                }
                // The hostname has never resolved. Create our InetSocketAddress(es) as unresolved
                this.addr = InetSocketAddress.createUnresolved(this.hostname, this.port);
                if (this.electionPort > 0){
                    this.electionAddr = InetSocketAddress.createUnresolved(this.hostname,
                                                                           this.electionPort);
                }
            }
        }

        public InetAddress getReachableAddress(String hostname) throws UnknownHostException {
            InetAddress[] addresses = InetAddress.getAllByName(hostname);
            for (InetAddress a : addresses) {
                try {
                    if (a.isReachable(5000)) {
                        return a;
                    } 
                } catch (IOException e) {
                    LOG.warn(""IP address {} is unreachable"", a);
                }
            }
            // All the IP address is unreachable, just return the first one.
            return addresses[0];
        }
{code}","[<JIRA Version: name='3.4.11', id='12339207'>]",Bug,ZOOKEEPER-2691,Minor,Jiafu Jiang,Fixed,2017-05-26T22:29:34.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,recreateSocketAddresses may recreate the unreachable IP address,2017-06-20T16:11:36.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.4.11', id='12339207'>]",5.0
Mark Fenes,"[<JIRA Component: name='documentation', id='12312422'>]",2017-02-09T19:34:02.000+0000,Michael Han,"In ZOOKEEPER-2574, the documentation change (https://github.com/apache/zookeeper/pull/111/) was done directly on the generated document files instead of on the document source. This JIRA is created to track the work of porting the doc change on the doc source so the change of the doc will not get lost between releases.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",Bug,ZOOKEEPER-2690,Minor,Michael Han,Fixed,2017-11-15T20:29:15.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Update documentation source for ZOOKEEPER-2574,2017-11-17T10:59:37.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>]",5.0
Rakesh Radhakrishnan,"[<JIRA Component: name='tests', id='12312427'>]",2017-02-09T15:47:33.000+0000,Mohammad Arshad,"Following test classes failed when branch-3.4 is run on java 6.
{noformat}
org.apache.zookeeper.server.quorum.auth.MiniKdcTest
org.apache.zookeeper.server.quorum.auth.QuorumKerberosAuthTest
org.apache.zookeeper.server.quorum.auth.QuorumKerberosHostBasedAuthTest
{noformat}

Error message is {{org/apache/kerby/kerberos/kerb/KrbException : Unsupported major.minor version 51.0}}","[<JIRA Version: name='3.4.10', id='12338036'>]",Bug,ZOOKEEPER-2689,Critical,Mohammad Arshad,Fixed,2017-02-13T18:13:38.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Fix Kerberos Authentication related test cases,2017-07-19T12:55:16.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",4.0
,[],2017-02-08T14:23:59.000+0000,Gregory Reshetniak,"Issuing rmr /vault leads to Node does not exist: /vault/core/_lock/_c_e393e8a4d2c984178373be528a25404a-lock-0000000028

I know that rmr is getting deprecated in next version, but I think this might be cluster consistency bug.

Please advice.",[],Bug,ZOOKEEPER-2688,Major,Gregory Reshetniak,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"rmr leads to ""Node does not exist""",2017-02-09T14:03:21.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",3.0
Mohammad Arshad,"[<JIRA Component: name='server', id='12312382'>]",2017-02-08T07:19:58.000+0000,Mohammad Arshad,"Leader server enters into deadlock while shutting down. This happens some time only.
The reason and deadlock flow is same as ZOOKEEPER-2380.
shutdown was removed from synchronized block in ZOOKEEPER-2380
Now shutdown is called from synchronized block from another place.
{code}
// check leader running status
if (!this.isRunning()) {
    shutdown(""Unexpected internal error"");
    return;
}
{code}","[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2687,Major,Mohammad Arshad,Fixed,2017-02-15T16:24:48.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Deadlock while shutting down the Leader server.,2017-05-18T03:43:58.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",7.0
,"[<JIRA Component: name='java client', id='12312381'>]",2017-02-07T09:08:01.000+0000,shamim khan,want to implement SSL in zookeeeper. But not able to implement as version issue. So how can we implement in zookeeper-3.4.5-3. ,[],Bug,ZOOKEEPER-2685,Major,shamim khan,Won't Fix,2017-02-08T04:14:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,How to implement SSL in zookeeper-3.4.5-3,2017-02-08T04:14:57.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",2.0
Kfir Lev-Ari,"[<JIRA Component: name='server', id='12312382'>]",2017-02-07T05:23:02.000+0000,Ryan Zhang,"We deployed our build with ZOOKEEPER-2024 and it quickly started to crash with the following error

atla-buh-05-sr1.prod.twttr.net: 2017-01-18 22:24:42,305 - ERROR [CommitProcessor:2] -org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:268) – Got cxid 0x119fa expected 0x11fc5 for client session id 1009079ba470055
atla-buh-05-sr1.prod.twttr.net: 2017-01-18 22:32:04,746 - ERROR [CommitProcessor:2] -org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:268) – Got cxid 0x698 expected 0x928 for client session id 4002eeb3fd0009d
atla-buh-05-sr1.prod.twttr.net: 2017-01-18 22:34:46,648 - ERROR [CommitProcessor:2] -org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:268) – Got cxid 0x8904 expected 0x8f34 for client session id 51b8905c90251
atla-buh-05-sr1.prod.twttr.net: 2017-01-18 22:43:46,834 - ERROR [CommitProcessor:2] -org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:268) – Got cxid 0x3a8d expected 0x3ebc for client session id 2051af11af900cc

clearly something is not right in the new commit processor per session queue implementation.","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2684,Blocker,Ryan Zhang,Fixed,2017-11-03T04:20:04.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Fix a crashing bug in the mixed workloads commit processor,2019-01-30T18:46:10.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",9.0
Mohammad Arshad,"[<JIRA Component: name='tests', id='12312427'>]",2017-02-06T18:01:41.000+0000,Mohammad Arshad,"*Error Message*
{noformat}
Leader failed to transition to LOOKING or FOLLOWING state
{noformat}
*Stacktrace*
{noformat}
junit.framework.AssertionFailedError: Leader failed to transition to LOOKING or FOLLOWING state
	at org.apache.zookeeper.server.quorum.RaceConditionTest.testRaceConditionBetweenLeaderAndAckRequestProcessor(RaceConditionTest.java:74)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:79)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.lang.Thread.run(Thread.java:745)
{noformat}
[CI Failures Reference|https://builds.apache.org/job/PreCommit-ZOOKEEPER-github-pr-build/279//testReport/org.apache.zookeeper.server.quorum/RaceConditionTest/testRaceConditionBetweenLeaderAndAckRequestProcessor/]","[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2683,Major,Mohammad Arshad,Fixed,2017-02-13T21:10:28.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,RaceConditionTest is flaky,2017-05-18T03:43:54.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",5.0
,"[<JIRA Component: name='java client', id='12312381'>]",2017-01-27T22:02:43.000+0000,Egor Ryashin,"Use CuratorFrameworkFactory.Builder and specify ExhibitorEnsembleProvider.
Call build() and start().
Internal ConnectionState.start() calls ensembleProvider.start() which should poll for hostnames to produce connectionString.
Without waiting (for connectionString) ConnectionState calls zooKeeper.closeAndReset() and ClientCnxn is created with empty connectionString. That leads to lame zooKeeper sending requests to localhost.
{noformat}
2017-01-27T22:56:17,618 INFO  [Agents-0] org.apache.curator.framework.imps.CuratorFrameworkImpl - Starting
2017-01-27T22:56:17,619 INFO  [Agents-0] org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString= sessionTimeout=60001 watcher=org.apache.curator.ConnectionState@4402fad2
2017-01-27T22:56:17,625 INFO  [Agents-0-SendThread(127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn - Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2017-01-27T22:56:18,632 WARN  [Agents-0-SendThread(127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused: no further information
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_74]
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:1.8.0_74]
        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350) ~[zookeeper-3.4.5.jar:3.4.5-1392090]
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068) [zookeeper-3.4.5.jar:3.4.5-1392090]
2017-01-27T22:56:19,733 INFO  [Agents-0-SendThread(127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn - Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2017-01-27T22:56:19,807 INFO  [Curator-ExhibitorEnsembleProvider-0] org.apache.curator.ensemble.exhibitor.ExhibitorEnsembleProvider - Connection string has changed. Old value (), new value (172.19.2.158:2181,172.19.2.15:2181,172.19.2.177:2181,172.19.2.4:2181,172.19.2.89:2181,172.19.2.72:2181)
2017-01-27T22:56:20,734 WARN  [Agents-0-SendThread(127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused: no further information
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_74]
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:1.8.0_74]
        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350) ~[zookeeper-3.4.5.jar:3.4.5-1392090]
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068) [zookeeper-3.4.5.jar:3.4.5-1392090]
2017-01-27T22:56:21,835 INFO  [Agents-0-SendThread(127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn - Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
{noformat}",[],Bug,ZOOKEEPER-2681,Major,Egor Ryashin,Invalid,2017-01-28T13:15:29.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ConnectionState does not sync startup of ExhibitorEnsembleProvider and Zookeeper connection,2017-01-28T13:15:29.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",1.0
Mohammad Arshad,"[<JIRA Component: name='server', id='12312382'>]",2017-01-27T06:03:14.000+0000,Mohammad Arshad,"DataNode.getChildren() API returns null and empty set if there are no children in it depending on when the API is called. DataNode.getChildren() API behavior should be changed and it should always return empty set if the node does not have any child

*DataNode.getChildren() API Current Behavior:*
# returns null initially
When DataNode is created and no children are added yet, DataNode.getChildren() returns null
# returns empty set after all the children are deleted:
created a Node
add a child
delete the child
DataNode.getChildren() returns empty set.

After fix DataNode.getChildren() should return empty set in all the above cases.","[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2680,Major,Mohammad Arshad,Fixed,2017-02-10T17:29:27.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Correct DataNode.getChildren() inconsistent behaviour.,2017-03-31T09:01:15.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.1', id='12326786'>]",7.0
Robert Joseph Evans,"[<JIRA Component: name='server', id='12312382'>]",2017-01-26T15:16:55.000+0000,Robert Joseph Evans,"I know this is long but please here me out.

I recently inherited a massive zookeeper ensemble.  The snapshot is 3.4 GB on disk.  Because of its massive size we have been running into a number of issues. There are lots of problems that we hope to fix with tuning GC etc, but the big one right now that is blocking us making a lot of progress on the rest of them is that when we lose a quorum because the leader left, for what ever reason, it can take well over 5 mins for a new quorum to be established.  So we cannot tune the leader without risking downtime.

We traced down where the time was being spent and found that each server was clearing the database so it would be read back in again before leader election even started.  Then as part of the sync phase each server will write out a snapshot to checkpoint the progress it made as part of the sync.

I will be putting up a patch shortly with some proposed changes in it.","[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2678,Major,Robert Joseph Evans,Fixed,2017-02-14T18:06:12.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Large databases take a long time to regain a quorum,2020-08-29T03:50:16.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>]",12.0
,[],2017-01-24T14:33:25.000+0000,berkani,,[],Bug,ZOOKEEPER-2674,Trivial,berkani,Invalid,2017-01-31T16:38:26.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Projet-Final,2017-01-31T16:38:26.000+0000,[],2.0
,[],2017-01-24T13:27:32.000+0000,berkani,,[],Bug,ZOOKEEPER-2673,Trivial,berkani,Invalid,2017-01-31T16:38:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,projet,2017-01-31T16:38:56.000+0000,[],1.0
Rakesh Radhakrishnan,"[<JIRA Component: name='server', id='12312382'>]",2017-01-23T03:59:45.000+0000,Mohammad Arshad,"branch-3.4 code compilation is failing. Following are the compilation erros:
{code}
compile-test:
    [mkdir] Created dir: D:\gitHome\zookeeperTrunk\build\test\classes
    [javac] Compiling 146 source files to D:\gitHome\zookeeperTrunk\build\test\classes
    [javac] warning: [options] bootstrap class path not set in conjunction with -source 1.6
    [javac] D:\gitHome\zookeeperTrunk\src\java\test\org\apache\zookeeper\server\PurgeTxnTest.java:464: error: cannot find symbol
    [javac]         ZooKeeper zk = ClientBase.createZKClient(HOSTPORT);
    [javac]                                  ^
    [javac]   symbol:   method createZKClient(String)
    [javac]   location: class ClientBase
    [javac] D:\gitHome\zookeeperTrunk\src\java\test\org\apache\zookeeper\server\PurgeTxnTest.java:503: error: cannot find symbol
    [javac]         zk = ClientBase.createZKClient(HOSTPORT);
    [javac]                        ^
    [javac]   symbol:   method createZKClient(String)
    [javac]   location: class ClientBase
    [javac] Note: Some input files use or override a deprecated API.
{code}","[<JIRA Version: name='3.4.10', id='12338036'>]",Bug,ZOOKEEPER-2671,Major,Mohammad Arshad,Fixed,2017-01-23T06:55:29.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Fix compilation error in branch-3.4,2017-03-31T09:01:11.000+0000,[],5.0
Enis Soztutar,"[<JIRA Component: name='server', id='12312382'>]",2017-01-19T19:32:46.000+0000,Yan Fitterer,"ZooKeeper server becomes slow over time when native GSSAPI is used. The connection to the server starts taking upto 10 seconds.
This is happening with ZooKeeper-3.4.6 and is fairly reproducible.

Debug logs:
{noformat}
2015-07-02 00:58:49,318 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:NIOServerCnxnFactory@197] - Accepted socket connection from /<client_ip>:47942
2015-07-02 00:58:49,318 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperSaslServer@78] - serviceHostname is '<zookeeper-server>'
2015-07-02 00:58:49,318 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperSaslServer@79] - servicePrincipalName is 'zookeeper'
2015-07-02 00:58:49,318 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperSaslServer@80] - SASL mechanism(mech) is 'GSSAPI'
2015-07-02 00:58:49,324 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperSaslServer@106] - Added private credential to subject: [GSSCredential: 
zookeeper@<zookeeper-server> 1.2.840.113554.1.2.2 Accept [class sun.security.jgss.wrapper.GSSCredElement]]
2015-07-02 00:58:59,441 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@810] - Session establishment request from client /<client_ip>:47942 client's lastZxid is 0x0
2015-07-02 00:58:59,441 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@868] - Client attempting to establish new session at /<client_ip>:47942
2015-07-02 00:58:59,448 [myid:] - DEBUG [SyncThread:0:FinalRequestProcessor@88] - Processing request:: sessionid:0x14e486028785c81 type:createSession cxid:0x0 zxid:0x110e79 txntype:-10 reqpath:n/a
2015-07-02 00:58:59,448 [myid:] - DEBUG [SyncThread:0:FinalRequestProcessor@160] - sessionid:0x14e486028785c81 type:createSession cxid:0x0 zxid:0x110e79 txntype:-10 reqpath:n/a
2015-07-02 00:58:59,448 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@617] - Established session 0x14e486028785c81 with negotiated timeout 10000 for client /<client_ip>:47942
2015-07-02 00:58:59,452 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@949] - Responding to client SASL token.
2015-07-02 00:58:59,452 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@953] - Size of client SASL token: 706
2015-07-02 00:58:59,460 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@984] - Size of server SASL response: 161
2015-07-02 00:58:59,462 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@949] - Responding to client SASL token.
2015-07-02 00:58:59,462 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@953] - Size of client SASL token: 0
2015-07-02 00:58:59,462 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@984] - Size of server SASL response: 32
2015-07-02 00:58:59,463 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@949] - Responding to client SASL token.
2015-07-02 00:58:59,463 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@953] - Size of client SASL token: 32
2015-07-02 00:58:59,464 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:SaslServerCallbackHandler@118] - Successfully authenticated client: authenticationID=<user_principal>;  authorizationID=<user_principal>.
2015-07-02 00:58:59,464 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@964] - adding SASL authorization for authorizationID: <user_principal>
2015-07-02 00:58:59,465 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@494] - Processed session termination for sessionid: 0x14e486028785c81
2015-07-02 00:58:59,467 [myid:] - DEBUG [SyncThread:0:FinalRequestProcessor@88] - Processing request:: sessionid:0x14e486028785c81 type:closeSession cxid:0x1 zxid:0x110e7a txntype:-11 reqpath:n/a
2015-07-02 00:58:59,467 [myid:] - DEBUG [SyncThread:0:FinalRequestProcessor@160] - sessionid:0x14e486028785c81 type:closeSession cxid:0x1 zxid:0x110e7a txntype:-11 reqpath:n/a
2015-07-02 00:58:59,467 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:NIOServerCnxn@1007] - Closed socket connection for client /<client_ip>:47942 which had sessionid 0x14e486028785c81
{noformat}

If you see, after adding the credentials to privateCredential set, it takes roughly 10 seconds to reach to session establishment request. From the code it looks like Subject.doAs() is taking a lot of time.

I connected it to jdb while it was waiting and got following stacktrace:
{noformat}
NIOServerCxn.Factory:0.0.0.0/0.0.0.0:58909:
  [1] java.util.HashMap$TreeNode.find (HashMap.java:1,865)
  [2] java.util.HashMap$TreeNode.find (HashMap.java:1,861)
  [3] java.util.HashMap$TreeNode.find (HashMap.java:1,861)
  [4] java.util.HashMap$TreeNode.find (HashMap.java:1,861)
  [5] java.util.HashMap$TreeNode.find (HashMap.java:1,861)
  [6] java.util.HashMap$TreeNode.find (HashMap.java:1,861)
  [7] java.util.HashMap$TreeNode.find (HashMap.java:1,861)
  [8] java.util.HashMap$TreeNode.putTreeVal (HashMap.java:1,981)
  [9] java.util.HashMap.putVal (HashMap.java:637)
  [10] java.util.HashMap.put (HashMap.java:611)
  [11] java.util.HashSet.add (HashSet.java:219)
  [12] javax.security.auth.Subject$ClassSet.populateSet (Subject.java:1,418)
  [13] javax.security.auth.Subject$ClassSet.<init> (Subject.java:1,372)
  [14] javax.security.auth.Subject.getPrivateCredentials (Subject.java:767)
  [15] sun.security.jgss.GSSUtil$1.run (GSSUtil.java:340)
  [16] sun.security.jgss.GSSUtil$1.run (GSSUtil.java:332)
  [17] java.security.AccessController.doPrivileged (native method)
  [18] sun.security.jgss.GSSUtil.searchSubject (GSSUtil.java:332)
  [19] sun.security.jgss.wrapper.NativeGSSFactory.getCredFromSubject (NativeGSSFactory.java:53)
  [20] sun.security.jgss.wrapper.NativeGSSFactory.getCredentialElement (NativeGSSFactory.java:116)
  [21] sun.security.jgss.GSSManagerImpl.getCredentialElement (GSSManagerImpl.java:193)
  [22] sun.security.jgss.GSSCredentialImpl.add (GSSCredentialImpl.java:427)
  [23] sun.security.jgss.GSSCredentialImpl.<init> (GSSCredentialImpl.java:62)
  [24] sun.security.jgss.GSSManagerImpl.createCredential (GSSManagerImpl.java:154)
  [25] com.sun.security.sasl.gsskerb.GssKrb5Server.<init> (GssKrb5Server.java:108)
  [26] com.sun.security.sasl.gsskerb.FactoryImpl.createSaslServer (FactoryImpl.java:85)
  [27] javax.security.sasl.Sasl.createSaslServer (Sasl.java:524)
  [28] org.apache.zookeeper.server.ZooKeeperSaslServer$1.run (ZooKeeperSaslServer.java:118)
  [29] org.apache.zookeeper.server.ZooKeeperSaslServer$1.run (ZooKeeperSaslServer.java:114)
  [30] java.security.AccessController.doPrivileged (native method)
  [31] javax.security.auth.Subject.doAs (Subject.java:422)
  [32] org.apache.zookeeper.server.ZooKeeperSaslServer.createSaslServer (ZooKeeperSaslServer.java:114)
  [33] org.apache.zookeeper.server.ZooKeeperSaslServer.<init> (ZooKeeperSaslServer.java:48)
  [34] org.apache.zookeeper.server.NIOServerCnxn.<init> (NIOServerCnxn.java:100)
  [35] org.apache.zookeeper.server.NIOServerCnxnFactory.createConnection (NIOServerCnxnFactory.java:161)
  [36] org.apache.zookeeper.server.NIOServerCnxnFactory.run (NIOServerCnxnFactory.java:202)
  [37] java.lang.Thread.run (Thread.java:745)
{noformat}

This doesn't happen when we use JGSS, I think because adding credential to privateCredential set for every connection is causing Subject.doAS() to take much longer time.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.2', id='12331981'>]",Bug,ZOOKEEPER-2670,Major,Yan Fitterer,Duplicate,2017-02-09T17:52:26.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,CLONE - Connections fo ZooKeeper server becomes slow over time with native GSSAPI,2017-02-09T17:52:26.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.0', id='12316644'>]",4.0
,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2017-01-19T11:30:37.000+0000,Zhenghua Chen,"We have a zookeeper cluster with 3 nodes named s1, s2, s3
By mistake, we shut down the ethernet interface of s2, and zk follower  shut down(zk process remains there)
Later, after ethernet up again, s2 failed to reconnect to leader s3 to be a follower

follower s2 keeps printing log like this:
{quote}
2017-01-19 16:40:58,956 WARN  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:7181] o.a.z.s.q.Learner - Got zxid 0x320001019f expected 0x1
2017-01-19 16:40:58,956 ERROR [SyncThread:1] o.a.z.s.ZooKeeperCriticalThread - Severe unrecoverable error, from thread : SyncThread:1
java.nio.channels.ClosedChannelException: null
	at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:99)
	at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:250)
	at org.apache.zookeeper.server.persistence.Util.padLogFile(Util.java:215)
	at org.apache.zookeeper.server.persistence.FileTxnLog.padFile(FileTxnLog.java:241)
	at org.apache.zookeeper.server.persistence.FileTxnLog.append(FileTxnLog.java:219)
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.append(FileTxnSnapLog.java:314)
	at org.apache.zookeeper.server.ZKDatabase.append(ZKDatabase.java:470)
	at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:140)
2017-01-19 16:40:58,956 INFO  [SyncThread:1] o.a.z.s.ZooKeeperServerListenerImpl - Thread SyncThread:1 exits, error code 1
2017-01-19 16:40:58,956 INFO  [SyncThread:1] o.a.z.s.SyncRequestProcessor - SyncRequestProcessor exited!
2017-01-19 16:40:58,957 INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:7181] o.a.z.s.q.Learner - shutdown called
java.lang.Exception: shutdown Follower
	at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:164)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:850)
{quote}

And, leader s3 keeps printing log like this:
{quote}
2017-01-19 16:30:50,452 INFO  [LearnerHandler-/192.168.40.51:35949] o.a.z.s.q.LearnerHandler - Follower sid: 1 : info : org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@95258f0
2017-01-19 16:30:50,452 INFO  [LearnerHandler-/192.168.40.51:35949] o.a.z.s.q.LearnerHandler - Synchronizing with Follower sid: 1 maxCommittedLog=0x320001019e minCommittedLog=0x320000ffaa peerLastZxid=0x2300000000
2017-01-19 16:30:50,453 WARN  [LearnerHandler-/192.168.40.51:35949] o.a.z.s.q.LearnerHandler - Unhandled proposal scenario
2017-01-19 16:30:50,453 INFO  [LearnerHandler-/192.168.40.51:35949] o.a.z.s.q.LearnerHandler - Sending SNAP
2017-01-19 16:30:50,453 INFO  [LearnerHandler-/192.168.40.51:35949] o.a.z.s.q.LearnerHandler - Sending snapshot last zxid of peer is 0x2300000000  zxid of leader is 0x320001019esent zxid of db as 0x320001019e
2017-01-19 16:30:50,461 INFO  [LearnerHandler-/192.168.40.51:35949] o.a.z.s.q.LearnerHandler - Received NEWLEADER-ACK message from 1
2017-01-19 16:30:51,738 ERROR [LearnerHandler-/192.168.40.51:35934] o.a.z.s.q.LearnerHandler - Unexpected exception causing shutdown while sock still open
java.net.SocketTimeoutException: Read timed out
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:152)
	at java.net.SocketInputStream.read(SocketInputStream.java:122)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
	at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:83)
	at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:99)
	at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:542)
{quote}

we execute netstat, found lots of close wait socket in s2,  and never closed.
{quote}
tcp6   10865      0 192.168.40.51:47181     192.168.40.57:7288      CLOSE_WAIT  2217/java           
tcp6    2576      0 192.168.40.51:57181     192.168.40.57:7288      CLOSE_WAIT  2217/java           
{quote}
seems that s2 has a connection leak.

after restart zk process of s2, it works fine.
",[],Bug,ZOOKEEPER-2669,Major,Zhenghua Chen,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,follower failed to  reconnect to leader after a network error,2021-11-24T07:54:18.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",11.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2017-01-18T22:55:48.000+0000,Devaraj Das,"I was trying to see if ZK can be configured to always do client authentication (globally and not per znode). I came across this https://cwiki.apache.org/confluence/display/ZOOKEEPER/Client-Server+mutual+authentication and it describes a config key requireClientAuthScheme that when set to 'sasl' should do the job. But upon looking at code (in master and in branch-3.5), I don't see any reference to it.
Raising this jira to update the wiki (assuming I am on the right track). There are probably ways to update the wiki otherwise but I wanted to get some attention on this before we did that. (cc [~phunt], [~ekoontz]).",[],Bug,ZOOKEEPER-2668,Major,Devaraj Das,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Remove reference to requireClientAuthScheme from https://cwiki.apache.org/confluence/display/ZOOKEEPER/Client-Server+mutual+authentication,2019-09-17T15:28:49.000+0000,[],6.0
,"[<JIRA Component: name='java client', id='12312381'>]",2017-01-17T11:35:17.000+0000,Hari Krishna Dara,"ZOOKEEPER-2139 added support for connecting to multiple ZK services, but this also introduced a bug that causes a cryptic NPE. The client sees the below sort of error messages:

{noformat}
Exception while trying to create SASL client: java.lang.NullPointerException
SASL authentication with Zookeeper Quorum member failed: javax.security.sasl.SaslException: saslClient failed to initialize properly: it's null.
Error while calling watcher
java.lang.NullPointerException
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:581)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:532)
        at org.apache.hadoop.hbase.zookeeper.PendingWatcher.process(PendingWatcher.java:40)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:579)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:554)
{noformat}

The line at {{ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:581)}} points to the middle line below, where {{event.getState()}} is {{null}}:

{noformat}
private void connectionEvent(WatchedEvent event) {
    switch(event.getState()) {
       case SyncConnected:
{noformat}

However, the event's state is {{null}} because of a couple of other bugs, particularly an NPE that gets a mention in the log without a stacktrace. This first NPE causes an incorrect initialization of the event and results in the second NPE with the stacktrace.

The reason for the first NPE comes from this code in {{ZookeeperSaslClient}}:

{noformat}
            if (!initializedLogin) {
                ...
            }
            Subject subject = login.getSubject();
{noformat}

Before the patch for ZOOKEEPER-2139, both the {{login}} and {{initializedLogin}} were {{static}} fields of {{ZookeeperSaslClient}}. To support multiple ZK clients, the {{login}} field was changed from {{static}} to instance field, however the {{initializedLogin}} field was left as {{static}} field. Because of this, the subsequent attempts to connect to ZK think that the login doesn't need to be done and go ahead and blindly use the {{login}} variable which causes the NPE.

At the core, the fix is simply to change {{initializedLogin}} to instance variable, but we have made a few additional changes to improve the logging and handle state. I will attach a patch soon. ",[],Bug,ZOOKEEPER-2667,Major,Hari Krishna Dara,Invalid,2017-01-18T12:23:00.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,NPE in the patch for ZOOKEEPER-2139 when multiple connections are made,2017-01-18T12:23:00.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",3.0
,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='server', id='12312382'>]",2017-01-16T05:51:25.000+0000,ZHE CHEN,"We have a service A, which has 2 instances A1 and A2. 
We also have another 2 services, B and C. B has 2 instances B1 and B2. C has 2 instances C1 and C2.

A1 and A2 both register child watch for B and C. 2 individual watches, of course.

I restart B1 and C1 nearly at the same time. Then, theoretically A1 and A2 
both should receive 2 events about the child change of service B and C.
However, the real result is, A1 received the 2 children changes of service B and C separately, A2 only received the children change of service B. Moreover, A2 got the children change of service B many many times when service B only changed once at that time (I add auto re-registration so A2 can receive the event more than once).

Till now, it only happened once. If it happens again, maybe I will provide some logs.",[],Bug,ZOOKEEPER-2666,Major,ZHE CHEN,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,the watch function called many times when it should be called once,2017-01-23T22:38:14.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",3.0
,"[<JIRA Component: name='jmx', id='12312451'>]",2017-01-11T13:41:07.000+0000,linbo.liao,"My laptop is Macbook Pro with macOS Sierra (IP: 192.168.2.102). An VM (IP: 192.168.2.107) is running on VirtualBox.

Deploy zookeeper-3.4.9 on VM, enable the remote JMX with option:

-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=8415 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.rmi.port=8415
-Djava.rmi.server.hostname=192.168.2.107

Test with jconsole on Mac,  connect 192.168.2.107:8415 works fine.

Runnign zkCli.sh failed
$ bin/zkCli.sh
Error: JMX connector server communication error: service:jmx:rmi://localhost.localdomain:8415

$ cat /etc/hosts
127.0.0.1       localhost.localdomain localhost
::1     localhost6.localdomain6 localhost6",[],Bug,ZOOKEEPER-2663,Major,linbo.liao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Enable remote jmx, zkCli.sh start failed with jmx communication error",2017-01-11T13:41:07.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",2.0
,"[<JIRA Component: name='java client', id='12312381'>]",2017-01-10T11:20:55.000+0000,Yaohui Wu,"I create and close ZooKeeper for 10 times.  It costs about 5055 ms for the first time.

See attached files for some test code and output.",[],Bug,ZOOKEEPER-2661,Major,Yaohui Wu,Not A Problem,2017-01-11T08:58:10.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,It costs about 5055 ms to create Zookeeper object for the first time.,2017-01-23T19:46:27.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",3.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2017-01-07T13:14:34.000+0000,Yongcheng Liu,"1. currentEpoch is bigger than acceptedEpoch, ZK will throw IOException when start loadDataBase.
2. function bug. In function setAcceptedEpoch and setCurrentEpoch, it is modify memory variable first, then write epoch to file. If write file failed, the memory has been modified.

solution as follow:
for example,

	public void setAcceptedEpoch(long e) throws IOException {
		acceptedEpoch = e;
		writeLongToFile(ACCEPTED_EPOCH_FILENAME, e);
	}

need to modify as follow:

	public void setAcceptedEpoch(long e) throws IOException {
		writeLongToFile(ACCEPTED_EPOCH_FILENAME, e);
		acceptedEpoch = e;
	}
",[],Bug,ZOOKEEPER-2660,Major,Yongcheng Liu,Duplicate,2017-01-20T18:03:44.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"acceptedEpoch and currentEpoch data inconsistency, ZK process can not start!",2017-01-20T18:03:44.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.4.9', id='12334700'>]",3.0
Olaf Flebbe,"[<JIRA Component: name='build', id='12312383'>]",2016-12-22T17:40:31.000+0000,Olaf Flebbe,"While compiling Bigtop on Fedora 25 we found that there is an issue with the autoconf detection of cppunit: See BIGTOP-2642 for error.

Some background regarding the issue can be found here: https://bugzilla.redhat.com/show_bug.cgi?id=1311694

The fedora maintainers encourage use of pkg-config rather crufty *.m4 autoconf magic by only supplying pkg-config files *.pc.

The patch is surprisingly easy but adds the additional requirement for pkg-config which should be available on every well maintained system for ages.

Please see for me proposed patch. Works for me for Fedora 25, Centos 6, MacOSX with HomeBrew.

",[],Bug,ZOOKEEPER-2654,Major,Olaf Flebbe,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Support Fedora 25: use pkg-config instead of obsolete M4 macros,2016-12-22T18:14:00.000+0000,[],1.0
,[],2016-12-22T17:03:20.000+0000,Lasaro Camargos,"Hi all.
After shutting zk down and upgrading to centos 7, ZK would not start with exception

Removing file: Dec 19, 2016 10:55:08 PM /hedvig/hpod/log/version-2/log.300ee0308
Removing file: Dec 19, 2016 7:11:23 PM  /hedvig/hpod/data/version-2/snapshot.300ee0307
java.lang.RuntimeException: Unable to run quorum server
        at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:558)
        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:500)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:153)
        at com.hedvig.hpod.service.PodnetService$1.run(PodnetService.java:2262)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: The current epoch, 3, is older than the last zxid, 17179871862
        at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:539)
        ... 4 more

All logs are empty, and the following snapshot and commit logs exist

find .
.
./log
./log/version-2
./log/version-2/log.40000010a
./log/version-2/log.300ef712b
./log/version-2/log.300f0659e
./log/version-2/.ignore
./data
./data/version-2
./data/version-2/snapshot.400000109
./data/version-2/currentEpoch
./data/version-2/acceptedEpoch
./data/version-2/snapshot.300ef712a
./data/version-2/snapshot.300f0659d
./data/myid.bak
./data/myid


On other nodes we had the same exception but no commit log deletion.
java.lang.RuntimeException: Unable to run quorum server
       at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:558)
        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:500)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:153)
        at com.hedvig.hpod.service.PodnetService$1.run(PodnetService.java:2262)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: The current epoch, 3, is older than the last zxid, 17179871862
        at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:539)

./log
./log/version-2
./log/version-2/log.300f06cfc
./log/version-2/log.300f03890
./log/version-2/.ignore
./data
./data/version-2
./data/version-2/snapshot.300f06cfb
./data/version-2/snapshot.300f06f10
./data/version-2/currentEpoch
./data/version-2/acceptedEpoch
./data/version-2/snapshot.300f0388f
./data/myid.bak
./data/myid


/log
./log/version-2
./log/version-2/log.300f06dbf
./log/version-2/log.300ed96fc
./log/version-2/log.300ef1048
./log/version-2/.ignore
./data
./data/version-2
./data/version-2/snapshot.300f06dbe
./data/version-2/currentEpoch
./data/version-2/acceptedEpoch
./data/version-2/snapshot.300ed96fb
./data/version-2/snapshot.300ef1048
./data/myid.bak
./data/myid

The symptoms look like ZOOKEEPER-1549, but we are running 3.4.9 here.

Any ideas?",[],Bug,ZOOKEEPER-2653,Major,Lasaro Camargos,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,epoch files do not match snapshots and logs,2016-12-22T17:03:20.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",2.0
Rakesh Radhakrishnan,[],2016-12-22T06:15:38.000+0000,Rakesh Radhakrishnan,"The commit of ZOOKEEPER-2479 has introduced a compilation error(due to diamond operator usage) in {{branch-3.4}}, which uses {{JDK 1.6}}","[<JIRA Version: name='3.4.10', id='12338036'>]",Bug,ZOOKEEPER-2652,Major,Rakesh Radhakrishnan,Fixed,2016-12-22T06:34:21.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Fix HierarchicalQuorumTest.java,2017-03-31T09:01:12.000+0000,"[<JIRA Version: name='3.4.10', id='12338036'>]",4.0
Rakesh Radhakrishnan,"[<JIRA Component: name='build', id='12312383'>]",2016-12-22T01:47:27.000+0000,Christopher Tubbs,"Trying to build downstream in Fedora, and discovered that the 3.4.9 release tarball is missing the {{src/pom.template}} file. It is present in the {{release-3.4.9}} tag, so I grabbed it from there to patch downstream.","[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2651,Major,Christopher Tubbs,Fixed,2017-01-11T05:28:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Missing src/pom.template in release,2017-03-31T09:01:09.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>]",6.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-12-18T18:05:05.000+0000,Hadriel Kaplan,"If a client creates a Container node, but does not also create a child within that Container, the Container will never be deleted. This may seem like a bug in the client for not subsequently creating a child, but we can't assume the client remains connected, or that the client didn't just change its mind (due to some recipe being canceled, for example).

The bug is in ContainerManager.getCandidates(), which only considers a node a candidate if its Cversion > 0. The comments indicate this was done intentionally, to avoid a race condition whereby the Container was created right before a cleaning period, and would get cleaned up before the child could be created - so to avoid that the check is performed to verify the Cversion > 0.

Instead, I propose that if the Cversion is 0 but the Ctime is more than a checkIntervalMs old, then it be deleted. In other words, if the Container node has been around for a whole cleaning round already and no child has been  created since, then go ahead and clean it up.

I can provide a patch if others agree with such a change.



",[],Bug,ZOOKEEPER-2648,Major,Hadriel Kaplan,Not A Bug,2017-01-23T22:11:05.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Container node never gets deleted if it never had children,2017-08-01T18:35:49.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Flavio Paiva Junqueira,[],2016-12-16T11:33:43.000+0000,Flavio Paiva Junqueira,The commit of ZOOKEEPER-761 has introduced a compilation error in one of the test cases. It is a pretty straightforward fix.,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2647,Blocker,Flavio Paiva Junqueira,Fixed,2017-01-05T17:57:36.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Fix TestReconfigServer.cc,2017-05-18T03:43:58.000+0000,[],5.0
Flavio Paiva Junqueira,[],2016-12-15T22:22:48.000+0000,Flavio Paiva Junqueira,Need to update build.xml 1.5->1.6.,"[<JIRA Version: name='3.4.10', id='12338036'>]",Bug,ZOOKEEPER-2646,Major,Flavio Paiva Junqueira,Fixed,2016-12-22T04:48:53.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Java target in branch 3.4 doesn't match documentation ,2017-03-31T09:01:15.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",5.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-12-15T07:36:40.000+0000,Tom Pijl,"When starting an embedded _QuorumPeerMain_ by executing the _runFromConfig()_ and providing _QuorumPeerConfig_ properties:
{code}standaloneEnabled=false
initLimit=5
syncLimit=2
clientPort=4101
server.1=nlbantpijl01.infor.com:2101:3101:participant;4101
dataDir=/Storage/zookeeper/server001{code}
an NullPointerException is thrown in the _QuorumPeerConfig_ class in the method _backupOldConfig()_ because the property configFileStr is null.

A check must be made at the start of the method _backupOldConfig()_ if the property _configFileStr_ is null. If so just exit the method. In the embedded mode there is no config file, so no need to create a backup.",[],Bug,ZOOKEEPER-2645,Major,Tom Pijl,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,If embedded QuorumPeerMain is started with Properties no backupOldConfig should be done,2016-12-15T08:04:07.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",4.0
Jordan Zimmerman,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>]",2016-12-07T11:23:24.000+0000,Jordan Zimmerman,"ZOOKEEPER-2014 moved the reconfig() methods into a new class, ZooKeeperAdmin. It appears this was done to document that these are methods have access restrictions. However, this change breaks Apache Curator (and possibly other clients). Curator APIs will have to be changed and/or special methods need to be added. A breaking change of this kind should only be done when the benefit is overwhelming. In this case, the same information can be conveyed with documentation and possibly a deprecation notice.

Revert the creation of the ZooKeeperAdmin class and move the reconfig() methods back to the ZooKeeper class with additional documentation.","[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2642,Blocker,Jordan Zimmerman,Fixed,2017-02-11T23:34:43.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeper reconfig API backward compatibility fix,2019-01-30T16:16:38.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",7.0
Michael Han,"[<JIRA Component: name='jute', id='12312385'>]",2016-11-24T21:28:56.000+0000,Michael Han,"C client build is broken after ZOOKEEPER-2628 is merged in. After a little debug, I find out that the build is broken because the zookeeper.jute.h and zookeeper.jute.c are not completely generated. 

* The culprit is the code change introduced in ZOOKEEPER-2628, where we wraps {code}JRecord.genCCode{code} with a try / catch / finally block and the file writers were prematurely closed in finally block which prevents remaining of the zookeeper.jute.h/c file being generated. 

* The fix to {code}JRecord.genCCode{code} in ZOOKEEPER-2628 was made because a find bug warning was directly associated with the code. Due to the subtlety of the file writer ownership, we did not capture the issue during code review. 

* The build break was not captured in pre-commit builds as well ([an example|https://builds.apache.org/job/PreCommit-ZOOKEEPER-github-pr-build/72//console]), where we get all tests passed including C client tests. I suspect we might have another bug with cached generated files that should be regenerated but we don't - need more investigation on this one.

* The fix is simple by revert the change to this specific method. Findbug does not complain anymore because the previous warning that appertain to this code block was fixed at the call site of {code}JRecord.genCCode{code}. So by reverting the change we still have zero find bug warnings.
","[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2636,Blocker,Michael Han,Fixed,2016-11-25T06:10:39.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Fix C build break.,2017-05-18T03:44:00.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",5.0
Michael Han,"[<JIRA Component: name='documentation', id='12312422'>]",2016-11-24T16:38:59.000+0000,Flavio Paiva Junqueira,"Some recent commits did not regenerate the documentation even though they had documentation changes, we need to do it before releasing.","[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2635,Blocker,Flavio Paiva Junqueira,Fixed,2017-03-21T19:10:11.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Regenerate documentation,2017-05-18T03:44:00.000+0000,[],5.0
,"[<JIRA Component: name='java client', id='12312381'>]",2016-11-20T13:01:01.000+0000,nayeem,"We can create zk node with null data as given bellow.
ZkConnect connector = new ZkConnect();
ZooKeeper zk = connector.connect(""host:port"");
String newNode = ""/nayeemDate3"";
String strdata = String.valueOf('\u0000');
connector.createNode(newNode, strdata.getBytes());

When we get the data for the zknode
2016-11-17 23:55:48,926 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:5181:NIOServerCnxn@349] - caught end of stream exception
EndOfStreamException: Unable to read additional data from client sessionid 0x1585061acbd0613, likely client has closed socket
        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:220)
        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
        at java.lang.Thread.run(Thread.java:745)
2016-11-17 23:55:48,926 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:5181:NIOServerCnxn@1001] - Closed socket connection for client /10.10.72.93:48005 which had sessionid 0x1585061acbd0613

To resolve the issue workaround is to delete the zknode, 

is it the right behaviour or is this a bug.
data from zkcli 
[zk: 10.10.72.93:5181(CONNECTED) 1] ls /nayeemDate3
[]
[zk: 10.10.72.93:5181(CONNECTED) 2] get /nayeemDate3
null
cZxid = 0xdc47
ctime = Fri Nov 18 13:29:43 IST 2016
mZxid = 0xdc47
mtime = Fri Nov 18 13:29:43 IST 2016
pZxid = 0xdc47
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 0
numChildren = 0
[zk: 10.10.72.93:5181(CONNECTED) 3] stat /nayeemDate3
cZxid = 0xdc47
ctime = Fri Nov 18 13:29:43 IST 2016
mZxid = 0xdc47
mtime = Fri Nov 18 13:29:43 IST 2016
pZxid = 0xdc47
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 0
numChildren = 0
 ",[],Bug,ZOOKEEPER-2634,Major,nayeem,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,null data in zknode data,2018-08-27T03:09:31.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",3.0
Raghavendra Prabhu,"[<JIRA Component: name='contrib-zkfuse', id='12312644'>]",2016-11-17T11:11:50.000+0000,Raghavendra Prabhu,"The build in contrib/zkfuse fails with

{noformat}

make
(CDPATH=""${ZSH_VERSION+.}:"" && cd . && /bin/sh /home/raghu/zookeeper/src/contrib/zkfuse/missing autoheader)
rm -f stamp-h1
touch config.h.in
cd . && /bin/sh ./config.status config.h
config.status: creating config.h
config.status: config.h is unchanged
make  all-recursive
make[1]: Entering directory '/home/raghu/zookeeper/src/contrib/zkfuse'
Making all in src
make[2]: Entering directory '/home/raghu/zookeeper/src/contrib/zkfuse/src'
g++ -DHAVE_CONFIG_H -I. -I..    -I/home/raghu/zookeeper/src/contrib/zkfuse/../../c/include -I/home/raghu/zookeeper/src/contrib/zkfuse/../../c/generated -I../include -I/usr/include -D_FILE_OFFSET_BITS=64 -D_REENTRANT -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -MT zkfuse.o -MD -MP -MF .deps/zkfuse.Tpo -c -o zkfuse.o zkfuse.cc
g++ -DHAVE_CONFIG_H -I. -I..    -I/home/raghu/zookeeper/src/contrib/zkfuse/../../c/include -I/home/raghu/zookeeper/src/contrib/zkfuse/../../c/generated -I../include -I/usr/include -D_FILE_OFFSET_BITS=64 -D_REENTRANT -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -MT zkadapter.o -MD -MP -MF .deps/zkadapter.Tpo -c -o zkadapter.o zkadapter.cc
In file included from zkadapter.h:34:0,
                 from zkadapter.cc:24:
event.h:216:9: error: reference to ‘shared_ptr’ is ambiguous
         shared_ptr<AbstractEventWrapper> m_eventWrapper;
         ^~~~~~~~~~
In file included from /usr/include/boost/throw_exception.hpp:42:0,
                 from /usr/include/boost/smart_ptr/shared_ptr.hpp:27,
                 from /usr/include/boost/shared_ptr.hpp:17,
                 from event.h:30,
                 from zkadapter.h:34,
                 from zkadapter.cc:24:
/usr/include/boost/exception/exception.hpp:148:11: note: candidates are: template<class T> class boost::shared_ptr
     class shared_ptr;
           ^~~~~~~~~~
In file included from /usr/include/c++/6.2.1/bits/shared_ptr.h:52:0,
                 from /usr/include/c++/6.2.1/memory:82,
                 from /usr/include/boost/config/no_tr1/memory.hpp:21,
                 from /usr/include/boost/smart_ptr/shared_ptr.hpp:23,
                 from /usr/include/boost/shared_ptr.hpp:17,
                 from event.h:30,
                 from zkadapter.h:34,
                 from zkadapter.cc:24:
/usr/include/c++/6.2.1/bits/shared_ptr_base.h:343:11: note:                 template<class _Tp> class std::shared_ptr
     class shared_ptr;
           ^~~~~~~~~~
In file included from zkadapter.h:34:0,
                 from zkadapter.cc:24:
event.h: In constructor ‘zkfuse::GenericEvent::GenericEvent(int, zkfuse::AbstractEventWrapper*)’:
event.h:189:27: error: class ‘zkfuse::GenericEvent’ does not have any field named ‘m_eventWrapper’
             m_type(type), m_eventWrapper(eventWrapper) {
                           ^~~~~~~~~~~~~~
event.h: In member function ‘void* zkfuse::GenericEvent::getEvent() const’:
event.h:204:41: error: ‘m_eventWrapper’ was not declared in this scope
         void *getEvent() const { return m_eventWrapper->getWrapee(); }
                                         ^~~~~~~~~~~~~~
In file included from zkadapter.h:34:0,
                 from zkfuse.cc:54:
event.h:216:9: error: reference to ‘shared_ptr’ is ambiguous
         shared_ptr<AbstractEventWrapper> m_eventWrapper;
         ^~~~~~~~~~
In file included from /usr/include/boost/throw_exception.hpp:42:0,
                 from /usr/include/boost/smart_ptr/detail/shared_count.hpp:27,
                 from /usr/include/boost/smart_ptr/weak_ptr.hpp:17,
                 from /usr/include/boost/weak_ptr.hpp:16,
                 from zkfuse.cc:50:
/usr/include/boost/exception/exception.hpp:148:11: note: candidates are: template<class T> class boost::shared_ptr
     class shared_ptr;
           ^~~~~~~~~~
In file included from /usr/include/c++/6.2.1/bits/shared_ptr.h:52:0,
                 from /usr/include/c++/6.2.1/memory:82,
                 from /usr/include/boost/smart_ptr/weak_ptr.hpp:16,
                 from /usr/include/boost/weak_ptr.hpp:16,
                 from zkfuse.cc:50:
/usr/include/c++/6.2.1/bits/shared_ptr_base.h:343:11: note:                 template<class _Tp> class std::shared_ptr
     class shared_ptr;
           ^~~~~~~~~~
In file included from zkadapter.h:34:0,
                 from zkfuse.cc:54:
event.h: In constructor ‘zkfuse::GenericEvent::GenericEvent(int, zkfuse::AbstractEventWrapper*)’:
event.h:189:27: error: class ‘zkfuse::GenericEvent’ does not have any field named ‘m_eventWrapper’
             m_type(type), m_eventWrapper(eventWrapper) {
                           ^~~~~~~~~~~~~~
event.h: In member function ‘void* zkfuse::GenericEvent::getEvent() const’:
event.h:204:41: error: ‘m_eventWrapper’ was not declared in this scope
         void *getEvent() const { return m_eventWrapper->getWrapee(); }
                                         ^~~~~~~~~~~~~~
zkadapter.cc: In member function ‘bool zk::ZooKeeperAdapter::deleteNode(const string&, bool, int)’:
zkadapter.cc:676:52: error: no matching function for call to ‘zk::ZooKeeperAdapter::getNodeChildren(std::vector<std::__cxx11::basic_string<char> >&, const string&, bool)’
             getNodeChildren( nodeList, path, false );
                                                    ^
In file included from zkadapter.cc:24:0:
zkadapter.h:440:14: note: candidate: void zk::ZooKeeperAdapter::getNodeChildren(std::vector<std::__cxx11::basic_string<char> >&, const string&, zk::ZKEventListener*, void*)
         void getNodeChildren(vector<string> &children,
              ^~~~~~~~~~~~~~~
zkadapter.h:440:14: note:   no known conversion for argument 3 from ‘bool’ to ‘zk::ZKEventListener* {aka zkfuse::EventListener<zk::ZKWatcherEvent>*}’
make[2]: *** [Makefile:310: zkadapter.o] Error 1
make[2]: *** Waiting for unfinished jobs....
make[2]: *** [Makefile:310: zkfuse.o] Error 1
make[2]: Leaving directory '/home/raghu/zookeeper/src/contrib/zkfuse/src'
make[1]: *** [Makefile:352: all-recursive] Error 1
make[1]: Leaving directory '/home/raghu/zookeeper/src/contrib/zkfuse'
make: *** [Makefile:293: all] Error 2
   
   
=================================================================================================================


make
make  all-recursive
make[1]: Entering directory '/home/raghu/zookeeper/src/contrib/zkfuse'
Making all in src
make[2]: Entering directory '/home/raghu/zookeeper/src/contrib/zkfuse/src'
g++ -DHAVE_CONFIG_H -I. -I..    -I/home/raghu/zookeeper/src/contrib/zkfuse/../../c/include -I/home/raghu/zookeeper/src/contrib/zkfuse/../../c/generated -I../include -I/usr/include -D_FILE_OFFSET_BITS=64 -D_REENTRANT -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -MT zkfuse.o -MD -MP -MF .deps/zkfuse.Tpo -c -o zkfuse.o zkfuse.cc
g++ -DHAVE_CONFIG_H -I. -I..    -I/home/raghu/zookeeper/src/contrib/zkfuse/../../c/include -I/home/raghu/zookeeper/src/contrib/zkfuse/../../c/generated -I../include -I/usr/include -D_FILE_OFFSET_BITS=64 -D_REENTRANT -march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -MT zkadapter.o -MD -MP -MF .deps/zkadapter.Tpo -c -o zkadapter.o zkadapter.cc
zkadapter.cc: In member function ‘bool zk::ZooKeeperAdapter::deleteNode(const string&, bool, int)’:
zkadapter.cc:676:52: error: no matching function for call to ‘zk::ZooKeeperAdapter::getNodeChildren(std::vector<std::__cxx11::basic_string<char> >&, const string&, bool)’
             getNodeChildren( nodeList, path, false );
                                                    ^
In file included from zkadapter.cc:24:0:
zkadapter.h:440:14: note: candidate: void zk::ZooKeeperAdapter::getNodeChildren(std::vector<std::__cxx11::basic_string<char> >&, const string&, zk::ZKEventListener*, void*)
         void getNodeChildren(vector<string> &children,
              ^~~~~~~~~~~~~~~
zkadapter.h:440:14: note:   no known conversion for argument 3 from ‘bool’ to ‘zk::ZKEventListener* {aka zkfuse::EventListener<zk::ZKWatcherEvent>*}’
make[2]: *** [Makefile:310: zkadapter.o] Error 1
make[2]: *** Waiting for unfinished jobs....
mv -f .deps/zkfuse.Tpo .deps/zkfuse.Po
make[2]: Leaving directory '/home/raghu/zookeeper/src/contrib/zkfuse/src'
make[1]: *** [Makefile:352: all-recursive] Error 1
make[1]: Leaving directory '/home/raghu/zookeeper/src/contrib/zkfuse'
make: *** [Makefile:293: all] Error 2
   
{noformat}

in two different places.

Fixed here: https://github.com/ronin13/zookeeper/commit/726a8eda08e4022fcbcb0581ec2650e07e39910b ","[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2633,Minor,Raghavendra Prabhu,Fixed,2017-01-23T00:43:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Build failure in contrib/zkfuse with gcc 6.x,2017-03-31T09:01:15.000+0000,[],6.0
Michael Han,[],2016-11-05T00:49:49.000+0000,Michael Han,"Findbug tool used by Jenkins bot is upgraded to 3.0.1 from 2.0.3 according to Infra team, and this leads to 20 new warnings produced by findbug. The warning reports can be found on [pre commit builds|https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/] with build number >= 3513. These warnings need to be triaged and fixed if they are legitimate.
","[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2628,Major,Michael Han,Fixed,2016-11-24T16:21:27.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Investigate and fix findbug warnings,2017-05-18T03:44:02.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",6.0
Michael Han,"[<JIRA Component: name='c client', id='12312380'>]",2016-11-03T20:52:30.000+0000,Michael Han,"While working on ZOOKEEPER-2014, I noticed a discrepancy between Java and C client regarding the error codes definition. There is a {noformat}ZRWSERVERFOUND = -122{noformat} definition in C client which is not present in Java client's KeeperException.Code definitions. 

This discrepancy was introduced by ZOOKEEPER-827, where the C client logic was simulating the Java client's logic when doing a read/write server search while client is in read only mode. Once client finds a valid read/write server, client will try to disconnect and reconnect with this read/write server, as we always prefer r/w server in ro mode. The way Java client is doing this disconnect/reconnect process is by throwing a RWServerFoundException (instead of a KeeperException) to set the client in disconnected state, then wait for client reconnect with r/w server address set before throwing the exception. C client did similar but instead of having an explicitly disconnect / clean up routine, the client was relying on handle_error to do the job where ZRWSERVERFOUND was introduced.

I propose we remove ZRWSERVERFOUND error code from C client and use an explicit routine instead of handle_error when we do r/w server search in C client for two reasons:
* ZRWSERVERFOUND is not something ZK client users would need to know. It's a pure implementation detail that's used to alter the connection state of the client, and ZK client users have no desire nor need to handle such errors, as R/W server scanning and connect is handled transparently by ZK client library.
* To maintain consistency between Java and C client regarding error codes definition. Without removing this from C client, we would need to replace RWServerFoundException in Java client with a new KeeperException, and again with the reason mentioned above, we don't need a KeeperException for this because such implementation detail does not have to be exposed to end users (unless, we provided alternative for users to opt-out automate R/W server switching when in read only mode which we don't.).","[<JIRA Version: name='3.5.3', id='12335444'>]",Bug,ZOOKEEPER-2627,Major,Michael Han,Fixed,2016-12-03T17:17:27.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Remove ZRWSERVERFOUND from C client and replace handle_error with something more semantically explicit for r/w server reconnect.,2017-05-18T03:44:02.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",5.0
,"[<JIRA Component: name='scripts', id='12312384'>]",2016-11-02T13:04:04.000+0000,Arne Bachmann,"I put the log4j.properties into the conf folder, plus a symlink to the base zookeeper folder, as described in the documentation.
Neither of them seem to be picked up, as my rolling logger is not recognized (no logs created), and also bin/zkServer.sh print-cmd shows wrong logger configuration. Is that a problem of the start script or did I put the properties file into the wrong place?

Note however, that also my additional java command-line options (from JAVA_TOOL_OPTIONS) don't get picked up by the start script, as can be seen by ps aux | grep java (e.g. -Xmx1000m instead of -Xmx500 as I defined it).
The script's refer to a lot of environment variables that aren't explained in the documentation and nowhere defined; I can't get it to run.",[],Bug,ZOOKEEPER-2626,Major,Arne Bachmann,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,log4j.properties don't get respected,2018-01-26T16:51:49.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",3.0
,"[<JIRA Component: name='scripts', id='12312384'>]",2016-11-02T12:59:02.000+0000,Arne Bachmann,"I provision a vagrant vm that installs zookeeper into /home/vagrant/zk and adjusts all owner and read/write rights.
With the vagrant user, I start zookeeper as bin/zkServer.sh start /vagrant/data/zoo.cfg
However, the folder data? (or data^M) gets created with the PID inside, instead of putting it into the data folder, which contains the version-2 folder.
Since I'm using the official start scripts, I'm at a loss.
Also, the data? folder comes with root:root ownership, which is strange, as zKServer.sh is executed from the vagrant user.",[],Bug,ZOOKEEPER-2625,Minor,Arne Bachmann,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zkServer.sh creates PID file in the folder data?/ instead of data/,2019-02-19T21:58:24.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",4.0
,[],2016-10-28T02:29:48.000+0000,Diego Ongaro,"I wasn't sure if check version (opcode 13) was permitted outside of a multi op, so I tried it. My server crashed with a NullPointerException and became unusable until restarted. I guess it's not allowed, but perhaps the server should handle this more gracefully?

Here are the server logs:
{noformat}
Accepted socket connection from /0:0:0:0:0:0:0:1:51737
Session establishment request from client /0:0:0:0:0:0:0:1:51737 client's lastZxid is 0x0
Connection request from old client /0:0:0:0:0:0:0:1:51737; will be dropped if server is in r-o mode
Client attempting to establish new session at /0:0:0:0:0:0:0:1:51737
:Fsessionid:0x10025651faa0000 type:createSession cxid:0x0 zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
Processing request:: sessionid:0x10025651faa0000 type:createSession cxid:0x0 zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
Got zxid 0x60000065e expected 0x1
Creating new log file: log.60000065e
Committing request:: sessionid:0x10025651faa0000 type:createSession cxid:0x0 zxid:0x60000065e txntype:-10 reqpath:n/a
Processing request:: sessionid:0x10025651faa0000 type:createSession cxid:0x0 zxid:0x60000065e txntype:-10 reqpath:n/a
:Esessionid:0x10025651faa0000 type:createSession cxid:0x0 zxid:0x60000065e txntype:-10 reqpath:n/a
sessionid:0x10025651faa0000 type:createSession cxid:0x0 zxid:0x60000065e txntype:-10 reqpath:n/a
Add a buffer to outgoingBuffers, sk sun.nio.ch.SelectionKeyImpl@28e9f397 is valid: true
Established session 0x10025651faa0000 with negotiated timeout 20000 for client /0:0:0:0:0:0:0:1:51737
:Fsessionid:0x10025651faa0000 type:check cxid:0x1 zxid:0xfffffffffffffffe txntype:unknown reqpath:/
Processing request:: sessionid:0x10025651faa0000 type:check cxid:0x1 zxid:0xfffffffffffffffe txntype:unknown reqpath:/
Processing request:: sessionid:0x10025651faa0000 type:check cxid:0x1 zxid:0xfffffffffffffffe txntype:unknown reqpath:/
Exception causing close of session 0x10025651faa0000: Connection reset by peer
:Esessionid:0x10025651faa0000 type:check cxid:0x1 zxid:0xfffffffffffffffe txntype:unknown reqpath:/
IOException stack trace
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:320)
	at org.apache.zookeeper.server.NIOServerCnxnFactory$IOWorkRequest.doWork(NIOServerCnxnFactory.java:530)
	at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:162)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Unexpected exception
java.lang.NullPointerException
	at org.apache.zookeeper.server.ZKDatabase.addCommittedProposal(ZKDatabase.java:252)
	at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:127)
	at org.apache.zookeeper.server.quorum.CommitProcessor$CommitWorkRequest.doWork(CommitProcessor.java:362)
	at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:162)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Committing request:: sessionid:0x10025651faa0000 type:error cxid:0x1 zxid:0x60000065f txntype:-1 reqpath:n/a
Unregister MBean [org.apache.ZooKeeperService:name0=ReplicatedServer_id1,name1=replica.1,name2=Follower,name3=Connections,name4=""0:0:0:0:0:0:0:1"",name5=0x10025651faa0000]
Exception thrown by downstream processor, unable to continue.
CommitProcessor exited loop!
Closed socket connection for client /0:0:0:0:0:0:0:1:51737 which had sessionid 0x10025651faa0000
{noformat}

And here's a one-liner to repro, which does a ConnectRequest followed by a {{CheckVersion(path=""/"", version=89235}}}:
{noformat}
echo AAAALAAAAAAAAAAAAAAAAAAAJxAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAEQAAAAEAAAANAAAAAS8AAVyT | base64 --decode | nc localhost 2181 >/dev/null
{noformat}

This is against master as of a couple of weeks ago (f78061a). I haven't checked to see which versions are affected.",[],Bug,ZOOKEEPER-2623,Minor,Diego Ongaro,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,CheckVersion outside of Multi causes NullPointerException,2016-11-03T13:54:23.000+0000,[],4.0
Flavio Paiva Junqueira,[],2016-10-26T14:17:49.000+0000,Flavio Paiva Junqueira,"The method simply returns and there is some code commented out:

{code}
        // if (isTraceEnabled(log, mask)) {
        // logTraceMessage(LOG, mask, direction + "" ""
        // + FollowerHandler.packetToString(qp));
        // }
{code}

There are calls to this trace method, so I think we should fix it.","[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2622,Trivial,Flavio Paiva Junqueira,Fixed,2017-01-28T05:52:58.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooTrace.logQuorumPacket does nothing,2020-01-23T18:16:31.000+0000,[],7.0
Amichai Rothman,"[<JIRA Component: name='scripts', id='12312384'>]",2016-10-26T09:45:17.000+0000,Amichai Rothman,"The ZooKeeper scripts fail due to missing cygpath path conversion in a MINGW32 environment, such as when running from git bash (installed by default when installing Git for Windows).

The fix is to add the line
{quote}
MINGW*) cygwin=true ;;
{quote}
near the bottom of the zkEnv.sh script, in the case statement that checks for a cygwin environment.
","[<JIRA Version: name='3.4.15', id='12344988'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-2621,Major,Amichai Rothman,Fixed,2019-03-18T05:27:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeper doesn't start on MINGW32 (Windows),2019-05-20T17:51:03.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",6.0
,[],2016-10-22T00:01:28.000+0000,Diego Ongaro,"According to the USENIX ATC 2010 [paper|https://www.usenix.org/conference/usenix-atc-10/zookeeper-wait-free-coordination-internet-scale-systems], ZooKeeper provides ""FIFO client order: all requests from a given client are executed in the order that they were sent by the client."" I believe applications written using the Java client library are unable to rely on this guarantee, and any current application that does so is broken. Other client libraries are also likely to be affected.

Consider this application, which is simplified from the algorithm described on Page 4 (right column) of the paper:
{code}
  zk = new ZooKeeper(...)
  zk.createAsync(""/data-23857"", ""..."", callback)
  zk.createSync(""/pointer"", ""/data-23857"")
{code}
Assume an empty ZooKeeper database to begin with and no other writers. Applying the above definition, if the ZooKeeper database contains /pointer, it must also contain /data-23857.

Now consider this series of unfortunate events:
{code}
  zk = new ZooKeeper(...)
  // The library establishes a TCP connection.
  zk.createAsync(""/data-23857"", ""..."", callback)
  // The library/kernel closes the TCP connection because it times out, and
  // the create of /data-23857 is doomed to fail with ConnectionLoss. Suppose
  // that it never reaches the server.
  // The library establishes a new TCP connection.
  zk.createSync(""/pointer"", ""/data-23857"")
  // The create of /pointer succeeds.
{code}
That's the problem: subsequent operations get assigned to the new connection and succeed, while earlier operations fail.

In general, I believe it's impossible to have a system with the following three properties:
 # FIFO client order for asynchronous operations,
 # Failing operations when connections are lost, AND
 # Transparently reconnecting when connections are lost.

To argue this, consider an application that issues a series of pipelined operations, then upon noticing a connection loss, issues a series of recovery operations, repeating the recovery procedure as necessary. If a pipelined operation fails, all subsequent operations in the pipeline must also fail. Yet the client must also carry on eventually: the recovery operations cannot be trivially failed forever. Unfortunately, the client library does not know where the pipelined operations end and the recovery operations begin. At the time of a connection loss, subsequent pipelined operations may or may not be queued in the library; others might be upcoming in the application thread. If the library re-establishes a connection too early, it will send pipelined operations out of FIFO client order.


I considered a possible workaround of having the client diligently check its callbacks and watchers for connection loss events, and do its best to stop the subsequent pipelined operations at the first sign of a connection loss. In addition to being a large burden for the application, this does not solve the problem all the time. In particular, if the callback thread is delayed significantly (as can happen due to excessive computation or scheduling hiccups), the application may not learn about the connection loss event until after the connection has been re-established and after dependent pipelined operations have already been transmitted over the new connection.


I suggest the following API changes to fix the problem:
 - Add a method ZooKeeper.getConnection() returning a ZKConnection object. ZKConnection would wrap a TCP connection. It would include all synchronous and asynchronous operations currently defined on the ZooKeeper class. Upon a connection loss on a ZKConnection, all subsequent operations on the same ZKConnection would return a Connection Loss error. Upon noticing, the client would need to call ZooKeeper.getConnection() again to get a working ZKConnection object, and it would execute its recovery procedure on this new connection.
 - Deprecate all asynchronous methods on the ZooKeeper object. These are unsafe to use if the caller assumes they're getting FIFO client order.
 - No changes to the protocols or servers are required.

I recognize this could cause a lot of code churn for both ZooKeeper and projects that use it. On the other hand, the existing asynchronous calls in applications should now be audited anyhow.


The code affected by this issue may be difficult to contain:
 - It likely affects all ZooKeeper client libraries that provide both asynchronous operations and transparent reconnection. That's probably all versions of the official Java client library, as well as most other client libraries.
 - It affects all applications using those libraries that depend on the FIFO client order of asynchronous operations. I don't know how common that is, but the paper implies that FIFO client order is important.
 - Fortunately, the issue can only manifest itself when connections are lost and transparently reestablished. In practice, it may also require a long pipeline or a significant delay in the application thread while the library establishes a new connection.
 - In case you're wondering, this issue occurred to me while working on a new client library for Go. I haven't seen this issue in the wild, but I was able to produce it locally by placing sleep statements in a Java program and closing its TCP connections.


I'm new to this community, so I'm looking forward to the discussion. Let me know if I can clarify any of the above.",[],Bug,ZOOKEEPER-2619,Major,Diego Ongaro,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Client library reconnecting breaks FIFO client order,2016-11-03T14:54:20.000+0000,[],9.0
tony mancill,[],2016-10-18T05:14:09.000+0000,tony mancill,"While working on the Debian packaging of ZooKeeper, some misspellings were detected in the source that affect the documentation, logging, and program output.

There is a PR against github containing the patch here:  https://github.com/apache/zookeeper/pull/87","[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2617,Trivial,tony mancill,Fixed,2017-02-14T02:45:38.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,correct a few spelling typos,2017-05-18T03:50:02.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",10.0
,[],2016-10-17T22:08:49.000+0000,Benjamin Jaton,"Only 1 of the 3 nodes of the ensemble is started. The server successfully started in readonly (""Read-only server started"").

{code:title=client}System.setProperty(""readonlymode.enabled"", ""true"");

String cs = ""QA-E8WIN11:2181,QA-E8WIN12:2181,QA-E8WIN13:2181"";
ZooKeeper zk = new ZooKeeper(cs, 30000, null, true);
		
// wait for connection
while (!zk.getState().isConnected()) {
	Thread.sleep(1000);
	logger.error(zk.getState());
}
zk.getData(""/"", false, new Stat());

logger.error(""DONE"");{code}


The client code above manages to acquire a connection (""CONNECTEDREADONLY"") but the subsequent getData fails with ConnectionLoss:

{code:title=client log}2016-10-17 14:37:43 ERROR TestCuratorReadOnly:31 - CONNECTEDREADONLY
2016-10-17 14:39:49 ERROR o.a.z.ClientCnxn:526 - Error while calling watcher 
java.lang.NullPointerException
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:524) [zookeeper-3.5.2-alpha.jar:3.5.2-alpha--1]
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:499) [zookeeper-3.5.2-alpha.jar:3.5.2-alpha--1]
Exception in thread ""main"" org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1956)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1985)
	at TestCuratorReadOnly.main(TestCuratorReadOnly.java:33){code}

Full server logs are attached, but here are the relevant parts:

{code:title=server log}
2016-10-17 14:37:31,375 [myid:1] - INFO  [Thread-2:ReadOnlyZooKeeperServer@73] - Read-only server started
(...)
2016-10-17 14:37:55,241 [myid:1] - INFO  [NIOServerCxnFactory.AcceptThread:/0.0.0.0:2181:NIOServerCnxnFactory$AcceptThread@296] - Accepted socket connection from /10.11.12.4:40800
2016-10-17 14:37:55,250 [myid:1] - INFO  [NIOWorkerThread-1:ZooKeeperServer@964] - Client attempting to establish new session at /10.11.12.4:40800
2016-10-17 14:37:55,255 [myid:1] - INFO  [ProcessThread(sid:1 cport:-1)::ZooKeeperServer@678] - Established session 0x100024619520000 with negotiated timeout 30000 for client /10.11.12.4:40800
(...)
 [org.apache.ZooKeeperService:name0=ReplicatedServer_id1,name1=replica.1,name2=ReadOnlyServer,name3=Connections,name4=10.11.12.4,name5=0x100024619520000]
2016-10-17 14:38:26,929 [myid:1] - INFO  [ProcessThread(sid:1 cport:-1)::NIOServerCnxn@607] - Closed socket connection for client /10.11.12.4:40800 which had sessionid 0x100024619520000{code}

The client and server are using official 3.5.2-alpha.

{code:title=zoo.cfg}autopurge.purgeInterval=3
initLimit=10
syncLimit=5
autopurge.snapRetainCount=3
snapCount=10000
minSessionTimeout=5000
maxSessionTimeout=600000
tickTime=2000
admin.commandURL=/commands
quorumListenOnAllIPs=true
dataDir=C:/workspace/zookeeper-3.5.2-alpha/data
admin.serverPort=8080
admin.enableServer=false
standaloneEnabled=false
dynamicConfigFile=C:/workspace/zookeeper-3.5.2-alpha/conf/zoo.cfg.dynamic.10000046b{code}

{code:title=zoo.cfg.dynamic.10000046b}server.1=QA-E8WIN11:2888:3888:participant;0.0.0.0:2181
server.2=QA-E8WIN12:2888:3888:participant;0.0.0.0:2181
server.3=QA-E8WIN13:2888:3888:participant;0.0.0.0:2181{code}",[],Bug,ZOOKEEPER-2616,Critical,Benjamin Jaton,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZK client fails to connect to ReadOnly server,2017-01-30T20:34:37.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",5.0
Camille Fournier,"[<JIRA Component: name='server', id='12312382'>]",2016-10-14T04:59:03.000+0000,guoping.gp,"The same issue (https://issues.apache.org/jira/browse/ZOOKEEPER-1382) still can be found even with zookeeper 3.4.6.

this issue cause our production zookeeper cluster leak about 1 million watchs, after restart the server one by one, the watch count decrease to only about 40000.

I can reproduce the issue on my mac,here it is:
------------------------------------------------------------------------

pguodeMacBook-Air:bin pguo$ echo srvr | nc localhost 6181
Zookeeper version: 3.4.6-1569965, built on 02/20/2014 09:09 GMT
Latency min/avg/max: 0/1156/128513
Received: 539
Sent: 531
Connections: 1
Outstanding: 0
Zxid: 0x100000037
Mode: follower
Node count: 5
------------------------
pguodeMacBook-Air:bin pguo$ echo cons | nc localhost 6181
 /127.0.0.1:55759[1](queued=0,recved=5,sent=5,sid=0x157be2732d0000e,lop=PING,est=1476372631116,to=15000,lcxid=0x1,lzxid=0xffffffffffffffff,lresp=1476372646260,llat=8,minlat=0,avglat=6,maxlat=17)
 /0:0:0:0:0:0:0:1:55767[0](queued=0,recved=1,sent=0)

------------------------
pguodeMacBook-Air:bin pguo$ echo wchp | nc localhost 6181
/curator_exists_watch
       	0x357be48e4d90007
       	0x357be48e4d90009
       	0x157be2732d0000e


as above 4-letter's report shows, 	0x357be48e4d90007 and 0x357be48e4d90009 are leaked after the two sessions expired ",[],Bug,ZOOKEEPER-2615,Major,guoping.gp,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper server holds onto dead/expired session ids in the watch data structures,2022-02-03T08:50:14.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",7.0
Thomas Schüttel,[],2016-10-12T07:02:31.000+0000,Vishal Khandelwal,ZOOKEEPER-1576 handles UnknownHostException and it good to have this change for 3.4 branch as well. Porting the changes to 3.4 after resolving the conflicts,"[<JIRA Version: name='3.4.11', id='12339207'>]",Bug,ZOOKEEPER-2614,Major,Vishal Khandelwal,Fixed,2017-08-01T15:55:37.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Port ZOOKEEPER-1576 to branch3.4,2017-08-19T11:24:56.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",8.0
,[],2016-10-11T17:31:22.000+0000,venkata puvvada,"We are facing very big problem in production in Zookeeper. Its been working perfectly from many months, but suddenly without done any change in config. We had below problem. Because of the below problem, we are unable to do anything on this collection.

2016-10-11 09:54:56,448 [myid:2] - INFO  [ProcessThread(sid:2 cport:-1)::PrepRequestProcessor@651] - Got user-level KeeperException when processing sessionid:0x156f39ec13e00b9 type:setData cxid:0x4728359 zxid:0xa00560776 txntype:-1 reqpath:n/a Error Path:/solr/configs/constants/managed-schema Error:KeeperErrorCode = BadVersion for /solr/configs/constants/managed-schema
^C
",[],Bug,ZOOKEEPER-2613,Major,venkata puvvada,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,user-level KeeperException,2016-10-11T17:31:22.000+0000,[],1.0
,[],2016-10-11T17:31:21.000+0000,venkata puvvada,"We are facing very big problem in production in Zookeeper. Its been working perfectly from many months, but suddenly without done any change in config. We had below problem. Because of the below problem, we are unable to do anything on this collection.

2016-10-11 09:54:56,448 [myid:2] - INFO  [ProcessThread(sid:2 cport:-1)::PrepRequestProcessor@651] - Got user-level KeeperException when processing sessionid:0x156f39ec13e00b9 type:setData cxid:0x4728359 zxid:0xa00560776 txntype:-1 reqpath:n/a Error Path:/solr/configs/constants/managed-schema Error:KeeperErrorCode = BadVersion for /solr/configs/constants/managed-schema
^C
",[],Bug,ZOOKEEPER-2612,Major,venkata puvvada,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,user-level KeeperException,2016-10-11T17:31:21.000+0000,[],2.0
Eyal leshem,"[<JIRA Component: name='c client', id='12312380'>]",2016-10-09T08:24:31.000+0000,Eyal leshem,"The actual problem is in the function ""removeWatcherFromList"" - 
That when we check if we need to delete the watch -  we compare the WatcherCtx to one node before the one we want to delete.. ","[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2611,Critical,Eyal leshem,Fixed,2016-10-09T20:47:42.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zoo_remove_watchers - can remove the wrong watch ,2018-12-16T14:50:13.000+0000,[],5.0
,[],2016-10-07T07:17:53.000+0000,HIthu Anand,"hithu@linux:~/opt/gridlabd-src-2_0_2363$ autoreconf -isf
configure.ac:95: error: AC_SUBST: `DX_FLAG_[]DX_CURRENT_FEATURE' is not a valid shell variable name
m4/dx_doxygen.m4:77: DX_REQUIRE_PROG is expanded from...
m4/dx_doxygen.m4:117: DX_ARG_ABLE is expanded from...
m4/dx_doxygen.m4:178: DX_INIT_DOXYGEN is expanded from...
configure.ac:95: the top level
autom4te: /usr/bin/m4 failed with exit status: 1
aclocal: error: echo failed with exit status: 1
autoreconf: aclocal failed with exit status: 1


hithu@linux:~$ doxygen --version
1.8.6
hithu@linux:~$ autoconf --version
autoconf (GNU Autoconf) 2.69

hithu@linux:~$ automake --version
automake (GNU automake) 1.14.1
",[],Bug,ZOOKEEPER-2607,Major,HIthu Anand,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"doxygen-related, ./configure fails",2016-10-07T07:24:00.000+0000,[],2.0
Ted Yu,[],2016-10-04T13:00:12.000+0000,Ted Yu,"{code}
            LOG.info(""Setting authorizedID: "" + userNameBuilder);
            ac.setAuthorizedID(userNameBuilder.toString());
        } catch (IOException e) {
            LOG.error(""Failed to set name based on Kerberos authentication rules."");
        }
{code}
On one cluster, we saw the following:
{code}
2016-10-04 02:18:16,484 - ERROR [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:SaslServerCallbackHandler@137] - Failed to set name based on Kerberos authentication rules.
{code}
It would be helpful if the log contains information about the IOException.","[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2606,Minor,Ted Yu,Fixed,2016-10-17T08:48:51.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,SaslServerCallbackHandler#handleAuthorizeCallback() should log the exception,2017-03-31T09:01:15.000+0000,[],4.0
,[],2016-09-29T18:55:32.000+0000,Joe Wang,"Not sure if it's a bug, or just a consequence of a design decision.

Recently we had an issue where faulty clients were issuing create requests at an abnormally high rate, which caused zookeeper to generate more snapshots than our cron job could clean up. This filled up the disk on our zookeeper hosts and brought the cluster down.

Is there a reason why Zookeeper uses a write-ahead log instead only flushing successful transactions to disk? If only successful transactions are flushed and counted towards snapCount, then even if a client is spamming requests to create a node that already exists, it wouldn't cause a flood of snapshots to be persisted to disk.",[],Bug,ZOOKEEPER-2605,Minor,Joe Wang,Not A Problem,2016-09-29T19:20:39.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Snapshot generation fills up disk space due to high volume of requests.,2016-09-29T19:20:41.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",2.0
,[],2016-09-27T11:23:51.000+0000,sunqb,"i use zkclient to connect zookeeper-server , but sometime when i close my zkclient ,the Temporary node can't be deleted。i have search this bug,and i fixed it by delete the data dir","[<JIRA Version: name='3.4.6', id='12323310'>]",Bug,ZOOKEEPER-2604,Major,sunqb,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Temporary node has not been deleted,2018-03-14T07:22:46.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",4.0
,[],2016-09-26T10:48:43.000+0000,Mateusz Moneta,"Hello,

When you try to restart zookeeper via {{service zookeepr restart}} it ends in state:
{noformat}
root@m1:/etc/systemd# service zookeeper status
● zookeeper.service - LSB: Apache ZooKeeper server
   Loaded: loaded (/etc/init.d/zookeeper)
   Active: active (exited) since Mon 2016-09-26 10:38:47 UTC; 58s ago
  Process: 55495 ExecStop=/etc/init.d/zookeeper stop (code=exited, status=0/SUCCESS)
  Process: 55504 ExecStart=/etc/init.d/zookeeper start (code=exited, status=0/SUCCESS)
{noformat}

After that {{service zookeeper start}} won't work. Only way is to do {{service zookeeper stop}} and then {{start}} or {{service zookeepr --full-restart}} (which does basically the same).",[],Bug,ZOOKEEPER-2603,Minor,Mateusz Moneta,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Init script restart command broken on Debian Jessie,2016-09-26T10:48:43.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.4.9', id='12334700'>]",2.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2016-09-22T21:38:34.000+0000,Benjamin Reed,"we had the following strange production bug:

there was an ephemeral znode for a session that was no longer active.  it happened even in the absence of failures.

we are running with local sessions enabled and slightly different logic than the open source zookeeper, but code inspection shows that the problem is also in open source.

the triggering condition was server overload. we had a traffic burst and it we were having commit latencies of over 30 seconds.

after digging through logs/code we realized from the logs that the create session txn for the ephemeral node started (in the PrepRequestProcessor) at 11:23:04 and committed at 11:23:38 (the ""Adding global session"" is output in the commit processor). it took 34 seconds to commit the createSession, during that time the session expired. due to delays it appears that the interleave was as follows:

1) create session hits prep request processor and create session txn generated 11:23:04
2) time passes as the create session is going through zab
3) the session expires, close session is generated, and close session txn generated 11:23:23
4) the create session gets committed and the session gets re-added to the sessionTracker 11:23:38
5) the create ephemeral node hits prep request processor and a create txn generated 11:23:40
6) the close session gets committed (all ephemeral nodes for the session are deleted) and the session is deleted from sessionTracker
7) the create ephemeral node gets committed

the root cause seems to be that the gobal sessions are managed by both the PrepRequestProcessor and the CommitProcessor. also with the local session upgrading we can have changes in flight before our sessions commits. i think there are probably two places to fix:

1) changes to session tracker should not happen in prep request processor.
2) we should not have requests in flight while create session is in process. there are two options to prevent this:
a) when a create session is generated in makeUpgradeRequest, we need to start queuing the requests from the clients and only submit them once the create session is committed
b) the client should explicitly detect that it needs to change from local session to global session and explicitly open a global session and get the commit before it sends an ephemeral create request

option 2a) is a more transparent fix, but architecturally and in the long term i think 2b) might be better.",[],Bug,ZOOKEEPER-2600,Major,Benjamin Reed,Cannot Reproduce,2016-09-24T09:36:46.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,dangling ephemerals on overloaded server with local sessions,2016-09-24T15:46:13.000+0000,[],4.0
,[],2016-09-22T14:36:49.000+0000,Rakesh Kumar Singh,"Start an quorum with 3 (let say A, B, C) zookeepers, stop 2 zookeepers and let one running, install and configure new quorum (A, A2, A3, A4, A5) where A is common but now has configuration of new quorum. When start A, it getting synced the configuration with previous quorum

Steps to reproduce:-
1. Configure and start quorum of 3 nodes (A, B, C) -> 1st quorum
2. stop 2 nodes and let running 3rd node (say C)
3. Create new quorum of 5 nodes (A, A2, A3, A4, A5) where A has same IP and port which was used in 1st quorum but A's configuration is as per new quorum (where details of A, A2, A3, A4, A5) are present and not B & C.
4. Now start 2nd quorum. Here A's dynamic configuration is getting changed according to 1st quorum

Problems:-
1. Now A node is neither syncing all data with 1st quorum nor with 2nd quorum
2. Big security flaw and the whole quorum can be screwed",[],Bug,ZOOKEEPER-2599,Critical,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Quorum with 3 nodes, stop 2 nodes and let one running, install and configure new quorum where one node details is common but now has configuration of new quorum. common node getting synced the configuration with previous quorum",2016-09-22T14:36:49.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",3.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2016-09-21T18:01:09.000+0000,Srinivas Neginhal,"Steps to reproduce:
1. Create a three node cluster: Node1, Node2 and Node3.
    Each node is a VM that runs:
    1. ZK in a docker container
    2. Two clients, A and B that use ZK for group membership and leader election. The clients create sequential ephemeral nodes when they come up. 
2. The three ZK's running in the containers form an ensemble.
3. Power off/on Node 2 and Node 3 in a loop
4. After a few times, the ephemeral nodes seen by the three nodes are different.

Here is the output of some four letter commands with the ensemble in the state:

1. conf:

ZK 1:

# echo conf| nc 10.0.0.1 1300
clientPort=1300
secureClientPort=-1
dataDir=/moot/persistentStore/zkWorkspace/version-2
dataDirSize=67293721
dataLogDir=/moot/persistentStore/zkWorkspace/version-2
dataLogSize=67293721
tickTime=2000
maxClientCnxns=60
minSessionTimeout=4000
maxSessionTimeout=40000
serverId=1
initLimit=100
syncLimit=20
electionAlg=3
electionPort=1200
quorumPort=1100
peerType=0
membership: 
server.1=10.0.0.1:1100:1200:participant;10.0.0.1:1300;8e64c644-d0fa-414f-bab2-3c8c80364410
server.2=10.0.0.2:1100:1200:participant;10.0.0.2:1300;38bf19b8-d4cb-4dac-b328-7bbf0ee1e2c4
server.3=10.0.0.3:1100:1200:participant;10.0.0.3:1300;e1415d59-e857-43e6-ba9b-01daeb31a434


ZK 2:

# echo conf| nc 10.0.0.2 1300
clientPort=1300
secureClientPort=-1
dataDir=/moot/persistentStore/zkWorkspace/version-2
dataDirSize=1409480873
dataLogDir=/moot/persistentStore/zkWorkspace/version-2
dataLogSize=1409480873
tickTime=2000
maxClientCnxns=60
minSessionTimeout=4000
maxSessionTimeout=40000
serverId=2
initLimit=100
syncLimit=20
electionAlg=3
electionPort=1200
quorumPort=1100
peerType=0
membership: 
server.1=10.0.0.1:1100:1200:participant;10.0.0.1:1300;8e64c644-d0fa-414f-bab2-3c8c80364410
server.2=10.0.0.2:1100:1200:participant;10.0.0.2:1300;38bf19b8-d4cb-4dac-b328-7bbf0ee1e2c4
server.3=10.0.0.3:1100:1200:participant;10.0.0.3:1300;e1415d59-e857-43e6-ba9b-01daeb31a434

ZK 3:

# echo conf| nc 10.0.0.3 1300
clientPort=1300
secureClientPort=-1
dataDir=/moot/persistentStore/zkWorkspace/version-2
dataDirSize=1409505467
dataLogDir=/moot/persistentStore/zkWorkspace/version-2
dataLogSize=1409505467
tickTime=2000
maxClientCnxns=60
minSessionTimeout=4000
maxSessionTimeout=40000
serverId=3
initLimit=100
syncLimit=20
electionAlg=3
electionPort=1200
quorumPort=1100
peerType=0
membership: 
server.1=10.0.0.1:1100:1200:participant;10.0.0.1:1300;8e64c644-d0fa-414f-bab2-3c8c80364410
server.2=10.0.0.2:1100:1200:participant;10.0.0.2:1300;38bf19b8-d4cb-4dac-b328-7bbf0ee1e2c4
server.3=10.0.0.3:1100:1200:participant;10.0.0.3:1300;e1415d59-e857-43e6-ba9b-01daeb31a434


2. mntr:

ZK 1:

# echo mntr| nc 10.0.0.1 1300
zk_version 3.5.1-alpha--1, built on 09/07/2016 00:34 GMT
zk_avg_latency 0
zk_max_latency 471
zk_min_latency 0
zk_packets_received 32556
zk_packets_sent 32564
zk_num_alive_connections 7
zk_outstanding_requests 0
zk_server_state leader
zk_znode_count 58
zk_watch_count 51
zk_ephemerals_count 5
zk_approximate_data_size 5251
zk_open_file_descriptor_count 52
zk_max_file_descriptor_count 1048576
zk_followers 2
zk_synced_followers 2
zk_pending_syncs 0

ZK 2:

# echo mntr| nc 10.0.0.2 1300
zk_version 3.5.1-alpha--1, built on 09/07/2016 00:34 GMT
zk_avg_latency 1
zk_max_latency 227
zk_min_latency 0
zk_packets_received 30905
zk_packets_sent 30936
zk_num_alive_connections 6
zk_outstanding_requests 0
zk_server_state follower
zk_znode_count 58
zk_watch_count 82
zk_ephemerals_count 5
zk_approximate_data_size 5251
zk_open_file_descriptor_count 49
zk_max_file_descriptor_count 1048576

ZK 3:

# echo mntr| nc 10.0.0.3 1300
zk_version 3.5.1-alpha--1, built on 09/07/2016 00:34 GMT
zk_avg_latency 4
zk_max_latency 590
zk_min_latency 0
zk_packets_received 6192
zk_packets_sent 6191
zk_num_alive_connections 2
zk_outstanding_requests 0
zk_server_state follower
zk_znode_count 64
zk_watch_count 17
zk_ephemerals_count 11
zk_approximate_data_size 5806
zk_open_file_descriptor_count 45
zk_max_file_descriptor_count 1048576


3. dump showing the inconsistency:

ZK 1:

# echo dump| nc 10.0.0.1 1300
SessionTracker dump:
Session Sets (17)/(12):
0 expire at Tue Sep 20 18:22:35 UTC 2016:
0 expire at Tue Sep 20 18:22:37 UTC 2016:
0 expire at Tue Sep 20 18:22:39 UTC 2016:
0 expire at Tue Sep 20 18:22:41 UTC 2016:
0 expire at Tue Sep 20 18:22:43 UTC 2016:
0 expire at Tue Sep 20 18:22:45 UTC 2016:
0 expire at Tue Sep 20 18:22:49 UTC 2016:
0 expire at Tue Sep 20 18:22:51 UTC 2016:
0 expire at Tue Sep 20 18:22:53 UTC 2016:
0 expire at Tue Sep 20 18:22:55 UTC 2016:
0 expire at Tue Sep 20 18:22:57 UTC 2016:
4 expire at Tue Sep 20 18:22:59 UTC 2016:
 0x100061435f7000d
 0x10000d9e4460004
 0x100061435f70002
 0x10000d9e4460003
4 expire at Tue Sep 20 18:23:03 UTC 2016:
 0x2000001141a0002
 0x2000001141a0000
 0x2000001141a0005
 0x100061435f70010
1 expire at Tue Sep 20 18:23:07 UTC 2016:
 0x2000001141a0001
1 expire at Tue Sep 20 18:23:09 UTC 2016:
 0x100061435f70000
1 expire at Tue Sep 20 18:23:11 UTC 2016:
 0x2000001141a000f
1 expire at Tue Sep 20 18:23:13 UTC 2016:
 0x300000188c30001
ephemeral nodes dump:
Sessions with Ephemerals (5):
0x100061435f70000:
 /moot/gmle/ServiceDirectory/ActiveNodes/member0000000064
0x2000001141a000f:
 /moot/gmle/ServiceDirectory/ActiveNodes/member0000000066
0x2000001141a0001:
 /moot/gmle/ServiceDirectory/ActiveNodes/member0000000065
0x2000001141a0000:
 /moot/gmle/ActiveControllerCluster/member0000000065
0x2000001141a0005:
 /moot/gmle/ActiveControllerCluster/member0000000066
Connections dump:
Connections Sets (5)/(10):
0 expire at Tue Sep 20 18:22:35 UTC 2016:
1 expire at Tue Sep 20 18:22:45 UTC 2016:
 ip: /10.0.0.1:45591 sessionId: 0x0
0 expire at Tue Sep 20 18:22:55 UTC 2016:
5 expire at Tue Sep 20 18:23:05 UTC 2016:
 ip: /10.0.0.3:34734 sessionId: 0x100061435f7000d
 ip: /10.0.0.1:42963 sessionId: 0x10000d9e4460003
 ip: /10.0.0.3:34739 sessionId: 0x100061435f70010
 ip: /10.0.0.2:45750 sessionId: 0x100061435f70002
 ip: /10.0.0.1:42961 sessionId: 0x10000d9e4460004
1 expire at Tue Sep 20 18:23:15 UTC 2016:
 ip: /10.0.0.1:42964 sessionId: 0x100061435f70000

ZK 2:

# echo dump| nc 10.0.0.2 1300
SessionTracker dump:
Global Sessions(13):
0x10000d9e4460003 30000ms
0x10000d9e4460004 30000ms
0x100061435f70000 40000ms
0x100061435f70002 30000ms
0x100061435f7000d 30000ms
0x100061435f70010 30000ms
0x100061435f70584 4000ms
0x2000001141a0000 40000ms
0x2000001141a0001 40000ms
0x2000001141a0002 30000ms
0x2000001141a0005 40000ms
0x2000001141a000f 40000ms
0x300000188c30001 40000ms
ephemeral nodes dump:
Sessions with Ephemerals (5):
0x100061435f70000:
 /moot/gmle/ServiceDirectory/ActiveNodes/member0000000064
0x2000001141a000f:
 /moot/gmle/ServiceDirectory/ActiveNodes/member0000000066
0x2000001141a0001:
 /moot/gmle/ServiceDirectory/ActiveNodes/member0000000065
0x2000001141a0000:
 /moot/gmle/ActiveControllerCluster/member0000000065
0x2000001141a0005:
 /moot/gmle/ActiveControllerCluster/member0000000066
Connections dump:
Connections Sets (4)/(6):
0 expire at Tue Sep 20 18:25:13 UTC 2016:
1 expire at Tue Sep 20 18:25:23 UTC 2016:
 ip: /10.0.0.2:38021 sessionId: 0x0
1 expire at Tue Sep 20 18:25:33 UTC 2016:
 ip: /10.0.0.2:35422 sessionId: 0x2000001141a0002
4 expire at Tue Sep 20 18:25:43 UTC 2016:
 ip: /10.0.0.2:35419 sessionId: 0x2000001141a0001
 ip: /10.0.0.1:59025 sessionId: 0x2000001141a0000
 ip: /10.0.0.2:35427 sessionId: 0x2000001141a0005
 ip: /10.0.0.3:56967 sessionId: 0x2000001141a000f

ZK 3:

# echo dump| nc 10.0.0.3 1300
SessionTracker dump:
Global Sessions(23):
0x10000d9e4460003 30000ms
0x10000d9e4460004 30000ms
0x100055a50b00001 30000ms
0x100055a50b00003 40000ms
0x100055a50b0000c 40000ms
0x100061435f70000 40000ms
0x100061435f70002 30000ms
0x100061435f7000d 30000ms
0x100061435f70010 30000ms
0x100061435f70585 4000ms
0x2000001141a0000 40000ms
0x2000001141a0001 40000ms
0x2000001141a0002 30000ms
0x2000001141a0005 40000ms
0x2000001141a000f 40000ms
0x200000130750000 40000ms
0x200000130750001 40000ms
0x200000130750002 30000ms
0x200000130750004 40000ms
0x20000013075000d 30000ms
0x3000000e4860000 30000ms
0x3000000e4860002 40000ms
0x300000188c30001 40000ms
ephemeral nodes dump:
Sessions with Ephemerals (11):
0x100061435f70000:
 /moot/gmle/ServiceDirectory/ActiveNodes/member0000000064
0x3000000e4860002:
 /moot/gmle/ActiveControllerCluster/member0000000027
0x100055a50b0000c:
 /moot/gmle/ServiceDirectory/ActiveNodes/member0000000027
0x100055a50b00003:
 /moot/gmle/ActiveControllerCluster/member0000000025
0x200000130750004:
 /moot/gmle/ActiveControllerCluster/member0000000026
0x200000130750000:
 /moot/gmle/ServiceDirectory/ActiveNodes/member0000000026
0x2000001141a000f:
 /moot/gmle/ServiceDirectory/ActiveNodes/member0000000066
0x200000130750001:
 /moot/gmle/ServiceDirectory/ActiveNodes/member0000000025
0x2000001141a0001:
 /moot/gmle/ServiceDirectory/ActiveNodes/member0000000065
0x2000001141a0000:
 /moot/gmle/ActiveControllerCluster/member0000000065
0x2000001141a0005:
 /moot/gmle/ActiveControllerCluster/member0000000066
Connections dump:
Connections Sets (4)/(2):
0 expire at Tue Sep 20 18:25:40 UTC 2016:
1 expire at Tue Sep 20 18:25:50 UTC 2016:
 ip: /10.0.0.3:52784 sessionId: 0x0
0 expire at Tue Sep 20 18:26:10 UTC 2016:
1 expire at Tue Sep 20 18:26:20 UTC 2016:
 ip: /10.0.0.3:50222 sessionId: 0x300000188c30001",[],Bug,ZOOKEEPER-2598,Major,Srinivas Neginhal,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Data Inconsistency after power off/on of some nodes,2016-10-17T10:46:22.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",4.0
,"[<JIRA Component: name='c client', id='12312380'>]",2016-09-20T14:10:59.000+0000,Scott Thompson,"Nodes fail to connect when a sub-domain is present in the FQDN.

The sub-domain is dropped from the hostname string when calling gethostname in zookeeper.c.

machine.sub.domain.com
becomes
machine.domain.com

#ifdef HAVE_GETHOSTNAME
  gethostname(buf, sizeof(buf));
  LOG_INFO(LOGCALLBACK(zh), ""Client environment:host.name=%s"", buf);",[],Bug,ZOOKEEPER-2596,Minor,Scott Thompson,Invalid,2016-09-20T21:11:43.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper.c - gethostname drops subdomain returning only partial FQDN,2016-09-21T08:28:40.000+0000,[],2.0
,[],2016-09-20T13:13:17.000+0000,Neha Bathra,"user1 sets ACL on one znode for user 2
example :
create /xyz data sasl:user2/xyz@XYZ.COM:cdr


now user3 can login to zkCli and delete /xyz if it has no children nodes, even when it does not have access",[],Bug,ZOOKEEPER-2595,Major,Neha Bathra,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"znode created with acl enabled on it can be deleted by any unauthorised user, when it has no child znodes",2016-09-20T13:15:06.000+0000,[],2.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-09-19T15:01:15.000+0000,Rakesh Kumar Singh,"Zookeeper is not recoverable once running system( machine on which zookeeper is running) is out of space 
Steps to reproduce:-
1. Install zookeeper on standalone mode and start zookeeper
2. Make the machine physical memory full
3. Connect through client to zookeeper and trying create some znodes with some data.
4. After sometime creating further znode will not happened as complete memory is occupied
5. Now start creating space in that machine
6. Again connect through a client. Connection is fine. Now try to execute any command like ""ls / "" it fails even though now space is more than 11gb

Client log:-
BLR1000007042:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin # df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvda2       36G   24G   11G  70% /
udev            1.9G  116K  1.9G   1% /dev
tmpfs           1.9G     0  1.9G   0% /dev/shm
BLR1000007042:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin # ./zkCli.sh
Connecting to localhost:2181
2016-09-19 22:50:20,227 [myid:] - INFO  [main:Environment@109] - Client environment:zookeeper.version=3.5.1-alpha--1, built on 08/18/2016 08:20 GMT
2016-09-19 22:50:20,231 [myid:] - INFO  [main:Environment@109] - Client environment:host.name=BLR1000007042
2016-09-19 22:50:20,231 [myid:] - INFO  [main:Environment@109] - Client environment:java.version=1.7.0_79
2016-09-19 22:50:20,234 [myid:] - INFO  [main:Environment@109] - Client environment:java.vendor=Oracle Corporation
2016-09-19 22:50:20,234 [myid:] - INFO  [main:Environment@109] - Client environment:java.home=/usr/java/jdk1.7.0_79/jre
2016-09-19 22:50:20,234 [myid:] - INFO  [main:Environment@109] - Client environment:java.class.path=/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../build/classes:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../build/lib/*.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/slf4j-log4j12-1.7.5.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/slf4j-api-1.7.5.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/servlet-api-2.5-20081211.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/netty-3.7.0.Final.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/log4j-1.2.16.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/jline-2.11.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/jetty-util-6.1.26.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/jetty-6.1.26.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/javacc.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/jackson-mapper-asl-1.9.11.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/jackson-core-asl-1.9.11.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/commons-cli-1.2.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/ant-eclipse-1.0-jvm1.2.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../zookeeper-3.5.1-alpha.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../src/java/lib/ant-eclipse-1.0-jvm1.2.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../conf:/usr/java/jdk1.7.0_79/lib
2016-09-19 22:50:20,234 [myid:] - INFO  [main:Environment@109] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2016-09-19 22:50:20,234 [myid:] - INFO  [main:Environment@109] - Client environment:java.io.tmpdir=/tmp
2016-09-19 22:50:20,234 [myid:] - INFO  [main:Environment@109] - Client environment:java.compiler=<NA>
2016-09-19 22:50:20,235 [myid:] - INFO  [main:Environment@109] - Client environment:os.name=Linux
2016-09-19 22:50:20,235 [myid:] - INFO  [main:Environment@109] - Client environment:os.arch=amd64
2016-09-19 22:50:20,235 [myid:] - INFO  [main:Environment@109] - Client environment:os.version=3.0.76-0.11-default
2016-09-19 22:50:20,235 [myid:] - INFO  [main:Environment@109] - Client environment:user.name=root
2016-09-19 22:50:20,235 [myid:] - INFO  [main:Environment@109] - Client environment:user.home=/root
2016-09-19 22:50:20,235 [myid:] - INFO  [main:Environment@109] - Client environment:user.dir=/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin
2016-09-19 22:50:20,235 [myid:] - INFO  [main:Environment@109] - Client environment:os.memory.free=52MB
2016-09-19 22:50:20,237 [myid:] - INFO  [main:Environment@109] - Client environment:os.memory.max=227MB
2016-09-19 22:50:20,238 [myid:] - INFO  [main:Environment@109] - Client environment:os.memory.total=57MB
2016-09-19 22:50:20,241 [myid:] - INFO  [main:ZooKeeper@716] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@3865db85
Welcome to ZooKeeper!
2016-09-19 22:50:20,264 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1138] - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2016-09-19 22:50:20,270 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@980] - Socket connection established, initiating session, client: /127.0.0.1:47801, server: localhost/127.0.0.1:2181
JLine support is enabled
[INFO] Unable to bind key for unsupported operation: backward-delete-word
[INFO] Unable to bind key for unsupported operation: backward-delete-word
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[zk: localhost:2181(CONNECTING) 0] ls /
2016-09-19 22:50:35,280 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1251] - Client session timed out, have not heard from server in 15011ms for sessionid 0x0, closing socket connection and attempting reconnect
Exception in thread ""main"" org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:2255)
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:2283)
	at org.apache.zookeeper.cli.LsCommand.exec(LsCommand.java:93)
	at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:674)
	at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:577)
	at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:360)
	at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:320)
	at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:280)
BLR1000007042:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin # 


-{color:blue} Server log

2016-09-19 22:34:13,380 [myid:] - INFO  [main:QuorumPeerConfig@114] - Reading configuration from: /home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../conf/zoo.cfg
2016-09-19 22:34:13,386 [myid:] - INFO  [main:QuorumPeerConfig@316] - clientPortAddress is 0.0.0.0/0.0.0.0:2181
2016-09-19 22:34:13,386 [myid:] - INFO  [main:QuorumPeerConfig@320] - secureClientPort is not set
2016-09-19 22:34:13,389 [myid:] - INFO  [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3
2016-09-19 22:34:13,389 [myid:] - INFO  [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 0
2016-09-19 22:34:13,390 [myid:] - INFO  [main:DatadirCleanupManager@101] - Purge task is not scheduled.
2016-09-19 22:34:13,390 [myid:] - WARN  [main:QuorumPeerMain@122] - Either no config or no quorum defined in config, running  in standalone mode
2016-09-19 22:34:13,402 [myid:] - INFO  [main:QuorumPeerConfig@114] - Reading configuration from: /home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../conf/zoo.cfg
2016-09-19 22:34:13,402 [myid:] - INFO  [main:QuorumPeerConfig@316] - clientPortAddress is 0.0.0.0/0.0.0.0:2181
2016-09-19 22:34:13,402 [myid:] - INFO  [main:QuorumPeerConfig@320] - secureClientPort is not set
2016-09-19 22:34:13,403 [myid:] - INFO  [main:ZooKeeperServerMain@113] - Starting server
2016-09-19 22:34:13,416 [myid:] - INFO  [main:Environment@109] - Server environment:zookeeper.version=3.5.1-alpha--1, built on 08/18/2016 08:20 GMT
2016-09-19 22:34:13,416 [myid:] - INFO  [main:Environment@109] - Server environment:host.name=BLR1000007042
2016-09-19 22:34:13,416 [myid:] - INFO  [main:Environment@109] - Server environment:java.version=1.7.0_79
2016-09-19 22:34:13,417 [myid:] - INFO  [main:Environment@109] - Server environment:java.vendor=Oracle Corporation
2016-09-19 22:34:13,417 [myid:] - INFO  [main:Environment@109] - Server environment:java.home=/usr/java/jdk1.7.0_79/jre
2016-09-19 22:34:13,419 [myid:] - INFO  [main:Environment@109] - Server environment:java.class.path=/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../build/classes:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../build/lib/*.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/slf4j-log4j12-1.7.5.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/slf4j-api-1.7.5.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/servlet-api-2.5-20081211.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/netty-3.7.0.Final.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/log4j-1.2.16.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/jline-2.11.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/jetty-util-6.1.26.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/jetty-6.1.26.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/javacc.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/jackson-mapper-asl-1.9.11.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/jackson-core-asl-1.9.11.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/commons-cli-1.2.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/ant-eclipse-1.0-jvm1.2.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../zookeeper-3.5.1-alpha.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../src/java/lib/ant-eclipse-1.0-jvm1.2.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../conf:/usr/java/jdk1.7.0_79/lib
2016-09-19 22:34:13,420 [myid:] - INFO  [main:Environment@109] - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2016-09-19 22:34:13,420 [myid:] - INFO  [main:Environment@109] - Server environment:java.io.tmpdir=/tmp
2016-09-19 22:34:13,420 [myid:] - INFO  [main:Environment@109] - Server environment:java.compiler=<NA>
2016-09-19 22:34:13,420 [myid:] - INFO  [main:Environment@109] - Server environment:os.name=Linux
2016-09-19 22:34:13,420 [myid:] - INFO  [main:Environment@109] - Server environment:os.arch=amd64
2016-09-19 22:34:13,421 [myid:] - INFO  [main:Environment@109] - Server environment:os.version=3.0.76-0.11-default
2016-09-19 22:34:13,421 [myid:] - INFO  [main:Environment@109] - Server environment:user.name=root
2016-09-19 22:34:13,421 [myid:] - INFO  [main:Environment@109] - Server environment:user.home=/root
2016-09-19 22:34:13,421 [myid:] - INFO  [main:Environment@109] - Server environment:user.dir=/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin
2016-09-19 22:34:13,421 [myid:] - INFO  [main:Environment@109] - Server environment:os.memory.free=51MB
2016-09-19 22:34:13,422 [myid:] - INFO  [main:Environment@109] - Server environment:os.memory.max=889MB
2016-09-19 22:34:13,422 [myid:] - INFO  [main:Environment@109] - Server environment:os.memory.total=57MB
2016-09-19 22:34:13,424 [myid:] - INFO  [main:ZooKeeperServer@858] - minSessionTimeout set to 4000
2016-09-19 22:34:13,424 [myid:] - INFO  [main:ZooKeeperServer@867] - maxSessionTimeout set to 40000
2016-09-19 22:34:13,424 [myid:] - INFO  [main:ZooKeeperServer@156] - Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 datadir /home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/zoo_log/version-2 snapdir /home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/data/version-2
2016-09-19 22:34:13,453 [myid:] - INFO  [main:Slf4jLog@67] - Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2016-09-19 22:34:13,477 [myid:] - INFO  [main:Slf4jLog@67] - jetty-6.1.26
2016-09-19 22:34:13,510 [myid:] - INFO  [main:Slf4jLog@67] - Started SelectChannelConnector@0.0.0.0:8080
2016-09-19 22:34:13,514 [myid:] - INFO  [main:JettyAdminServer@105] - Started AdminServer on address 0.0.0.0, port 8080 and command URL /commands
2016-09-19 22:34:13,521 [myid:] - INFO  [main:NIOServerCnxnFactory@673] - Configuring NIO connection handler with 10s sessionless connection timeout, 1 selector thread(s), 8 worker threads, and 64 kB direct buffers.
2016-09-19 22:34:13,523 [myid:] - INFO  [main:NIOServerCnxnFactory@686] - binding to port 0.0.0.0/0.0.0.0:2181
2016-09-19 22:34:13,537 [myid:] - INFO  [main:FileTxnSnapLog@298] - Snapshotting: 0x0 to /home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/data/version-2/snapshot.0
2016-09-19 22:34:13,567 [myid:] - INFO  [main:ContainerManager@64] - Using checkIntervalMs=60000 maxPerMinute=10000
2016-09-19 22:35:41,907 [myid:] - INFO  [NIOServerCxnFactory.AcceptThread:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory$AcceptThread@296] - Accepted socket connection from /0:0:0:0:0:0:0:1:49485
2016-09-19 22:35:41,917 [myid:] - INFO  [NIOWorkerThread-1:ZooKeeperServer@964] - Client attempting to establish new session at /0:0:0:0:0:0:0:1:49485
2016-09-19 22:35:41,919 [myid:] - INFO  [SyncThread:0:FileTxnLog@200] - Creating new log file: log.1
2016-09-19 22:35:41,952 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@678] - Established session 0x100632436270000 with negotiated timeout 30000 for client /0:0:0:0:0:0:0:1:49485
2016-09-19 22:40:21,211 [myid:] - INFO  [NIOServerCxnFactory.AcceptThread:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory$AcceptThread@296] - Accepted socket connection from /10.18.221.194:34892
2016-09-19 22:40:21,218 [myid:] - INFO  [NIOWorkerThread-8:ZooKeeperServer@964] - Client attempting to establish new session at /10.18.221.194:34892
2016-09-19 22:40:21,221 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@678] - Established session 0x100632436270001 with negotiated timeout 30000 for client /10.18.221.194:34892
2016-09-19 22:40:40,298 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@649] - Processed session termination for sessionid: 0x100632436270001
2016-09-19 22:40:40,301 [myid:] - INFO  [NIOWorkerThread-3:MBeanRegistry@119] - Unregister MBean [org.apache.ZooKeeperService:name0=StandaloneServer_port2181,name1=Connections,name2=10.18.221.194,name3=0x100632436270001]
2016-09-19 22016-09-19 22:43:47,733 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@498] - shutting down
2016-09-19 22:44:39,892 [myid:] - INFO  [NIOServerCxnFactory.AcceptThread:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory$AcceptThread@296] - Accepted socket connection from /127.0.0.1:47796
2016-09-19 22:44:39,898 [myid:] - INFO  [NIOWorkerThread-2:ZooKeeperServer@964] - Client attempting to establish new session at /127.0.0.1:47796
2016-09-19 22:45:15,883 [myid:] - INFO  [NIOServerCxnFactory.AcceptThread:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory$AcceptThread@296] - Accepted socket connection from /0:0:0:0:0:0:0:1:49493
2016-09-19 22:45:15,890 [myid:] - INFO  [NIOWorkerThread-3:ZooKeeperServer@964] - Client attempting to establish new session at /0:0:0:0:0:0:0:1:49493
2016-09-19 22:45:16,000 [myid:] - INFO  [ConnnectionExpirer:NIOServerCnxn@606] - Closed socket connection for client /127.0.0.1:47796 which had sessionid 0x100632436270012
2016-09-19 22:45:46,000 [myid:] - INFO  [ConnnectionExpirer:NIOServerCnxn@606] - Closed socket connection for client /0:0:0:0:0:0:0:1:49493 which had sessionid 0x100632436270013
2016-09-19 22:47:42,512 [myid:] - INFO  [NIOServerCxnFactory.AcceptThread:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory$AcceptThread@296] - Accepted socket connection from /0:0:0:0:0:0:0:1:49494
2016-09-19 22:47:42,519 [myid:] - INFO  [NIOWorkerThread-4:ZooKeeperServer@964] - Client attempting to establish new session at /0:0:0:0:0:0:0:1:49494
2016-09-19 22:48:16,001 [myid:] - INFO  [ConnnectionExpirer:NIOServerCnxn@606] - Closed socket connection for client /0:0:0:0:0:0:0:1:49494 which had sessionid 0x100632436270014
2016-09-19 22:50:20,268 [myid:] - INFO  [NIOServerCxnFactory.AcceptThread:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory$AcceptThread@296] - Accepted socket connection from /127.0.0.1:47801
2016-09-19 22:50:20,275 [myid:] - INFO  [NIOWorkerThread-5:ZooKeeperServer@964] - Client attempting to establish new session at /127.0.0.1:47801
2016-09-19 22:50:56,000 [myid:] - INFO  [ConnnectionExpirer:NIOServerCnxn@606] - Closed socket connection for client /127.0.0.1:47801 which had sessionid 0x100632436270015

{color}",[],Bug,ZOOKEEPER-2592,Critical,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper is not recoverable once running system( machine on which zookeeper is running) is out of space,2018-11-22T01:04:02.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>]",7.0
Edward Ribeiro,"[<JIRA Component: name='security', id='12329414'>, <JIRA Component: name='server', id='12312382'>]",2016-09-18T02:16:31.000+0000,Edward Ribeiro,"Container nodes check the ACL before creation, but the deletion doesn't check  the ACL rights. The code below succeeds even tough we removed ACL access permissions for ""/a"".

{code}
        zk.create(""/a"", null, Ids.OPEN_ACL_UNSAFE, CreateMode.CONTAINER);
        ArrayList<ACL> list = new ArrayList<>();
        list.add(new ACL(0, Ids.ANYONE_ID_UNSAFE));
        zk.setACL(""/"", list, -1);

        zk.delete(""/a"", -1);
{code}",[],Bug,ZOOKEEPER-2591,Major,Edward Ribeiro,Not A Bug,2016-09-19T14:41:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,The deletion of Container znode doesn't check ACL delete permission,2019-03-19T12:45:47.000+0000,[],6.0
Ling Mao,[],2016-09-18T01:51:30.000+0000,Edward Ribeiro,"As hinted  [here|https://github.com/apache/zookeeper/blob/master/src/java/main/org/apache/zookeeper/server/FinalRequestProcessor.java#L298], even if a parent znode path has restricted READ access it's possible to issue an exists() operation on any child znode of that given path.

 For example, the snippet below doesn't throw {{NoAuthExceptio}}, even tough it removes ACL rights to ""/"":

{code}
        zk.create(""/a"", null, Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
        ArrayList<ACL> acls = new ArrayList<>();
        acls.add(new ACL(0, Ids.ANYONE_ID_UNSAFE));

        zk.setACL(""/"", acls, -1);

        Stat r = zk.exists(""/a"", false);
{code}

Also, in the above example, what if the removed READ access for ""/a""? Should we allow a call to exists(""/a"") to succeed even if it returns the znode metadata info?",[],Bug,ZOOKEEPER-2590,Major,Edward Ribeiro,,,This issue is being actively worked on at the moment by the assignee.,In Progress,0.0,exists() should check read ACL permission,2020-03-30T02:51:46.000+0000,[],2.0
,[],2016-09-17T06:18:26.000+0000,Rakesh Kumar Singh,"Not able to access znode if  IP ACL is set on a znode when zookeeper started in ssl mode.

Steps to reproduce:-
1. Start zookeeper in SSL (standalone) mode
2. Create a znode
3. set ip ACL and connect the zkCli and try to access, it does not allow.

[zk: localhost:2181(CONNECTED) 3] setAcl /test ip:127.0.0.1:crdwa
[zk: localhost:2181(CONNECTED) 5] quit

>> start the zkCli with 127.0.0.1 and trying access the znode
[zk: 127.0.0.1:2181(CONNECTED) 0] get -s /test
Authentication is not valid : /test
[zk: 127.0.0.1:2181(CONNECTED) 1] getAcl /test
'ip,'127.0.0.1
: cdrwa
[zk: 127.0.0.1:2181(CONNECTED) 2] get /test
Authentication is not valid : /test
",[],Bug,ZOOKEEPER-2589,Major,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Not able to access znode if  IP ACL is set on a znode when zookeeper started in ssl mode,2016-09-21T05:12:37.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",3.0
Rakesh Kumar Singh,"[<JIRA Component: name='java client', id='12312381'>]",2016-09-16T05:44:37.000+0000,Rakesh Kumar Singh,"[zk: localhost:2181(CONNECTED) 2] redo -1
Exception in thread ""main"" java.lang.NullPointerException
	at java.util.StringTokenizer.<init>(StringTokenizer.java:199)
	at java.util.StringTokenizer.<init>(StringTokenizer.java:221)
	at org.apache.zookeeper.ZooKeeperMain$MyCommandOptions.parseCommand(ZooKeeperMain.java:219)
	at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:638)
	at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:577)
	at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:360)
	at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:320)
	at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:280)",[],Bug,ZOOKEEPER-2587,Minor,Rakesh Kumar Singh,Duplicate,2016-10-06T11:31:15.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Not handled negative scenario for redo command,2016-10-06T11:31:16.000+0000,[],2.0
,[],2016-09-16T04:35:18.000+0000,prashant,"three issues 
1)zoo_aremove_watchers() does not remove a watch if path has more than one watch.
but it works in below cases.
it removes watch if path has only one watch.
and it removes all watches if watcher function arguments is NULL.
Seen in 
zookeeper.version=3.5.1-alpha--1, built on 06/09/2016 18:31 GMT
Not sure if this is fixed in later versions.

2) If zoo_aremove_watchers()  is called with local=1, then client hangs in waiting for mutex in mt_adaptor.c:102

void notify_sync_completion(struct sync_completion *sc)
{
    pthread_mutex_lock(&sc->lock);
...

3) Acts like sync API if no node and no watcher on path.
it does not call async completion callback in this case.
",[],Bug,ZOOKEEPER-2586,Major,prashant,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zoo_aremove_watchers() does not remove a watch of path has more than one watch,2018-12-16T14:46:23.000+0000,[],3.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-09-15T07:16:03.000+0000,Rakesh Kumar Singh,"Set ACL with SSL is not working

Steps to reproduce:-
1. Start zookeeper in ssl mode in standalone
2. Connect zookeeper from zookeeper client (using zkCli.sh)
3. add auth and set ACL as below and then quit the client :-

[zk: localhost:2181(CONNECTED) 0] addauth digest u1:p1
[zk: localhost:2181(CONNECTED) 1] create /test_auth hello
Created /test_auth
[zk: localhost:2181(CONNECTED) 2] setAcl /test_auth auth:u1:p1:crdwa
[zk: localhost:2181(CONNECTED) 3] get /test_auth
hello
[zk: localhost:2181(CONNECTED) 4] quit

4. Connect again zookeeper from zookeeper client (using zkCli.sh)
5. Try to access the znode, try to set the data and so on, everything is allowed

[zk: localhost:2181(CONNECTED) 2] set /test_auth hello1
[zk: localhost:2181(CONNECTED) 3] get /test_auth
hello1
[zk: localhost:2181(CONNECTED) 1] getAcl /test_auth
'x509,'CN=locahost%2COU=CS%2CO=HUAWEI%2CL=Shenzhen%2CST=Guangdong%2CC=CHINA
: cdrwa
'digest,'u1:fpT/y03U+EjItKZOSLGvjnJlyng=
: cdrwa",[],Bug,ZOOKEEPER-2585,Critical,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ACL with SSL is not working,2016-09-21T08:48:30.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",3.0
Rakesh Kumar Singh,"[<JIRA Component: name='server', id='12312382'>]",2016-09-15T04:48:17.000+0000,Rakesh Kumar Singh,"when setquota for a znode and set ip/user ACL on /zookeeper/quota, still able to delete the quota from client with another ip though it says ""Authentication is not valid""


>> Set quota and ip ACL from one client (with IP 10.18.101.80)
[zk: 10.18.101.80:2181(CONNECTED) 9] setquota -n 10 /test
[zk: 10.18.101.80:2181(CONNECTED) 10] setAcl /zookeeper/quota ip:10.18.101.80:crdwa
[zk: 10.18.101.80:2181(CONNECTED) 11] 


>> Try to delete the set quota using different client(with ip 10.18.219.50)
[zk: 10.18.219.50:2181(CONNECTED) 22] listquota /test
absolute path is /zookeeper/quota/test/zookeeper_limits
Output quota for /test count=10,bytes=-1
Output stat for /test count=1,bytes=5
[zk: 10.18.219.50:2181(CONNECTED) 23] delquota /test
Authentication is not valid : /zookeeper/quota/test
[zk: 10.18.219.50:2181(CONNECTED) 24] listquota /test
absolute path is /zookeeper/quota/test/zookeeper_limits
quota for /test does not exist.

>> Here quota has been deleted though it is saying ""Authentication is not valid.."" which is not correct.
Now try to set the quota from another ip itself, it fails which is as expected

[zk: 10.18.219.50:2181(CONNECTED) 25] setquota -n 10 /test
Authentication is not valid : /zookeeper/quota/test
[zk: 10.18.219.50:2181(CONNECTED) 26] listquota /test
absolute path is /zookeeper/quota/test/zookeeper_limits
quota for /test does not exist.


>> Sameway when we set user ACL...
[zk: 10.18.101.80:2181(CONNECTED) 26] addauth digest user:pass
[zk: 10.18.101.80:2181(CONNECTED) 27] create /test hello
Node already exists: /test
[zk: 10.18.101.80:2181(CONNECTED) 28] delete /test
[zk: 10.18.101.80:2181(CONNECTED) 29] create /test hello
Created /test
[zk: 10.18.101.80:2181(CONNECTED) 30] 
[zk: 10.18.101.80:2181(CONNECTED) 30] setquota -n 10 /test
[zk: 10.18.101.80:2181(CONNECTED) 31] setAcl /zookeeper/quota auth:user:pass:crdwa
[zk: 10.18.101.80:2181(CONNECTED) 32] 


[zk: 10.18.219.50:2181(CONNECTED) 27] listquota /test
absolute path is /zookeeper/quota/test/zookeeper_limits
Output quota for /test count=10,bytes=-1
Output stat for /test count=1,bytes=5
[zk: 10.18.219.50:2181(CONNECTED) 28] delquota /test
Authentication is not valid : /zookeeper/quota/test
[zk: 10.18.219.50:2181(CONNECTED) 29] listquota /test
absolute path is /zookeeper/quota/test/zookeeper_limits
quota for /test does not exist.
[zk: 10.18.219.50:2181(CONNECTED) 30]",[],Bug,ZOOKEEPER-2584,Major,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"when setquota for a znode and set ip/user ACL on /zookeeper/quota, still able to delete the quota from client with another ip though it says ""Authentication is not valid""",2019-05-28T11:03:05.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",2.0
Rakesh Kumar Singh,"[<JIRA Component: name='server', id='12312382'>]",2016-09-14T14:30:51.000+0000,Rakesh Kumar Singh,"Using one client able to access the znode with localhost but fails from another client when IP ACL is set for znode using 127.0.0.1


Start zookeeper in cluster mode.

Client 1 :-

[zk: localhost:2181(CONNECTED) 11] create /ip_test hello
Created /ip_test
[zk: localhost:2181(CONNECTED) 12] setAcl /ip_test
ip_test    ip_test4   
[zk: localhost:2181(CONNECTED) 12] setAcl /ip_test ip:127.0.0.1:crdwa
[zk: localhost:2181(CONNECTED) 13] get /ip_test
hello
[zk: localhost:2181(CONNECTED) 14] set /ip_test hi
[zk: localhost:2181(CONNECTED) 15] 


Client 2 :-

[zk: localhost:2181(CONNECTED) 0] get /ip_test
Authentication is not valid : /ip_test
[zk: localhost:2181(CONNECTED) 1] getAcl /ip_test
'ip,'127.0.0.1
: cdrwa

[zk: localhost:2181(CONNECTED) 2] quit

now quit the client connection and connect again using 127.0.0.1 (like :- ./zkCli.sh -server 127.0.0.1:2181)

[zk: 127.0.0.1:2181(CONNECTED) 0] get /ip_test
hi",[],Bug,ZOOKEEPER-2583,Minor,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Using one client able to access the znode with localhost but fails from another client when IP ACL is set for znode using 127.0.0.1,2016-09-23T05:04:09.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-09-14T13:38:58.000+0000,Rakesh Kumar Singh,"When addauth twice for same user but different password, it is adding 2 digest corresponding to both username, password and so we can able to access znode with user and any of these password which does not seem to be correct

Steps:-
[zk: localhost:2181(CONNECTED) 0] addauth digest user1:pass1
[zk: localhost:2181(CONNECTED) 1] addauth digest user1:pass

[zk: localhost:2181(CONNECTED) 9] create /user_test5 hello
Created /user_test5
[zk: localhost:2181(CONNECTED) 10] setAcl /user_test5 auth:user1:pass1:crdwa
[zk: localhost:2181(CONNECTED) 11] getAcl /user_test5
'digest,'user1:+7K83PhyQ3ijGj0ADmljf0quVwQ=
: cdrwa
'digest,'user1:UZIsvOKp29j8vAahJzjgpA1VTOk=
: cdrwa


Here we can see 2 entries for same user (user1) with different password

Now disconnect the client and connect again using zkCli.sh
addauth digest user1:<any of 2 password>, we can able to access the znode.

[zk: localhost:2181(CONNECTED) 0] get /user_test5
Authentication is not valid : /user_test5
[zk: localhost:2181(CONNECTED) 1] addauth digest user1:pass
[zk: localhost:2181(CONNECTED) 2] get /user_test5
hello

Same way, it will allow n number of entry if we addauth for same user with n number of password
",[],Bug,ZOOKEEPER-2582,Major,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"When addauth twice for same user but different password, it is adding 2 digest corresponding to both username, password and so we can able to access znode with user and any of these password which does not seem to be correct",2016-09-20T19:29:46.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",3.0
Ling Mao,"[<JIRA Component: name='server', id='12312382'>]",2016-09-14T07:12:58.000+0000,Rakesh Kumar Singh,"Not handled NullPointerException while creating key manager and trustManager:-


2016-09-14 13:35:23,488 [myid:1] - ERROR [CommitProcWorkThread-1:X509AuthenticationProvider@78] - Failed to create key manager
org.apache.zookeeper.common.X509Exception$KeyManagerException: java.lang.NullPointerException
	at org.apache.zookeeper.common.X509Util.createKeyManager(X509Util.java:129)
	at org.apache.zookeeper.server.auth.X509AuthenticationProvider.<init>(X509AuthenticationProvider.java:75)
	at org.apache.zookeeper.server.auth.ProviderRegistry.initialize(ProviderRegistry.java:42)
	at org.apache.zookeeper.server.auth.ProviderRegistry.getProvider(ProviderRegistry.java:68)
	at org.apache.zookeeper.server.PrepRequestProcessor.checkACL(PrepRequestProcessor.java:319)
	at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:324)
	at org.apache.zookeeper.server.quorum.CommitProcessor$CommitWorkRequest.doWork(CommitProcessor.java:296)
	at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:162)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.zookeeper.common.X509Util.createKeyManager(X509Util.java:113)
	... 10 more
2016-09-14 13:35:23,489 [myid:1] - ERROR [CommitProcWorkThread-1:X509AuthenticationProvider@90] - Failed to create trust manager
org.apache.zookeeper.common.X509Exception$TrustManagerException: java.lang.NullPointerException
	at org.apache.zookeeper.common.X509Util.createTrustManager(X509Util.java:158)
	at org.apache.zookeeper.server.auth.X509AuthenticationProvider.<init>(X509AuthenticationProvider.java:87)
	at org.apache.zookeeper.server.auth.ProviderRegistry.initialize(ProviderRegistry.java:42)
	at org.apache.zookeeper.server.auth.ProviderRegistry.getProvider(ProviderRegistry.java:68)
	at org.apache.zookeeper.server.PrepRequestProcessor.checkACL(PrepRequestProcessor.java:319)
	at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:324)
	at org.apache.zookeeper.server.quorum.CommitProcessor$CommitWorkRequest.doWork(CommitProcessor.java:296)
	at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:162)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.zookeeper.common.X509Util.createTrustManager(X509Util.java:143)
	... 10 more","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2581,Major,Rakesh Kumar Singh,Fixed,2017-09-11T20:57:03.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Not handled NullPointerException while creating key manager and trustManager,2017-09-11T21:37:09.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",6.0
Rakesh Kumar Singh,"[<JIRA Component: name='java client', id='12312381'>]",2016-09-14T04:48:00.000+0000,Rakesh Kumar Singh,"set IP acl and try to set again from another machine:-

[zk: localhost:2181(CONNECTED) 11] setAcl /ip_test ip:10.18.101.80:crdwa
KeeperErrorCode = NoAuth for /ip_test",[],Bug,ZOOKEEPER-2580,Minor,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ErrorMessage is not correct when set IP acl and try to set again from another machine,2016-09-23T05:04:51.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",3.0
Abraham Fine,[],2016-09-13T20:49:38.000+0000,Abraham Fine,"If the directories specified for the dataDir or the snapDir are not writeable, the server does not fail until it actually tries to write there. It should fail when it starts.","[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2579,Major,Abraham Fine,Fixed,2016-09-18T21:37:06.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeper server should verify that dataDir and snapDir are writeable before starting,2017-03-31T09:01:08.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>]",4.0
,[],2016-09-13T20:30:32.000+0000,Bjorn Stange,"In bin/zkEnv.sh The ZOOCFG variable is duplicated over itself if already set. For example, in my use case it was being set in zookeeper-env.sh. The problem seems to arise from this line (line 61 on the master branch at the time of this submission): ZOOCFG=""$ZOOCFGDIR/$ZOOCFG"". This overwrites the value of ZOOCFG by appending the old value to the value of ZOOCFGDIR, which is problematic if it was already initialized as the absolute path to a file. The behavior of overwriting the value of the variable in this way seems to be specific to the case where ZOOCFG is not initialized. The final state of ZOOCFG seemingly is the absolute path to the zookeeper configuration file. This behavior assumes that it is the filename only, which is where the bug arises. ",[],Bug,ZOOKEEPER-2578,Minor,Bjorn Stange,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zkEnv.sh does not set $ZOOCFG properly if already set ,2016-09-21T15:37:17.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>, <JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.2.2', id='12314335'>, <JIRA Version: name='3.2.3', id='12314847'>, <JIRA Version: name='3.3.0', id='12313976'>, <JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.4.1', id='12318650'>, <JIRA Version: name='3.4.2', id='12319196'>, <JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.3.5', id='12319081'>, <JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>]",3.0
Patrick D. Hunt,"[<JIRA Component: name='build', id='12312383'>]",2016-09-12T16:46:48.000+0000,Patrick D. Hunt,"After moving from svn to git the precommit job is failing. I've disabled it temporarily.
https://builds.apache.org/view/S-Z/view/ZooKeeper/job/PreCommit-ZOOKEEPER-Build/","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2576,Blocker,Patrick D. Hunt,Fixed,2016-09-13T03:23:26.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,After svn to git migration ZooKeeper Precommit jenkins job is failing.,2016-10-16T14:58:24.000+0000,[],3.0
kevin.chen,"[<JIRA Component: name='java client', id='12312381'>]",2016-09-12T05:34:54.000+0000,Prabhunath Yadav,"while creating node using command (random arguments like this).
create /  /./// or some wrong format it shows the message 
/./// does not have the form scheme:id:perm
with Exception in thread ""main"" org.apache.zookeeper.KeeperException$InvalidACLException: KeeperErrorCode=InvalidACL
.....

It should give the accurate message but it should not get closed or quit.",[],Bug,ZOOKEEPER-2575,Minor,Prabhunath Yadav,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,/./// does not have the form scheme:id:perm and client is quit.,2016-11-23T09:41:04.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",4.0
Abhishek Rai,"[<JIRA Component: name='server', id='12312382'>]",2016-09-09T20:20:40.000+0000,Abhishek Rai,"As part of the fix for ZOOKEEPER-1797, the call to FileTxnSnapLog.getSnapshotLogs() was removed from PurgeTxnLog.java.  As a result, some old-looking but required txn log files can be deleted, resulting in data corruption or loss.

For example, consider the following:

1. Configuration:
autopurge.snapRetainCount=3

2. Following files exist:
log.100 spans transactions from zxid=100 till zxid=140 (inclusive)
snapshot.110 - snapshot as of zxid=110
snapshot.120 - snapshot as of zxid=120
snapshot.130 - snapshot as of zxid=130

Above scenario is possible when snapshotting has happened multiple times but without accompanying log rollover, which is possible if the server was running as a learner.

3. PurgeTxnLog retains all snapshots but deletes log.100 because its zxid is older than the zxid of the oldest snapshot (110).  This results in loss of transactions in the range 131-140.

Before the fix for ZOOKEEPER-1797, this was avoided by the call to FileTxnSnapLog.getSnapshotLogs() which finds and retains the newest txn log file with starting zxid < oldest retained snapshot's highest zxid.","[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2574,Major,Abhishek Rai,Fixed,2017-01-23T01:31:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,PurgeTxnLog can inadvertently delete required txn log files,2017-11-15T21:29:01.000+0000,"[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>]",9.0
Edward Ribeiro,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='server', id='12312382'>]",2016-09-09T13:35:48.000+0000,Mohammad Arshad,"Modify {{org.apache.zookeeper.version.Info.REVISION}} to store git repo revision
Currently {{org.apache.zookeeper.version.Info.REVISION}} stores the svn repo revision which is of type int
But after migrating to git repo the git repo's revision(commit 63f5132716c08b3d8f18993bf98eb46eb42f80fb) can not be stored in this variable.
So either we should modify this variable to string to introduce new variable to store the git revision and leave the svn revision variable unchanged.
build.xml, and org.apache.zookeeper.version.util.VerGen also need to be modified. ","[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2573,Major,Mohammad Arshad,Fixed,2017-01-25T12:52:27.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Modify Info.REVISION to adapt git repo,2017-04-11T22:45:59.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>]",7.0
Ling Mao,"[<JIRA Component: name='server', id='12312382'>]",2016-09-09T05:50:32.000+0000,Michael Han,"In FileTxnLog.truncate, we have:
{code}
public boolean truncate(long zxid) throws IOException {
        FileTxnIterator itr = null;
        try {
            itr = new FileTxnIterator(this.logDir, zxid);
            PositionInputStream input = itr.inputStream;
            if(input == null) {
                throw new IOException(""No log files found to truncate! This could "" +
                        ""happen if you still have snapshots from an old setup or "" +
                        ""log files were deleted accidentally or dataLogDir was changed in zoo.cfg."");
            }
            long pos = input.getPosition();
            // now, truncate at the current position
            RandomAccessFile raf=new RandomAccessFile(itr.logFile,""rw"");
            raf.setLength(pos);
            raf.close();
            while(itr.goToNextLog()) {
                if (!itr.logFile.delete()) {
                    LOG.warn(""Unable to truncate {}"", itr.logFile);
                }
            }
        } finally {
            close(itr);
        }
        return true;
    }
{code}

{{raf}} here can be potentially in a state of not closed after leaving the method, if there is an (IO) exception thrown from setLength.",[],Bug,ZOOKEEPER-2572,Major,Michael Han,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Potential resource leak in FileTxnLog.truncate,2020-03-28T15:07:32.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.4.11', id='12339207'>]",5.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-09-09T05:33:41.000+0000,Michael Han,"In QuorumPeer.writeLongToFile we have:
{code}
try {
            bw.write(Long.toString(value));
            bw.flush();
            
            out.flush();
        } catch (IOException e) {
            LOG.error(""Failed to write new file "" + file, e);
            // worst case here the tmp file/resources(fd) are not cleaned up
            //   and the caller will be notified (IOException)
            aborted = true;
            out.abort();
            throw e;
        } finally {
            if (!aborted) {
                // if the close operation (rename) fails we'll get notified.
                // worst case the tmp file may still exist
                out.close();
            }
        }
{code}

So if any unchecked exception thrown during write (e.g. out of memory, you never know), the output stream will not be closed. The fix is can be made by having the flag set at the end of the try block instead of of in the catch block, which only catch a specific type of exception (which is what ZOOKEEPER-1835 did, thus the same issue does not exist in 3.5.x branch.).",[],Bug,ZOOKEEPER-2571,Major,Michael Han,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Potential resource leak in QuorumPeer.writeLongToFile,2018-06-22T04:49:02.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.4.11', id='12339207'>]",1.0
Mohammad Arshad,[],2016-09-08T16:27:05.000+0000,Mohammad Arshad,"ZooKeeper clients are timed out when ZooKeeper servers are very busy. Clients throw below exception and fail all the pending operations
{code}
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
{code}
Clients log bellow information
{noformat}
2016-09-22 01:49:08,001 [myid:127.0.0.1:11228] - WARN  [main-SendThread(127.0.0.1:11228):ClientCnxn$SendThread@1181] - Client session timed out, have not heard from server in 13908ms for sessionid 0x20000d21b280000
2016-09-22 01:49:08,001 [myid:127.0.0.1:11228] - INFO  [main-SendThread(127.0.0.1:11228):ClientCnxn$SendThread@1229] - Client session timed out, have not heard from server in 13908ms for sessionid 0x20000d21b280000, closing socket connection and attempting reconnect
{noformat}
*STEPS TO REPRODECE:*
# Create multi operation
{code}
List<Op> ops = new ArrayList<Op>();
        for (int i = 0; i < N; i++) {
            Op create = Op.create(rootNode + ""/"" + i, """".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE,
                    CreateMode.PERSISTENT);
            ops.add(create);
        }
{code}
Chose N in such a way that the total multi operation request  size is less than but near 1 MB.  For bigger request size increase jute.maxbuffer in servers
# Submit the multi operation request
{code} zooKeeper.multi(ops);{code} 
# After repeating above steps few times issue is reproduced
",[],Bug,ZOOKEEPER-2570,Critical,Mohammad Arshad,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper clients are timed out when ZooKeeper servers are very busy,2016-10-13T18:59:47.000+0000,[],4.0
Rakesh Kumar Singh,"[<JIRA Component: name='security', id='12329414'>]",2016-09-08T13:58:52.000+0000,Rakesh Kumar Singh,"Plain password is stored when set individual ACL using digest scheme instead of storing the username and encoded hash string of <username:password>

[zk: localhost:2181(CONNECTED) 13] addauth digest user:pass
[zk: localhost:2181(CONNECTED) 14] setAcl /newNode digest:user:pass:crdwa
[zk: localhost:2181(CONNECTED) 15] getAcl /newNode
'digest,'user:pass
: cdrwa
[zk: localhost:2181(CONNECTED) 16]",[],Bug,ZOOKEEPER-2569,Major,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,plain password is stored when set individual ACL using digest scheme,2016-09-23T05:05:10.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",3.0
,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2016-09-08T11:18:28.000+0000,Prabhunath Yadav,"For Example : 
String myNode=""/MyNode""+new Date() ;
 connector.createNode(newNode, new Date().toString().getBytes());

and createNode is defined as:
public void createNode(String path, byte[] data) throws Exception
    {
        zk.create(path, data, Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
    }

if we delete the node from command
delete /MyNodeFri Aug 12 09:42:16 GMT+05:30 2016
 then we get exception saying 
Command failed:java.lang.NumberFormatException: for input string : ""Aug"" 

How to delete such node ? may rmr command can remove but why delete command not working ?",[],Bug,ZOOKEEPER-2568,Minor,Prabhunath Yadav,Information Provided,2016-09-08T22:36:18.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Node created having name with space is not deleted with delete command,2016-09-13T02:19:23.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",1.0
Rakesh Kumar Singh,"[<JIRA Component: name='java client', id='12312381'>]",2016-09-08T11:00:11.000+0000,Rakesh Kumar Singh,"Error message is not correct when wrong argument is passed for ""reconfig"" cmd

Steps to reproduce:-
1. Start zookeeper in cluster mode
2. use reconfig cmd with wrong argument (pass : instead of ;)
[zk: localhost:2181(CONNECTED) 10] reconfig -remove 3 -add 3=10.18.221.194:2888:3888:2181
KeeperErrorCode = BadArguments for 

Here error message is not complete and informative on client console.

The log is as below:-
2016-09-08 18:54:08,701 [myid:1] - INFO  [ProcessThread(sid:1 cport:-1)::PrepRequestProcessor@512] - Incremental reconfig
2016-09-08 18:54:08,702 [myid:1] - INFO  [ProcessThread(sid:1 cport:-1)::PrepRequestProcessor@843] - Got user-level KeeperException when processing sessionid:0x100299b7eac0000 type:reconfig cxid:0x7 zxid:0x400000004 txntype:-1 reqpath:n/a Error Path:Reconfiguration failed Error:KeeperErrorCode = BadArguments for Reconfiguration failed
",[],Bug,ZOOKEEPER-2567,Minor,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Error message is not correct when wrong argument is passed for ""reconfig"" cmd",2016-11-23T14:53:12.000+0000,[],3.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-09-08T09:51:36.000+0000,Athyab Ameer,"space should be truncated while reading password for keystore/truststore which is required to configure while SSL enabled.
As of now if we configure the password with any heading/trailing space, the zookeeper server will fail to start.",[],Bug,ZOOKEEPER-2566,Minor,Athyab Ameer,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,space should be truncated while reading password for keystore/truststore which is required to configure while SSL enabled,2016-09-08T22:37:23.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",2.0
kevin.chen,"[<JIRA Component: name='server', id='12312382'>]",2016-09-08T09:29:40.000+0000,Rakesh Kumar Singh,"listquota <path> should display the quota even it is set on parent/child node. As of now if we have a parent-child hierarchy for example n1->n2->n3 and quota is set for n2. If we try to get quota details on n1 and n3 using listquota, it says no quota set but if try to set the quota on those nodes it fails saying quota is already set on parent node...
So listquota should fetch the quota set on any node in hierarchy with exact path (on which quota is set) even though this api is called on any other node in that hierarchy.",[],Bug,ZOOKEEPER-2565,Minor,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,listquota <path> should display the quota even it is set on parent/child node,2019-05-26T06:47:55.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",7.0
Rakesh Kumar Singh,"[<JIRA Component: name='server', id='12312382'>]",2016-09-08T06:57:41.000+0000,Rakesh Kumar Singh,"No message is prompted when trying to delete quota with different quota option.

Steps to reproduce:-
1. Start zookeeper in cluster mode 
2. Create some node and set quota like
setquota -n 10 /test
3. Now try to delete as below:-
delquota -b /test

Here no message/exception is prompted. We should prompt message like 
""Byte Quota does not exist for /test""
",[],Bug,ZOOKEEPER-2564,Minor,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,No message is prompted when trying to delete quota with different quota option,2019-05-28T10:09:40.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",4.0
Rakesh Kumar Singh,"[<JIRA Component: name='server', id='12312382'>]",2016-09-08T06:19:59.000+0000,Rakesh Kumar Singh,"delquota -[n|b] is not deleting the set quota properly

Steps to reproduce:-
1. Start zookeeper in cluster mode (ssl)
2. create some node say /test
3. Run command as listquota says (as expected)
quota for /test does not exist
4. setquota let say
setquota -n 10 /test
5. Now try to delete this as below
delquota -n /test
6. now check the quota

[zk: localhost:2181(CONNECTED) 1] listquota /test
absolute path is /zookeeper/quota/test/zookeeper_limits
Output quota for /test count=-1,bytes=-1
Output stat for /test count=1,bytes=5

7. Here it is not deleted quota node for test
8. Now try to set some new quota
It fails as it is not deleted correctly while delete

[zk: localhost:2181(CONNECTED) 3] setquota -n 11 /test
Command failed: java.lang.IllegalArgumentException: /test has a parent /zookeeper/quota/test which has a quota

But through delquota it is able to delete
","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2563,Major,Rakesh Kumar Singh,Fixed,2019-07-17T11:42:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,A revisit to setquota,2019-07-17T21:33:58.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",3.0
,[],2016-09-07T18:54:31.000+0000,Ramnatthan Alagappan,"I am running a three node ZooKeeper cluster. 

Renames of acceptedEpoch.tmp to acceptedEpoch and currentEpoch.tmp to currentEpoch have to persisted to disk by explicitly issuing fsync on the parent directory. If not, the rename might not hit the disk immediately and if a crash occurs at this point, then the server would fail to start with the following error in the log. If this happens on two more or nodes, then the cluster can become unavailable.   

[myid:] - INFO  [main:QuorumPeerConfig@103] - Reading configuration from: /tmp/zoo2.cfg
[myid:] - INFO  [main:QuorumPeer$QuorumServer@149] - Resolved hostname: 127.0.0.2 to address: /127.0.0.2
[myid:] - INFO  [main:QuorumPeer$QuorumServer@149] - Resolved hostname: 127.0.0.4 to address: /127.0.0.4
[myid:] - INFO  [main:QuorumPeer$QuorumServer@149] - Resolved hostname: 127.0.0.3 to address: /127.0.0.3
[myid:] - INFO  [main:QuorumPeerConfig@331] - Defaulting to majority quorums
[myid:1] - INFO  [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3
[myid:1] - INFO  [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 0
[myid:1] - INFO  [main:DatadirCleanupManager@101] - Purge task is not scheduled.
[myid:1] - INFO  [main:QuorumPeerMain@127] - Starting quorum peer
[myid:1] - INFO  [main:NIOServerCnxnFactory@89] - binding to port 0.0.0.0/0.0.0.0:2182
[myid:1] - INFO  [main:QuorumPeer@1019] - tickTime set to 2000
[myid:1] - INFO  [main:QuorumPeer@1039] - minSessionTimeout set to -1
[myid:1] - INFO  [main:QuorumPeer@1050] - maxSessionTimeout set to -1
[myid:1] - INFO  [main:QuorumPeer@1065] - initLimit set to 5
[myid:1] - INFO  [main:FileSnap@83] - Reading snapshot /run/shm/dice-4636/113-98-129-z_majority_RO_OM_0=60_1=55/rdir-0/version-2/snapshot.100000002
[myid:1] - ERROR [main:QuorumPeer@557] - Unable to load database on disk
java.io.IOException: The accepted epoch, 1 is less than the current epoch, 2
	at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:554)
	at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:500)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:153)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:111)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)
2016-04-15 03:24:57,144 [myid:1] - ERROR [main:QuorumPeerMain@89] - Unexpected exception, exiting abnormally
java.lang.RuntimeException: Unable to run quorum server 
	at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:558)
	at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:500)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:153)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:111)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)
Caused by: java.io.IOException: The accepted epoch, 1 is less than the current epoch, 2
	at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:554)
	... 4 more

Similarly, when new log file is created, the parent directory needs be explicitly fsynced to persist the log file. Otherwise a data loss might be possible (We have reproduced the above issues).  Please see this: https://www.quora.com/Linux/When-should-you-fsync-the-containing-directory-in-addition-to-the-file-itself and http://research.cs.wisc.edu/wind/Publications/alice-osdi14.pdf. ","[<JIRA Version: name='3.4.7', id='12325149'>]",Bug,ZOOKEEPER-2562,Major,Ramnatthan Alagappan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Safely persist renames of *epoch.tmp to *epoch by issuing fsync on parent directory -- Possible cluster unavailability otherwise,2016-09-07T18:54:31.000+0000,[],1.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-09-07T17:40:38.000+0000,Athyab Ameer,"Possible Cluster Unvailability

I am running a three node ZooKeeper cluster. Each node runs Linux. 

I see the below sequence of system calls when ZooKeeper appends a user data item to the log file.

1 write(""/data/version-2/log.200000001"", offset=65, count=12)
2 write(""/data/version-2/log.200000001"", offset=77, count=16323)
3 write(""/data/version-2/log.200000001"", offset=16400, count=4209)
4 write(""/data/version-2/log.200000001"", offset=20609, count=1)
5 fdatasync(""/data//version-2/log.200000001"")

Now, a crash could happen just after operation 4 but before the final fdatasync. In this situation, the file system could persist the 4th operation and fail to persist the 3rd operation because of the crash and there is fsync in between them. In such cases, ZooKeeper server fails to start with the following messages in its log file:

[myid:] - INFO  [main:QuorumPeerConfig@103] - Reading configuration from: /tmp/zoo2.cfg
[myid:] - INFO  [main:QuorumPeer$QuorumServer@149] - Resolved hostname: 127.0.0.2 to address: /127.0.0.2
[myid:] - INFO  [main:QuorumPeer$QuorumServer@149] - Resolved hostname: 127.0.0.4 to address: /127.0.0.4
[myid:] - INFO  [main:QuorumPeer$QuorumServer@149] - Resolved hostname: 127.0.0.3 to address: /127.0.0.3
[myid:] - INFO  [main:QuorumPeerConfig@331] - Defaulting to majority quorums
[myid:1] - INFO  [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3
[myid:1] - INFO  [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 0
[myid:1] - INFO  [main:DatadirCleanupManager@101] - Purge task is not scheduled.
[myid:1] - INFO  [main:QuorumPeerMain@127] - Starting quorum peer
[myid:1] - INFO  [main:NIOServerCnxnFactory@89] - binding to port 0.0.0.0/0.0.0.0:2182
[myid:1] - INFO  [main:QuorumPeer@1019] - tickTime set to 2000
[myid:1] - INFO  [main:QuorumPeer@1039] - minSessionTimeout set to -1
[myid:1] - INFO  [main:QuorumPeer@1050] - maxSessionTimeout set to -1
[myid:1] - INFO  [main:QuorumPeer@1065] - initLimit set to 5
[myid:1] - INFO  [main:FileSnap@83] - Reading snapshot /data/version-2/snapshot.100000002
[myid:1] - ERROR [main:QuorumPeer@557] - Unable to load database on disk
java.io.IOException: CRC check failed
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:635)
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:158)
	at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:223)
	at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:510)
	at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:500)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:153)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:111)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)
2016-04-15 04:00:32,795 [myid:1] - ERROR [main:QuorumPeerMain@89] - Unexpected exception, exiting abnormally
java.lang.RuntimeException: Unable to run quorum server 
	at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:558)
	at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:500)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:153)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:111)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)
Caused by: java.io.IOException: CRC check failed
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:635)
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:158)
	at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:223)
	at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:510)
	... 4 more

The same happens when the 3rd and 4th writes hit the disk but the 2nd operation does not. 

Now, two nodes of a three node cluster can easily reach this state, rendering the entire cluster unavailable. ZooKeeper, on recovery should be able to handle such checksum mismatches gracefully to maintain cluster availability.","[<JIRA Version: name='3.4.8', id='12326517'>]",Bug,ZOOKEEPER-2561,Major,Athyab Ameer,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,CLONE - Possible Cluster Unavailability,2016-09-07T17:40:41.000+0000,[],1.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-09-07T15:12:59.000+0000,Ramnatthan Alagappan,"Possible Cluster Unvailability

I am running a three node ZooKeeper cluster. Each node runs Linux. 

I see the below sequence of system calls when ZooKeeper appends a user data item to the log file.

1 write(""/data/version-2/log.200000001"", offset=65, count=12)
2 write(""/data/version-2/log.200000001"", offset=77, count=16323)
3 write(""/data/version-2/log.200000001"", offset=16400, count=4209)
4 write(""/data/version-2/log.200000001"", offset=20609, count=1)
5 fdatasync(""/data//version-2/log.200000001"")

Now, a crash could happen just after operation 4 but before the final fdatasync. In this situation, the file system could persist the 4th operation and fail to persist the 3rd operation because of the crash and there is fsync in between them. In such cases, ZooKeeper server fails to start with the following messages in its log file:

[myid:] - INFO  [main:QuorumPeerConfig@103] - Reading configuration from: /tmp/zoo2.cfg
[myid:] - INFO  [main:QuorumPeer$QuorumServer@149] - Resolved hostname: 127.0.0.2 to address: /127.0.0.2
[myid:] - INFO  [main:QuorumPeer$QuorumServer@149] - Resolved hostname: 127.0.0.4 to address: /127.0.0.4
[myid:] - INFO  [main:QuorumPeer$QuorumServer@149] - Resolved hostname: 127.0.0.3 to address: /127.0.0.3
[myid:] - INFO  [main:QuorumPeerConfig@331] - Defaulting to majority quorums
[myid:1] - INFO  [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3
[myid:1] - INFO  [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 0
[myid:1] - INFO  [main:DatadirCleanupManager@101] - Purge task is not scheduled.
[myid:1] - INFO  [main:QuorumPeerMain@127] - Starting quorum peer
[myid:1] - INFO  [main:NIOServerCnxnFactory@89] - binding to port 0.0.0.0/0.0.0.0:2182
[myid:1] - INFO  [main:QuorumPeer@1019] - tickTime set to 2000
[myid:1] - INFO  [main:QuorumPeer@1039] - minSessionTimeout set to -1
[myid:1] - INFO  [main:QuorumPeer@1050] - maxSessionTimeout set to -1
[myid:1] - INFO  [main:QuorumPeer@1065] - initLimit set to 5
[myid:1] - INFO  [main:FileSnap@83] - Reading snapshot /data/version-2/snapshot.100000002
[myid:1] - ERROR [main:QuorumPeer@557] - Unable to load database on disk
java.io.IOException: CRC check failed
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:635)
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:158)
	at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:223)
	at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:510)
	at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:500)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:153)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:111)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)
2016-04-15 04:00:32,795 [myid:1] - ERROR [main:QuorumPeerMain@89] - Unexpected exception, exiting abnormally
java.lang.RuntimeException: Unable to run quorum server 
	at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:558)
	at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:500)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:153)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:111)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)
Caused by: java.io.IOException: CRC check failed
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:635)
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:158)
	at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:223)
	at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:510)
	... 4 more

The same happens when the 3rd and 4th writes hit the disk but the 2nd operation does not. 

Now, two nodes of a three node cluster can easily reach this state, rendering the entire cluster unavailable. ZooKeeper, on recovery should be able to handle such checksum mismatches gracefully to maintain cluster availability.","[<JIRA Version: name='3.4.8', id='12326517'>]",Bug,ZOOKEEPER-2560,Major,Ramnatthan Alagappan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Possible Cluster Unavailability,2016-09-07T17:40:41.000+0000,[],2.0
Rakesh Kumar Singh,"[<JIRA Component: name='server', id='12312382'>]",2016-09-07T12:14:55.000+0000,Rakesh Kumar Singh,"Failed to delete the set quota for ephemeral node when the node is deleted because of client session closed

[zk: localhost:2181(CONNECTED) 0] create -e /e_test hello
Created /e_test
[zk: localhost:2181(CONNECTED) 1] setquota -n 10 /e_test
[zk: localhost:2181(CONNECTED) 2] listquota /e_test
absolute path is /zookeeper/quota/e_test/zookeeper_limits
Output quota for /e_test count=10,bytes=-1
Output stat for /e_test count=1,bytes=5

Now close the client connection and so the ephemeral node gets deleted. But the corresponding quota is not getting deleted as below:-

[zk: localhost:2181(CONNECTED) 0] ls /
[test, test1, test3, zookeeper]
[zk: localhost:2181(CONNECTED) 1] listquota /e_test
absolute path is /zookeeper/quota/e_test/zookeeper_limits
Output quota for /e_test count=10,bytes=-1
Output stat for /e_test count=0,bytes=0
[zk: localhost:2181(CONNECTED) 2] 


and so now again create the ephemeral node with same node and try to set the quota, it will fail.

[zk: localhost:2181(CONNECTED) 2] create -e /e_test hello
Created /e_test
[zk: localhost:2181(CONNECTED) 3] setquota -n 10 /e_test
Command failed: java.lang.IllegalArgumentException: /e_test has a parent /zookeeper/quota/e_test which has a quota
[zk: localhost:2181(CONNECTED) 4] ",[],Bug,ZOOKEEPER-2559,Major,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,the quota should be deleted when its parasitic path doesn't exist,2019-07-01T07:06:02.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>]",1.0
Michael Han,"[<JIRA Component: name='c client', id='12312380'>]",2016-09-06T22:57:16.000+0000,Michael Han,"We have code like this in {{create_buffer_iarchive}} and {{create_buffer_oarchive}}:

{code}
    struct iarchive *ia = malloc(sizeof(*ia));
    struct buff_struct *buff = malloc(sizeof(struct buff_struct));
    if (!ia) return 0;
    if (!buff) {
        free(ia);
        return 0;
    }
{code}

If first malloc failed but second succeeds, then the memory allocated with second malloc will not get freed when the function returned. One could argue that if first malloc failed the second will also fail (i.e. when system run out of memory), but I could also see the possibility of the opposite (the first malloc failed because heap fragmentation but the second succeeds).","[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2558,Minor,Michael Han,Fixed,2016-09-08T02:59:41.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Potential memory leak in recordio.c,2017-03-31T09:01:10.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>]",3.0
Rakesh Kumar Singh,"[<JIRA Component: name='server', id='12312382'>]",2016-09-06T10:43:54.000+0000,Rakesh Kumar Singh,"peerType remains as ""observer"" in zoo.cfg even though we change the node from observer to participant runtime

Steps to reproduce:-
1. Start zookeeper in cluster with one node as observer by configuring 
peerType=observer in zoo.cfg and server.2=10.18.219.50:2888:3888:observer;2181
2. Start the cluster
3. start a client and change the node from observer to participant, the configuration related to peertype remained same though other things like clientport got from zoo.cfg

>reconfig -remove 2 -add 2=10.18.219.50:2888:3888:participant;2181

We should either remove this parameter or update with correct node type at run time",[],Bug,ZOOKEEPER-2556,Minor,Rakesh Kumar Singh,Fixed,2016-11-03T15:41:36.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"peerType remains as ""observer"" in zoo.cfg even though we change the node from observer to participant runtime",2016-11-03T16:48:36.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>]",5.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-09-06T09:59:59.000+0000,Rakesh Kumar Singh,"zookeeper started in observer mode takes long time (some times 25 seconds) to come to observer state once leader restarted and so client connected to observer mode has to wait for longer time to get service

Steps to reproduce:-
1. Start zookeeper in cluster mode in which one node is in observer mode
2. stop the leader node (some times we need to wait for 30 secs to reproduce this issue)
3. Start the leader node
4. Check the observer node status -
It will be in 
""Error contacting service. It is probably not running.""

and takes long time (25 secs) to come to observer mode. And hence client connected to this node will not get service during this time.
Log at observer node is as below:-

2016-09-06 17:49:14,774 [myid:2] - WARN  [WorkerSender[myid=2]:QuorumCnxManager@459] - Cannot open channel to 3 at election address /10.18.221.194:3888
java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:391)
	at java.net.Socket.connect(Socket.java:579)
	at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:444)
	at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:485)
	at org.apache.zookeeper.server.quorum.QuorumCnxManager.toSend(QuorumCnxManager.java:421)
	at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.process(FastLeaderElection.java:486)
	at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.run(FastLeaderElection.java:465)
	at java.lang.Thread.run(Thread.java:722)
{color:red}2016-09-06 17:49:14,776 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection@688] - Notification: 2 (message format version), 3 (n.leader), 0x2a00000001 (n.zxid), 0x7 (n.round), LOOKING (n.state), 1 (n.sid), 0x2b (n.peerEPoch), LOOKING (my state)100000000 (n.config version)
2016-09-06 17:49:40,377 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):FastLeaderElection@928] - Notification time out: 51200 {color}
2016-09-06 17:49:40,378 [myid:2] - INFO  [WorkerSender[myid=2]:QuorumCnxManager@278] - Have smaller server identifier, so dropping the connection: (3, 2)
2016-09-06 17:49:40,379 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection@688] - Notification: 2 (message format version), 3 (n.leader), 0x2a00000001 (n.zxid), 0x7 (n.round), FOLLOWING (n.state), 1 (n.sid), 0x2c (n.peerEPoch), LOOKING (my state)100000000 (n.config version)
2016-09-06 17:49:40,381 [myid:2] - INFO  [/10.18.219.50:3888:QuorumCnxManager$Listener@637] - Received connection request /10.18.221.194:34085
2016-09-06 17:49:40,388 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection@688] - Notification: 2 (message format version), 3 (n.leader), 0x2a00000001 (n.zxid), 0x7 (n.round), LEADING (n.state), 3 (n.sid), 0x2c (n.peerEPoch), LOOKING (my state)100000000 (n.config version)
2016-09-06 17:49:40,388 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):MBeanRegistry@119] - Unregister MBean [org.apache.ZooKeeperService:name0=ReplicatedServer_id2,name1=replica.2,name2=LeaderElection]
2016-09-06 17:49:40,389 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):QuorumPeer@1049] - OBSERVING
2016-09-06 17:49:40,389 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):ZooKeeperServer@858] - minSessionTimeout set to 4000
2016-09-06 17:49:40,389 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):ZooKeeperServer@867] - maxSessionTimeout set to 40000
2016-09-06 17:49:40,389 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):ZooKeeperServer@156] - Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 datadir /home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/data/version-2 snapdir /home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/data/version-2
2016-09-06 17:49:40,389 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):ObserverZooKeeperServer@56] - syncEnabled =true
2016-09-06 17:49:40,389 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Observer@72] - Observing /10.18.221.194:2888
2016-09-06 17:49:40,396 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):FileSnap@83] - Reading snapshot /home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/data/version-2/snapshot.2a00000001
2016-09-06 17:49:40,410 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Learner@369] - Getting a snapshot from leader
2016-09-06 17:49:40,411 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Learner@509] - Learner received NEWLEADER message
2016-09-06 17:49:40,411 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):FileTxnSnapLog@298] - Snapshotting: 0x2c00000000 to /home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/data/version-2/snapshot.2c00000000
2016-09-06 17:49:40,417 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Learner@493] - Learner received UPTODATE message
2016-09-06 17:49:40,417 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):CommitProcessor@254] - Configuring CommitProcessor with 8 worker threads.
",[],Bug,ZOOKEEPER-2555,Major,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zookeeper started in observer mode takes long time to come to observer state once leader restarted and so client connected to observer mode has to wait for longer time to get service,2016-09-06T10:00:57.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",1.0
Mohammad Arshad,"[<JIRA Component: name='server', id='12312382'>]",2016-09-05T19:14:26.000+0000,Mohammad Arshad,"Try to add new observer server using reconfig API, server gets added as participant.
STEPS:
# create 3 node cluster.
{code}
server.0=127.0.0.1:11223:11224:participant;127.0.0.1:11222
server.1=127.0.0.1:11226:11227:participant;127.0.0.1:11225
server.2=127.0.0.1:11229:11230:participant;127.0.0.1:11228
{code}
# Suppose the 2 is the leader in the above cluster. Configure the new server as
{code}
server.2=127.0.0.1:11229:11230:participant;127.0.0.1:11228
server.3=127.0.0.1:11232:11233:observer;127.0.0.1:11231
{code}
# Connect to 1 and execute the reconfig command
{code}
zkClient.reconfig(""server.3=127.0.0.1:11232:11233:observer;127.0.0.1:11231"", null, null, -1, null, null);
{code}
# Verify sever 3. It was supposed to run as observer but it is running as participant",[],Bug,ZOOKEEPER-2554,Critical,Mohammad Arshad,Not A Problem,2016-09-06T17:14:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,reconfig can not add new server as observer,2016-09-06T18:22:11.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-09-05T17:20:17.000+0000,Ramnatthan Alagappan,"I am running a three node ZooKeeper cluster. 

When a new log file is created by ZooKeeper, I see the following sequence of system calls:

1. creat(new_log)
2. write(new_log, count=16) // This is a log header I believe/
3. truncate(new_log, from 16 bytes to 16 KBytes) // I have configured the log size to be 16K. 

When the above sequence of operations complete, it is reasonable to expect the newly created log file to contain the header(16 bytes) and then filled with zeros till the end of the log.

But when a crash occurs (due to a power failure), while the truncate system call is in progress, it is possible for the log to contain garbage data when the system restarts from the crash. Note that if the crash occurs just after the truncate system call completes, then there is no problem. Basically, the truncate needs to be atomically persisted for ZooKeeper to recover from crashes correctly  or (more realistically) the recovery code needs to deal with the case of expecting garbage in a newly created log. 

As mentioned, if a crash occurs during the truncate system call, then ZooKeeper will fail to start with the following exception. Here is the stack trace:

java.io.IOException: Unreasonable length = -295704495
        at org.apache.jute.BinaryInputArchive.checkLength(BinaryInputArchive.java:127)
        at org.apache.jute.BinaryInputArchive.readBuffer(BinaryInputArchive.java:92)
        at org.apache.zookeeper.server.persistence.Util.readTxnBytes(Util.java:233)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:625)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:652)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.init(FileTxnLog.java:552)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.<init>(FileTxnLog.java:527)
        at org.apache.zookeeper.server.persistence.FileTxnLog.read(FileTxnLog.java:354)
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:132)
        at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:223)
        at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:510)
        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:500)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:153)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:111)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)
[myid:1] - ERROR [main:QuorumPeerMain@89] - Unexpected exception, exiting abnormally
java.lang.RuntimeException: Unable to run quorum server
        at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:558)
        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:500)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:153)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:111)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)
Caused by: java.io.IOException: Unreasonable length = -295704495
        at org.apache.jute.BinaryInputArchive.checkLength(BinaryInputArchive.java:127)
        at org.apache.jute.BinaryInputArchive.readBuffer(BinaryInputArchive.java:92)
        at org.apache.zookeeper.server.persistence.Util.readTxnBytes(Util.java:233)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:625)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:652)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.init(FileTxnLog.java:552)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.<init>(FileTxnLog.java:527)
        at org.apache.zookeeper.server.persistence.FileTxnLog.read(FileTxnLog.java:354)
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:132)
        at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:223)
        at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:510)
        ... 4 more


Next, it is possible for two nodes of a 3-node  ZooKeeper cluster to reach the same state. In that case, they both will fail to startup, rendering the entire cluster unavailable. ",[],Bug,ZOOKEEPER-2553,Major,Ramnatthan Alagappan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper cluster unavailable due to corrupted log file during power failures -- java.io.IOException: Unreasonable length,2020-11-18T16:51:13.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>]",6.0
Edward Ribeiro,[],2016-09-05T12:43:03.000+0000,Rakesh Radhakrishnan,"Couple of issues listed on http://zookeeper.apache.org/
doc/r3.4.9/releasenotes.html that are either 'Open' or 'Patch available'. For example, issues were wrongly marked as ""3.4.8"" fix version in jira and has caused the trouble.

This jira to cross check all the jira issues present in the release note and check the correctness.","[<JIRA Version: name='3.4.10', id='12338036'>]",Bug,ZOOKEEPER-2552,Major,Rakesh Radhakrishnan,Fixed,2016-12-15T10:14:00.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Revisit release note doc and remove the items which are not related to the released version,2017-05-13T01:05:06.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",4.0
Mohammad Arshad,"[<JIRA Component: name='documentation', id='12312422'>]",2016-09-05T07:01:37.000+0000,Mohammad Arshad,"ZooKeeper documentation has hadoop logo on each page's header. There is no significance to put the hadoop logo on ZooKeeper project. So hadoop  logo should be removed from  Zookeeper  as  ZooKeeper is independent of hadoop project, ",[],Bug,ZOOKEEPER-2551,Minor,Mohammad Arshad,Won't Fix,2021-01-07T11:10:57.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Remove Hadoop Logo from ZooKeeper documentation,2021-01-07T11:10:57.000+0000,[],1.0
,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>, <JIRA Component: name='tests', id='12312427'>]",2016-09-05T06:05:23.000+0000,KangYin," I'm studying on the Test of ZooKeeper 3.3.3 but got a test failure when I run  _testResyncBySnapThenDiffAfterFollowerCrashes_ in _FollowerResyncConcurrencyTest.java_.

{quote}
2016-09-05 13:57:35,072 - INFO  [main:QuorumBase@307] - FINISHED testResyncBySnapThenDiffAfterFollowerCrashes

java.util.concurrent.TimeoutException: Did not connect

	at org.apache.zookeeper.test.ClientBase$CountdownWatcher.waitForConnected(ClientBase.java:119)
	at org.apache.zookeeper.test.FollowerResyncConcurrencyTest.testResyncBySnapThenDiffAfterFollowerCrashes(FollowerResyncConcurrencyTest.java:95)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at junit.framework.TestCase.runTest(TestCase.java:168)
	at junit.framework.TestCase.runBare(TestCase.java:134)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at junit.framework.TestSuite.runTest(TestSuite.java:232)
	at junit.framework.TestSuite.run(TestSuite.java:227)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:157)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:119)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:42)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:234)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
{quote}

Which happened in _FollowerResyncConcurrencyTest.java_ at line 92.

{quote}
        index = (index == 1) ? 2 : 1;
        qu.shutdown(index);
        final ZooKeeper zk3 = new DisconnectableZooKeeper(""127.0.0.1:"" + qu.getPeer(3).peer.getClientPort(), 1000,watcher3);
        {color:red}watcher3.waitForConnected(CONNECTION_TIMEOUT);{color}
        zk3.create(""/mybar"", null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);
{quote}

I checked the Log Message, and I guess it is probably because of the following ERROR (marked as blue):

{quote}
2016-09-05 13:56:54,928 - INFO  [main-SendThread():ClientCnxn$SendThread@1041] - Opening socket connection to server /127.0.0.1:11237
2016-09-05 13:56:54,930 - INFO  [main-SendThread(127.0.0.1:11237):ClientCnxn$SendThread@949] - Socket connection established to 127.0.0.1/127.0.0.1:11237, initiating session
2016-09-05 13:56:54,930 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11237:NIOServerCnxn$Factory@251] - Accepted socket connection from /127.0.0.1:33566
2016-09-05 13:56:54,957 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11237:NIOServerCnxn@777] - Client attempting to establish new session at /127.0.0.1:33566
 {color:blue}
2016-09-05 13:56:55,000 - INFO  [SyncThread:3:FileTxnLog@197] - Creating new log file: log.100000001
2016-09-05 13:56:55,000 - WARN  [QuorumPeer:/0:0:0:0:0:0:0:0:11235:Follower@116] - Got zxid 0x100000001 expected 0x1
2016-09-05 13:56:55,000 - INFO  [SyncThread:2:FileTxnLog@197] - Creating new log file: log.100000001
2016-09-05 13:56:55,078 - ERROR [CommitProcessor:3:CommitProcessor@146] - Unexpected exception causing CommitProcessor to exit
java.lang.AssertionError
	at org.apache.zookeeper.jmx.MBeanRegistry.register(MBeanRegistry.java:66)
	at org.apache.zookeeper.server.NIOServerCnxn.finishSessionInit(NIOServerCnxn.java:1552)
	at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:183)
	at org.apache.zookeeper.server.quorum.Leader$ToBeAppliedRequestProcessor.processRequest(Leader.java:540)
	at org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:73)
2016-09-05 13:56:55,078 - INFO  [CommitProcessor:3:CommitProcessor@148] - CommitProcessor exited loop!
{color}

2016-09-05 13:56:55,931 - INFO  [main-SendThread(127.0.0.1:11237):ClientCnxn$SendThread@1157] - Client session timed out, have not heard from server in 1001ms for sessionid 0x0, closing socket connection and attempting reconnect
2016-09-05 13:56:58,035 - INFO  [main-SendThread(127.0.0.1:11237):ClientCnxn$SendThread@1041] - Opening socket connection to server 127.0.0.1/127.0.0.1:11237
2016-09-05 13:56:58,036 - INFO  [main-SendThread(127.0.0.1:11237):ClientCnxn$SendThread@949] - Socket connection established to 127.0.0.1/127.0.0.1:11237, initiating session
{quote}

I'll very appreciate it if I can get some help from you genius people.
Thanks.
",[],Bug,ZOOKEEPER-2550,Minor,KangYin,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,FollowerResyncConcurrencyTest failed in ZooKeeper 3.3.3,2021-01-06T10:37:40.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",2.0
Yuliya Feldman,"[<JIRA Component: name='server', id='12312382'>]",2016-09-03T06:39:23.000+0000,Yuliya Feldman,"As NettyServerCnxn.sendResponse() allows all the exception to bubble up it can stop main ZK requests processing thread and make Zookeeper server look like it is hanging, while it just can not process any request anymore.

Idea is to catch all the exceptions in NettyServerCnxn.sendResponse() , convert them to IOException and allow it propagating up","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-2549,Major,Yuliya Feldman,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,As NettyServerCnxn.sendResponse() allows all the exception to bubble up it can stop main ZK requests processing thread,2022-02-03T08:36:25.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",5.0
Mohammad Arshad,"[<JIRA Component: name='contrib', id='12312700'>]",2016-09-02T20:23:01.000+0000,Mohammad Arshad,"ZooInspector is very usefully tool but seems its windows scripts are not maintained. 
zooInspector.cmd commands fails with bellow error:
{noformat}
D:\workspace\ZooInspector>zooInspector.cmd

D:\workspace\ZooInspector>#!/bin/sh
'#!' is not recognized as an internal or external command,
operable program or batch file.

D:\workspace\ZooInspector># Licensed to the Apache Software Foundation (ASF) under one or more
'#' is not recognized as an internal or external command,
operable program or batch file.
{noformat}","[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2548,Major,Mohammad Arshad,Fixed,2016-09-07T22:14:14.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zooInspector does not start on Windows,2017-05-18T03:44:03.000+0000,[],3.0
Mohammad Arshad,[],2016-09-02T17:16:34.000+0000,Mohammad Arshad,"IP based ACL is not working with NettyServerCnxnFactory.

Scenario:
1) Configure serverCnxnFactory= org.apache.zookeeper.server.NettyServerCnxnFactory and start ZooKeeper server
2) Create a znode  ""/n"" with ACL(ZooDefs.Perms.ALL, new Id(""ip"", ""127.0.0.1/8"")
3) Create child node /n/n1. Child node creation fails.
But the same above scenario works with NIOServerCnxnFactory
",[],Bug,ZOOKEEPER-2547,Major,Mohammad Arshad,Duplicate,2021-01-07T11:08:23.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,IP ACL is not working with NettyServerCnxnFactory,2021-01-07T11:08:23.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-09-02T11:56:03.000+0000,Rakesh Kumar Singh,"Started throwing ""Error Path:null Error:KeeperErrorCode = ReconfigInProgress"" error when trying to change the cluster using reconfig and the IO hangged at one node.

Steps:-
1. Start Zookeeper in cluster mode
2. try to reconfig the cluster using ""reconfig"" command from one node's client (194) like
""reconfig -remove 3 -add 3=10.18.221.194:2888:3888;2181
3. make the IO busy for 5-10 secs at 194 node and then release
4. Again execute the above reconfig command
It is failing to execute even after 3-4 mins.
Server log is attached. (Complete server log is attached)

2016-09-02 18:12:05,845 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):QuorumPeer@1074] - LEADING
2016-09-02 18:12:05,848 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Leader@63] - TCP NoDelay set to: true
2016-09-02 18:12:05,848 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Leader@83] - zookeeper.leader.maxConcurrentSnapshots = 10
2016-09-02 18:12:05,848 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Leader@85] - zookeeper.leader.maxConcurrentSnapshotTimeout = 5
2016-09-02 18:12:05,849 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):ZooKeeperServer@858] - minSessionTimeout set to 4000
2016-09-02 18:12:05,849 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):ZooKeeperServer@867] - maxSessionTimeout set to 40000
2016-09-02 18:12:05,849 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):ZooKeeperServer@156] - Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 datadir /home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/data/version-2 snapdir /home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/data/version-2
2016-09-02 18:12:05,850 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Leader@412] - LEADING - LEADER ELECTION TOOK - 5
2016-09-02 18:12:05,852 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):FileTxnSnapLog@298] - Snapshotting: 0x100000001 to /home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/data/version-2/snapshot.100000001
2016-09-02 18:12:06,854 [myid:2] - INFO  [LearnerHandler-/10.18.101.80:55632:LearnerHandler@382] - Follower sid: 1 : info : 10.18.101.80:2888:3888:participant;0.0.0.0:2181
2016-09-02 18:12:06,869 [myid:2] - INFO  [LearnerHandler-/10.18.101.80:55632:LearnerHandler@683] - Synchronizing with Follower sid: 1 maxCommittedLog=0x100000001 minCommittedLog=0x100000001 lastProcessedZxid=0x100000001 peerLastZxid=0x100000001
2016-09-02 18:12:06,869 [myid:2] - INFO  [LearnerHandler-/10.18.101.80:55632:LearnerHandler@727] - Sending DIFF zxid=0x100000001 for peer sid: 1
2016-09-02 18:12:06,888 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Leader@1245] - Have quorum of supporters, sids: [ [1, 2] ]; starting up and setting last processed zxid: 0x200000000
2016-09-02 18:12:06,890 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):CommitProcessor@254] - Configuring CommitProcessor with 8 worker threads.
2016-09-02 18:12:06,898 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):ContainerManager@64] - Using checkIntervalMs=60000 maxPerMinute=10000
2016-09-02 18:12:18,886 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection@688] - Notification: 2 (message format version), 3 (n.leader), 0x0 (n.zxid), 0xffffffffffffffff (n.round), LEADING (n.state), 3 (n.sid), 0x1 (n.peerEPoch), LEADING (my state)200000028 (n.config version)
2016-09-02 18:13:47,869 [myid:2] - INFO  [ProcessThread(sid:2 cport:-1)::PrepRequestProcessor@512] - Incremental reconfig
2016-09-02 18:13:47,872 [myid:2] - ERROR [ProcessThread(sid:2 cport:-1)::QuorumPeer@1383] - setLastSeenQuorumVerifier called with stale config 8589934593. Current version: 8589934632
2016-09-02 18:14:15,545 [myid:2] - INFO  [ProcessThread(sid:2 cport:-1)::PrepRequestProcessor@843] - Got user-level KeeperException when processing sessionid:0x1000aa5ce650000 type:reconfig cxid:0x3 zxid:0x200000002 txntype:-1 reqpath:n/a Error Path:null Error:KeeperErrorCode = ReconfigInProgress
2016-09-02 18:14:56,442 [myid:2] - INFO  [NIOServerCxnFactory.AcceptThread:/0.0.0.0:2181:NIOServerCnxnFactory$AcceptThread@296] - Accepted socket connection from /10.18.219.50:48388
2016-09-02 18:14:56,454 [myid:2] - INFO  [NIOWorkerThread-1:NIOServerCnxn@485] - Processing srvr command from /10.18.219.50:48388
2016-09-02 18:14:56,467 [myid:2] - INFO  [NIOWorkerThread-1:NIOServerCnxn@606] - Closed socket connection for client /10.18.219.50:48388 (no session established for client)
2016-09-02 18:17:18,365 [myid:2] - INFO  [ProcessThread(sid:2 cport:-1)::PrepRequestProcessor@843] - Got user-level KeeperException when processing sessionid:0x1000aa5ce650000 type:reconfig cxid:0x4 zxid:0x200000003 txntype:-1 reqpath:n/a Error Path:null Error:KeeperErrorCode = ReconfigInProgress
2016-09-02 18:19:23,699 [myid:2] - INFO  [ProcessThread(sid:2 cport:-1)::PrepRequestProcessor@843] - Got user-level KeeperException when processing sessionid:0x1000aa5ce650000 type:reconfig cxid:0x5 zxid:0x200000004 txntype:-1 reqpath:n/a Error Path:null Error:KeeperErrorCode = ReconfigInProgress
",[],Bug,ZOOKEEPER-2546,Major,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Started throwing ""Error Path:null Error:KeeperErrorCode = ReconfigInProgress"" error when trying to change the cluster using reconfig and the IO hangged at one node",2016-09-02T11:56:03.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",1.0
Rakesh Kumar Singh,"[<JIRA Component: name='server', id='12312382'>]",2016-09-02T06:53:46.000+0000,Rakesh Kumar Singh,"Keep maintaining the old zoo.cfg.dynamic* files which will be getting created every time when ""reconfig"" is executed

Steps to reproduce:-1
1. Setup the zookeeper in cluster mode and start
2. trying running reconfig command like 
>reconfig -remove 3 -add 1=10.18.101.80:2888:3888;2181
3. It will create new zoo.cfg.dynamic in conf folder
The problem is it is not deleting the old zoo.cfg.dynamic* files which will keep eating the memory",[],Bug,ZOOKEEPER-2545,Major,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Keep maintaining the old zoo.cfg.dynamic* files which will keep eating system memory which is getting generated as part of reconfig execution,2016-09-23T05:08:32.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-09-02T05:45:41.000+0000,Rakesh Kumar Singh,"root@BLR1000010865:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin# ./zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../conf/zoo.cfg
./zkServer.sh: line 223: [: =: unary operator expected",[],Bug,ZOOKEEPER-2544,Minor,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,""" unary operator expected"" message on the console while trying to get the zookeeper status",2017-10-08T16:41:23.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>]",2.0
Rakesh Radhakrishnan,[],2016-08-31T17:40:11.000+0000,Flavio Paiva Junqueira,"We need to update the NOTICE file in the 3.4 branch as we did for 3.5 and trunk, see ZOOKEEPER-2459.","[<JIRA Version: name='3.4.10', id='12338036'>]",Bug,ZOOKEEPER-2542,Blocker,Flavio Paiva Junqueira,Fixed,2016-12-12T23:04:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Update NOTICE file with Netty notice in 3.4,2017-03-31T09:01:15.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",4.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-08-30T05:58:24.000+0000,Rakesh Kumar Singh,"We can't configure the ""secureClientPort"" in dynamicConfiguration and connect through client

Steps to reproduce:-
1. Configure the zookeeper in cluster mode with SSL mode
2. comment the clientport and secureClientport details from zoo.cfg file
3. Configure the secureClientport in dynamicConfiguration as below:-

server.1=10.18.101.80:2888:3888:participant;2181
server.2=10.18.219.50:2888:3888:participant;2181
server.3=10.18.221.194:2888:3888:participant;2181

4. Start the cluster
5. Start one client using zkCli.sh and try to connect to any one of the cluster, it fails

Client log:-
BLR1000007042:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin # ./zkCli.sh 
/usr/bin/java
Connecting to localhost:2181
2016-08-30 13:42:33,574 [myid:] - INFO  [main:Environment@109] - Client environment:zookeeper.version=3.5.1-alpha--1, built on 08/18/2016 08:20 GMT
2016-08-30 13:42:33,578 [myid:] - INFO  [main:Environment@109] - Client environment:host.name=BLR1000007042
2016-08-30 13:42:33,578 [myid:] - INFO  [main:Environment@109] - Client environment:java.version=1.7.0_79
2016-08-30 13:42:33,581 [myid:] - INFO  [main:Environment@109] - Client environment:java.vendor=Oracle Corporation
2016-08-30 13:42:33,581 [myid:] - INFO  [main:Environment@109] - Client environment:java.home=/usr/java/jdk1.7.0_79/jre
2016-08-30 13:42:33,581 [myid:] - INFO  [main:Environment@109] - Client environment:java.class.path=/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../build/classes:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../build/lib/*.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/slf4j-log4j12-1.7.5.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/slf4j-api-1.7.5.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/servlet-api-2.5-20081211.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/netty-3.7.0.Final.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/log4j-1.2.16.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/jline-2.11.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/jetty-util-6.1.26.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/jetty-6.1.26.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/javacc.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/jackson-mapper-asl-1.9.11.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/jackson-core-asl-1.9.11.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/commons-cli-1.2.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/ant-eclipse-1.0-jvm1.2.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../zookeeper-3.5.1-alpha.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../src/java/lib/ant-eclipse-1.0-jvm1.2.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../conf:
2016-08-30 13:42:33,582 [myid:] - INFO  [main:Environment@109] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2016-08-30 13:42:33,582 [myid:] - INFO  [main:Environment@109] - Client environment:java.io.tmpdir=/tmp
2016-08-30 13:42:33,582 [myid:] - INFO  [main:Environment@109] - Client environment:java.compiler=<NA>
2016-08-30 13:42:33,582 [myid:] - INFO  [main:Environment@109] - Client environment:os.name=Linux
2016-08-30 13:42:33,582 [myid:] - INFO  [main:Environment@109] - Client environment:os.arch=amd64
2016-08-30 13:42:33,583 [myid:] - INFO  [main:Environment@109] - Client environment:os.version=3.0.76-0.11-default
2016-08-30 13:42:33,583 [myid:] - INFO  [main:Environment@109] - Client environment:user.name=root
2016-08-30 13:42:33,583 [myid:] - INFO  [main:Environment@109] - Client environment:user.home=/root
2016-08-30 13:42:33,583 [myid:] - INFO  [main:Environment@109] - Client environment:user.dir=/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin
2016-08-30 13:42:33,583 [myid:] - INFO  [main:Environment@109] - Client environment:os.memory.free=52MB
2016-08-30 13:42:33,586 [myid:] - INFO  [main:Environment@109] - Client environment:os.memory.max=227MB
2016-08-30 13:42:33,587 [myid:] - INFO  [main:Environment@109] - Client environment:os.memory.total=57MB
2016-08-30 13:42:33,591 [myid:] - INFO  [main:ZooKeeper@716] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@272f15b0
Welcome to ZooKeeper!
2016-08-30 13:42:33,681 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1138] - Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
JLine support is enabled
[INFO] Unable to bind key for unsupported operation: backward-delete-word
[INFO] Unable to bind key for unsupported operation: backward-delete-word
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[zk: localhost:2181(CONNECTING) 0] 2016-08-30 13:42:33,975 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxnSocketNetty$ZKClientPipelineFactory@363] - SSL handler added for channel: null
2016-08-30 13:42:34,004 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@980] - Socket connection established, initiating session, client: /0:0:0:0:0:0:0:1:47374, server: localhost/0:0:0:0:0:0:0:1:2181
2016-08-30 13:42:34,006 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxnSocketNetty$1@146] - channel is connected: [id: 0xd4aaee7b, /0:0:0:0:0:0:0:1:47374 => localhost/0:0:0:0:0:0:0:1:2181]
2016-08-30 13:42:34,030 [myid:] - INFO  [New I/O worker #1:ClientCnxnSocketNetty$ZKClientHandler@377] - channel is disconnected: [id: 0xd4aaee7b, /0:0:0:0:0:0:0:1:47374 :> localhost/0:0:0:0:0:0:0:1:2181]
2016-08-30 13:42:34,030 [myid:] - INFO  [New I/O worker #1:ClientCnxnSocketNetty@201] - channel is told closing
2016-08-30 13:42:34,030 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1253] - channel for sessionid 0x0 is lost, closing socket connection and attempting reconnect
2016-08-30 13:42:34,033 [myid:] - WARN  [New I/O worker #1:ClientCnxnSocketNetty$ZKClientHandler@432] - Exception caught: [id: 0xd4aaee7b, /0:0:0:0:0:0:0:1:47374 :> localhost/0:0:0:0:0:0:0:1:2181] EXCEPTION: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
	at org.jboss.netty.handler.ssl.SslHandler$6.run(SslHandler.java:1580)
	at org.jboss.netty.channel.socket.ChannelRunnableWrapper.run(ChannelRunnableWrapper.java:40)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:71)
	at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:57)
	at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
	at org.jboss.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
	at org.jboss.netty.handler.ssl.SslHandler.channelClosed(SslHandler.java:1566)
	at org.jboss.netty.channel.Channels.fireChannelClosed(Channels.java:468)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:376)
	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:93)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2016-08-30 13:42:34,230 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1138] - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2016-08-30 13:42:34,240 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxnSocketNetty$ZKClientPipelineFactory@363] - SSL handler added for channel: null
2016-08-30 13:42:34,241 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@980] - Socket connection established, initiating session, client: /127.0.0.1:60295, server: localhost/127.0.0.1:2181


Server log (It is not starting in secureMode though all the required configuration is done for ssl except secureClientport which is configured in dynamicConfiguration):-
2016-08-30 13:40:13,436 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0.0.0.0:2181)(secure=disabled):FastLeaderElection@928] - Notification time out: 800
2016-08-30 13:40:14,239 [myid:1] - WARN  [QuorumPeer[myid=1](plain=/0.0.0.0:2181)(secure=disabled):QuorumCnxManager@459] - Cannot open channel to 2 at election address /10.18.219.50:3888
java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:444)
	at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:485)
	at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:513)
	at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:919)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1040)
201",[],Bug,ZOOKEEPER-2541,Major,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"We can't configure the ""secureClientPort"" in dynamicConfiguration and connect through client",2016-08-30T05:58:24.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",3.0
Rakesh Kumar Singh,"[<JIRA Component: name='server', id='12312382'>]",2016-08-30T05:16:09.000+0000,Rakesh Kumar Singh,"When start zookeeper server by configuring the server details in dynamic configuration with passing the client port, wrong log info is logged:-

Configure the server details as below which contains client port as well and remove the client port from zoo.cfg (as it is duplicate) :-
server.1=10.18.101.80:2888:3888:participant;2181
server.2=10.18.219.50:2888:3888:participant;2181
server.3=10.18.221.194:2888:3888:participant;2181

Start the cluster, we can see message as 

2016-08-30 17:00:33,984 [myid:] - INFO  [main:QuorumPeerConfig@306] - clientPort is not set

which is not correct",[],Bug,ZOOKEEPER-2540,Minor,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"When start zookeeper server by configuring the server details in dynamic configuration with passing the client port, wrong log info is logged",2016-09-23T05:09:19.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",1.0
Rakesh Kumar Singh,"[<JIRA Component: name='java client', id='12312381'>]",2016-08-29T14:09:23.000+0000,Rakesh Kumar Singh,"Throwing nullpointerException when run the command ""config -c"" when client port is mentioned as separate and not like new style
1. Configure the zookeeper to start in cluster mode like below-

clientPort=2181

server.1=10.18.101.80:2888:3888
server.2=10.18.219.50:2888:3888
server.3=10.18.221.194:2888:3888

and not like below:-
server.1=10.18.101.80:2888:3888:participant;2181
server.2=10.18.219.50:2888:3888:participant;2181
server.3=10.18.221.194:2888:3888:participant;2181

2. Start the cluster and one client using >zkCli.sh
3. execute command ""config -c""
It is throwing nullpointerException:-


root@BLR1000010865:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin# ./zkCli.sh 
Connecting to localhost:2181
2016-08-29 21:45:19,558 [myid:] - INFO  [main:Environment@109] - Client environment:zookeeper.version=3.5.1-alpha--1, built on 08/18/2016 08:20 GMT
2016-08-29 21:45:19,561 [myid:] - INFO  [main:Environment@109] - Client environment:host.name=BLR1000010865
2016-08-29 21:45:19,562 [myid:] - INFO  [main:Environment@109] - Client environment:java.version=1.7.0_17
2016-08-29 21:45:19,564 [myid:] - INFO  [main:Environment@109] - Client environment:java.vendor=Oracle Corporation
2016-08-29 21:45:19,564 [myid:] - INFO  [main:Environment@109] - Client environment:java.home=/usr/lib/jvm/oracle_jdk7/jre
2016-08-29 21:45:19,564 [myid:] - INFO  [main:Environment@109] - Client environment:java.class.path=/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../build/classes:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../build/lib/*.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/slf4j-log4j12-1.7.5.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/slf4j-api-1.7.5.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/servlet-api-2.5-20081211.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/netty-3.7.0.Final.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/log4j-1.2.16.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/jline-2.11.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/jetty-util-6.1.26.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/jetty-6.1.26.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/javacc.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/jackson-mapper-asl-1.9.11.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/jackson-core-asl-1.9.11.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/commons-cli-1.2.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../lib/ant-eclipse-1.0-jvm1.2.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../zookeeper-3.5.1-alpha.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../src/java/lib/ant-eclipse-1.0-jvm1.2.jar:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin/../conf:
2016-08-29 21:45:19,564 [myid:] - INFO  [main:Environment@109] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2016-08-29 21:45:19,564 [myid:] - INFO  [main:Environment@109] - Client environment:java.io.tmpdir=/tmp
2016-08-29 21:45:19,564 [myid:] - INFO  [main:Environment@109] - Client environment:java.compiler=<NA>
2016-08-29 21:45:19,565 [myid:] - INFO  [main:Environment@109] - Client environment:os.name=Linux
2016-08-29 21:45:19,565 [myid:] - INFO  [main:Environment@109] - Client environment:os.arch=amd64
2016-08-29 21:45:19,565 [myid:] - INFO  [main:Environment@109] - Client environment:os.version=4.4.0-31-generic
2016-08-29 21:45:19,565 [myid:] - INFO  [main:Environment@109] - Client environment:user.name=root
2016-08-29 21:45:19,565 [myid:] - INFO  [main:Environment@109] - Client environment:user.home=/root
2016-08-29 21:45:19,565 [myid:] - INFO  [main:Environment@109] - Client environment:user.dir=/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin
2016-08-29 21:45:19,565 [myid:] - INFO  [main:Environment@109] - Client environment:os.memory.free=114MB
2016-08-29 21:45:19,567 [myid:] - INFO  [main:Environment@109] - Client environment:os.memory.max=227MB
2016-08-29 21:45:19,568 [myid:] - INFO  [main:Environment@109] - Client environment:os.memory.total=119MB
2016-08-29 21:45:19,570 [myid:] - INFO  [main:ZooKeeper@716] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@149ee0f1
Welcome to ZooKeeper!
2016-08-29 21:45:19,596 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1138] - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2016-08-29 21:45:19,603 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@980] - Socket connection established, initiating session, client: /127.0.0.1:43574, server: localhost/127.0.0.1:2181
JLine support is enabled
2016-08-29 21:45:19,630 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1400] - Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x20044a0c51d0000, negotiated timeout = 30000

WATCHER::

WatchedEvent state:SyncConnected type:None path:null
[zk: localhost:2181(CONNECTED) 0] 
[zk: localhost:2181(CONNECTED) 0] config -c
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.zookeeper.server.util.ConfigUtils.getClientConfigStr(ConfigUtils.java:56)
	at org.apache.zookeeper.cli.GetConfigCommand.exec(GetConfigCommand.java:64)
	at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:674)
	at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:577)
	at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:360)
	at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:320)
	at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:280)
root@BLR1000010865:/home/Rakesh/Zookeeper/18_Aug/cluster/zookeeper-3.5.1-alpha/bin# 
","[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2539,Minor,Rakesh Kumar Singh,Fixed,2016-09-08T14:22:48.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Throwing nullpointerException when run the command ""config -c"" when client port is mentioned as separate and not like new style",2017-05-18T03:43:56.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>]",4.0
Rakesh Kumar Singh,"[<JIRA Component: name='server', id='12312382'>]",2016-08-26T09:08:22.000+0000,Rakesh Kumar Singh,"Scenario 1 :-
When provide path for ""dataDir"" with heading space, it is taking correct path (by trucating space) for snapshot but zookeeper_server.pid is getting created in root (/) folder
Steps to reproduce:-
1. Configure the dataDir
dataDir= /home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/data
Here there is a space after dataDir=
2. Start Zookeeper Server
3. The snapshot is getting created at location mentioned above by truncating the heading space but
zookeeper_server.pid is getting created at root (/) folder
","[<JIRA Version: name='3.5.3', id='12335444'>]",Bug,ZOOKEEPER-2537,Major,Rakesh Kumar Singh,Duplicate,2016-09-12T22:13:03.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"When provide path for ""dataDir"" with heading space, it is taking correct path (by trucating space) for snapshot but zookeeper_server.pid is getting created in root (/) folder",2019-12-19T23:02:01.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>]",2.0
Rakesh Kumar Singh,"[<JIRA Component: name='server', id='12312382'>]",2016-08-26T08:57:14.000+0000,Rakesh Kumar Singh,"Scenario 1:-
When provide path for ""dataDir"" with trailing space, it is taking correct path (by trucating space) for snapshot but creating temporary file with some junk folder name for zookeeper_server.pid

Steps to reproduce:-
1. Configure the dataDir
dataDir=/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/data 
Here there is a space after /data 
2. Start Zookeeper Server
3. The snapshot is getting created at location mentioned above by truncating the trailing space but
one temp folder with junk name (like -> D29D4X~J) is getting created for zookeeper_server.pid

Scenario 2:-
When configure the heading and trailing space in above mentioned scenario. the temp folder is getting created in zookeeper/bin folder","[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2536,Major,Rakesh Kumar Singh,Fixed,2016-09-08T04:07:21.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"When provide path for ""dataDir"" with trailing space, it is taking correct path (by trucating space) for snapshot but creating temporary file with some junk folder name for zookeeper_server.pid",2017-05-18T03:43:59.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>]",4.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-08-26T06:58:32.000+0000,Rakesh Kumar Singh,"zookeeper_server's pid should be created once server is started completely and not before, problem with current approach :-
Scenario:-
1. Configure below in zoo.cfg
dataDir=/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/data
2. Start zookeeper server
3. Change the dataDir to suppose
dataDir=/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/data1
4. Again start zookeeper without stopping zookeeper.

Though it will fail to start the server as port is already bind, it will create ""zookeeper_server.pid"" file with different PID inside and ""version-2"" folder

Now again revertback the dataDir path and stop the server, the new created folder and file at step 4 remained
",[],Bug,ZOOKEEPER-2535,Major,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"zookeeper_server's pid should be created once server is started completely and not before, problem with current approach",2016-08-26T07:00:44.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",1.0
,"[<JIRA Component: name='security', id='12329414'>]",2016-08-26T05:46:58.000+0000,Rakesh Kumar Singh,"Configuration for IP address (zookeeper.server.ip=) should be introduced and zookeeper server should bind to that particular IP only not to all IPs of the system. As of now zookeeper is binding to 0.0.0.0 (all IPs - 127.0.0.1, IPv4, IPv6) of the system.",[],Bug,ZOOKEEPER-2534,Major,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Configuration for IP address (zookeeper.server.ip=) should be introduced and zookeeper server should bind to that particular IP only not to all IPs of the system,2016-08-26T05:47:45.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>]",1.0
Rakesh Kumar Singh,"[<JIRA Component: name='java client', id='12312381'>]",2016-08-25T09:16:18.000+0000,Rakesh Kumar Singh,"Close the zkCli using ""close"" command and then connect using ""connect"" then provide some invalid input, it closing the channel and connecting again

Steps to reproduce:-
1. Connect the Zookeeper server using zkCli
2. close the connection using ""close""
3. Connect again using ""connect host""
4. Once connected, input space "" "" and hit enter
It is closing the channel and establishing again.
Console log is as below:-

[zk: localhost:2181(CONNECTED) 5] close
2016-08-25 16:59:04,854 [myid:] - INFO  [main:ClientCnxnSocketNetty@201] - channel is told closing
2016-08-25 16:59:04,855 [myid:] - INFO  [main:ZooKeeper@1110] - Session: 0x101a00305cc0008 closed
[zk: localhost:2181(CLOSED) 6] 2016-08-25 16:59:04,855 [myid:] - INFO  [main-EventThread:ClientCnxn$EventThread@542] - EventThread shut down for session: 0x101a00305cc0008
2016-08-25 16:59:04,856 [myid:] - INFO  [New I/O worker #1:ClientCnxnSocketNetty$ZKClientHandler@377] - channel is disconnected: [id: 0xd9735868, /0:0:0:0:0:0:0:1:44595 :> localhost/0:0:0:0:0:0:0:1:2181]
2016-08-25 16:59:04,856 [myid:] - INFO  [New I/O worker #1:ClientCnxnSocketNetty@201] - channel is told closing
connect 10.18.101.80
2016-08-25 16:59:14,410 [myid:] - INFO  [main:ZooKeeper@716] - Initiating client connection, connectString=10.18.101.80 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@19c50523
[zk: 10.18.101.80(CONNECTING) 7] 2016-08-25 16:59:14,417 [myid:] - INFO  [main-SendThread(10.18.101.80:2181):ClientCnxn$SendThread@1138] - Opening socket connection to server 10.18.101.80/10.18.101.80:2181. Will not attempt to authenticate using SASL (unknown error)
2016-08-25 16:59:14,426 [myid:] - INFO  [main-SendThread(10.18.101.80:2181):ClientCnxnSocketNetty$ZKClientPipelineFactory@363] - SSL handler added for channel: null
2016-08-25 16:59:14,428 [myid:] - INFO  [New I/O worker #10:ClientCnxn$SendThread@980] - Socket connection established, initiating session, client: /10.18.101.80:58871, server: 10.18.101.80/10.18.101.80:2181
2016-08-25 16:59:14,428 [myid:] - INFO  [New I/O worker #10:ClientCnxnSocketNetty$1@146] - channel is connected: [id: 0xa8f6b724, /10.18.101.80:58871 => 10.18.101.80/10.18.101.80:2181]
2016-08-25 16:59:14,473 [myid:] - INFO  [New I/O worker #10:ClientCnxn$SendThread@1400] - Session establishment complete on server 10.18.101.80/10.18.101.80:2181, sessionid = 0x101a00305cc0009, negotiated timeout = 30000

WATCHER::

WatchedEvent state:SyncConnected type:None path:null",[],Bug,ZOOKEEPER-2533,Minor,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Close the zkCli using ""close"" command and then connect using ""connect"" then provide some invalid input, it closing the channel and connecting again",2016-09-23T05:10:34.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",1.0
Rakesh Kumar Singh,"[<JIRA Component: name='java client', id='12312381'>]",2016-08-25T09:11:01.000+0000,Rakesh Kumar Singh,"1. a) Connect to zookeeper using zkCli
 b) just input space and then hit enter

2. a) Connect to zookeeper using zkCli and hit enter it will come as connected 
 b) just input space and then hit enter

Console log is as below:-
[zk: localhost:2181(CONNECTING) 0] 2016-08-25 16:54:48,143 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxnSocketNetty$ZKClientPipelineFactory@363] - SSL handler added for channel: null
2016-08-25 16:54:48,175 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@980] - Socket connection established, initiating session, client: /0:0:0:0:0:0:0:1:44592, server: localhost/0:0:0:0:0:0:0:1:2181
2016-08-25 16:54:48,178 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxnSocketNetty$1@146] - channel is connected: [id: 0xd03f4226, /0:0:0:0:0:0:0:1:44592 => localhost/0:0:0:0:0:0:0:1:2181]
2016-08-25 16:54:48,288 [myid:] - INFO  [New I/O worker #1:ClientCnxn$SendThread@1400] - Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x101a00305cc0005, negotiated timeout = 30000

WATCHER::

WatchedEvent state:SyncConnected type:None path:null
   
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.zookeeper.ZooKeeperMain$MyCommandOptions.getArgArray(ZooKeeperMain.java:171)
	at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:613)
	at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:577)
	at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:360)
	at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:320)
	at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:280)



----------------------------------------
After connection is established, input space and hit enter


[zk: localhost:2181(CONNECTING) 0] 2016-08-25 16:56:22,445 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxnSocketNetty$ZKClientPipelineFactory@363] - SSL handler added for channel: null
2016-08-25 16:56:22,481 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@980] - Socket connection established, initiating session, client: /0:0:0:0:0:0:0:1:44594, server: localhost/0:0:0:0:0:0:0:1:2181
2016-08-25 16:56:22,484 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxnSocketNetty$1@146] - channel is connected: [id: 0xe6d3a461, /0:0:0:0:0:0:0:1:44594 => localhost/0:0:0:0:0:0:0:1:2181]
2016-08-25 16:56:22,597 [myid:] - INFO  [New I/O worker #1:ClientCnxn$SendThread@1400] - Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x101a00305cc0007, negotiated timeout = 30000

WATCHER::

WatchedEvent state:SyncConnected type:None path:null

[zk: localhost:2181(CONNECTED) 0]  
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.zookeeper.ZooKeeperMain$MyCommandOptions.getArgArray(ZooKeeperMain.java:171)
	at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:613)
	at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:577)
	at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:360)
	at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:320)
	at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:280)","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-2532,Minor,Rakesh Kumar Singh,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,zkCli throwing nullpointerException instead it should provide help when invalid input is entered,2022-02-03T08:36:24.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>]",4.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-08-25T06:58:44.000+0000,Rakesh Kumar Singh,"Configuration as ""zookeeper.secure=true/false"" can be introduced and reading and verifying all ssl related configuration (like secureport, keystore, truststore, corresponding password) should be done only when ""zookeeper.secure"" flag is true",[],Bug,ZOOKEEPER-2531,Major,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Configuration as ""zookeeper.secure=true/false"" can be introduced and reading and verifying all ssl related configuration (like secureport, keystore, truststore, corresponding password) should be done only when ""zookeeper.secure"" flag is true",2016-09-16T11:36:50.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-08-25T06:40:12.000+0000,Rakesh Kumar Singh,"When zookeeper started in SSL mode, set a ""get"" watcher on a znode from zkCli client, restart the zkCli, the ""Data"" watcher still present. Trying removing that watcher fails saying no watcher available

Steps to reproduce:-

Start Zookeeper server in ssl mode by configuring all required ssl configuration
Start zkCli and set a ""Data"" watcher ""get -w ""
Restart the zkCli client
Check the watcher. Still the Data watcher is available
Try to remove the watcher using removewachers, it fails saying no watcher available.
BLR1000007042:~ # echo wchs | netcat localhost 3181
1 connections watching 1 paths
Total watches:1
BLR1000007042:~ # echo wchs | netcat localhost 3181
1 connections watching 1 paths
Total watches:1

Client log as below:-

[zk: localhost:2181(CONNECTED) 0] get -w /test
hello1
[zk: localhost:2181(CONNECTED) 1] quit
2016-08-25 14:22:00,706 [myid:] - INFO [main:ClientCnxnSocketNetty@201] - channel is told closing
2016-08-25 14:22:00,706 [myid:] - INFO [main:ZooKeeper@1110] - Session: 0x1019f8940e20000 closed
2016-08-25 14:22:00,706 [myid:] - INFO [main-EventThread:ClientCnxn$EventThread@542] - EventThread shut down for session: 0x1019f8940e20000
2016-08-25 14:22:00,707 [myid:] - INFO [New I/O worker #1:ClientCnxnSocketNetty$ZKClientHandler@377] - channel is disconnected: [id: 0x9dab735e, /127.0.0.1:57415 :> localhost/127.0.0.1:2181]
2016-08-25 14:22:00,707 [myid:] - INFO [New I/O worker #1:ClientCnxnSocketNetty@201] - channel is told closing
BLR1000007042:/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin # ./zkCli.sh
/usr/bin/java
Connecting to localhost:2181
2016-08-25 14:22:15,079 [myid:] - INFO [main:Environment@109] - Client environment:zookeeper.version=3.5.1-alpha--1, built on 08/18/2016 08:20 GMT
2016-08-25 14:22:15,083 [myid:] - INFO [main:Environment@109] - Client environment:host.name=BLR1000007042
2016-08-25 14:22:15,084 [myid:] - INFO [main:Environment@109] - Client environment:java.version=1.7.0_79
2016-08-25 14:22:15,086 [myid:] - INFO [main:Environment@109] - Client environment:java.vendor=Oracle Corporation
2016-08-25 14:22:15,086 [myid:] - INFO [main:Environment@109] - Client environment:java.home=/usr/java/jdk1.7.0_79/jre
2016-08-25 14:22:15,086 [myid:] - INFO [main:Environment@109] - Client environment:java.class.path=/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin/../build/classes:/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin/../build/lib/*.jar:/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin/../lib/slf4j-log4j12-1.7.5.jar:/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin/../lib/slf4j-api-1.7.5.jar:/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin/../lib/servlet-api-2.5-20081211.jar:/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin/../lib/netty-3.7.0.Final.jar:/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin/../lib/log4j-1.2.16.jar:/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin/../lib/jline-2.11.jar:/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin/../lib/jetty-util-6.1.26.jar:/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin/../lib/jetty-6.1.26.jar:/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin/../lib/javacc.jar:/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin/../lib/jackson-mapper-asl-1.9.11.jar:/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin/../lib/jackson-core-asl-1.9.11.jar:/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin/../lib/commons-cli-1.2.jar:/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin/../lib/ant-eclipse-1.0-jvm1.2.jar:/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin/../zookeeper-3.5.1-alpha.jar:/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin/../src/java/lib/ant-eclipse-1.0-jvm1.2.jar:/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin/../conf:
2016-08-25 14:22:15,087 [myid:] - INFO [main:Environment@109] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2016-08-25 14:22:15,087 [myid:] - INFO [main:Environment@109] - Client environment:java.io.tmpdir=/tmp
2016-08-25 14:22:15,087 [myid:] - INFO [main:Environment@109] - Client environment:java.compiler=
2016-08-25 14:22:15,087 [myid:] - INFO [main:Environment@109] - Client environment:os.name=Linux
2016-08-25 14:22:15,087 [myid:] - INFO [main:Environment@109] - Client environment:os.arch=amd64
2016-08-25 14:22:15,087 [myid:] - INFO [main:Environment@109] - Client environment:os.version=3.0.76-0.11-default
2016-08-25 14:22:15,087 [myid:] - INFO [main:Environment@109] - Client environment:user.name=root
2016-08-25 14:22:15,087 [myid:] - INFO [main:Environment@109] - Client environment:user.home=/root
2016-08-25 14:22:15,088 [myid:] - INFO [main:Environment@109] - Client environment:user.dir=/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin
2016-08-25 14:22:15,088 [myid:] - INFO [main:Environment@109] - Client environment:os.memory.free=52MB
2016-08-25 14:22:15,090 [myid:] - INFO [main:Environment@109] - Client environment:os.memory.max=227MB
2016-08-25 14:22:15,090 [myid:] - INFO [main:Environment@109] - Client environment:os.memory.total=57MB
2016-08-25 14:22:15,095 [myid:] - INFO [main:ZooKeeper@716] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@272f15b0
Welcome to ZooKeeper!
2016-08-25 14:22:15,182 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@1138] - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
JLine support is enabled
[INFO] Unable to bind key for unsupported operation: backward-delete-word
[INFO] Unable to bind key for unsupported operation: backward-delete-word
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[zk: localhost:2181(CONNECTING) 0] 2016-08-25 14:22:15,502 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxnSocketNetty$ZKClientPipelineFactory@363] - SSL handler added for channel: null
2016-08-25 14:22:15,537 [myid:] - INFO [New I/O worker #1:ClientCnxn$SendThread@980] - Socket connection established, initiating session, client: /127.0.0.1:57420, server: localhost/127.0.0.1:2181
2016-08-25 14:22:15,540 [myid:] - INFO [New I/O worker #1:ClientCnxnSocketNetty$1@146] - channel is connected: [id: 0xfc4fe483, /127.0.0.1:57420 => localhost/127.0.0.1:2181]
2016-08-25 14:22:15,673 [myid:] - INFO [New I/O worker #1:ClientCnxn$SendThread@1400] - Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1019f8940e20001, negotiated timeout = 30000

WATCHER::

WatchedEvent state:SyncConnected type:None path:null

[zk: localhost:2181(CONNECTED) 0] removewatches /test -a
2016-08-25 14:24:14,420 [myid:] - ERROR [New I/O worker #1:ClientCnxn@725] - Failed to find watcher!
org.apache.zookeeper.KeeperException$NoWatcherException: KeeperErrorCode = No such watcher for /test
at org.apache.zookeeper.ZooKeeper$ZKWatchManager.containsWatcher(ZooKeeper.java:377)
at org.apache.zookeeper.ZooKeeper$ZKWatchManager.removeWatcher(ZooKeeper.java:252)
at org.apache.zookeeper.WatchDeregistration.unregister(WatchDeregistration.java:58)
at org.apache.zookeeper.ClientCnxn.finishPacket(ClientCnxn.java:712)
at org.apache.zookeeper.ClientCnxn.access$1500(ClientCnxn.java:97)
at org.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:948)
at org.apache.zookeeper.ClientCnxnSocketNetty$ZKClientHandler.messageReceived(ClientCnxnSocketNetty.java:419)
at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109)
at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
KeeperErrorCode = No such watcher for /test
[zk: localhost:2181(CONNECTED) 1]",[],Bug,ZOOKEEPER-2530,Major,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"When zookeeper started in SSL mode, set a ""get"" watcher on a znode from zkCli client, restart the zkCli, the ""Data"" watcher still present. Trying removing that watcher fails saying no watcher available",2017-06-28T07:08:51.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-08-24T20:45:01.000+0000,Ramnatthan Alagappan,"ZooKeeper cluster can become unavailable if power failures happen at certain specific points in time. 

Details:

I am running a three-node ZooKeeper cluster. I perform a simple update from a client machine. 

When I try to update a value, ZooKeeper creates a new log file (for example, when the current log is fully utilized). First, it creates the file and appends some header information to the newly created log. The system call sequence looks like below:

creat(log.200000001)
append(log.200000001, offset=0,  count=16)

Now, if a power failure happens just after the creat of the log file but before the append of the header information, the node simply crashes with an EOF exception. If the same problem occurs at two or more nodes in my three-node cluster, the entire cluster becomes unavailable as the majority of servers have crashed because of the above problem.  

A power failure at the same time across multiple nodes may be possible in single data center or single rack deployment scenarios. 




",[],Bug,ZOOKEEPER-2528,Critical,Ramnatthan Alagappan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper cluster can become unavailable due to power failures,2017-03-09T20:39:38.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-08-24T11:34:52.000+0000,Rakesh Kumar Singh,"Connection watch is not getting cleared when watch is created as part of get and it is fired as part of set and client is closed
Steps to reproduce:-

Configure the Zookeeper in ssl mode and start the same
connect to zookeeper using ./zkCli.sh
Check the watch status as zero.
set watch as below :-
get -w /test
Check the watch it is like below:-
BLR1000007042:/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin # echo wchs | netcat 10.18.101.80 2181
1 connections watching 1 paths
Total watches:1
let the watch fire as part of below-
set /test hello
Here watch is fired when set is done
Close the client
Check for the watch. It is not zero but 1
BLR1000007042:/home/Rakesh/Zookeeper/18_Aug/zookeeper-3.5.1-alpha/bin # echo wchs | netcat 10.18.101.80 2181
1 connections watching 0 paths
Total watches:0
If we repeat again and again it will keep increasing.
Tried without SSL mode and it is working fine in that mode.",[],Bug,ZOOKEEPER-2527,Major,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Connection watch is not getting cleared when watch is created as part of get and it is fired as part of set and client is closed,2017-06-28T07:08:10.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-08-23T09:36:56.000+0000,Rakesh Kumar Singh,"Throwing Exception at zookeeper server side whenever client is closing the connection

2016-08-17 20:38:09,030 [myid:] - WARN [New I/O worker #4:ClientCnxnSocketNetty$ZKClientHandler@432] - Exception caught: [id: 0xbb7a218d, /0:0:0:0:0:0:0:1:41679 :> localhost/0:0:0:0:0:0:0:1:2181] EXCEPTION: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
at org.jboss.netty.handler.ssl.SslHandler$6.run(SslHandler.java:1580)
at org.jboss.netty.channel.socket.ChannelRunnableWrapper.run(ChannelRunnableWrapper.java:40)
at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:71)
at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:57)
at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
at org.jboss.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
at org.jboss.netty.handler.ssl.SslHandler.channelClosed(SslHandler.java:1566)
at org.jboss.netty.channel.Channels.fireChannelClosed(Channels.java:468)
at org.jboss.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:376)
at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:93)
at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109)
at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)",[],Bug,ZOOKEEPER-2525,Minor,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Throwing Exception at zookeeper server side whenever client is closing the connection,2016-08-23T09:36:56.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",2.0
,[],2016-08-23T09:35:26.000+0000,Rakesh Kumar Singh,"Start zkServer in ssl mode, start zkCli in non-ssl mode but on ssl port then try to quit at client, it takes almost 30 seconds to quit.

Steps to reproduce:-

Configure the details required for SSL in zkServer and zkclient
Make ""-Dzookeeper.client.secure=false"" for client
Configure the clientPort=2181 and secureClientPort=3181 zoo.cfg file
Start zookeeper server and then client as ""zkCli.sh -server :3181
Then quit at client console
It takes almost 30 seconds to quit.

Log at server side is attached.
Log at client side is as below:-

[zk: 10.18.101.80:3181(CONNECTING) 0] quit
2016-08-18 15:02:19,076 [myid:] - INFO [New I/O worker #1:ClientCnxnSocketNetty$ZKClientHandler@377] - channel is disconnected: [id: 0x07b576fd, /10.18.101.80:42228 :> 10.18.101.80/10.18.101.80:3181]
2016-08-18 15:02:19,077 [myid:] - INFO [New I/O worker #1:ClientCnxnSocketNetty@201] - channezkServer.txtl is told closing
2016-08-18 15:02:19,080 [myid:] - INFO [main:ClientCnxnSocketNetty@201] - channel is told closing
2016-08-18 15:02:19,080 [myid:] - INFO [main:ZooKeeper@1110] - Session: 0x0 closed
2016-08-18 15:02:19,080 [myid:] - INFO [main-EventThread:ClientCnxn$EventThread@542] - EventThread shut down for session: 0x0",[],Bug,ZOOKEEPER-2524,Major,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Start zkServer in ssl mode, start zkCli in non-ssl mode but on ssl port then try to quit at client, it takes almost 30 seconds to quit ",2016-08-23T09:35:26.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-08-23T09:33:35.000+0000,Rakesh Kumar Singh,"Scenario1 :-

Configure zookeeper for all but configure wrong ssl password
Start the zookeeper server, it starts fine. No logs that zookeeper is not started in ssl mode though it has not started in ssl mode but normal mode.
Try to connect with client, it will failed to connect in ssl port or normal port (as client is started in ssl mode)
Scenario 2:-

Configure the ssl port as 0 and start the server
The log level is in info saying not binding ..",[],Bug,ZOOKEEPER-2523,Minor,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,No proper logging when zookeeper failed to start in ssl mode.,2016-08-23T09:33:35.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",1.0
,[],2016-08-23T09:12:32.000+0000,Rakesh Kumar Singh,By default we configure the clientPort as 2181 in zookeeper and secureClientPort as 3181 in SSL enable zookeeper. But when start the corresponding client (which is in ssl enabled mode) fails to connect because it is trying to connect 2181 in secure mode.,[],Bug,ZOOKEEPER-2522,Minor,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Need to document that default ""secureClientPort"" for Client will be 2181 and not ""clientPort"" when starting in SSL enable mode",2016-08-23T09:12:32.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-08-23T06:49:47.000+0000,Rakesh Kumar Singh,"space should be truncated while reading password for keystore/truststore which is required to configure while SSL enabled.
As of now if we configure the password with any heading/trailing space, the zookeeper server will fail to start.",[],Bug,ZOOKEEPER-2521,Minor,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,space should be truncated while reading password for keystore/truststore which is required to configure while SSL enabled,2022-02-03T08:50:12.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",5.0
,"[<JIRA Component: name='security', id='12329414'>]",2016-08-23T06:44:34.000+0000,Rakesh Kumar Singh,"When enable SSL support we need to configure the keystore and truststore details and corresponding password in zkEnv.sh/bat file. Currenlty we can only pass plain password which is not accepted because of security issue.
We should provide the provision to handle the latest encryption mechanism to handle this.",[],Bug,ZOOKEEPER-2520,Major,Rakesh Kumar Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper should accept and handle encrypted password (AES) which is required to be passed for keystore and truststore in case of SSL support,2016-08-23T06:44:34.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",3.0
Andrew Grasso,"[<JIRA Component: name='c client', id='12312380'>]",2016-08-22T12:45:36.000+0000,Andrew Grasso,"0 does not correspond to any of the defined states for the zookeeper handle, so a client should not expect to see this value. But in the function {{handle_error}}, we set {{zh->state = 0}}, which a client may then see. Instead, we should set our state to be {{ZOO_CONNECTING_STATE}}. 

At some point the code moved away from 0 as a valid state and introduced the defined states. This broke the fix to ZOOKEEPER-800, which checks if state is 0 to know if the handle has been created but has not yet connected. We now use {{ZOO_NOTCONNECTED_STATE}} to mean this, so the check for this in {{zoo_add_auth}} must be changed.

We saw this error in 3.4.6, but I believe it remains present in trunk.",[],Bug,ZOOKEEPER-2519,Major,Andrew Grasso,,,This issue is being actively worked on at the moment by the assignee.,In Progress,0.0,zh->state should not be 0 while handle is active,2016-09-22T18:36:09.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",3.0
Mohammad Arshad,[],2016-08-18T23:41:52.000+0000,Benjamin Jaton,"In ClientCnxnSocket.java the parsing of the system property is erroneous:

{code}packetLen = Integer.getInteger(
  clientConfig.getProperty(ZKConfig.JUTE_MAXBUFFER),
  ZKClientConfig.CLIENT_MAX_PACKET_LENGTH_DEFAULT
);{code}

Javadoc of Integer.getInteger states ""The first argument is treated as the name of a system property"", whereas here the value of the property is passed.

Instead I believe the author meant to write something like:

{code}packetLen = Integer.parseInt(
  clientConfig.getProperty(
    ZKConfig.JUTE_MAXBUFFER,
    String.valueOf(ZKClientConfig.CLIENT_MAX_PACKET_LENGTH_DEFAULT)
  )
);{code}
","[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2517,Blocker,Benjamin Jaton,Fixed,2016-12-30T22:02:36.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,jute.maxbuffer is ignored,2017-05-18T03:43:55.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",8.0
,"[<JIRA Component: name='c client', id='12312380'>]",2016-08-17T18:43:54.000+0000,Hadriel Kaplan,"The C-client has a function called {{calculate_interval()}} in {{zookeeper.c}}, whose purpose is to determine the number of milliseconds difference between a start and end time. 

Unfortunately its logic is invalid, if the number of microseconds of the end time happens to be less than the number of microseconds of the start time - which it will be about half the time, since the end time could be in the next second interval. Such a case would yield a very big negative number, making the function return an invalid value.

Instead of re-creating the wheel, the {{calculate_interval()}} should use the {{timersub()}} function from {{time.h}} if it's available - if it's not #define'd, then #define it. (it's a macro, and the source code for it is readily available)
",[],Bug,ZOOKEEPER-2516,Minor,Hadriel Kaplan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,C client calculates invalid time interval for pings et al,2016-08-24T22:27:00.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",2.0
,[],2016-08-17T16:34:52.000+0000,Paul Millar,"While calling SessionTrackerImpl#shutdown does result in the thread eventually stopping, it takes up to expirationInterval (3 seconds, by default) for the thread to finally die.

Since the thread is not a daemon, this delays the shutdown of any application that makes use of ZooKeeper.

I believe the issue is simple to fix: if the shutdown method notified the thread from within object's monitor then this issue will be resolved.",[],Bug,ZOOKEEPER-2515,Major,Paul Millar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,SessionTrackerImpl non-daemon thread slow to shutdown,2016-08-17T16:34:52.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",4.0
Mohammad Arshad,"[<JIRA Component: name='server', id='12312382'>]",2016-08-14T00:46:35.000+0000,Alexander Shraer,"In Learner.java there are exceptions being thrown in case majorChange = true, i.e., a reconfig is encountered in the stream of updates from the leader. There may be two problems in the way such exceptions are thrown:
1. important actions, e.g., processTxn, will not be done if an exception is thrown
2. its unclear that the learner will be able to continue where it left off in the process of syncing with the leader, if that sync is interrupted by an exception.

This requires further investigation. Whereas similar code in Follower and Observer is extensively tested, this code in Learner isn't tested as much. We could build on the test case developed in ZOOKEEPER-2172 to make sure this code works properly.",[],Bug,ZOOKEEPER-2513,Critical,Alexander Shraer,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,majorChange exceptions during leader sync,2016-09-05T23:02:57.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",4.0
Ted Dunning,[],2016-08-10T23:50:38.000+0000,Ted Dunning,"junit.framework.Assert is deprecated. The code should use org.junit.Assert instead.

Patch coming shortly.",[],Bug,ZOOKEEPER-2510,Major,Ted Dunning,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,org.apache.zookeeper.server.NettyServerCnxnTest uses wrong import for junit,2022-02-03T08:50:19.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>]",1.0
Ted Dunning,[],2016-08-10T04:55:36.000+0000,Ted Dunning,"The Netty connection handling logic fails to clean up watches on connection close. This causes memory to leak.

I will have a repro script available soon and a fix. I am not sure how to build a unit test since we would need to build an entire server and generate keys and such. Advice on that appreciated.

","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-2509,Major,Ted Dunning,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Secure mode leaks memory,2022-02-03T08:36:25.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>]",6.0
Ling Mao,"[<JIRA Component: name='server', id='12312382'>]",2016-08-05T16:01:00.000+0000,Michael Han,"In ZK documentation, we have:

""The myid file consists of a single line containing only the text of that machine's id. So myid of server 1 would contain the text ""1"" and nothing else. The id must be unique within the ensemble and should have a value between 1 and 255.""

This however is not enforced in code, which should be fixed either in documentation that we remove the restriction of the range 1-255 or in code we enforce such constraint.
 Discussion thread:
 [http://zookeeper-user.578899.n2.nabble.com/Is-myid-actually-limited-to-1-255-td7581270.html]

 ",[],Bug,ZOOKEEPER-2503,Major,Michael Han,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,do a hard constraints on the number of myid which must between 0 and 255,2019-04-21T11:06:02.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.4.11', id='12339207'>]",8.0
Abraham Fine,[],2016-08-03T15:39:05.000+0000,Abraham Fine,,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2500,Major,Abraham Fine,Fixed,2016-08-05T21:07:20.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Fix compilation warnings for CliException classes,2017-05-18T03:44:04.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",3.0
Michael Han,"[<JIRA Component: name='c client', id='12312380'>]",2016-08-03T05:01:14.000+0000,Michael Han,"In C client, we use reference counting to decide if a given zh handle can be destroyed or not. This requires we always make sure to call api_prolog (which increment the counter) and api_epilog (which decrease the counter) in pairs, for a given call context. 

In zookeeper_process, there is a place where the code will return without invoking api_epilog, which would lead to potential zh resource leak.","[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.3', id='12335444'>]",Bug,ZOOKEEPER-2498,Major,Michael Han,Fixed,2016-08-03T18:22:01.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Potential resource leak in C client when processing unexpected / out of order response,2016-09-04T05:27:18.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.2', id='12331981'>]",4.0
,[],2016-08-01T07:52:39.000+0000,Kazuaki Banzai,"If a client tries to execute some illegal operations inside a transaction, ZooKeeper throws an exception.

Some exceptions such as NodeExistsException should have a path to indicate where the exception occurred.
ZooKeeper clients can get the path by calling method getPath.
However, this method returns null if the exception occurs inside a transaction.
For example, when a client calls create /a and create /a in a transaction,
ZooKeeper throws NodeExistsException but getPath returns null.
In normal operation (outside transactions), the path information is set correctly.
The patch only shows this bug occurs with NoNode exception and NodeExists exception,
but this bug seems to occur with any exception which needs a path information:
When an error occurred in a transaction, ZooKeeper creates an ErrorResult instance to represent error result.

However, the ErrorResult class doesn't have a field for a path where an error occurred(See src/java/main/org/apache/zookeeper/OpResult.java for more details).",[],Bug,ZOOKEEPER-2496,Major,Kazuaki Banzai,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"When inside a transaction, some exceptions do not have path information set.",2016-09-27T12:24:08.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.1', id='12326786'>]",7.0
,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='server', id='12312382'>]",2016-07-31T20:51:46.000+0000,Ramnatthan Alagappan,"ZooKeeper cluster completely stalls with *no* transactions making progress when a storage related error (such as *ENOSPC, EDQUOT, EIO*) is encountered by the current *leader*. 

Surprisingly, the same errors in some circumstances cause the node to completely crash and therefore allowing other nodes in the cluster to become the leader and make progress with transactions. Interestingly, the same errors if encountered while initializing a new log file causes the current leader to go to weird state (but does not crash) where it thinks it is the leader (and so does not allow others to become the leader). *This causes the entire cluster to freeze. *

Here is the stacktrace of the leader:

------------------------------------------------

2016-07-11 15:42:27,502 [myid:3] - INFO  [SyncThread:3:FileTxnLog@199] - Creating new log file: log.200000001
2016-07-11 15:42:27,505 [myid:3] - ERROR [SyncThread:3:ZooKeeperCriticalThread@49] - Severe unrecoverable error, from thread : SyncThread:3
java.io.IOException: Disk quota exceeded
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:345)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at org.apache.zookeeper.server.persistence.FileTxnLog.append(FileTxnLog.java:211)
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.append(FileTxnSnapLog.java:314)
	at org.apache.zookeeper.server.ZKDatabase.append(ZKDatabase.java:476)
	at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:140)

------------------------------------------------

From the trace and the code, it looks like the problem happens only when a new log file is initialized and only when there are errors in two cases:

1. Error during the append of *log header*.
2. Error during *padding zero bytes to the end of the log*.
 
If similar errors happen when writing some other blocks of data, then the node just completely crashes allowing others to be elected as a new leader. These two blocks of the newly created log file are special as they take a different error recovery code path -- the node does not completely crash but rather certain threads are killed but supposedly the quorum holding thread stays up thereby preventing others to become the new leader.  This causes the other nodes to think that there is no problem with the leader but the cluster just becomes unavailable for any subsequent operations such as read/write. ",[],Bug,ZOOKEEPER-2495,Major,Ramnatthan Alagappan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Cluster unavailable on disk full(ENOSPC), disk quota(EDQUOT), disk write error(EIO) errors",2020-04-16T12:01:26.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>]",5.0
Abraham Fine,"[<JIRA Component: name='documentation', id='12312422'>]",2016-07-28T19:35:27.000+0000,Abraham Fine,[~rakeshr] mades this suggestion in ZOOKEEPER-2477 and I thought this was worth implementing in its own jira,[],Bug,ZOOKEEPER-2494,Major,Abraham Fine,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"reword documentation to say ""a simple shell to execute file-like operations"" instead of ""a shell in which to execute simple file-system-like operations""",2022-02-03T08:50:26.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.2', id='12331981'>]",2.0
spooky000,[],2016-07-28T02:09:40.000+0000,spooky000,"gethostname return error before Win32WSAStartup on windows.

in log_env function.
gethostname(buf, sizeof(buf));
LOG_INFO(LOGCALLBACK(zh), ""Client environment:host.name=%s"", buf);

buf will be uninitialized buffer.
","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-2492,Trivial,spooky000,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,gethostname return error before Win32WSAStartup on windows.,2022-02-03T08:36:22.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",2.0
Andrew Schwartzmeyer,"[<JIRA Component: name='c client', id='12312380'>]",2016-07-28T01:59:14.000+0000,spooky000,"Visual Studio 2015  supports snprintf.
#define snprintf _snprintf throw error.
","[<JIRA Version: name='3.5.4', id='12340141'>]",Bug,ZOOKEEPER-2491,Minor,spooky000,Duplicate,2017-07-27T22:03:54.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,C client build error in vs 2015 ,2019-12-19T23:01:51.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",4.0
spooky000,"[<JIRA Component: name='c client', id='12312380'>]",2016-07-27T06:17:10.000+0000,spooky000,"in addrvec_contains function
this memcmp always return false on windows release build.

for (i = 0; i < avec->count; i++)
{
    if(memcmp(&avec->data[i], addr, INET_ADDRSTRLEN) == 0)
        return 1;
}

because..
#define INET_ADDRSTRLEN  16 on linux.
#define INET_ADDRSTRLEN  22 on windows.




","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",Bug,ZOOKEEPER-2490,Major,spooky000,Duplicate,2020-04-14T10:01:14.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,infinitely connect on windows,2020-05-11T15:41:06.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",3.0
gaoshu,"[<JIRA Component: name='server', id='12312382'>]",2016-07-25T23:05:44.000+0000,Michael Han,"Access to shuttingDownLE in QuorumPeer is not synchronized here:

https://github.com/apache/zookeeper/blob/3c37184e83a3e68b73544cebccf9388eea26f523/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeer.java#L1066
https://github.com/apache/zookeeper/blob/3c37184e83a3e68b73544cebccf9388eea26f523/src/java/main/org/

The access should be synchronized as the same variable might be accessed 
in QuormPeer::restartLeaderElection, which is synchronized.",[],Bug,ZOOKEEPER-2488,Major,Michael Han,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Unsynchronized access to shuttingDownLE in QuorumPeer,2022-02-03T08:50:15.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",5.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2016-07-19T11:11:39.000+0000,Kerem Yazici,"We have 5 node ensemble setup in our tst environment and we are seeing the error below on two of the nodes. We have 3 nodes on the first data centre and 2 nodes on the second data centre and all of them are managed by exhbitor.

The problem is that the nodes from the same data centres cannot talk to the leader node on the same data centre but can talk to the other nodes without any issue. 

If I bounce the leader node to force another node to get elected then the other nodes that are on the same data centre start throwing the below exception.

I'm sure the problem is with the dns name resolution but I would like to understand how zookeeper resolves these dns names and what might be the issue here so I can go back to our unix team and get this fixed.


{code}
2016-07-19 10:48:54,711 [myid:4] - WARN  [WorkerSender[myid=4]:QuorumCnxManager@400] - Cannot open channel to 5 at election address server1.dns.name/192.168.1.3:4882
java.net.ConnectException: Connection refused
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:589)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:381)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.toSend(QuorumCnxManager.java:354)
        at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.process(Fast       LeaderElection.java:452)
        at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.run(FastLead       erElection.java:433)
        at java.lang.Thread.run(Thread.java:745)

{code}



{code:title=zoo.cfg|borderStyle=solid}
#Auto-generated by Exhibitor - Fri Jul 15 11:30:52 BST 2016
#Fri Jul 15 11:30:52 BST 2016
server.2=server2.dns.name\:4881\:4882\:observer
autopurge.purgeInterval=4
server.1=server1.dns.name\:4881\:4882
initLimit=50
syncLimit=2
clientPort=4880
tickTime=2001
server.5=server5.dns.name\:4881\:4882
dataDir=/opt/app/datafabric/data/zookeeper
server.4=server4.dns.name\:4881\:4882
dataLogDir=/path/to/datalogdir
server.3=server3.dns.name\:4881\:4882
~
{code}",[],Bug,ZOOKEEPER-2480,Minor,Kerem Yazici,Not A Problem,2016-07-19T15:45:23.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"Exhibitor and zookeeper, Cannot open channel to x at election address",2016-07-19T15:45:23.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>]",1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='documentation', id='12312422'>]",2016-07-19T01:12:01.000+0000,Richard Shaw,"There's a duplicate 'the' on the Zookeeper wiki
Zab in words

Phase 1: Establish an epoch
4.1",[],Bug,ZOOKEEPER-2478,Trivial,Richard Shaw,Fixed,2016-07-19T05:02:13.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Duplicate 'the' on Zab in words Wiki page,2019-01-31T13:48:21.000+0000,[],4.0
Abraham Fine,"[<JIRA Component: name='documentation', id='12312422'>]",2016-07-17T17:09:30.000+0000,Patrick D. Hunt,"The documentation tends to refer to the c cli shell when citing examples of how to interact with ZK, rather than using the Java cli shell. Given the Java cli is much better maintained and more featureful the docs should refer to that instead. Also the c cli was originally meant to be a sample/example of c client usage rather than a true cli tool.","[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2477,Major,Patrick D. Hunt,Fixed,2016-07-29T22:02:35.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,documentation should refer to Java cli shell and not C cli shell,2016-09-04T05:28:45.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.2', id='12331981'>]",5.0
Alexander Shraer,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2016-07-16T00:25:52.000+0000,Jordan Zimmerman,"Contrary to the documentation, it is not possible to upgrade via reconfig a Participant+Observer cluster to a Participant+Participant cluster. KeeperException.NewConfigNoQuorum is thrown instead.

PrepRequestProcessor should recognize this special case and let it pass. Test will be enclosed shortly. I'll work on a fix as well, but I imagine that [~shralex] will want to look at it.",[],Bug,ZOOKEEPER-2476,Critical,Jordan Zimmerman,Not A Bug,2016-07-17T00:21:32.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Not possible to upgrade via reconfig a Participant+Observer cluster to a Participant+Participant cluster,2016-07-17T00:21:32.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",2.0
Mohammad Arshad,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='documentation', id='12312422'>]",2016-07-14T14:06:05.000+0000,Mohammad Arshad,"Generate zookeeper api doc using {{ant javadoc}} command
open build/docs/api/index.html, ZKClientConfig is not present","[<JIRA Version: name='3.5.10', id='12349434'>, <JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-2475,Major,Mohammad Arshad,Invalid,2021-01-07T10:59:41.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Include ZKClientConfig API in zoookeeper javadoc,2021-03-28T08:54:30.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",3.0
Ling Mao,"[<JIRA Component: name='java client', id='12312381'>]",2016-07-14T12:52:16.000+0000,Timothy James Ward,"The new constructors for ZooKeeper instances take a ZKClientConfig, which is great, however there is no way to reattach to an existing session.

New constructors should be added to allow passing a session id and password when using ZKClientConfig.

 ","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-2474,Major,Timothy James Ward,Fixed,2019-03-12T18:09:16.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,add a way for client to reattach to a session when using ZKClientConfig,2019-05-20T17:50:46.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",4.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-07-11T02:31:51.000+0000,yangkun,"When debug zookeeper, run ZooKeeperServerMain then pass 4 args, e.g: 2181 F:\\zk\\data 2000 30, that is:

clientPort = 2181
dataDir = F:\\zk\\data
tickTime = 2000
maxClientCnxns = 30
But ServerConfig#parse(String[]) method has a little problem：

public void parse(String[] args) {
    ...
    if (args.length == 3) {
        tickTime = Integer.parseInt(args[2]);
    }
    if (args.length == 4) {
        maxClientCnxns = Integer.parseInt(args[3]);
    }
}
The problem is:

    if (args.length == 4) {
        maxClientCnxns = Integer.parseInt(args[3]);
    }
It can't parse tickTime, igone the tickTime.This coe snippet should be:

    if (args.length == 4) {
        tickTime = Integer.parseInt(args[2]);
        maxClientCnxns = Integer.parseInt(args[3]);
    }",[],Bug,ZOOKEEPER-2472,Minor,yangkun,Duplicate,2016-07-11T03:18:59.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ServerConfig#parse(String[]) parse params has problems,2016-07-11T03:43:14.000+0000,[],2.0
Michael Han,"[<JIRA Component: name='java client', id='12312381'>]",2016-07-08T15:52:07.000+0000,Dan Benediktson,"ClientCnxnSocket uses a member variable ""now"" to track the current time, and lastSend / lastHeard variables to track socket liveness. Implementations, and even ClientCnxn itself, are expected to call both updateNow() to reset ""now"" to System.currentTimeMillis, and then call updateLastSend()/updateLastHeard() on IO completions.

This is a fragile contract, so it's not surprising that there's a bug resulting from it: ClientCnxn.SendThread.run() calls updateLastSendAndHeard() as soon as startConnect() returns, but it does not call updateNow() first. I expect when this was written, either the expectation was that startConnect() was an asynchronous operation and that updateNow() would have been called very recently, or simply the requirement to call updateNow() was forgotten at this point. As far as I can see, this bug has been present since the ""updateNow"" method was first introduced in the distant past. As it turns out, since startConnect() calls HostProvider.next(), which can sleep, quite a lot of time can pass, leaving a big gap between ""now"" and now.

If you are using very short session timeouts (one of our ZK ensembles has many clients using a 1-second timeout), this is potentially disastrous, because the sleep time may exceed the connection timeout itself, which can potentially result in the Java client being stuck in a perpetual reconnect loop. The exact code path it goes through in this case is complicated, because there has to be a previously-closed socket still waiting in the selector (otherwise, the first timeout evaluation will not fail because ""now"" still hasn't been updated, and then the actual connect timeout will be applied in ClientCnxnSocket.doTransport()) so that select() will harvest the IO from the previous socket and updateNow(), resulting in the next loop through ClientCnxnSocket.SendThread.run() observing the spurious timeout and failing. In practice it does happen to us fairly frequently; we only got to the bottom of the bug yesterday. Worse, when it does happen, the Zookeeper client object is rendered unusable: it's stuck in a perpetual reconnect loop where it keeps sleeping, opening a socket, and immediately closing it.

I have a patch. Rather than calling updateNow() right after startConnect(), my fix is to remove the ""now"" member variable and the updateNow() method entirely, and to instead just call System.currentTimeMillis() whenever time needs to be evaluated. I realize there is a benefit (aside from a trivial micro-optimization not worth worrying about) to having the time be ""fixed"", particularly for truth in the logging: if time is fixed by an updateNow() call, then the log for a timeout will still show exactly the same value the code reasoned about. However, this benefit is in my opinion not enough to merit the fragility of the contract which led to this (for us) highly impactful and difficult-to-find bug in the first place.

I'm currently running ant tests locally against my patch on trunk, and then I'll upload it here.",[],Bug,ZOOKEEPER-2471,Major,Dan Benediktson,,,This issue is being actively worked on at the moment by the assignee.,In Progress,0.0,"Java Zookeeper Client incorrectly considers time spent sleeping as time spent connecting, potentially resulting in infinite reconnect loop",2019-12-13T01:26:08.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>]",8.0
Edward Ribeiro,"[<JIRA Component: name='server', id='12312382'>]",2016-07-08T04:59:48.000+0000,Alexander Shraer,"Based on bug report from ykgarfield:

ServerConfig#parse(String[]) method has the following code:

    public void parse(String[] args) {
        ...
        if (args.length == 3) {
                tickTime = Integer.parseInt(args[2]);
        }
        if (args.length == 4) {
                maxClientCnxns = Integer.parseInt(args[3]);
        }
    }
    ```

So if args.length == 4 tickTime isn't parsed.","[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2470,Trivial,Alexander Shraer,Fixed,2016-12-22T03:12:38.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ServerConfig#parse(String[])  ignores tickTime,2017-03-31T09:01:16.000+0000,"[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>]",8.0
Sergey Shelukhin,[],2016-07-08T00:55:19.000+0000,Sergey Shelukhin,"{noformat}
                        int retry = 1;
                        while (retry >= 0) {
                            try {
                                reLogin();
                                break;
                            } catch (LoginException le) {
                                if (retry > 0) {
                                    --retry;
                                    // sleep for 10 seconds.
                                    try {
                                        Thread.sleep(10 * 1000);
                                    } catch (InterruptedException e) {
                                        LOG.error(""Interrupted during login retry after LoginException:"", le);
                                        throw le;
                                    }
                                } else {
                                    LOG.error(""Could not refresh TGT for principal: "" + principal + ""."", le);
                                }
                            }
                        }
{noformat}
will retry forever. Should return like the one above",[],Bug,ZOOKEEPER-2469,Major,Sergey Shelukhin,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,infinite loop in ZK re-login,2016-08-05T19:06:57.000+0000,[],9.0
SREENIVASULUDANDU,"[<JIRA Component: name='java client', id='12312381'>]",2016-07-07T05:34:42.000+0000,Joshi Shankar,"Setquota and delquota commands does not work 
Steps to reproduce:
1. Create Test Node 1
    create -c /TestZookeeperNodeNumber ""testdata""

2. Create Test Node 2
    create -c /TestZookeeperNodeBytes ""testdatabytes""
    
3. Set Quota Using 
    setquota -n 1 /TestZookeeperNodeNumber

4. Set Quota Using 
     setquota -b 10 /TestZookeeperNodeBytes
    AlreadySelectedException is thrown by Apache CLI. It is  bug in Apache CLI (https://issues.apache.org/jira/browse/CLI-183)
    
   We can fix by upgrading Apache CLI From(*commons-cli-1.2.jar*) to (*commons-cli-1.3.1.jar*) 
Client Operation Log:


[zk: localhost:2181(CONNECTED) 2] create -c /TestZookeeperNodeNumber ""testdata""
Created /TestZookeeperNodeNumber
[zk: localhost:2181(CONNECTED) 3] create -c /TestZookeeperNodeBytes ""testdatabytes""
Created /TestZookeeperNodeBytes
[zk: localhost:2181(CONNECTED) 4] setquota -n 1 /TestZookeeperNodeNumber
[zk: localhost:2181(CONNECTED) 5] setquota -b 10 /TestZookeeperNodeBytes
The option 'b' was specified but an option from this group has already been selected: 'n'
ZooKeeper -server host:port cmd args
        addauth scheme auth
        close
        config [-c] [-w] [-s]
        connect host:port
        create [-s] [-e] [-c] path [data] [acl]
        delete [-v version] path
        deleteall path
        delquota [-n|-b] path
        get [-s] [-w] path
        getAcl [-s] path
        history
        listquota path
        ls [-s] [-w] path
        ls2 path [watch]
        printwatches on|off
        quit
        reconfig [-s] [-v version] [[-file path] | [-members serverID=host:port1:port2;port3[,...]*]
] | [-add serverId=host:port1:port2;port3[,...]]* [-remove serverId[,...]*]
        redo cmdno
        removewatches path [-c|-d|-a] [-l]
        rmr path
        set [-s] [-v version] path data
        setAcl [-s] [-v version] path acl
        setquota -n|-b val path
        stat [-w] path
        sync path
",[],Bug,ZOOKEEPER-2468,Major,Joshi Shankar,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,SetQuota and DelQuota,2016-12-12T08:39:26.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.1', id='12326786'>]",5.0
Rakesh Kumar Singh,"[<JIRA Component: name='java client', id='12312381'>]",2016-07-07T02:51:02.000+0000,Joshi Shankar,"When negative value of argument is passed to redo command .

[zk: localhost:2181(CONNECTED) 0] redo -1
Exception in thread ""main"" java.lang.NullPointerException
        at java.util.StringTokenizer.<init>(Unknown Source)
        at java.util.StringTokenizer.<init>(Unknown Source)
        at org.apache.zookeeper.ZooKeeperMain$MyCommandOptions.parseCommand(ZooKeeperMain.java:227)
        at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:645)
        at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:588)
        at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:360)
        at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:323)
        at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:282)","[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2467,Minor,Joshi Shankar,Fixed,2016-10-16T13:18:38.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,NullPointerException when redo Command is passed negative value,2017-03-31T09:01:13.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>]",6.0
Michael Han,"[<JIRA Component: name='c client', id='12312380'>]",2016-07-05T21:04:28.000+0000,Flavio Paiva Junqueira,"I've been looking at {{Zookeeper_simpleSystem::testFirstServerDown}} and I observed the following behavior. The list of servers to connect contains two servers, let's call them S1 and S2. The client never connects, but the odd bit is the sequence of servers that the client tries to connect to:

{noformat}
S1
S2
S1
S1
S1
<keeps repeating S1>
{noformat}

It intrigued me that S2 is only tried once and never again. Checking the code, here is what happens. Initially, {{zh->reconfig}} is 1, so in {{zoo_cycle_next_server}} we return an address from {{get_next_server_in_reconfig}}, which is taken from {{zh->addrs_new}} in this test case. The attempt to connect fails, and {{handle_error}} is invoked in the error handling path. {{handle_error}} actually invokes {{addrvec_next}} which changes the address pointer to the next server on the list.

After two attempts, it decides that it has tried all servers in {{zoo_cycle_next_server}} and sets {{zh->reconfig}} to zero. Once {{zh->reconfig == 0}}, we have that each call to {{zoo_cycle_next_server}} moves the address pointer to the next server in {{zh->addrs}}. But, given that {{handle_error}} also moves the pointer to the next server, we end up moving the pointer ahead twice upon every failed attempt to connect, which is wrong.","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-2466,Major,Flavio Paiva Junqueira,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Client skips servers when trying to connect,2022-02-03T08:36:23.000+0000,[],7.0
Edward Ribeiro,"[<JIRA Component: name='documentation', id='12312422'>]",2016-07-05T17:38:03.000+0000,Chris Nauroth,"As reported by [~eribeiro], all of the documentation pages show a copyright notice dating ""2008-2013"".  This issue tracks updating the copyright notice on all documentation pages to show the current year.","[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2465,Blocker,Chris Nauroth,Fixed,2016-12-30T22:59:52.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Documentation copyright notice is out of date.,2017-03-31T09:01:12.000+0000,[],8.0
Jordan Zimmerman,"[<JIRA Component: name='server', id='12312382'>]",2016-07-04T22:21:01.000+0000,Stefano Salmaso,"I would like to expose you to a problem that we are experiencing.
We are using a cluster of 7 zookeeper and we use them to implement a distributed lock using Curator (http://curator.apache.org/curator-recipes/shared-reentrant-lock.html)
So .. we tried to play with the servers to see if everything worked properly and we stopped and start servers to see that the system worked well
(like stop 03, stop 05, stop 06, start 05, start 06, start 03)

We saw a strange behavior.
The number of znodes grew up without stopping (normally we had 4000 or 5000, we got to 60,000 and then we stopped our application)

In zookeeeper logs I saw this (on leader only, one every minute)

2016-07-04 14:53:50,302 [myid:7] - ERROR [ContainerManagerTask:ContainerManager$1@84] - Error checking containers
java.lang.NullPointerException
       at org.apache.zookeeper.server.ContainerManager.getCandidates(ContainerManager.java:151)
       at org.apache.zookeeper.server.ContainerManager.checkContainers(ContainerManager.java:111)
       at org.apache.zookeeper.server.ContainerManager$1.run(ContainerManager.java:78)
       at java.util.TimerThread.mainLoop(Timer.java:555)
       at java.util.TimerThread.run(Timer.java:505)

We have not yet deleted the data ... so the problem can be reproduced on our servers","[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2464,Major,Stefano Salmaso,Fixed,2017-02-11T15:09:49.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,NullPointerException on ContainerManager,2021-05-06T12:14:48.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",9.0
,[],2016-07-03T15:38:17.000+0000,Flavio Paiva Junqueira,"I noticed that all multi tests seem to be timing out and they are failing silently. This is the output that I'm observing:

{noformat}
Zookeeper_multi::testCreate : assertion : elapsed 10001
Zookeeper_multi::testCreateDelete : assertion : elapsed 10001
Zookeeper_multi::testInvalidVersion : assertion : elapsed 10001
Zookeeper_multi::testNestedCreate : assertion : elapsed 10001
Zookeeper_multi::testSetData : assertion : elapsed 10001
Zookeeper_multi::testUpdateConflict : assertion : elapsed 10001
Zookeeper_multi::testDeleteUpdateConflict : assertion : elapsed 10001
Zookeeper_multi::testAsyncMulti : assertion : elapsed 10001
Zookeeper_multi::testMultiFail : assertion : elapsed 10001
Zookeeper_multi::testCheck : assertion : elapsed 10001
Zookeeper_multi::testWatch : assertion : elapsed 10001
Zookeeper_multi::testSequentialNodeCreateInAsyncMulti : assertion : elapsed 10001
{noformat}","[<JIRA Version: name='3.5.3', id='12335444'>]",Bug,ZOOKEEPER-2463,Blocker,Flavio Paiva Junqueira,Cannot Reproduce,2016-11-24T16:31:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,TestMulti is broken in the C client,2019-12-19T23:01:56.000+0000,[],2.0
Enrico Olivelli,"[<JIRA Component: name='java client', id='12312381'>]",2016-06-30T07:39:13.000+0000,Enrico Olivelli,"during the vote of 3.5.2-ALPHA RC 0 we found a Maven dependency to javacc in published pom for zookeeper

{code}
<dependency>
<groupId>net.java.dev.javacc</groupId>
<artifactId>javacc</artifactId>
<version>5.0</version><scope>compile</scope>
</dependency>
{code}

this dependency appears not to be useful and should be removed

this was the tested pom: https://repository.apache.org/content/groups/staging/org/apache/zookeeper/zookeeper/3.5.2-alpha/zookeeper-3.5.2-alpha.pom","[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2460,Critical,Enrico Olivelli,Fixed,2017-03-16T18:09:45.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Remove javacc dependency from public Maven pom,2017-05-18T03:49:46.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",7.0
Flavio Paiva Junqueira,[],2016-06-29T22:43:50.000+0000,Flavio Paiva Junqueira,"Bubbling up the Netty notice. According to the ALv2 item 4, we need to include it in our top notice, it isn't sufficient to have it in the bundle. ","[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2459,Blocker,Flavio Paiva Junqueira,Fixed,2016-06-30T04:13:06.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Update NOTICE file with Netty notice,2016-07-21T20:18:27.000+0000,[],4.0
Flavio Paiva Junqueira,[],2016-06-29T21:48:58.000+0000,Flavio Paiva Junqueira,"In ZOOKEEPER-2235, we changed the license of the servlet-api dependency to the correct one ALv2, but didn't remove the CDDL license file, which is incorrect. This jira removes the incorrect license file.","[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2458,Major,Flavio Paiva Junqueira,Fixed,2016-06-30T03:59:10.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Remove license file for servlet-api dependency,2016-07-21T20:18:18.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",3.0
Flavio Paiva Junqueira,[],2016-06-29T21:47:20.000+0000,Flavio Paiva Junqueira,"In ZOOKEEPER-2235, we changed the license of the servlet-api dependency to the correct one ALv2, but didn't remove the CDDL license file, which is incorrect. This jira removes the incorrect license file.","[<JIRA Version: name='3.5.2', id='12331981'>]",Bug,ZOOKEEPER-2457,Major,Flavio Paiva Junqueira,Duplicate,2016-06-29T21:55:13.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Remove license file for servlet-api dependency,2019-12-19T23:01:57.000+0000,[],1.0
,"[<JIRA Component: name='c client', id='12312380'>]",2016-06-28T06:01:36.000+0000,pradeep,"Hi Folks,

I am hitting an error in my C client code and below are the set of operations I perform:

  1.  Zookeeper Client connected to Zookeeper server S1 and a new server S2 gets added.
  2.  monitor zookeeper server config at the client and on change of server config, call zoo_set_server
from the client
  3.  client can issue operations like zoo_get just after the call to zoo_set_servers
  4.  I can see that the zookeeper thread logs connect to the new server just after the zoo_get
call

2016-04-11 03:46:50,655:1207(0xf26ffb40):ZOO_INFO@check_events@2345: initiated connection
to server [128.0.0.5:61728]

2016-04-11 03:46:50,658:1207(0xf26ffb40):ZOO_INFO@check_events@2397: session establishment
complete on server [128.0.0.5:61728], sessionId=0x4000001852c000c, negotiated timeout=20000

  5.  Some times I find errors like below:

2016-04-11 03:46:50,662:1207(0xf26ffb40):ZOO_ERROR@handle_socket_error_msg@2923: Socket [128.0.0.5:61728]
zk retcode=-2, errno=115(Operation now in progress): unexpected server response: expected
0x570b82fa, but received 0x570b82f9

  1.
zoo_get returns (-2) indicating that ZRUNTIMEINCONSISTENCY<http://zookeeper.sourcearchive.com/documentation/3.2.2plus-pdfsg3/zookeeper_8h_bb1a0a179f313b2e44ee92369c438a4c.html#bb1a0a179f313b2e44ee92369c438a4c9eabb281ab14c74db3aff9ab456fa7fe>


What is the issue here? should I be retry the operation zoo_get operation? Or should I wait
for the zoo_set_server to complete (like wait for the connection establishment notification)

Thanks,",[],Bug,ZOOKEEPER-2455,Major,pradeep,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,unexpected server response ZRUNTIMEINCONSISTENCY,2022-02-03T08:50:13.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",6.0
,"[<JIRA Component: name='c client', id='12312380'>]",2016-06-25T16:49:28.000+0000,Markus Thies,"It seems that this is a bug equivalent to the issue ZOOKEEPER-1374.

make[5]: Entering directory '/home/pi/Downloads/mesos-0.28.2/build/3rdparty/zookeeper-3.4.5/src/c'
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -DTHREADED -g -O2 -D_GNU_SOURCE -MT libzkmt_la-mt_adaptor.lo -MD -MP -MF "".deps/libzkmt_la-mt_adaptor.Tpo"" -c -o libzkmt_la-mt_adaptor.lo `test -f 'src/mt_adaptor.c' || echo './'`src/mt_adaptor.c; \
then mv -f "".deps/libzkmt_la-mt_adaptor.Tpo"" "".deps/libzkmt_la-mt_adaptor.Plo""; else rm -f "".deps/libzkmt_la-mt_adaptor.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O2 -D_GNU_SOURCE -MT libzkmt_la-mt_adaptor.lo -MD -MP -MF .deps/libzkmt_la-mt_adaptor.Tpo -c src/mt_adaptor.c  -fPIC -DPIC -o libzkmt_la-mt_adaptor.o
/tmp/ccs0G1lb.s: Assembler messages:
/tmp/ccs0G1lb.s:1589: Error: bad instruction `lock xaddl r1,[r0]'
Makefile:743: recipe for target 'libzkmt_la-mt_adaptor.lo' failed",[],Bug,ZOOKEEPER-2453,Minor,Markus Thies,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Cannot compile on ARM: ""Error: bad instruction `lock xaddl""",2016-06-29T02:51:29.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",7.0
Abraham Fine,"[<JIRA Component: name='quorum', id='12312379'>]",2016-06-23T20:17:42.000+0000,Chris Nauroth,"Via code inspection, I see that the ""server.nnn"" configuration key does not support literal IPv6 addresses because the property value is split on "":"". In v3.4.3, the problem is in QuorumPeerConfig:

{noformat}
String parts[] = value.split("":"");
InetSocketAddress addr = new InetSocketAddress(parts[0],
                        Integer.parseInt(parts[1]));
{noformat}

In the current trunk (http://svn.apache.org/viewvc/zookeeper/trunk/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeer.java?view=markup) this code has been refactored into QuorumPeer.QuorumServer, but the bug remains:

{noformat}
String serverClientParts[] = addressStr.split("";"");
String serverParts[] = serverClientParts[0].split("":"");
addr = new InetSocketAddress(serverParts[0],
                        Integer.parseInt(serverParts[1]));
{noformat}

This bug probably affects very few users because most will naturally use a hostname rather than a literal IP address. But given that IPv6 addresses are supported for clients via ZOOKEEPER-667 it seems that server support should be fixed too.","[<JIRA Version: name='3.4.9', id='12334700'>]",Bug,ZOOKEEPER-2452,Critical,Chris Nauroth,Fixed,2016-08-11T16:14:03.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Back-port ZOOKEEPER-1460 to 3.4 for IPv6 literal address support.,2016-09-04T05:27:05.000+0000,[],4.0
,[],2016-06-23T19:32:43.000+0000,Sergey Shelukhin,"Is there (or could there be) a way to set up security for ZK client that doesn't involve calls like {noformat}
System.setProperty(ZooKeeperSaslClient.LOGIN_CONTEXT_NAME_KEY, SASL_LOGIN_CONTEXT_NAME);
{noformat}?
I was looking at an unrelated security configuration issue and stumbled upon this pattern; we use (at least) 2 ZK connections from the same process, that (for now) use the same config but different context names, one of which is in a library out of our control. Unless I'm missing something with this pattern it seems extremely brittle. Or unless there's an alternative approach already; if there is, hadoop-common and hive don't use it atm, old approach seems prevalent.

There should be an approach that is at least slightly more solid, like say public globals... maybe even threadlocals!

",[],Bug,ZOOKEEPER-2451,Major,Sergey Shelukhin,Duplicate,2016-06-23T19:56:38.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,do not make use of system properties for security configuration,2016-06-23T19:56:38.000+0000,[],3.0
Michael Han,"[<JIRA Component: name='security', id='12329414'>, <JIRA Component: name='server', id='12312382'>]",2016-06-22T21:11:40.000+0000,Michael Han,"This JIRA recreates ZOOKEEPER-2432 which was deleted as the collateral damage during the spamming fighting effort Apache Infrastructure Team did weeks ago. Recreate the JIRA for the record so external documentations can link back to this JIRA.

The SslHandler in Netty before 3.9.2 allows remote attackers to cause a denial of service (infinite loop and CPU consumption) via a crafted SSLv2Hello message [1]. We are using netty 3.7.x in ZK for 3.4/3.5/3.6, which is affected by this vulnerability.

[1] http://cve.mitre.org/cgi-bin/cvename.cgi?name=2014-3488
[2] http://netty.io/news/
","[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2450,Critical,Michael Han,Fixed,2016-06-22T21:12:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Upgrade Netty version due to security vulnerability (CVE-2014-3488),2016-07-21T20:18:38.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",2.0
Vishal Khandelwal,[],2016-06-15T12:56:26.000+0000,Vishal Khandelwal,"StaticHostProvider --> resolveAndShuffle method adds all of the address which are valid in the quorum to the list, shuffles them and sends back to client connection class. If after shuffling if first node appear to be the one which is not reachable, Clientcnx.SendThread.run will keep on connecting to the failure till a timeout and the moves to a different node. This adds up random delay in zookeeper connection in case a host is down. Rather we could check if host is reachable in StaticHostProvider and ignore isReachable is false. Same as we do for UnknownHostException Exception.

This can tested using following test code by providing a valid host which is not reachable. for quick test comment Collections.shuffle(tmpList, sourceOfRandomness); in StaticHostProvider.resolveAndShuffle

{code}
 @Test
  public void test() throws Exception {
    EventsWatcher watcher = new EventsWatcher();
    QuorumUtil qu = new QuorumUtil(1);
    qu.startAll();
    
    ZooKeeper zk =
        new ZooKeeper(""<hostnamet:2181,"" + qu.getConnString(), 180 * 1000, watcher);
    
    watcher.waitForConnected(CONNECTION_TIMEOUT * 5);
    Assert.assertTrue(""connection Established"", watcher.isConnected());
    zk.close();    
  }
{code}

Following fix can be added to StaticHostProvider.resolveAndShuffle
{code}
 if(taddr.isReachable(4000 // can be some value)) {
                      tmpList.add(new InetSocketAddress(taddr, address.getPort()));
                    } 
{code}","[<JIRA Version: name='3.4.9', id='12334700'>]",Bug,ZOOKEEPER-2447,Major,Vishal Khandelwal,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Zookeeper adds  good delay when one of the quorum host is not reachable,2016-11-11T07:30:19.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>]",7.0
,"[<JIRA Component: name='build', id='12312383'>]",2016-06-08T10:03:01.000+0000,Sergio Fernández,"Assembling a {{NOTICE}} file in a project that uses Zookeeper I've realized the {{pom.xml}} does not declare the license, at least in the whole {{3.4.x}} branch, e.g., https://repo1.maven.org/maven2/org/apache/zookeeper/zookeeper/3.4.8/zookeeper-3.4.8.pom",[],Bug,ZOOKEEPER-2446,Critical,Sergio Fernández,Duplicate,2016-06-08T10:32:02.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,License missing in the pom.xml,2016-06-08T10:32:02.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-06-08T08:56:59.000+0000,Ahaha,"I want to add a new node to a test cluster with three node. So I tried with the folling steps: 1. I copied one foller zookeeper directory, and edit clientPort, dataDir , logDir, myid file, and then add a new record server.newId=hostname:newPort:newEPort into zoo.cfg, for test I keep the leader configuration there and remove the other two configuration, I tested ,this failed. So I open the zoo.cfg , copied the lead configuration line just edit the serverId, and this should be a wrong configuration file as the server does not match with the serverid. But strangely , it successed. ",[],Bug,ZOOKEEPER-2445,Critical,Ahaha,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Add a new node to cluster, error config file but sucessed",2016-06-08T14:33:46.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",2.0
,"[<JIRA Component: name='c client', id='12312380'>]",2016-06-07T11:01:08.000+0000,Pawel Rozlach,https://github.com/apache/zookeeper/pull/70,[],Bug,ZOOKEEPER-2444,Major,Pawel Rozlach,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Fix NULL handling for getlogin() call,2016-06-09T04:58:51.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>]",3.0
Chris Nauroth,"[<JIRA Component: name='c client', id='12312380'>]",2016-06-07T09:42:49.000+0000,Paul Asmuth,"Hey,

I encountered some issues with the zookeepeer c client.

The problem starts in the zookeeper_init_internal method. A lot of initialization work is performed here and if any of the initialization routines fails, the code jumps to the ""abort"" label to perform various cleanup tasks [1]. The conceptual issue is that a bunch of the cleanup code tries to take locks on the zk structure that are only intialized in adaptor_init in line 1181 (at the very end of the zookeeper_init_internal method) [2]. So if we fail before reaching adaptor_init this causes trouble.

One specific instance of an invalid memory access that this causes is in free_completions [3]. Here, in line 1651 zoo_lock_auth will fail because it tries to grab an invalid mutex, after which the a_list struct is uninitialized (the linked list next pointer points to random memory) and subsequently the free routine segfaults.

An easy way to trigger this bug-path is to pass an invalid hostname, or do anything else that causes the zookeeper_init_internal method to fail before adaptor_init.

In my local checkout/codebase, I have added correct initialization for the a_list struct in the free_completions routine, which at least fixes the segfault for now. However this still leaves the issue that the cleanup code tries to grab a lot of invalid locks, which all fail. I think in order to fix this properly, one would need to do a larger refactoring of the code (add another adaptor_preinit routine to the adaptor interface maybe?) and I wasn't sure if that would be appreciated, so I didn't attach a patch for now. If someone wants me to try and clean this up, I would be happy to give it a try.

PS: I think this bug was introduced in SVN #1719528, which - as it seems - tried to work around the uninitialized locks problem by adding an int return code to all the lock_xxx functions, allowing them to indicate a failure. The change introduce the invalid memory access since some (always required) init code is only run after the lock was obtained successfully.

However, I think there is a much large issue with the change and I think it must be reverted. Trying to lock an uninitialized mutex is undefined behaviour on POSIX and may lead to deadlocks, etc.

>> If mutex does not refer to an initialized mutex object, the behavior of pthread_mutex_lock(), pthread_mutex_trylock(), and pthread_mutex_unlock() is undefined.

http://pubs.opengroup.org/onlinepubs/9699919799/functions/pthread_mutex_lock.html

[1] https://github.com/apache/zookeeper/blob/trunk/src/c/src/zookeeper.c#L1078
[2] https://github.com/apache/zookeeper/blob/trunk/src/c/src/zookeeper.c#L1181
[3] https://github.com/apache/zookeeper/blob/trunk/src/c/src/zookeeper.c#L1651

------------------

BACKTRACE

Program received signal SIGSEGV, Segmentation fault.

0x000000010004f6d5 in free_auth_completion (a_list=0x7fff5fbff048) at /deps/3rdparty/zookeeper/source/src/zookeeper.c:260
260             tmp = tmp->next;

#0  0x000000010004f6d5 in free_auth_completion (a_list=0x7fff5fbff048) at /deps/3rdparty/zookeeper/source/src/zookeeper.c:260
#1  0x000000010004f500 in free_completions (zh=0x1003022f0, callCompletion=1, reason=-116) at /deps/3rdparty/zookeeper/source/src/zookeeper.c:1219
#2  0x0000000100057bfd in cleanup_bufs (zh=0x1003022f0, callCompletion=1, rc=-116) at /deps/3rdparty/zookeeper/source/src/zookeeper.c:1227
#3  0x000000010004ee42 in destroy (zh=0x1003022f0) at /deps/3rdparty/zookeeper/source/src/zookeeper.c:393
#4  0x000000010004eaf3 in zookeeper_init (host=0x1006005b0 ""xxxinvalidhostname:2181"", watcher=0x100007670 <xxx::zk_watch_cb(_zhandle*, int, int, char const*, void*)>, 
    recv_timeout=10000, clientid=0x0, context=0x100600350, flags=0) at /deps/3rdparty/zookeeper/source/src/zookeeper.c:877",[],Bug,ZOOKEEPER-2443,Major,Paul Asmuth,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Invalid Memory Access (SEGFAULT) and undefined behaviour in c client,2016-06-15T04:13:28.000+0000,[],3.0
Michael Han,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2016-06-03T23:05:36.000+0000,Michael Han,"The function connectOne() in QuorumCnxManager.java sometimes fails to release a socket allocated by Socket():

{code}
 try {

                if (LOG.isDebugEnabled()) {
                    LOG.debug(""Opening channel to server "" + sid);
                }
                Socket sock = new Socket();
                setSockOpts(sock);
                sock.connect(self.getView().get(sid).electionAddr, cnxTO);
                if (LOG.isDebugEnabled()) {
                    LOG.debug(""Connected to server "" + sid);
                }
                initiateConnection(sock, sid);
            } catch (UnresolvedAddressException e) {
                // Sun doesn't include the address that causes this
                // exception to be thrown, also UAE cannot be wrapped cleanly
                // so we log the exception in order to capture this critical
                // detail.
                LOG.warn(""Cannot open channel to "" + sid
                        + "" at election address "" + electionAddr, e);
                throw e;
            } catch (IOException e) {
                LOG.warn(""Cannot open channel to "" + sid
                        + "" at election address "" + electionAddr,
                        e);
            }
{code}

Another place in Listener.run() where the client socket is not explicitly closed.","[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2442,Major,Michael Han,Fixed,2016-08-03T04:32:35.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Socket leak in QuorumCnxManager connectOne,2017-05-18T03:43:59.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",7.0
,[],2016-06-03T08:37:31.000+0000,Andras Erdei,"https://github.com/apache/zookeeper/blob/trunk/src/c/src/zookeeper.c#L560
maps getaddrinfo() return values indicating a transient failure (e.g. EAI_AGAIN) to same value (EINVAL) that zookeeper_init() uses to indicate permanent problems (like empty host spec or invalid port).
As a result client code has no way to decide whether it should re-try the initialization or abort (asking for manual intervention).

As discussed e.g. in https://issues.apache.org/jira/browse/MESOS-3790 zookeeper should most likely retry on this and other transient failures automagically.  Independently, the switch above should be fixed to map EAI_* values to different E* values allowing client code some flexibility in handling and reporting errors deemed permanent by zookeeper.

Note that there is a related bug https://issues.apache.org/jira/browse/ZOOKEEPER-1451 -- zookeeper also does not report the problem properly in its own logs, making debugging these problems even harder.
",[],Bug,ZOOKEEPER-2441,Major,Andras Erdei,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,C API maps getaddrinfo() transient and permanent failures to the same value,2016-06-03T08:37:31.000+0000,[],1.0
Ryan Zhang,"[<JIRA Component: name='quorum', id='12312379'>]",2016-06-02T21:01:32.000+0000,Ryan Zhang,"ZOOKEEPER-710 fixed the issue when the request is not a multi request. However, the multi request is handled a little bit differently as the code didn't throw the SESSIONMOVED exception. In addition, the exception is set in the request by the leader so it will be lost in the commit process and by the time the final processor sees it, it will be gone. ",[],Bug,ZOOKEEPER-2440,Major,Ryan Zhang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,permanent SESSIONMOVED error after client app reconnects to zookeeper cluster,2022-02-03T08:50:23.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",6.0
,[],2016-06-02T03:17:20.000+0000,Kazuaki Banzai,"Within a given client connection, the execution of commands on the ZooKeeper server is always ordered, as both synchronous and asynchronous commands are dispatched through queuePacket (directly or indirectly).

In other words, Zookeeper guarantees sequential consistency: updates from a client will be applied in the order that they were sent.

However, the order of asynchronous setACL is not correct on Ubuntu.
When asynchronous setACL is called BEFORE another API is called, asynchronous setACL is applied AFTER another API.

For example, if a client calls

(1) asynchronous setACL to remove all permissions of node ""/"" and

(2) synchronous create to create node ""/a"",
synchronous create should fail, but it succeeds on Ubuntu.

(We can see all permissions of node ""/"" are removed when the client calls getACL to node ""/"" after (2), so (1) is applied AFTER (2). If we call getACL between (1) and (2), the synchronous case works correctly but the asynchronous case still produces the bug.)

The attached unit test reproduces this scenario. It fails on Linux Ubuntu but succeeds on Mac OS X. If used on a heavily loaded server on Mac OS, the test sometimes fails as well but only rarely.",[],Bug,ZOOKEEPER-2439,Major,Kazuaki Banzai,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,The order of asynchronous setACL is not correct.,2016-09-19T08:05:52.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.1', id='12326786'>]",4.0
,[],2016-05-30T08:39:21.000+0000,Ed Rowe,"Consider this scenario:
# Ensemble of nodes A, B, C, D, E with A as the leader
# Nodes A, B get partitioned from C, D, E
# Leader A receives a write _before it detects_ that it has lost its quorum so it logs the write
# Nodes C, D, E elect node C as the leader
# Partition resolves, and nodes A, B rejoin the C, D, E ensemble with C continuing to lead

Depending on whether any updates have occurred in the C, D, E ensemble between steps 4 and 5, the re-joining nodes A,B will either receive a TRUNC or a SNAP. 

*The problems:*
# If updates have occurred in the C,D,E ensemble, SNAP is sent to the re-joining nodes. This occurs because the code in LearnerHandler.queueCommittedProposals() notices that truncation would cross epochs and bails out, leading to a SNAP being sent. A comment in the code says ""We cannot send TRUNC that cross epoch boundary. The learner will crash if it is asked to do so. We will send snapshot this those cases."" LearnerHandler.syncFollower() then logs an ERROR saying ""Unhandled scenario for peer sid: # fall back to use snapshot"" and a comment with this code says ""This should never happen, but we should fall back to sending snapshot just in case."" Presumably since queueCommittedProposals() is intentionally triggering the snapshot logic, this is not an ""unhandled scenario"" that warrants logging an ERROR nor is it a case that ""should never happen"". This inconsistency should be cleaned up. It might also be the case that a TRUNC would work fine in this scenario - see #2 below. 
# If no updates have occurred in the C,D,E ensemble, when nodes A,B rejoin LearnerHandler.syncFollower() goes into the ""Newer than commitedLog, send trunc and done"" clause and sends them a TRUNC. This seems to work fine. However, this would also seem to be a cross-epoch TRUNC, which per the comment discussed above in #1, is expected to cause a crash in the learner. I haven't found anything special about a TRUNC that crosses epochs that would cause a crash in the learner, and I believe that at the time of the TRUNC (or SNAP), the learner is in the same state in both scenarios. It is certainly the case (pending resolution of ZOOKEEPER-1549) that TRUNC is not able to remove data that has been snapshotted, so perhaps detecting “cross-epoch” is a shortcut for trying to detect that scenario? If the resolution of ZOOKEEPER-1549 does not allow TRUNC through a snapshot (or alternately does not allow a benign TRUNC through a snapshot that may not contain uncommitted data), then this case should probably also be a SNAP. If TRUNC is allowed in this case, then perhaps it should also be allowed for case #1, which would be more performant.
   
*While I certainly could have missed something, it would seem that either both cases should be SNAP or both should be a TRUNC given that the learner is in the same state in both cases*.
",[],Bug,ZOOKEEPER-2436,Minor,Ed Rowe,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Inconsistent truncation logic and out of sync logging and comments in recovery code,2016-05-30T08:40:42.000+0000,[],3.0
,"[<JIRA Component: name='build', id='12312383'>]",2016-05-29T07:32:21.000+0000,BourneHan,"Hi All, 

In my projects, I use three ZooKeeper server as an ensemble:
zk1 as a follower on 192.168.25.221,
zk2 as a follower on 192.168.25.222,
zk3 as the leader on 192.168.25.223.
My two programs using ZooKeepers C client run on 192.168.25.221 and 192.168.25.222.

When watched the ZOO_CONNECTED_STATE, my program will use the zookeeper to obtain a lock do the following:
1. Create a ZOO_EPHEMERAL | ZOO_SEQUENCE node under '/Lock/'.
2. Call getChildren( ) on the '/Lock/' node.
3. If the pathname created in step 1 has the lowest sequence number suffix, the program has the lock and do something,then release the lock simply delete the node created in step 1.
4. The program calls exists() with the watch flag set on the lowest sequence number node.
5. if exists( ) returns false, go to step 2. Otherwise, wait for a notification(ZOO_DELETED_EVENT) for the pathname from the previous step before going to step 2.

When I stop a follower such as zk1/zk2, everything is ok, my programs on 192.168.25.221 and 192.168.25.222 do its work orderly under the lock's control.

When I stop the leader such as zk3(I have restarted zk1/zk2), my program on 192.168.25.221 got the lock and release it normally, and my program on 192.168.25.222 detected existence of the node 
created by the program on 192.168.25.221, but keep waiting and can't receive the ZOO_DELETED_EVENT notification.

Does anyone else see the same problem？

PS:
1. The attachment is the log of the zookeeper on 192.168.25.221 and 192.168.25.222 when I stop the leader on 192.168.25.223
2. Actually I have other more programs using ZooKeepers C client run on 192.168.25.221, 192.168.25.222 and 192.168.25.223.
3. The system time on 192.168.25.221 is slower 1 minute and 33 seconds than 192.168.25.222 and 192.168.25.223. so when I stop the leader, it's 2016-05-28 22:33:34 on 192.168.25.221 and 2016-05-28 22:35:07 on 192.168.25.222. ",[],Bug,ZOOKEEPER-2435,Minor,BourneHan,Not A Bug,2016-05-30T14:56:59.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,miss event when the leader stop,2016-06-01T01:25:55.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-05-29T01:56:59.000+0000,Ed Rowe,"# If FileTxnLog.truncate() is unable to delete a log file, it calls LOG.warn() but otherwise does nothing. I think this should be a fatal error not a logged warning. Otherwise those log files are going be be read later when the DB is reloaded and data that should have been removed will still be present. 
# Learner.syncWithLeader() expects ZKDatabase.truncateLog() to return false on failure, and if this occurs it calls System.exit(). However, this will never happen because ZKDatabase.truncateLog() never returns false - instead an exception is thrown on failure. ZKDatabase.truncateLog() calls FileTxnSnapLog.truncateLog() which calls FileTxnLog.truncate(), each of which is documented to return false on failure but none of which ever does in practice. TruncateTest.testTruncationNullLog() clearly expects an exception on error in ZKDatabase.truncateLog() so there are conflicting expectations in the codebase. It appears that if Learner.syncWithLeader() encounters an exception, System.exit() will _not_ be called and instead we land in the main run loop where we'll start the whole thing again. So there are two things to deal with: a) whether we want to do system.exit or go back to the main run loop if truncation fails, and b) sort out the return false vs. throw exception discrepancy and make it consistent (and change the docs as needed).
   
I'm happy to propose a patch, but I'd need people with more experience in the codebase to weigh in on the questions above.
",[],Bug,ZOOKEEPER-2434,Minor,Ed Rowe,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Error handling issues in truncation code,2016-05-29T15:39:51.000+0000,[],5.0
Saurabh jain,"[<JIRA Component: name='security', id='12329414'>, <JIRA Component: name='server', id='12312382'>]",2016-05-17T15:19:36.000+0000,Saurabh Jain,"When connecting from a zookeeper client running in IBM WebSphere Application Server version 8.5.5, with SSL configured in ZooKeeper, the below mentioned exception is observed.

org.jboss.netty.channel.ChannelPipelineException: Failed to initialize a pipeline.
      at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:208)
      at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182)
      at org.apache.zookeeper.ClientCnxnSocketNetty.connect(ClientCnxnSocketNetty.java:112)
      at org.apache.zookeeper.ClientCnxn$SendThread.startConnect(ClientCnxn.java:1130)
      at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1158)
Caused by: org.apache.zookeeper.common.X509Exception$SSLContextException: Failed to create KeyManager
      at org.apache.zookeeper.common.X509Util.createSSLContext(X509Util.java:75)
      at org.apache.zookeeper.ClientCnxnSocketNetty$ZKClientPipelineFactory.initSSL(ClientCnxnSocketNetty.java:358)
      at org.apache.zookeeper.ClientCnxnSocketNetty$ZKClientPipelineFactory.getPipeline(ClientCnxnSocketNetty.java:348)
      at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:206)
      ... 4 more
Caused by: org.apache.zookeeper.common.X509Exception$KeyManagerException: java.security.NoSuchAlgorithmException: SunX509 KeyManagerFactory not available
      at org.apache.zookeeper.common.X509Util.createKeyManager(X509Util.java:129)
      at org.apache.zookeeper.common.X509Util.createSSLContext(X509Util.java:73)
      ... 7 more
Caused by: java.security.NoSuchAlgorithmException: SunX509 KeyManagerFactory not available
      at sun.security.jca.GetInstance.getInstance(GetInstance.java:172)
      at javax.net.ssl.KeyManagerFactory.getInstance(KeyManagerFactory.java:9)
      at org.apache.zookeeper.common.X509Util.createKeyManager(X509Util.java:118)


Reason : IBM websphere uses its own jre and supports only IbmX509 keymanager algorithm which is causing an exception when trying to get an key manager instance using SunX509 which is not supported.
Currently KeyManager algorithm name  (SunX509) is hardcoded in the class X509Util.java.

Possible fix: Instead of having algorithm name hardcoded to SunX509 we can fall back to the default algorithm supported by the underlying jre.

Instead of having this -
KeyManagerFactory kmf = KeyManagerFactory.getInstance(""SunX509"");
TrustManagerFactory tmf = TrustManagerFactory.getInstance(""SunX509"");

can we have ?
KeyManagerFactory kmf = KeyManagerFactory.getInstance(KeyManagerFactory.getDefaultAlgorithm());

TrustManagerFactory tmf = TrustManagerFactory.getInstance(TrustManagerFactory.getDefaultAlgorithm());",[],Bug,ZOOKEEPER-2429,Minor,Saurabh Jain,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,IbmX509 KeyManager and TrustManager algorithm not supported,2022-02-03T08:50:26.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",3.0
Prasanth Mathialagan,[],2016-05-02T14:56:30.000+0000,Flavio Paiva Junqueira,"This test case in SessionTest:
{noformat}
   testSessionReuse
{noformat}
is commented out.",[],Bug,ZOOKEEPER-2421,Major,Flavio Paiva Junqueira,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,testSessionReuse is commented out,2018-06-12T04:43:59.000+0000,[],8.0
Ed Rowe,"[<JIRA Component: name='server', id='12312382'>]",2016-05-02T05:13:54.000+0000,Ed Rowe,"Autopurge retains all log files whose zxid are >= the zxid of the oldest snapshot file that it is going to retain (in PurgeTxnLog retainNRecentSnapshots()). However, unless there is a log file with the same zxid as the oldest snapshot file being retained (and whether log file and snapshot file zxids are equal is timing dependent), loading the database from snapshots/logs will start with the log file _prior_ to the snapshot's zxid. Thus, to avoid data loss autopurge should retain the log file prior to the oldest retained snapshot as well, unless it verifies that it contains no zxids beyond what the snapshot contains or there is a log file whose zxid == snapshot zxid.
",[],Bug,ZOOKEEPER-2420,Major,Ed Rowe,Duplicate,2016-11-18T07:11:34.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Autopurge deletes log file prior to oldest retained snapshot even though restore may need it,2016-11-18T07:39:33.000+0000,[],7.0
Brian Nixon,"[<JIRA Component: name='server', id='12312382'>]",2016-04-28T16:57:10.000+0000,Nicholas Wolchko,"If the leader is having disk issues so that its on disk txnlog is behind the in memory commit log, it will send a DIFF that is missing the transactions in between the two.

Example:
There are 5 hosts in the cluster. 1 is the leader. 5 is disconnected.
We commit up to zxid 1000.
At zxid 450, the leader's disk stalls, but we still commit transactions because 2,3,4 are up and acking writes.
At zxid 1000, the txnlog on the leader has 1-450 and the commit log has 500-1000.
Then host 5 regains its connection to the cluster and syncs with the leader. It will receive a DIFF containing zxids 1-450 and 500-1000.

This is because queueCommittedProposals in the LearnerHandler just queues everything within its zxid range. It doesn't give an error if there is a gap between peerLastZxid and the iterator it is queueing from.","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2418,Critical,Nicholas Wolchko,Fixed,2019-07-03T18:54:34.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,txnlog diff sync can skip sending some transactions to followers,2019-07-04T01:17:04.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",5.0
Meyer Kizner,"[<JIRA Component: name='c client', id='12312380'>]",2016-04-21T04:07:20.000+0000,Tianyi Zhang,"code like this:
{code}
zoo_op_t ops[2];
zoo_op_result_t results[2];
zoo_create_op_init(ops, ""test"", ""1"", 1, &ZOO_OPEN_ACL_UNSAFE, 0, NULL, 0);
zoo_create_op_init(ops+1, ""/test/1"", ""2"", 1, &ZOO_OPEN_ACL_UNSAFE, 0, NULL, 0);
zoo_multi(zkhandle, 2, ops, results);
{code}
The ops->path is invalid, and it will cause double free in the line 3136 of zookeeper.c.
{code}
for (index=0; index < count; index++) {
        const zoo_op_t *op = ops+index;
        zoo_op_result_t *result = results+index;
        completion_list_t *entry = NULL;

        struct MultiHeader mh = { STRUCT_INITIALIZER(type, op->type), STRUCT_INITIALIZER(done, 0),    STRUCT_INITIALIZER(err, -1) };
        rc = rc < 0 ? rc : serialize_MultiHeader(oa, ""multiheader"", &mh);

        switch(op->type) {
            case ZOO_CREATE_OP: {
                struct CreateRequest req;

                rc = rc < 0 ? rc : CreateRequest_init(zh, &req,
                                        op->create_op.path, op->create_op.data,
                                        op->create_op.datalen, op->create_op.acl,
                                        op->create_op.flags);
                rc = rc < 0 ? rc : serialize_CreateRequest(oa, ""req"", &req);
                result->value = op->create_op.buf;
                result->valuelen = op->create_op.buflen;

                enter_critical(zh);
                entry = create_completion_entry(h.xid, COMPLETION_STRING, op_result_string_completion, result, 0, 0);
                leave_critical(zh);
-->             free_duplicate_path(req.path, op->create_op.path);
                break;
            }
{code}
This problem will happen when the 'rc' of last op is less than 0(maybe ZBADARGUMENTS or ZINVALIDSTATE).
In my case, rc of op[0] is  ZBADARGUMENTS, and the req.path of the ‘free_duplicate_path’ is still 'test' when execute op[1]. 
I‘m confused about why not break the for-loop  when the 'rc' is  less than 0?","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-2414,Major,Tianyi Zhang,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,c-client aborted when operate's path is invalid in zoo_amulti ,2022-02-03T08:36:25.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.4.8', id='12326517'>]",5.0
Jordan Zimmerman,"[<JIRA Component: name='server', id='12312382'>]",2016-04-13T18:35:50.000+0000,Jordan Zimmerman,ContainerManager creates a Timer object. It's stop() method cancel's the running task but doesn't close the Timer itself. This ends up leaking a Thread (internal to the Timer).,"[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2413,Major,Jordan Zimmerman,Fixed,2016-04-24T21:22:55.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ContainerManager doesn't close the Timer it creates when stop() is called,2016-07-21T20:18:16.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",5.0
,[],2016-04-12T13:32:11.000+0000,Yongcheng Liu,"the time begin to have problem is 20:59:47

(1) out of memory log:

2016-03-24 23:01:21,355 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:35112:LearnerHandler@330] - Follower sid: 6 : info : org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@5cf112b0
2016-03-24 23:01:21,355 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:35112:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x9000033a1 minCommittedLog=0x9000031ad peerLastZxid=0x90000280a
2016-03-24 23:01:21,355 [myid:4] - WARN  [LearnerHandler-/192.168.50.26:35112:LearnerHandler@446] - Unhandled proposal scenario
2016-03-24 23:01:21,355 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:35112:LearnerHandler@462] - Sending SNAP
2016-03-24 23:01:21,893 [myid:4] - INFO  [NIOServerCxn.Factory:/192.168.50.24:10540:NIOServerCnxnFactory@207] - Current connection (from /192.168.50.22 Cnxns = 4; totalCnxns = 15)
2016-03-24 23:01:22,625 [myid:4] - WARN  [NIOServerCxn.Factory:/192.168.50.24:10540:ZooKeeperServer@832] - Connection request from old client /192.168.50.22:49695; will be dropped if server is in r-o mode
2016-03-24 23:01:23,283 [myid:4] - INFO  [QuorumPeer[myid=4]/192.168.50.24:10540:Leader@493] - Shutting down
2016-03-24 23:01:24,102 [myid:4] - INFO  [QuorumPeer[myid=4]/192.168.50.24:10540:Leader@499] - Shutdown called
2016-03-24 23:01:24,040 [myid:4] - INFO  [SessionTracker:ZooKeeperServer@347] - Expiring session 0x453a6dc5b7a007e, timeout of 3500ms exceeded

Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ""NIOServerCxn.Factory:/192.168.50.24:10540""
2016-03-24 23:01:25,001 [myid:4] - WARN  [QuorumPeer[myid=4]/192.168.50.24:10540:QuorumPeer@827] - QuorumPeer main thread exited

Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ""LearnerHandler-/192.168.50.26:35112""

Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ""QuorumPeer[myid=4]/192.168.50.24:10540""
2016-03-24 23:01:24,227 [myid:4] - ERROR [LearnerHandler-/192.168.50.26:35112:NIOServerCnxnFactory$1@44] - Thread LearnerHandler Socket[addr=/192.168.50.26,port=35112,localport=10550] tickOfNextAckDeadline:38310 synced?:true queuedPacketLength:7910 died
2016-03-24 23:01:28,492 [myid:4] - INFO  [main:QuorumPeerMain@93] - Exiting normally



(2) this leader is very strange, db lastzxid not update, log as follow (after grep ""Synchronizing with""), we can see max commit zxid from leader is not update any more. From the beginning of 20:59:47, leader lastZxid not update.

2016-03-24 20:56:10,266 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:48439:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x90000333f minCommittedLog=0x90000314b peerLastZxid=0x90000280a
2016-03-24 20:57:59,203 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:46956:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x900003398 minCommittedLog=0x9000031a4 peerLastZxid=0x90000280a
2016-03-24 20:59:47,928 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:20601:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x9000033a1 minCommittedLog=0x9000031ad peerLastZxid=0x90000280a
2016-03-24 21:01:26,475 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:29622:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x9000033a1 minCommittedLog=0x9000031ad peerLastZxid=0x90000280a
2016-03-24 21:03:16,552 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:35717:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x9000033a1 minCommittedLog=0x9000031ad peerLastZxid=0x90000280a
2016-03-24 21:03:44,427 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:48197:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x9000033a1 minCommittedLog=0x9000031ad peerLastZxid=0x90000280a
2016-03-24 21:05:01,125 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:57826:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x9000033a1 minCommittedLog=0x9000031ad peerLastZxid=0x90000280a
2016-03-24 21:06:54,187 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:30137:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x9000033a1 minCommittedLog=0x9000031ad peerLastZxid=0x90000280a
2016-03-24 21:07:42,780 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:24255:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x9000033a1 minCommittedLog=0x9000031ad peerLastZxid=0x90000280a
2016-03-24 21:08:41,279 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:40909:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x9000033a1 minCommittedLog=0x9000031ad peerLastZxid=0x90000280a
2016-03-24 21:10:23,137 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:64166:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x9000033a1 minCommittedLog=0x9000031ad peerLastZxid=0x90000280a
2016-03-24 21:11:50,003 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:56070:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x9000033a1 minCommittedLog=0x9000031ad peerLastZxid=0x90000280a
2016-03-24 21:12:11,956 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:41423:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x9000033a1 minCommittedLog=0x9000031ad peerLastZxid=0x90000280a
2016-03-24 21:13:08,286 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:26757:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x9000033a1 minCommittedLog=0x9000031ad peerLastZxid=0x90000280a
2016-03-24 21:13:59,960 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:62785:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x9000033a1 minCommittedLog=0x9000031ad peerLastZxid=0x90000280a
2016-03-24 21:15:41,103 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:53141:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x9000033a1 minCommittedLog=0x9000031ad peerLastZxid=0x90000280a
2016-03-24 21:16:11,125 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:39551:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x9000033a1 minCommittedLog=0x9000031ad peerLastZxid=0x90000280a
2016-03-24 21:17:25,541 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:24638:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x9000033a1 minCommittedLog=0x9000031ad peerLastZxid=0x90000280a
2016-03-24 21:18:25,039 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:54723:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x9000033a1 minCommittedLog=0x9000031ad peerLastZxid=0x90000280a
2016-03-24 21:19:04,148 [myid:4] - INFO  [LearnerHandler-/192.168.50.26:37450:LearnerHandler@385] - Synchronizing with Follower sid: 6 maxCommittedLog=0x9000033a1 minCommittedLog=0x9000031ad peerLastZxid=0x90000280a


(3) we can see leader Snapshotting to the same file in different time, but receive different TxnZxid(0x900003b70 and 0x9000049dd), this show leader has not been updated lastZxid in db.

Snapshotting 1:
2016-03-24 21:33:27,214 [myid:4] - INFO  [Snapshot Thread:FileTxnSnapLog@253] - Snapshotting: 0x9000033a1 to /opt/dsware/agent/zk/data/version-2/snapshot.9000033a1
2016-03-24 21:33:27,333 [myid:4] - INFO  [SyncThread:4:FileTxnLog@199] - Creating new log file: log.900003b70

Snapshotting 2:
2016-03-24 22:41:26,601 [myid:4] - INFO  [Snapshot Thread:FileTxnSnapLog@253] - Snapshotting: 0x9000033a1 to /opt/dsware/agent/zk/data/version-2/snapshot.9000033a1
2016-03-24 22:41:26,662 [myid:4] - INFO  [SyncThread:4:FileTxnLog@199] - Creating new log file: log.9000049dd


(4) finally, this node(leader server) zxid is behind zk c client, log as follow:

2016-03-24 23:00:53,712 [myid:4] - WARN  [NIOServerCxn.Factory:/192.168.50.24:10540:ZooKeeperServer@832] - Connection request from old client /192.168.50.23:35043; will be dropped if server is in r-o mode
2016-03-24 23:00:53,713 [myid:4] - INFO  [NIOServerCxn.Factory:/192.168.50.24:10540:ZooKeeperServer@851] - Refusing session request for client /192.168.50.23:35043 as it has seen zxid 0x900004e1f our last zxid is 0x9000033a1 client must try another server","[<JIRA Version: name='3.4.8', id='12326517'>]",Bug,ZOOKEEPER-2412,Major,Yongcheng Liu,Duplicate,2016-05-10T13:57:37.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"leader zk out of memory,  and leader db lastZxid is not update when process set data. ",2016-05-10T13:57:37.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",4.0
,[],2016-04-11T09:03:44.000+0000,André Cruz,,[],Bug,ZOOKEEPER-2411,Major,André Cruz,Invalid,2016-04-11T09:04:24.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,email,2016-04-11T09:04:24.000+0000,[],1.0
,"[<JIRA Component: name='c client', id='12312380'>]",2016-04-01T06:59:51.000+0000,europelee,"1. start zookeeper cluster with multi servers
2. multi clients connect to different zookeeper server at the same time
3. clients use lock from zookeeper recipes lock's C implement.

for example:
Client A  create a node like x-025373e3a9960050-0000000067
and Client B create a node like x-015373e3a9960050-0000000068;
A is a lock owner now, then kill A,  as expect, B should become owner, but in fact B not.
Because in zoo_lock.c, function  zkr_lock_operation call child_floor to monitoring a pre node, but child_floor has bug, it caused B not check its prenode A.

B function child_floor just simply strcmp ""x-025373e3a9960050-0000000067"" with own node ""x-015373e3a9960050-0000000068"", 
it should only strcmp ""0000000067"" with ""0000000068"", not include session info.
besides, it is better that using binary search than travelling every node for looking for a pre node when there exists many nodes.
fix:
static char* child_floor(char **sorted_data, int len, char *element) {
    char* ret = NULL;

    int begin = 0;
    int end = len-1;
    int index = 0;

    while (begin <= end) {
        index = (begin+end)/2;
        int iCmpRet = strcmp(strrchr(sorted_data[index], '-')+1, strrchr(element, '-')+1);
        if (iCmpRet < 0) {
            begin = index + 1;
        }
        else {
            if (iCmpRet == 0) {
                if (index - 1 >= 0) {
                    ret = sorted_data[index-1];
                }
                break;
            }
            else {
                end = index - 1;
            }
        }
    }

    return ret;
}
",[],Bug,ZOOKEEPER-2409,Major,europelee,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zookeeper recipes lock's c implement: function child_floor bug,2016-04-01T06:59:51.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>]",2.0
,"[<JIRA Component: name='c client', id='12312380'>]",2016-04-01T02:44:05.000+0000,Kevin,"I lunch a client, then watch some node, call zoo_awget() to watch data, when the client exit, if the node data doesn't change, the callback 'watcher' won't be  invoked. and the memory of watcherCtx is not freed

ZOOAPI int zoo_awget(zhandle_t *zh, const char *path, 
        watcher_fn watcher, void* watcherCtx, 
        data_completion_t completion, const void *data);

I use valgrind to check and the result show the memory lost.",[],Bug,ZOOKEEPER-2408,Major,Kevin,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zoo_awget() ctx memory does not be freed if callback watcher was not invoked,2016-04-01T02:44:05.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",1.0
sunhaitao,[],2016-03-30T11:21:27.000+0000,sunhaitao,"EventThread in ClientCnxn can't be closed when SendThread exits because of auth failed during reconnection.
for send thread if it is in authfailed state, the send thread exits,but the event thread is still running.

observation:
use jstack tho check the thread running they find the send thread no longer exists but event thread is still threre
even when we call zookeeper.close(),the eventthread is still there.

Stack trace: 
sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:514)
",[],Bug,ZOOKEEPER-2407,Major,sunhaitao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,EventThread in ClientCnxn can't be closed when SendThread exits because of auth failed during reconnection,2022-02-03T08:50:25.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",4.0
,[],2016-03-29T19:35:54.000+0000,Mark Elrod,"The comment in zkEnv.sh indicates that /etc/zookeeper should be an option for the ZOOCFGDIR but the code beneath it does not look to see if it exists:

{noformat}
# We use ZOOCFGDIR if defined,                                                                                                                                                                
# otherwise we use /etc/zookeeper                                                                                                                                                             
# or the conf directory that is                                                                                                                                                               
# a sibling of this script's directory   


ZOOBINDIR=""${ZOOBINDIR:-/usr/bin}""
ZOOKEEPER_PREFIX=""${ZOOBINDIR}/..""

if [ ""x$ZOOCFGDIR"" = ""x"" ]
then
  if [ -e ""${ZOOKEEPER_PREFIX}/conf"" ]; then
    ZOOCFGDIR=""$ZOOBINDIR/../conf""
  else
    ZOOCFGDIR=""$ZOOBINDIR/../etc/zookeeper""
  fi
fi                                                                                                                                                     
{noformat}

Should this be something like:

{noformat}
if [ ""x$ZOOCFGDIR"" = ""x"" ]
then
  if [ -e ""/etc/zookeeper"" ]; then
    ZOOCFGDIR=""/etc/zookeeper""
  elif [ -e ""${ZOOKEEPER_PREFIX}/conf"" ]; then
    ZOOCFGDIR=""$ZOOBINDIR/../conf""
  else
    ZOOCFGDIR=""$ZOOBINDIR/../etc/zookeeper""
  fi
fi            
{noformat}

I am not sure if ZOOBINDIR/../etc/zookeeper is supposed to be an option or a typo but in the default setup ZOOBINDIR/../conf exists so even if it were changed to /etc/zookeeper it would never try to use it.
",[],Bug,ZOOKEEPER-2406,Minor,Mark Elrod,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,/etc/zookeeper isn't used for ZOOCFGDIR,2016-03-29T19:35:54.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",2.0
Michael Han,"[<JIRA Component: name='kerberos', id='12329415'>, <JIRA Component: name='security', id='12329414'>, <JIRA Component: name='server', id='12312382'>]",2016-03-26T19:04:25.000+0000,Patrick D. Hunt,"We're logging the kerberos ticket when in debug mode, probably not the best idea. This was identified as a ""critical"" issue by Fortify.

{noformat}
        for(KerberosTicket ticket: tickets) {
            KerberosPrincipal server = ticket.getServer();
            if (server.getName().equals(""krbtgt/"" + server.getRealm() + ""@"" + server.getRealm())) {
                LOG.debug(""Found tgt "" + ticket + ""."");
                return ticket;
            }
        }
{noformat}
","[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2405,Blocker,Patrick D. Hunt,Fixed,2016-05-25T20:47:18.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,getTGT() in Login.java mishandles confidential information,2016-07-21T20:18:38.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",5.0
Mohammad Arshad,"[<JIRA Component: name='documentation', id='12312422'>]",2016-03-23T11:40:59.000+0000,Mohammad Arshad,"Following properties used in the zookeeper server, also configurable, but not documented in admin guide.
# leader.nodelay
# follower.nodelay",[],Bug,ZOOKEEPER-2401,Minor,Mohammad Arshad,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Document leader.nodelay and follower.nodelay ZooKeeper server properties,2016-03-23T11:40:59.000+0000,[],1.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2016-03-23T09:00:26.000+0000,Andrey,"Steps to reproduce:
# Select deprecated algorithm in zoo.cfg: {code}electionAlg=0{code}
# Start zookeeper cluster: A(index 3),B(index 1),C(index 2) nodes
# Stop A node.
# Make some change to zk data. i.e. re-create ephemeral node. Make sure currentEpoch increased in B and C nodes.
# currentEpoch/acceptedEpoch in node A less than B/C epoch
# Stop node B. Zookeeper cluster is not available
# Start node A. In A's node logs:
{code}
LEADING [quorum.QuorumPeer] [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:15523]
LEADING - LEADER ELECTION TOOK - 1458721180995 [quorum.Leader]
Follower sid: 2 : info : org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@3a888c1
java.io.IOException: Follower is ahead of the leader, leader summary: 10 (current epoch), 42949672964 (last zxid)
        at org.apache.zookeeper.server.quorum.Leader.waitForEpochAck(Leader.java:894)
        at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:365)

...

Follower sid: 1 : info : org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@5001b9f5

...

java.lang.InterruptedException: Timeout while waiting for epoch to be acked by quorum
        at org.apache.zookeeper.server.quorum.Leader.waitForEpochAck(Leader.java:915)
        at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:394)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:799)
{code}

The logs above will be printed indefinitely and cluster won't recover.",[],Bug,ZOOKEEPER-2400,Major,Andrey,Not A Problem,2016-03-23T21:31:58.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZooKeeper not starting: Follower is ahead of the leader,2016-03-23T21:31:58.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",2.0
Mohammad Arshad,"[<JIRA Component: name='server', id='12312382'>]",2016-03-18T01:39:40.000+0000,Mohammad Arshad,"Zookeeper run-time dependency on log4j and slf4j-log4j12 was removed as part of ZOOKEEPER-1371 jira work.
Following things were done as part of ZOOKEEPER-1371
# Removed direct log4j API use from the code, instead used slf4j-api
# Changed  log4j  and slf4j-log4j12 run time dependency to test time dependency
# Upgraded log4j, slf4j-log4j12  and slf4j-api versions.
Here is the component wise version change
#* (zookeeper)ivy.xml
log4j: 1.2.15 -->1.7.5
#* src\contrib\loggraph\ivy.xml
slf4j-api: 1.6.1 -->1.7.5
slf4j-log4j12:  1.6.1 -->1.7.5
log4j: 1.2.15 -->1.7.5
#* src\contrib\rest\ivy.xml
slf4j-api: 1.6.1 -->1.7.5
slf4j-log4j12:  1.6.1 -->1.7.5
log4j: 1.2.15 -->1.7.5
#* src\contrib\zooinspector\ivy.xml
slf4j-api: 1.6.1 -->1.7.5
slf4j-log4j12:  1.6.1 -->1.7.5
log4j: 1.2.15 -->1.7.5

The major problem with ZOOKEEPER-1371 change is that it removed run time dependency. For more detail refer ZOOKEEPER-2342 discussion
Now as part of this jira revert back only run time dependency, #2, on log4j and slf4j-log4j12.


","[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2393,Blocker,Mohammad Arshad,Fixed,2016-03-19T21:56:20.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Revert run-time dependency on log4j and slf4j-log4j12,2016-07-21T20:18:39.000+0000,[],4.0
Kazuaki Banzai,"[<JIRA Component: name='server', id='12312382'>]",2016-03-16T12:34:49.000+0000,Kazuaki Banzai,"setMin/MaxSessionTimeout of ZookeeperServer are implemented in quite a weak way.
* -1 restores the default, but this is not documented.
* values < -1 are permitted but make no sense.
* min > max is permitted but makes not sense.",[],Bug,ZOOKEEPER-2391,Minor,Kazuaki Banzai,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,setMin/MaxSessionTimeout of ZookeeperServer are implemented in quite a weak way,2016-12-12T08:39:19.000+0000,[],3.0
,[],2016-03-15T00:02:34.000+0000,Jason Rosenberg,"I have rediscovered an issue, that was apparently posted a while back (link below).  It seems that if I configure an Observer node to be enabled for read-only mode, with syncEnabled = true, it properly syncs its transaction log with the quorum.  However, if I shut down the quorum participants, and the Observer automatically transitions to read-only mode, it does not load the saved transaction log, and thus rejects any client connection with a zxid > 0.  But If I restart the Observer node, it reloads it's persisted transaction log and serves read-only requests at the latest zxid.  Is this the correct behavior? Things run fine if instead of an observer, I do the same with a read-only participant.  In this case, it transitions without issue to a read-only server, and serves the current transaction log.

It seems to me this issue renders read-only observers completely useless.  What am I missing here?

I'm seeing this with 3.4.8

It seems this was discovered and reported a long time ago here:
http://grokbase.com/t/zookeeper/user/14c16b1d22/issue-with-zxid-during-observer-failover-to-read-only",[],Bug,ZOOKEEPER-2389,Major,Jason Rosenberg,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,read-only observer doesn't load transaction log when transitioning to read-only,2018-06-22T04:49:03.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.4.11', id='12339207'>]",1.0
Mohammad Arshad,"[<JIRA Component: name='tests', id='12312427'>]",2016-03-14T21:06:27.000+0000,Patrick D. Hunt,"The same two tests are failing consistently on Solaris in 3.5/trunk (I don't see similar failures in 3.4, jenkins is mostly green there)

org.apache.zookeeper.server.quorum.LocalPeerBeanTest.testClientAddress
org.apache.zookeeper.server.quorum.QuorumPeerTest.testQuorumPeerListendOnSpecifiedClientIP
","[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2388,Blocker,Patrick D. Hunt,Fixed,2016-03-17T06:16:07.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Unit tests failing on Solaris,2016-07-21T20:18:37.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",4.0
,[],2016-03-10T03:11:25.000+0000,Enis Soztutar,"Recently, we've observed a curious case where a quorum was not reached for days in a cluster of 3 nodes (zk0, zk1, zk2) and the middle node zk1 is unreachable from network. 

The leader election happens, and both zk0 and zk2 starts the vote. Then each server sends notifications to every other server including itself. The problem is that, zk1 vm is unavailable, so when we are trying to open up a socket to connect to that server with socket timeout of 5 seconds, it delays the notification processing of the vote sent from zk2 to zk2 (itself). The vote eventually comes after 5 sec, but by that time, the follower (zk0) already converted to the follower state. On the follower state, the follower try to connect to leader 5 times with 1 second timeout (5 sec in total). Since the leader does not start its peer port for 5 seconds after the follower starts, the follower always times out connecting to the leader. This cycle is repeating for hours / days even after restarting the servers several times. 

I believe this is related to the default timeouts (5 sec socket timeout) and follower to leader connection timeout (5 tries with 1 second timeout). Only after setting the {{zookeeper.cnxTimeout}} to 1 second, the quorum was operating. 

More logs coming shortly. 

zoo.cfg: 
{code}
server.3=zk2-hostname:2889:3889
server.2=zk1-hostname:2889:3889
server.1=zk0-hostname:2889:3889
{code}",[],Bug,ZOOKEEPER-2386,Major,Enis Soztutar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Cannot achieve quorum when middle server (in a q of 3) is unreacable,2016-09-14T10:40:14.000+0000,[],4.0
Mohammad Arshad,"[<JIRA Component: name='build', id='12312383'>]",2016-03-09T19:45:16.000+0000,Mohammad Arshad,"command {{ant tar}} fails with following error
{code}
D:\gitHome\zookeeperTrunk\build.xml:722: The following error occurred while executing this line:
D:\gitHome\zookeeperTrunk\src\contrib\build.xml:47: The following error occurred while executing this line:
D:\gitHome\zookeeperTrunk\src\contrib\build-contrib.xml:207: Unable to delete file D:\gitHome\zookeeperTrunk\src\java\lib\ivy-2.4.0.jar
{code}

This is happening only on windows.","[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2385,Blocker,Mohammad Arshad,Fixed,2016-03-15T16:17:14.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Zookeeper trunk build is failing on windows,2016-07-21T20:18:38.000+0000,[],5.0
Rakesh Radhakrishnan,"[<JIRA Component: name='jmx', id='12312451'>, <JIRA Component: name='server', id='12312382'>]",2016-03-08T17:00:14.000+0000,Steven Rowe,"In attempting to upgrade Solr's ZooKeeper dependency from 3.4.6 to 3.4.8 (SOLR-8724) I ran into test failures where attempts to create a node in a newly started standalone ZooKeeperServer were failing because of an assertion in MBeanRegistry.

ZooKeeperServer.startup() first sets up its request processor chain then registers itself in JMX, but if a connection comes in before the server's JMX registration happens, registration of the connection will fail because it trips the assertion that (effectively) its parent (the server) has already registered itself.

{code:java|title=ZooKeeperServer.java}
    public synchronized void startup() {
        if (sessionTracker == null) {
            createSessionTracker();
        }
        startSessionTracker();
        setupRequestProcessors();

        registerJMX();

        state = State.RUNNING;
        notifyAll();
    }
{code}

{code:java|title=MBeanRegistry.java}
    public void register(ZKMBeanInfo bean, ZKMBeanInfo parent)
        throws JMException
    {
        assert bean != null;
        String path = null;
        if (parent != null) {
            path = mapBean2Path.get(parent);
            assert path != null;
        }
{code}

This problem appears to be new with ZK 3.4.8 - AFAIK Solr never had this issue with ZK 3.4.6. ","[<JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2383,Blocker,Steven Rowe,Fixed,2017-01-03T17:54:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Startup race in ZooKeeperServer,2017-03-31T09:01:14.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>]",17.0
Mohammad Arshad,"[<JIRA Component: name='server', id='12312382'>]",2016-03-04T05:32:18.000+0000,Mohammad Arshad,"Zookeeper enters into deadlock while shutting down itself, thus making zookeeper service unavailable as deadlocked server is a leader. Here is the thread dump:
{code}
""QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled)"" #25 prio=5 os_prio=0 tid=0x00007fbc502a6800 nid=0x834 in Object.wait() [0x00007fbc4d9a8000]      java.lang.Thread.State: WAITING (on object monitor)      at java.lang.Object.wait(Native Method)      at java.lang.Thread.join(Thread.java:1245)      - locked <
0x00000000feb78000> (a org.apache.zookeeper.server.SyncRequestProcessor)      at java.lang.Thread.join(Thread.java:1319)      at org.apache.zookeeper.server.SyncRequestProcessor.shutdown(SyncRequestProcessor.java:196)      at org.apache.zookeeper.server.quorum.ProposalRequestProcessor.shutdown(ProposalRequestProcessor.java:90)      at org.apache.zookeeper.server.PrepRequestProcessor.shutdown(PrepRequestProcessor.java:1016)      at org.apache.zookeeper.server.quorum.LeaderRequestProcessor.shutdown(LeaderRequestProcessor.java:78)      at org.apache.zookeeper.server.ZooKeeperServer.shutdown(ZooKeeperServer.java:561)      - locked <
0x00000000feb61e20> (a org.apache.zookeeper.server.quorum.LeaderZooKeeperServer)      at org.apache.zookeeper.server.quorum.QuorumZooKeeperServer.shutdown(QuorumZooKeeperServer.java:169)      - locked <
0x00000000feb61e20> (a org.apache.zookeeper.server.quorum.LeaderZooKeeperServer)      at org.apache.zookeeper.server.quorum.LeaderZooKeeperServer.shutdown(LeaderZooKeeperServer.java:102)      - locked <
0x00000000feb61e20> (a org.apache.zookeeper.server.quorum.LeaderZooKeeperServer)      at org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:637)      at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:590)      - locked <
0x00000000feb781a0> (a org.apache.zookeeper.server.quorum.Leader)      at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1108)


""SyncThread:1"" #46 prio=5 os_prio=0 tid=0x00007fbc5848f000 nid=0x867 waiting for monitor entry [0x00007fbc4ca90000]      java.lang.Thread.State: BLOCKED (on object monitor)      at org.apache.zookeeper.server.quorum.Leader.processAck(Leader.java:784)      - waiting to lock <0x00000000feb781a0> (a org.apache.zookeeper.server.quorum.Leader)      at org.apache.zookeeper.server.quorum.AckRequestProcessor.processRequest(AckRequestProcessor.java:46)      at org.apache.zookeeper.server.SyncRequestProcessor.flush(SyncRequestProcessor.java:183)      at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:113)
{code}

Leader.lead() calls shutdown() from the synchronized block, it acquired lock on Leader.java instance
{code}
while (true) {
                synchronized (this) {
                long start = Time.currentElapsedTime();
				.....
{code}
In the shutdown flow SyncThread is trying to acquire lock on the same Leader.java instance. 

Leader thread acquired lock and waiting for SyncThread shutdown. SyncThread waiting for the lock to complete its shutdown.  This is how ZooKeeper entered into deadlock","[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2380,Blocker,Mohammad Arshad,Fixed,2016-06-23T21:20:37.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Deadlock between leader shutdown and forwarding ACK to the leader,2016-07-21T20:18:31.000+0000,[],8.0
Rakesh Radhakrishnan,"[<JIRA Component: name='build', id='12312383'>]",2016-03-03T15:22:12.000+0000,Patrick D. Hunt,"A recent commit seems to have broken findbugs, looks like it's in ZooKeeperSaslClient

see:
https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3075//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html","[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2379,Blocker,Patrick D. Hunt,Fixed,2016-03-05T00:07:03.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,recent commit broke findbugs qabot check,2016-07-21T20:18:43.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",5.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-02-25T19:45:05.000+0000,Dmitry Ryabkov,"If there is an empty TxnLog file in the log file folder, ZooKeeper server fails to start. This is the exception it logs:

2015-11-02 07:41:10.479 -0600 (,,,) main : ERROR org.apache.zookeeper.server.ZooKeeperServerMain - Unexpected exception, exiting abnormally

java.io.EOFException

                at java.io.DataInputStream.readInt(DataInputStream.java:392)

                at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)

                at org.apache.zookeeper.server.persistence.FileHeader.deserialize(FileHeader.java:64)

                at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.inStreamCreated(FileTxnLog.java:576)

                at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.createInputArchive(FileTxnLog.java:595)

                at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.goToNextLog(FileTxnLog.java:561)

                at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:643)

                at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:158)

                at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:223)

                at org.apache.zookeeper.server.ZooKeeperServer.loadData(ZooKeeperServer.java:272)

                at org.apache.zookeeper.server.ZooKeeperServer.startdata(ZooKeeperServer.java:399)

                at org.apache.zookeeper.server.NIOServerCnxnFactory.startup(NIOServerCnxnFactory.java:122)

                at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:113)

                at org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:86)

                at org.apache.zookeeper.server.ZooKeeperServerMain.main(ZooKeeperServerMain.java:52)

                at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:116)

                at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)

Zero-length log file can be created if FileTxnLog.append() crashes after it creates FileOutputStream but before it serializes and flushes the header.",[],Bug,ZOOKEEPER-2376,Major,Dmitry Ryabkov,Duplicate,2016-02-25T20:39:08.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Server fails to start if there is a zero-length TxnLog file present in the log directory,2016-02-25T20:39:08.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",3.0
yuemeng,"[<JIRA Component: name='java client', id='12312381'>]",2016-02-25T09:18:19.000+0000,yuemeng,"If there are exist many ZooKeeperSaslClient instance in one process,each ZooKeeperSaslClient instance will be call synchronize method( createSaslClient),But each ZooKeeperSaslClient instance will be lock the current object(that is say ,the synchronize only for lock it's own object) ,but many instances can access the static variable login,the synchronize can't prevent other threads access the static login object,it will be cause more than one ZooKeeperSaslClient  instances use the same login object,and login.startThreadIfNeeded() will be called more than one times for same login object。
it wll cause problem:
 ERROR | [Executor task launch worker-1-SendThread(fi1:24002)] | Exception while trying to create SASL client: java.lang.IllegalThreadStateException | org.apache.zookeeper.client.ZooKeeperSaslClient.createSaslClient(ZooKeeperSaslClient.java:305)","[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2375,Blocker,yuemeng,Fixed,2016-03-02T06:58:26.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Prevent multiple initialization of login object in each ZooKeeperSaslClient instance,2016-07-21T20:18:25.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.5.1', id='12326786'>]",8.0
,"[<JIRA Component: name='server', id='12312382'>]",2016-02-24T12:28:08.000+0000,zhupengfei,"This is the second time I faced the problem on ec2, my activemq stomp port have the same problem, but tcp message port works fine.

I have checked zookeeper.out, no error log found. And aws technical support tell it maybe caused by zookeeper.

OS Type:
Amazon Linux AMI

Network Test Result:
-bash-4.1$ netstat | grep 2181
-bash-4.1$ telnet localhost 2181
Trying 127.0.0.1...
^C
-bash-4.1$ netstat -tunpl|grep 2181
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp        0      0 :::2181                     :::*                        LISTEN      17923/java
-bash-4.1$ netstat -an |grep 2181
tcp        0      1 172.12.10.152:60171         172.12.10.152:2181          SYN_SENT    
tcp        0      0 :::2181                     :::*                        LISTEN      
tcp        0      1 ::ffff:127.0.0.1:36032      ::ffff:127.0.0.1:2181       SYN_SENT",[],Bug,ZOOKEEPER-2374,Blocker,zhupengfei,Cannot Reproduce,2016-03-17T14:32:36.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Can not telnet 2181 port on aws ec2 server,2016-03-17T14:32:44.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",2.0
,"[<JIRA Component: name='scripts', id='12312384'>]",2016-02-23T15:13:17.000+0000,Siddhartha,"If I try to execute 'bin/zkServer.sh status' while having
'-Dcom.sun.management.jmxremote.port=9011' in $JVMFLAGS,
zookeeper quits with ""Error: Exception thrown by the agent : java.rmi.server.ExportException: Port already in use: 9011"".

EIther some other means of getting status should be used, or some way of not setting JMX variables in this case should be added.

Thanks",[],Bug,ZOOKEEPER-2371,Minor,Siddhartha,Not A Bug,2016-02-26T01:42:58.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zkServer.sh status does not work if JMX Port is enabled,2016-02-26T01:42:58.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>]",1.0
,"[<JIRA Component: name='java client', id='12312381'>]",2016-02-23T07:19:10.000+0000,Chao Sun,"(My apology if this is not a bug.)

I'm trying to use a ZK client which has successfully authenticated with a secure ZK server using principal {{me/hostname@EXAMPLE.COM}}. However, the following simple commands failed:

{code}
[zk: hostname(CONNECTED) 0] create /zk-test ""1""
Created /zk-test
[zk: hostname(CONNECTED) 1] setAcl /zk-test sasl:me/hostname@EXAMPLE.COM:cdrwa
cZxid = 0x3e3b
ctime = Mon Feb 22 23:10:36 PST 2016
mZxid = 0x3e3b
mtime = Mon Feb 22 23:10:36 PST 2016
pZxid = 0x3e3b
cversion = 0
dataVersion = 0
aclVersion = 1
ephemeralOwner = 0x0
dataLength = 3
numChildren = 0
[zk: hostname(CONNECTED) 2] getAcl /zk-test
'sasl,'me/hostname@EXAMPLE.COM
: cdrwa
[zk: hostname(CONNECTED) 3] ls /zk-test
Authentication is not valid : /zk-test
[zk: hostname(CONNECTED) 4] create /zk-test/c ""2""
Authentication is not valid : /zk-test/c
{code}

I wonder what I did wrong here, or is this behavior intentional? how can I delete the znodes? Thanks.",[],Bug,ZOOKEEPER-2370,Major,Chao Sun,Not A Problem,2016-02-23T11:47:23.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Can't access Znodes after adding ACL with SASL,2017-03-24T16:09:42.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",3.0
emopers,[],2016-02-18T09:13:12.000+0000,emopers,"In ./src/java/main/org/apache/zookeeper/server/ZooKeeperServer.java
{code}
            ByteArrayOutputStream baos = new ByteArrayOutputStream();
            BinaryOutputArchive bos = BinaryOutputArchive.getArchive(baos);
            bos.writeInt(-1, ""len"");
            rsp.serialize(bos, ""connect"");
            if (!cnxn.isOldClient) {
                bos.writeBool(
                        this instanceof ReadOnlyZooKeeperServer, ""readOnly"");
            }
            baos.close();
            ByteBuffer bb = ByteBuffer.wrap(baos.toByteArray());
{code}

BinaryOutputArchive internally uses DataOutputStream as its stream, and when a DataOutputStream instance wraps an underlying ByteArrayOutputStream instance,
it is recommended to flush or close the DataOutputStream before invoking the underlying instances's toByteArray() . Also, it is a good practice to call flush/close explicitly as mentioned for example http://stackoverflow.com/questions/2984538/how-to-use-bytearrayoutputstream-and-dataoutputstream-simultaneously-java.
Moreover, ""baos.close()"" at second last line is not required as it is no-op according to [javadoc|http://docs.oracle.com/javase/7/docs/api/java/io/ByteArrayOutputStream.html]
{quote}
Closing a ByteArrayOutputStream has no effect. The methods in this class can be called after the stream has been closed without generating an IOException.
{quote}
The patch is to add flush method on ""bos"" before calling toByteArray on ""baos"". Similar behavior is also present in the following files:
./src/java/main/org/apache/zookeeper/ClientCnxn.java
./src/java/main/org/apache/zookeeper/server/ZKDatabase.java
./src/java/main/org/apache/zookeeper/server/persistence/Util.java
./src/java/main/org/apache/zookeeper/server/NIOServerCnxn.java

Let me know if this looks good. I can provide patch.",[],Bug,ZOOKEEPER-2369,Minor,emopers,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Flushing DataOutputStream before calling toByteArray on the underlying ByteArrayOutputStream,2016-03-04T00:14:33.000+0000,[],1.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2016-02-12T18:52:08.000+0000,Timothy James Ward,"If I have a set of three machines, all of which have locally defined hostnames A, B and C (i.e. B and C cannot look up A by name). I am unable to control the DNS setup, and I don't want to manually reimplement DNS using entries in the hosts file.

A is on IP 192.168.1.16
B is on IP 192.168.1.17
C is on IP 192.168.1.18

All of my ZK configuration uses literal IP addresses (no hostnames anywhere), however I still see a hostname appearing in the leader log (in this case the leader was C):

    java.net.UnknownHostException: B
    	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
    	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    	at java.net.Socket.connect(Socket.java:589)
    	at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:369)
    	at org.apache.zookeeper.server.quorum.QuorumCnxManager.receiveConnection(QuorumCnxManager.java:291)
    	at org.apache.zookeeper.server.quorum.QuorumCnxManager$Listener.run(QuorumCnxManager.java:558)

This is caused by the initiateConnection method of QuorumCnxManager, which contains the line:

    self.getElectionAddress().getHostName()

The use of getHostName() forces a reverse DNS lookup, which I do not want. The code should use getHostString() instead, which will use the actual data from config, and avoid unresolvable hosts being sent over the wire. This will mean that node C attempts to connect to 192.168.1.17, not ""B"".",[],Bug,ZOOKEEPER-2367,Critical,Timothy James Ward,Duplicate,2016-02-15T11:59:12.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Unable to establish quorum when hostnames are not resolvable between all of the nodes,2016-02-15T11:59:12.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='quorum', id='12312379'>]",2016-02-12T16:59:27.000+0000,Timothy James Ward,"The NIOServerCnxnFactory reconfigure method can leak server sockets, and hence make ports unusable until the JVM restarts:

The first line of the method takes a reference to the current ServerSocketChannel and then the next line replaces it. The subsequent interactions with the server socket can fail (for example if the reconfiguration tries to bind to an in-use port). If they fail *before* the  call to oldSS.close() then oldSS is *never* closed. This holds that port open forever, and prevents the user from rolling back to the previous port!

The code from reconfigure is shown below:

 ServerSocketChannel oldSS = ss;        
        try {
           this.ss = ServerSocketChannel.open();
           ss.socket().setReuseAddress(true);
           LOG.info(""binding to port "" + addr);
           ss.socket().bind(addr);
           ss.configureBlocking(false);
           acceptThread.setReconfiguring();
           oldSS.close();           
           acceptThread.wakeupSelector();
           try {
			  acceptThread.join();
		   } catch (InterruptedException e) {
			   LOG.error(""Error joining old acceptThread when reconfiguring client port "" + e.getMessage());
		   }
           acceptThread = new AcceptThread(ss, addr, selectorThreads);
           acceptThread.start();
        } catch(IOException e) {
           LOG.error(""Error reconfiguring client port to "" + addr + "" "" + e.getMessage());
        }



","[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2366,Blocker,Timothy James Ward,Fixed,2016-06-23T21:46:53.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Reconfiguration of client port causes a socket leak,2016-07-21T20:18:21.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",8.0
Biju Nair,"[<JIRA Component: name='java client', id='12312381'>]",2016-02-10T22:24:53.000+0000,Dan Fitch,"I have zookeeper running normally just fine in a 3-server cluster.

Then I try to configure zookeeper to use Kerberos, following docs in the Solr wiki here: https://cwiki.apache.org/confluence/display/solr/Kerberos+Authentication+Plugin

I can't even get to the fun Kerberos errors. When I start with {{JVMFLAGS=""-Djava.security.auth.login.config=/opt/zookeeper/jaas-server.conf""}}

and this jaas-server.conf:

{code}
Server {
com.sun.security.auth.module.Krb5LoginModule required
useKeyTab=true
keyTab=/keytabs/vdev-solr-01.keytab
storeKey=true
doNotPrompt=true
useTicketCache=false
debug=true
principal=""HTTP/<snip>"";
}
{code}

I get this in the log:

{code}
2016-02-10 16:16:51,327 [myid:1] - ERROR [main:ServerCnxnFactory@195] - No JAAS configuration section named 'Server' was foundin '/opt/zookeeper/jaas-server.conf'.
2016-02-10 16:16:51,328 [myid:1] - ERROR [main:QuorumPeerMain@89] - Unexpected exception, exiting abnormally
java.io.IOException: No JAAS configuration section named 'Server' was foundin '/opt/zookeeper/jaas-server.conf'.
        at org.apache.zookeeper.server.ServerCnxnFactory.configureSaslLogin(ServerCnxnFactory.java:196)
        at org.apache.zookeeper.server.NIOServerCnxnFactory.configure(NIOServerCnxnFactory.java:87)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:130)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:111)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)
{code}

(Note the ""foundin"" typo.)

I get the exact same error if the jaas-server.conf file exists, or does not.

So later I found that the Solr wiki was wrong and lost the double quotes around the keytab value. It would be nice if Zookeeper spewed a more useful message when it can't parse the configuration.",[],Bug,ZOOKEEPER-2365,Trivial,Dan Fitch,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,JAAS configuration section error is confusing,2022-02-14T17:27:48.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",6.0
Patrick D. Hunt,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='documentation', id='12312422'>]",2016-02-08T21:36:14.000+0000,Chris Nauroth,"""ant docs"" is failing on branch-3.5.  (Both trunk and branch-3.4 are fine.)  The root cause appears
to be a missing file on branch-3.5: src/docs/src/documentation/content/xdocs/releasenotes.xml.
 This causes Forrest to report a failure due to broken hyperlinks targeting releasenotes.html.","[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2364,Blocker,Chris Nauroth,Fixed,2016-03-21T20:46:29.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"""ant docs"" fails on branch-3.5 due to missing releasenotes.xml.",2016-08-16T21:44:52.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",3.0
,[],2016-02-05T17:09:17.000+0000,David Foregger,"h4. Background
ZOOKEEPER-1107 introduced a DatadirCleanupManager to automatically purge snapshots. This can be configured using autopurge.snapRetainCount and autopurge.purgeInterval. This is documented [here|http://zookeeper.apache.org/doc/r3.4.5/zookeeperAdmin.html#Ongoing+Data+Directory+Cleanup] and [there|http://zookeeper.apache.org/doc/r3.4.5/zookeeperAdmin.html#sc_advancedConfiguration].
h4. Symptoms
Autopurging does not work when running a standalone ZooKeeperServer. The DatadirCleanupManager is started by the QuorumPeerMain, but there is no similar setup ZooKeeperServerMain. ServerConfig does not hold autopurge properties.
h4. Expected Behavior
Starting a standalone zookeeper server should enable autopurging with the same behavior as a quorum server. ",[],Bug,ZOOKEEPER-2363,Major,David Foregger,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,DatadirCleanupManager never created by ZookeeperServerMain,2016-08-03T05:51:38.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",3.0
Atri Sharma,"[<JIRA Component: name='server', id='12312382'>]",2016-02-05T15:32:05.000+0000,Whitney Sorenson,"In this thread http://mail-archives.apache.org/mod_mbox/zookeeper-user/201602.mbox/%3CCAPbqGzicBkLLyVDm7RFM20z0y3X1v1P-C9-1%3D%3D1DDqRDTzdOmQ%40mail.gmail.com%3E , I discussed an issue I've now seen in multiple environments:

In a multi (using Curator), I write 2 new nodes. At some point, I issue 2 reads for these new nodes. In one read, I see one of the new nodes. In a subsequent read, I fail to see the other new node:

1. Starting state : { /foo = <does not exist>, /bar = <does not exist> }
2. In a multi, write: { /foo = A, /bar = B}
3. Read /foo as A
4. Read /bar as <does not exist> 

#3 and #4 are issued 100% sequentially.

It is not known at what point during #2, #3 starts.

Note: the reads are getChildren() calls.",[],Bug,ZOOKEEPER-2362,Critical,Whitney Sorenson,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper multi / transaction allows partial read,2018-11-22T01:09:12.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",9.0
Patrick D. Hunt,"[<JIRA Component: name='build', id='12312383'>]",2016-02-04T17:33:02.000+0000,Patrick D. Hunt,"I don't believe this affects us from a security perspective directly, however it's something we should clean up in our next release.

Afaict the only commons we use for shipping/production code is commons-cli. Our two release branches, 3.4 and 3.5, neither of them use commons-collections. I looked at the binary release artifact and it doesn't include the commons collections jar.

We do have a test that uses CollectionsUtils, but no shipping code. I downloaded our 3.4 and 3.5 artifacts, this is all I see:

phunt:~/Downloads/zd/5/zookeeper-3.5.1-alpha$ grep -R ""org.apache.commons.collections"" .
./src/java/test/org/apache/zookeeper/RemoveWatchesTest.java:import org.apache.commons.collections.CollectionUtils;
phunt:~/Downloads/zd/5/zookeeper-3.5.1-alpha$

Also in our ivy file we have

    <dependency org=""org.apache.rat"" name=""apache-rat-tasks""
                rev=""0.10"" conf=""releaseaudit->default""/>
    <dependency org=""commons-lang"" name=""commons-lang""
                rev=""2.6"" conf=""releaseaudit->default""/>
    <dependency org=""commons-collections"" name=""commons-collections""
                rev=""3.2.1"" conf=""releaseaudit->default""/>

So commons-collections is pulled in - but only for the release audit, which is something we do as a build verification activity but not part of the product itself.","[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.2', id='12331981'>]",Bug,ZOOKEEPER-2360,Blocker,Patrick D. Hunt,Fixed,2016-02-05T00:33:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Update commons collections version used by tests/releaseaudit,2016-07-21T20:18:32.000+0000,"[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>]",4.0
Ian Dimayuga,[],2016-01-28T02:35:00.000+0000,Ian Dimayuga,"NettyServerCnxn.close() neglects to call zkServer.removeCnxn the way NIOServerCnxn.close() does. Also, WatchLeakTest does not test watch leaks in Netty.",[],Bug,ZOOKEEPER-2358,Major,Ian Dimayuga,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,NettyServerCnxn leaks watches upon close,2022-02-03T08:50:17.000+0000,"[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>]",7.0
Mohammad Arshad,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2016-01-18T14:54:29.000+0000,Mohammad Arshad,"ZooKeeper ephemeral node is never deleted if follower fail while reading the proposal packet
The scenario is as follows:
# Configure three node ZooKeeper cluster, lets say nodes are A, B and C, start all, assume A is leader, B and C are follower
# Connect to any of the server and create ephemeral node /e1
# Close the session, ephemeral node /e1 will go for deletion
# While receiving delete proposal make Follower B to fail with {{SocketTimeoutException}}. This we need to do to reproduce the scenario otherwise in production environment it happens because of network fault.
# Remove the fault, just check that faulted Follower is now connected with quorum
# Connect to any of the server, create the same ephemeral node /e1, created is success.
# Close the session,  ephemeral node /e1 will go for deletion
# {color:red}/e1 is not deleted from the faulted Follower B, It should have been deleted as it was again created with another session{color}
# {color:green}/e1 is deleted from Leader A and other Follower C{color}","[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2355,Critical,Mohammad Arshad,Fixed,2017-08-03T15:58:24.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Ephemeral node is never deleted if follower fails while reading the proposal packet,2021-07-23T13:06:00.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.5.3', id='12335444'>]",21.0
Sujith Simon,[],2016-01-12T15:20:43.000+0000,Mohammad Arshad,"ZOOKEEPER-1653 is merged only to 3.4 branch. 
It should be merged to 3.5 and master branch as well.","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-2354,Major,Mohammad Arshad,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,ZOOKEEPER-1653 not merged in master and 3.5 branch,2022-02-03T08:36:22.000+0000,[],6.0
,"[<JIRA Component: name='build', id='12312383'>]",2016-01-05T20:14:28.000+0000,Wenjie Ding,"rpm ‘cd’ to the BUILD directory, delete the directory it just ‘cd’ to.  What was going on from that point was that it called some script which was loading /bin/bash and crashed since ‘getcwd’ cannot access the parent directory which had been deleted.

build output messages;
...
      [rpm] + cd /tmp/zkpython_build_root/BUILD
      [rpm] + '[' /tmp/zkpython_build_root/BUILD '!=' / ']'
      [rpm] + rm -rf /tmp/zkpython_build_root/BUILD
      [rpm] ++ dirname /tmp/zkpython_build_root/BUILD
      [rpm] + mkdir -p /tmp/zkpython_build_root
      [rpm] + mkdir /tmp/zkpython_build_root/BUILD
      [rpm] + /usr/lib/rpm/check-buildroot
      [rpm] shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory
      [rpm] + /usr/lib/rpm/redhat/brp-compress
      [rpm] shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory
      [rpm] chdir: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory
      [rpm] + /usr/lib/rpm/redhat/brp-strip /usr/bin/strip
      [rpm] shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory
      [rpm] + /usr/lib/rpm/redhat/brp-strip-static-archive /usr/bin/strip
      [rpm] shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory
      [rpm] + /usr/lib/rpm/redhat/brp-strip-comment-note /usr/bin/strip /usr/bin/objdump
      [rpm] shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory
      [rpm] + /usr/lib/rpm/brp-python-bytecompile
      [rpm] shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory
",[],Bug,ZOOKEEPER-2352,Major,Wenjie Ding,Won't Fix,2016-03-03T16:20:54.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,rpm build broke,2016-03-03T16:20:55.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",3.0
,"[<JIRA Component: name='scripts', id='12312384'>]",2016-01-04T03:34:00.000+0000,LEON YU,"%JAVA_HOME% does not work in zkEnv.cmd, so zkServer.cmd and zkClient.cmd can not run and zookeeper can not start.

Temporary solution to this is to use quotation marks ""%JAVA_HOME%"" to replace %JAVA_HOME% in zkEnv.cmd.",[],Bug,ZOOKEEPER-2351,Major,LEON YU,Duplicate,2016-01-04T23:02:32.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,%JAVA_HOME% in bin\zkEnv.cmd does not work on Windows 8 and Windows 10,2016-01-04T23:02:32.000+0000,"[<JIRA Version: name='3.4.7', id='12325149'>]",2.0
Ling Mao,"[<JIRA Component: name='documentation', id='12312422'>]",2015-12-21T14:17:50.000+0000,Raghavendra Prabhu,"The documentation states that 

{code}
ZooKeeper logs transactions to a transaction
log. After snapCount transactions are written to a log
file a snapshot is started and a new transaction log
file is created. The default snapCount is
100,000.
{code}

However, in implementation, snapshotting is done when logCount is somwhere in (snapCount/2, snapCount+1], based on the limit set at runtime:  

{code}
        if (logCount > (snapCount / 2 + randRoll)) {
{code}
as in 

https://github.com/apache/zookeeper/blob/trunk/src/java/main/org/apache/zookeeper/server/SyncRequestProcessor.java#L124","[<JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2349,Minor,Raghavendra Prabhu,Fixed,2017-09-11T20:43:29.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Update documentation for snapCount,2017-09-11T21:37:08.000+0000,[],7.0
,[],2015-12-18T10:17:51.000+0000,Echo Chen,"When client session expired, leader tried to remove it from session map and remove its EPHEMERAL znode, for example, /test_znode. This operation succeed on leader, but at the very same time, network fault happended and not synced to followers, a new leader election launched. After leader election finished, the new leader is not the old leader. we found the znode /test_znode still existed in the followers but not on leader

 *Scenario :* 
1) Create znode E.g.  
{{/rmstore/ZKRMStateRoot/RMAppRoot/application_1449644945944_0001/appattempt_1449644945944_0001_000001}}
2) Delete Znode. 
3) Network fault b/w follower and leader machines
4) leader election again and follower became leader.

Now data is not synced with new leader..After this client is not able to same znode.

",[],Bug,ZOOKEEPER-2348,Major,Echo Chen,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Data between leader and followers are not synchronized.,2019-04-30T04:56:40.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",10.0
Rakesh Radhakrishnan,[],2015-12-16T21:01:10.000+0000,Ted Yu,"HBase recently upgraded to zookeeper 3.4.7

In one of the tests, TestSplitLogManager, there is reproducible hang at the end of the test.
Below is snippet from stack trace related to zookeeper:
{code}
""main-EventThread"" daemon prio=5 tid=0x00007fd27488a800 nid=0x6f1f waiting on condition [0x000000011834b000]
   java.lang.Thread.State: WAITING (parking)
  at sun.misc.Unsafe.park(Native Method)
  - parking to wait for  <0x00000007c5b8d3a0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
  at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
  at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
  at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
  at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:501)

""main-SendThread(localhost:59510)"" daemon prio=5 tid=0x00007fd274eb4000 nid=0x9513 waiting on condition [0x0000000118042000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
  at java.lang.Thread.sleep(Native Method)
  at org.apache.zookeeper.client.StaticHostProvider.next(StaticHostProvider.java:101)
  at org.apache.zookeeper.ClientCnxn$SendThread.startConnect(ClientCnxn.java:997)
  at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1060)

""SyncThread:0"" prio=5 tid=0x00007fd274d02000 nid=0x730f waiting for monitor entry [0x00000001170ac000]
   java.lang.Thread.State: BLOCKED (on object monitor)
  at org.apache.zookeeper.server.ZooKeeperServer.decInProcess(ZooKeeperServer.java:512)
  - waiting to lock <0x00000007c5b62128> (a org.apache.zookeeper.server.ZooKeeperServer)
  at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:144)
  at org.apache.zookeeper.server.SyncRequestProcessor.flush(SyncRequestProcessor.java:200)
  at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:131)

""main-EventThread"" daemon prio=5 tid=0x00007fd2753a3800 nid=0x711b waiting on condition [0x0000000117a30000]
   java.lang.Thread.State: WAITING (parking)
  at sun.misc.Unsafe.park(Native Method)
  - parking to wait for  <0x00000007c9b106b8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
  at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
  at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
  at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
  at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:501)

""main"" prio=5 tid=0x00007fd276000000 nid=0x1903 in Object.wait() [0x0000000108aa1000]
   java.lang.Thread.State: WAITING (on object monitor)
  at java.lang.Object.wait(Native Method)
  - waiting on <0x00000007c5b66400> (a org.apache.zookeeper.server.SyncRequestProcessor)
  at java.lang.Thread.join(Thread.java:1281)
  - locked <0x00000007c5b66400> (a org.apache.zookeeper.server.SyncRequestProcessor)
  at java.lang.Thread.join(Thread.java:1355)
  at org.apache.zookeeper.server.SyncRequestProcessor.shutdown(SyncRequestProcessor.java:213)
  at org.apache.zookeeper.server.PrepRequestProcessor.shutdown(PrepRequestProcessor.java:770)
  at org.apache.zookeeper.server.ZooKeeperServer.shutdown(ZooKeeperServer.java:478)
  - locked <0x00000007c5b62128> (a org.apache.zookeeper.server.ZooKeeperServer)
  at org.apache.zookeeper.server.NIOServerCnxnFactory.shutdown(NIOServerCnxnFactory.java:266)
  at org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.shutdown(MiniZooKeeperCluster.java:301)
{code}
Note the address (0x00000007c5b66400) in the last hunk which seems to indicate some form of deadlock.

According to Camille Fournier:

We made shutdown synchronized. But decrementing the requests is
also synchronized and called from a different thread. So yeah, deadlock.
This came in with ZOOKEEPER-1907","[<JIRA Version: name='3.4.8', id='12326517'>]",Bug,ZOOKEEPER-2347,Blocker,Ted Yu,Fixed,2016-01-14T05:22:33.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Deadlock shutting down zookeeper,2016-01-14T05:22:33.000+0000,"[<JIRA Version: name='3.4.7', id='12325149'>]",10.0
Meyer Kizner,"[<JIRA Component: name='server', id='12312382'>]",2015-12-15T17:59:46.000+0000,Steve Loughran,"If a client can't authenticate via sasl then (a) the stack trace is lost on the server logs, and (b) it is exposed to the client as a connection refusal. This results in curator retrying many times before giving up —and with the cause being misinterpreted as a server-down problem, rather than a client-not-trusted problem",[],Bug,ZOOKEEPER-2346,Major,Steve Loughran,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,SASL Auth failure manifested to client as connection refusal,2016-11-28T19:44:57.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",5.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2015-12-15T11:06:54.000+0000,cheyang,"I'd like to setup 3-nodes zookeeper cluster with version 3.5.1. Because the network model of kubernetes is that pod and service has different ip address.  In order to deploy it into kubernetes, I have to make zookeeper pods itself to be 0.0.0.0, so it can be started correctly. the configuration as below:

zk1: zoo.cfg

    standaloneEnabled=false
    dynamicConfigFile=/opt/zookeeper/conf/zoo.cfg.dynamic

zoo.cfg.dynamic

    server.1=0.0.0.0:2888:3888:participant;2181
    server.2=10.62.56.192:2888:3888:participant;2181
    server.3=10.62.56.193:2888:3888:participant;2181

zk2: zoo.cfg

    standaloneEnabled=false
    dynamicConfigFile=/opt/zookeeper/conf/zoo.cfg.dynamic

zoo.cfg.dynamic

    server.1=10.62.56.191:2888:3888:participant;2181
    server.2=0.0.0.0:2888:3888:participant;2181
    server.3=10.62.56.193:2888:3888:participant;2181

zk3: zoo.cfg

    standaloneEnabled=false
    dynamicConfigFile=/opt/zookeeper/conf/zoo.cfg.dynamic

zoo.cfg.dynamic

   server.1=10.62.56.191:2888:3888:participant;2181
   server.2=10.62.56.192:2888:3888:participant;2181
   server.3=0.0.0.0:2888:3888:participant;218

The result is that:
1. Looks like the election is successful. a new dynamic file is generated in every node:/opt/zookeeper/conf/zoo.cfg.dynamic.100000000
like below
server.1=10.62.56.191:2888:3888:participant;0.0.0.0:2181
server.2=0.0.0.0:2888:3888:participant;0.0.0.0:2181
server.3=10.62.56.193:2888:3888:participant;0.0.0.0:2181

2. But the cluster doesn't really work, I saw the errors:

0:0:2181)(secure=disabled):Learner@273] - Unexpected exception, tries=3, remaining init limit=16997, connecting to /0.0.0.0:2888
java.net.ConnectException: Connection refused
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:204)
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:589)
        at org.apache.zookeeper.server.quorum.Learner.sockConnect(Learner.java:227)
        at org.apache.zookeeper.server.quorum.Learner.connectToLeader(Learner.java:256)
        at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:74)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1064)
2015-12-15 04:35:00,403 [myid:1] - INFO  
2015-12-15 04:35:00,585 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Follower@198] - shutdown called
java.lang.Exception: shutdown Follower
        at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:198)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1068)
","[<JIRA Version: name='3.5.1', id='12326786'>]",Bug,ZOOKEEPER-2343,Major,cheyang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper 3.5.1 failed to deploy into the kubernetes,2017-11-30T09:27:11.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",5.0
Chris Nauroth,[],2015-12-10T00:59:31.000+0000,Chris Nauroth,"ZOOKEEPER-1371 removed our source code dependency on Log4J.  It appears that this also removed the Log4J SLF4J binding jar from the runtime classpath.  Without any SLF4J binding jar available on the runtime classpath, it is impossible to write logs.

This JIRA investigated migration to Log4J 2 as a possible path towards resolving the bug introduced by ZOOKEEPER-1371.  At this point, we know this is not feasible short-term.  This JIRA remains open to track long-term migration to Log4J 2.",[],Bug,ZOOKEEPER-2342,Major,Chris Nauroth,Won't Do,2022-02-09T19:10:31.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Migrate to Log4J 2.,2022-02-09T19:22:00.000+0000,[],58.0
,[],2015-12-09T22:13:06.000+0000,Karl Mortensen,"I just downloaded ZooKeeper 3.4.7 (wouldn't let me put that version in the ""Affects Version/s"" field) and it doesn't work out of the box on Windows 7, which is brutal for folks who don't understand.

It complains that you don't have JAVA_HOME set right if you have it set to a path with spaces e.g. C:\program files\java\blah will fail.

All the following need quotes around and %VARIABLE% expansions to deal with potential spaces in the path:

* bin/zkCli.cmd
* bin/zkEnv.cmd
* bin/zkServer.cmd
 
Should be a trivial fix.

Definition of Done:
zkCli.cmd, zkEnv.cmd and zkServer.cm work out of the box on Windows 7.",[],Bug,ZOOKEEPER-2341,Trivial,Karl Mortensen,Duplicate,2015-12-09T22:24:04.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,bin/zkEnv.cmd needs quotes around %JAVA%,2015-12-09T22:24:04.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",2.0
Mohammad Arshad,[],2015-12-09T05:33:57.000+0000,Neha Bathra,"Currently, to enable jmx for zookeeper, need to comment the property JMXDISABLE as JMXDISABLE=false continues to disable JMX.","[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2340,Minor,Neha Bathra,Fixed,2015-12-10T05:33:43.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,JMX is disabled even if JMXDISABLE is false,2016-07-21T20:18:21.000+0000,[],7.0
Radu Brumariu,"[<JIRA Component: name='c client', id='12312380'>]",2015-12-08T16:57:26.000+0000,James DeFelice,I've observed socket FD leaks in Apache Mesos when using ZK to coordinate master leadership: https://issues.apache.org/jira/browse/MESOS-4065,"[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2338,Major,James DeFelice,Fixed,2017-12-06T22:59:39.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,c bindings should create socket's with SOCK_CLOEXEC to avoid fd leaks on fork/exec,2020-03-31T10:44:17.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.6.0', id='12326518'>]",5.0
Timothy James Ward,[],2015-12-08T15:14:52.000+0000,Timothy James Ward,"Some of the ZooKeeper tests use ""fake"" hostnames to trigger host resolution failures. The problem with this is that it uses valid hostnames which are sometimes configured in VMs.

At the moment I am unable to build cleanly because I get test failures on the two test methods that do this. The tests work equally well if syntactically invalid hostnames are used, and the test cases become more portable at the same time.

The affected test cases are:

org.apache.zookeeper.test.StaticHostProviderTest.testTwoInvalidHostAddresses and org.apache.zookeeper.test.StaticHostProviderTest.testOneInvalidHostAddresses

See GitHub pull request https://github.com/apache/zookeeper/pull/48 for a proposed fix","[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.2', id='12331981'>]",Bug,ZOOKEEPER-2337,Major,Timothy James Ward,Duplicate,2015-12-08T15:44:46.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Fake ""invalid"" hostnames used in tests are sometimes valid",2019-12-19T23:01:55.000+0000,"[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",2.0
Mohammad Arshad,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2015-12-06T05:28:53.000+0000,Mohammad Arshad,"There  are some compilation error in latest trunk code.
{code}
[javac] D:\gitHome\zookeeperTrunk\src\java\main\org\apache\zookeeper\ClientCnxn.java:49: error: package org.apache.log4j does not exist
    [javac] import org.apache.log4j.MDC;
    [javac]                        ^
    [javac] D:\gitHome\zookeeperTrunk\src\java\main\org\apache\zookeeper\ClientCnxn.java:1108: error: cannot find symbol
    [javac]             MDC.put(""myid"", hostPort);
    [javac]             ^
    [javac]   symbol:   variable MDC
    [javac]   location: class ClientCnxn.SendThread
    [javac] 2 errors

{code}

This compilation error got introduced by ZOOKEEPER-2330 patch. This patch used log4j api and log4 dependency has already been removed by ZOOKEEPER-1371
","[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2335,Major,Mohammad Arshad,Fixed,2015-12-06T19:01:21.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Java Compilation Error in ClientCnxn.java,2016-10-16T15:23:22.000+0000,[],4.0
Flavio Paiva Junqueira,[],2015-12-04T20:19:13.000+0000,Elias Levy,"The Zookeeper download page and mirrors only track the latest version of the mirror release versions.  The page has a link to the archives page at archive.apache.org, but that page is missing all releases after 3.3.2.  That means there are a large number of releases that disappear from the official download site when a new release is published.

In my particular case I was building a container based on 3.4.6.  Once 3.4.7 came out my build broke and it cannot be fixed as 3.4.7 can't be downloaded from anywhere official.",[],Bug,ZOOKEEPER-2334,Major,Elias Levy,Fixed,2015-12-16T15:35:18.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper Archives Out Date,2015-12-16T15:35:18.000+0000,[],5.0
Shaohui Liu,[],2015-11-27T13:42:02.000+0000,Shaohui Liu,"We found that the zookeeper server with version 3.4.6 failed to start for there is a empty txn log in log dir.  
I think we should skip the empty log file during restoring the datatree. 
Any suggestion?

{code}
2015-11-27 19:16:16,887 [myid:] - ERROR [main:ZooKeeperServerMain@63] - Unexpected exception, exiting abnormally
java.io.EOFException
at java.io.DataInputStream.readInt(DataInputStream.java:392)
at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
at org.apache.zookeeper.server.persistence.FileHeader.deserialize(FileHeader.java:64)
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.inStreamCreated(FileTxnLog.java:576)
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.createInputArchive(FileTxnLog.java:595)
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.goToNextLog(FileTxnLog.java:561)
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:643)
at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:158)
at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:223)
at org.apache.zookeeper.server.ZooKeeperServer.loadData(ZooKeeperServer.java:272)
at org.apache.zookeeper.server.ZooKeeperServer.startdata(ZooKeeperServer.java:399)
at org.apache.zookeeper.server.NIOServerCnxnFactory.startup(NIOServerCnxnFactory.java:122)
at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:113)
at org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:86)
at org.apache.zookeeper.server.ZooKeeperServerMain.main(ZooKeeperServerMain.java:52)
at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:116)
at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)
{code}","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-2332,Critical,Shaohui Liu,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Zookeeper failed to start for empty txn log,2022-02-03T08:36:22.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",17.0
Mohammad Arshad,"[<JIRA Component: name='java client', id='12312381'>]",2015-11-23T10:03:46.000+0000,Mohammad Arshad,"When kerberos is used as authentication mechanism, one login thread runs in the background for ZooKeeper client as well ZooKeepr server.
This problem is related to Zookeeper client and the scenario is as follows:

# Main application connects to Zookeeper
{code}
ZooKeeper zooKeeper = new ZooKeeper(zookeeperConnectionString, sessionTimeout, this)
{code}
# Completes it is work with zookeeper
# calls close() on zookeeper, and continues with rest of the application specific work

Thread dump, taken after 3rd step, shows that login thread is still alive
{code}
""Thread-1"" daemon prio=6 tid=0x04842c00 nid=0x1f04 waiting on condition [0x05b7f000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.zookeeper.Login$1.run(Login.java:180)
	at java.lang.Thread.run(Thread.java:722)
{code}","[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2330,Major,Mohammad Arshad,Fixed,2016-05-08T20:24:09.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeper close API does not close Login thread.,2016-07-21T20:18:37.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
Mohammad Arshad,[],2015-11-21T21:41:16.000+0000,Mohammad Arshad,"Currently ZooKeeper java code has 10 javac and 1 javadoc warning. These should be removed.

*javac warnings*
{noformat}
[javac] Compiling 228 source files to D:\gitHome\zookeeperTrunk\build\classes
    [javac] D:\gitHome\zookeeperTrunk\src\java\main\org\apache\zookeeper\ZooKeeperMain.java:226: warning: [rawtypes] found raw type: List
    [javac]             List args = new LinkedList();
    [javac]             ^
    [javac]   missing type arguments for generic class List<E>
    [javac]   where E is a type-variable:
    [javac]     E extends Object declared in interface List
    [javac] D:\gitHome\zookeeperTrunk\src\java\main\org\apache\zookeeper\ZooKeeperMain.java:226: warning: [rawtypes] found raw type: LinkedList
    [javac]             List args = new LinkedList();
    [javac]                             ^
    [javac]   missing type arguments for generic class LinkedList<E>
    [javac]   where E is a type-variable:
    [javac]     E extends Object declared in class LinkedList
    [javac] D:\gitHome\zookeeperTrunk\src\java\main\org\apache\zookeeper\ZooKeeperMain.java:233: warning: [unchecked] unchecked call to add(E) as a member of the raw type List
    [javac]                 args.add(value);
    [javac]                         ^
    [javac]   where E is a type-variable:
    [javac]     E extends Object declared in interface List
    [javac] D:\gitHome\zookeeperTrunk\src\java\main\org\apache\zookeeper\ZooKeeperMain.java:239: warning: [unchecked] unchecked conversion
    [javac]             cmdArgs = args;
    [javac]                       ^
    [javac]   required: List<String>
    [javac]   found:    List
    [javac] D:\gitHome\zookeeperTrunk\src\java\main\org\apache\zookeeper\jmx\ManagedUtil.java:62: warning: [rawtypes] found raw type: Enumeration
    [javac]         Enumeration enumer = r.getCurrentLoggers();
    [javac]         ^
    [javac]   missing type arguments for generic class Enumeration<E>
    [javac]   where E is a type-variable:
    [javac]     E extends Object declared in interface Enumeration
    [javac] D:\gitHome\zookeeperTrunk\src\java\main\org\apache\zookeeper\server\admin\AdminServer.java:33: warning: [serial] serializable class AdminServerException has no definition of serialVersionUID
    [javac]     public class AdminServerException extends Exception {
    [javac]            ^
    [javac] D:\gitHome\zookeeperTrunk\src\java\main\org\apache\zookeeper\server\admin\JettyAdminServer.java:142: warning: [serial] serializable class JettyAdminServer.CommandServlet has no definition of serialVersionUID
    [javac]     private class CommandServlet extends HttpServlet {
    [javac]             ^
    [javac] D:\gitHome\zookeeperTrunk\src\java\main\org\apache\zookeeper\server\util\KerberosUtil.java:39: warning: [rawtypes] found raw type: Class
    [javac]     getInstanceMethod = classRef.getMethod(""getInstance"", new Class[0]);
    [javac]                                                               ^
    [javac]   missing type arguments for generic class Class<T>
    [javac]   where T is a type-variable:
    [javac]     T extends Object declared in class Class
    [javac] D:\gitHome\zookeeperTrunk\src\java\main\org\apache\zookeeper\server\util\KerberosUtil.java:42: warning: [rawtypes] found raw type: Class
    [javac]          new Class[0]);
    [javac]              ^
    [javac]   missing type arguments for generic class Class<T>
    [javac]   where T is a type-variable:
    [javac]     T extends Object declared in class Class
    [javac] D:\gitHome\zookeeperTrunk\src\java\main\org\apache\zookeeper\server\util\OSMXBean.java:89: warning: [rawtypes] found raw type: Class
    [javac]                 new Class[0]);
    [javac]                     ^
    [javac]   missing type arguments for generic class Class<T>
    [javac]   where T is a type-variable:
    [javac]     T extends Object declared in class Class
    [javac] 10 warnings
{noformat}
*javadoc warning*
{noformat}
[javadoc] D:\gitHome\zookeeperTrunk\src\java\main\org\apache\zookeeper\server\PurgeTxnLog.java:172: warning - @return tag has no arguments.
{noformat}","[<JIRA Version: name='3.5.2', id='12331981'>]",Bug,ZOOKEEPER-2329,Minor,Mohammad Arshad,Fixed,2015-11-21T23:35:49.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Clear javac and javadoc warning from zookeeper,2016-07-21T20:18:15.000+0000,[],4.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2015-11-20T23:48:09.000+0000,Arun,"Hi, We have a 3 node zookeeper running with 3.4.6 version. myId 1 (leader) and 2 (follower) are working fine, myId 3 node starts successfully but when we check the status, we see below error, we also do not see this instance taking a load

Any help will be highly appreciated.

$./zkServer.sh status
JMX enabled by default
Using config: ../zookeeper/zookeeper-3.4.6/conf/zoo.cfg
Error contacting service. It is probably not running.

Server Logs: 

2015-11-20 23:38:41,863 [myid:3] - WARN  [RecvWorker:1:QuorumCnxManager$RecvWorker@780] - Connection broken for id 1, my id = 3, error = 
java.net.SocketException: Connection reset
at java.net.SocketInputStream.read(SocketInputStream.java:189)
at java.net.SocketInputStream.read(SocketInputStream.java:121)
at java.net.SocketInputStream.read(SocketInputStream.java:203)
at java.io.DataInputStream.readInt(DataInputStream.java:387)
at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:765)
2015-11-20 23:38:41,863 [myid:3] - WARN  [RecvWorker:2:QuorumCnxManager$RecvWorker@780] - Connection broken for id 2, my id = 3, error = 
java.net.SocketException: Connection reset
at java.net.SocketInputStream.read(SocketInputStream.java:189)
at java.net.SocketInputStream.read(SocketInputStream.java:121)
at java.net.SocketInputStream.read(SocketInputStream.java:203)
at java.io.DataInputStream.readInt(DataInputStream.java:387)
2015-11-20 23:23:33,320 [myid:] - INFO  [main:QuorumPeerConfig@103] - Reading configuration from: ../zookeeper/zookeeper-3.4.6/conf/zoo.cfg
2015-11-20 23:23:33,344 [myid:] - INFO  [main:QuorumPeerConfig@340] - Defaulting to majority quorums
2015-11-20 23:23:33,351 [myid:3] - INFO  [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3
2015-11-20 23:23:33,352 [myid:3] - INFO  [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 0
2015-11-20 23:23:33,353 [myid:3] - INFO  [main:DatadirCleanupManager@101] - Purge task is not scheduled.
2015-11-20 23:23:33,382 [myid:3] - INFO  [main:QuorumPeerMain@127] - Starting quorum peer
2015-11-20 23:23:33,410 [myid:3] - INFO  [main:NIOServerCnxnFactory@94] - binding to port 0.0.0.0/0.0.0.0:2181
2015-11-20 23:23:33,452 [myid:3] - INFO  [main:QuorumPeer@959] - tickTime set to 2000
2015-11-20 23:23:33,452 [myid:3] - INFO  [main:QuorumPeer@979] - minSessionTimeout set to -1
2015-11-20 23:23:33,453 [myid:3] - INFO  [main:QuorumPeer@990] - maxSessionTimeout set to -1
2015-11-20 23:23:33,453 [myid:3] - INFO  [main:QuorumPeer@1005] - initLimit set to 5
2015-11-20 23:23:33,493 [myid:3] - INFO  [Thread-1:QuorumCnxManager$Listener@504] - My election bind port: <host_name>/<IP_address>:3888
2015-11-20 23:23:33,512 [myid:3] - INFO  [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:2181:QuorumPeer@714] - LOOKING
2015-11-20 23:23:33,515 [myid:3] - INFO  [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:2181:FastLeaderElection@815] - New election. My id =  3, proposed zxid=0x0
2015-11-20 23:23:33,528 [myid:3] - INFO  [WorkerReceiver[myid=3]:FastLeaderElection@597] - Notification: 1 (message format version), 3 (n.leader), 0x0 (n.zxid), 0x1 (n.round), LOOKING (n.state), 3 (n.sid), 0x0 (n.peerEpoch) LOOKING (my state)
2015-11-20 23:23:33,731 [myid:3] - INFO  [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:2181:FastLeaderElection@849] - Notification time out: 400
2015-11-20 23:23:33,732 [myid:3] - INFO  [WorkerReceiver[myid=3]:FastLeaderElection@597] - Notification: 1 (message format version), 3 (n.leader), 0x0 (n.zxid), 0x1 (n.round), LOOKING (n.state), 3 (n.sid), 0x0 (n.peerEpoch) LOOKING (my state)
2015-11-20 23:23:34,136 [myid:3] - INFO  [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:2181:FastLeaderElection@849] - Notification time out: 800
2015-11-20 23:23:34,137 [myid:3] - INFO  [WorkerReceiver[myid=3]:FastLeaderElection@597] - Notification: 1 (message format version), 3 (n.leader), 0x0 (n.zxid), 0x1 (n.round), LOOKING (n.state), 3 (n.sid), 0x0 (n.peerEpoch) LOOKING (my state)
2015-11-20 23:23:34,938 [myid:3] - INFO  [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:2181:FastLeaderElection@849] - Notification time out: 1600
2015-11-20 23:23:34,939 [myid:3] - INFO  [WorkerReceiver[myid=3]:FastLeaderElection@597] - Notification: 1 (message format version), 3 (n.leader), 0x0 (n.zxid), 0x1 (n.round), LOOKING (n.state), 3 (n.sid), 0x0 (n.peerEpoch) LOOKING (my state)
2015-11-20 23:23:36,540 [myid:3] - INFO  [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:2181:FastLeaderElection@849] - Notification time out: 3200
2015-11-20 23:23:36,540 [myid:3] - INFO  [WorkerReceiver[myid=3]:FastLeaderElection@597] - Notification: 1 (message format version), 3 (n.leader), 0x0 (n.zxid), 0x1 (n.round), LOOKING (n.state), 3 (n.sid), 0x0 (n.peerEpoch) LOOKING (my state)
2015-11-20 23:23:39,741 [myid:3] - INFO  [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:2181:FastLeaderElection@849] - Notification time out: 6400
2015-11-20 23:23:39,742 [myid:3] - INFO  [WorkerReceiver[myid=3]:FastLeaderElection@597] - Notification: 1 (message format version), 3 (n.leader), 0x0 (n.zxid), 0x1 (n.round), LOOKING (n.state), 3 (n.sid), 0x0 (n.peerEpoch) LOOKING (my state)",[],Bug,ZOOKEEPER-2328,Major,Arun,Not A Problem,2016-04-19T14:18:25.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Connection broken for id 1,2016-04-19T14:18:25.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",4.0
,[],2015-11-20T01:16:11.000+0000,Andrew Pennebaker,"Zookeeper v3.5.0-alpha experiences an error when trying to create a simple data node.

Versions before and after 3.5.0-alpha work just fine, but this specific version oddly fails to create data nodes.

Source:

https://github.com/mcandre/docker-zookeeper/tree/3.5.0-alpha

Trace:

```
$ git clone git@github.com:mcandre/docker-zookeeper.git
$ cd docker-zookeeper.git
$ git checkout 3.5.0-alpha
$ make
CONTAINER=$(docker run -d -p 2181:2181 -p 2888:2888 -p 3888:3888 mcandre/docker-zookeeper:3.5.0-alpha)
docker exec $CONTAINER sh -c 'echo ""create /dog moon"" | zkCli.sh'
Connecting to localhost:2181
2015-11-20 01:05:54,951 [myid:] - INFO  [main:Environment@109] - Client environment:zookeeper.version=3.5.0-alpha-1615249, built on 08/01/2014 22:13 GMT
2015-11-20 01:05:54,958 [myid:] - INFO  [main:Environment@109] - Client environment:host.name=190002648bbc
2015-11-20 01:05:54,958 [myid:] - INFO  [main:Environment@109] - Client environment:java.version=1.7.0_85
2015-11-20 01:05:54,969 [myid:] - INFO  [main:Environment@109] - Client environment:java.vendor=Oracle Corporation
2015-11-20 01:05:54,970 [myid:] - INFO  [main:Environment@109] - Client environment:java.home=/usr/lib/jvm/java-7-openjdk-amd64/jre
2015-11-20 01:05:54,970 [myid:] - INFO  [main:Environment@109] - Client environment:java.class.path=/zookeeper-3.5.0-alpha/bin/../build/classes:/zookeeper-3.5.0-alpha/bin/../build/lib/*.jar:/zookeeper-3.5.0-alpha/bin/../lib/slf4j-log4j12-1.7.5.jar:/zookeeper-3.5.0-alpha/bin/../lib/slf4j-api-1.7.5.jar:/zookeeper-3.5.0-alpha/bin/../lib/servlet-api-2.5-20081211.jar:/zookeeper-3.5.0-alpha/bin/../lib/netty-3.7.0.Final.jar:/zookeeper-3.5.0-alpha/bin/../lib/log4j-1.2.16.jar:/zookeeper-3.5.0-alpha/bin/../lib/jline-2.11.jar:/zookeeper-3.5.0-alpha/bin/../lib/jetty-util-6.1.26.jar:/zookeeper-3.5.0-alpha/bin/../lib/jetty-6.1.26.jar:/zookeeper-3.5.0-alpha/bin/../lib/javacc.jar:/zookeeper-3.5.0-alpha/bin/../lib/jackson-mapper-asl-1.9.11.jar:/zookeeper-3.5.0-alpha/bin/../lib/jackson-core-asl-1.9.11.jar:/zookeeper-3.5.0-alpha/bin/../lib/commons-cli-1.2.jar:/zookeeper-3.5.0-alpha/bin/../zookeeper-3.5.0-alpha.jar:/zookeeper-3.5.0-alpha/bin/../src/java/lib/*.jar:/zookeeper-3.5.0-alpha/bin/../conf:
2015-11-20 01:05:54,971 [myid:] - INFO  [main:Environment@109] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2015-11-20 01:05:54,972 [myid:] - INFO  [main:Environment@109] - Client environment:java.io.tmpdir=/tmp
2015-11-20 01:05:54,972 [myid:] - INFO  [main:Environment@109] - Client environment:java.compiler=<NA>
2015-11-20 01:05:54,973 [myid:] - INFO  [main:Environment@109] - Client environment:os.name=Linux
2015-11-20 01:05:54,973 [myid:] - INFO  [main:Environment@109] - Client environment:os.arch=amd64
2015-11-20 01:05:54,974 [myid:] - INFO  [main:Environment@109] - Client environment:os.version=4.0.9-boot2docker
2015-11-20 01:05:54,974 [myid:] - INFO  [main:Environment@109] - Client environment:user.name=root
2015-11-20 01:05:54,974 [myid:] - INFO  [main:Environment@109] - Client environment:user.home=/root
2015-11-20 01:05:54,975 [myid:] - INFO  [main:Environment@109] - Client environment:user.dir=/
2015-11-20 01:05:54,975 [myid:] - INFO  [main:Environment@109] - Client environment:os.memory.free=26MB
2015-11-20 01:05:54,977 [myid:] - INFO  [main:Environment@109] - Client environment:os.memory.max=247MB
2015-11-20 01:05:54,977 [myid:] - INFO  [main:Environment@109] - Client environment:os.memory.total=30MB
2015-11-20 01:05:54,987 [myid:] - INFO  [main:ZooKeeper@709] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@3c407d5
Welcome to ZooKeeper!
2015-11-20 01:05:55,062 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1093] - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2015-11-20 01:05:55,099 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@963] - Socket connection established to localhost/127.0.0.1:2181, initiating session
JLine support is enabled
2015-11-20 01:05:55,154 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1209] - Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect
[zk: localhost:2181(CONNECTING) 0] create /dog moon
Exception in thread ""main"" org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /dog
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1067)
	at org.apache.zookeeper.cli.CreateCommand.exec(CreateCommand.java:78)
	at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:670)
	at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:573)
	at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:356)
	at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:316)
	at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:276)
make: *** [run] Error 1
```","[<JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.5.1', id='12326786'>]",Bug,ZOOKEEPER-2327,Major,Andrew Pennebaker,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"""ConnectionLoss for /dog""",2016-12-12T08:36:02.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",3.0
Andrew Grasso,"[<JIRA Component: name='server', id='12312382'>]",2015-11-18T22:21:56.000+0000,Andrew Grasso,"When loading state from snapshots on startup, FileTxnSnapLog.java ignores the result of FileSnap.deserialize, which is -1L if no valid snapshots are found. Recovery proceeds with dt.lastProcessed == 0, its initial value.

The result is that Zookeeper will process the transaction logs and then begin serving requests with a different state than the rest of the ensemble.

To reproduce:
In a healthy zookeeper cluster of size >= 3, shut down one node.
Either delete all snapshots for this node or change all to be empty files.
Restart the node.

We believe this can happen organically if a node runs out of disk space.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2325,Critical,Andrew Grasso,Fixed,2018-10-26T10:04:07.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Data inconsistency if all snapshots empty or missing,2019-08-21T16:59:07.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",15.0
Mohammad Arshad,"[<JIRA Component: name='java client', id='12312381'>]",2015-11-18T20:29:53.000+0000,Mohammad Arshad,"ZooKeeper client enters into infinite AuthFailedException cycle. For every operation its throws AuthFailedException
Here is the create operation exception
{code}
org.apache.zookeeper.KeeperException$AuthFailedException: KeeperErrorCode = AuthFailed for /continuousRunningZKClient
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1753)
{code}

This can be reproduced easily with the following steps:

# Reduce the ZooKeeper client principal max life for example set 2 min.  use command {color:blue} modprinc -maxlife 2min zkcli  {color} in kadmin. (This is done to reduce the issue reproduce time)
# Connect Client to ZooKeeper quorum,let it gets connected and some operations are done successfully
# Disconnect the Client's network, by pulling out the Ethernet cable or by any way. Now the Client is in disconnected state, no operation is expected,Client tries to reconnect to different-different servers in the ZooKeeper quorum.
# After two minutes Client tries to get new Keberos ticket and it fails.
# Connect the Client to network. Client comes in connected state but AuthFailedException for every operation.","[<JIRA Version: name='3.5.2', id='12331981'>]",Bug,ZOOKEEPER-2323,Major,Mohammad Arshad,Fixed,2016-08-11T20:00:24.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZooKeeper client enters into infinite AuthFailedException cycle if its unable to recreate Kerberos ticket,2016-08-11T20:00:24.000+0000,"[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>]",5.0
,[],2015-11-16T11:38:03.000+0000,Neha Bathra,"This happened with a long run scenario where the client is connected to Zk and leader re-election happens in some time interval 
StackTrace:

2015-11-05 03:15:42,953 [myid:1] - WARN [NIOWorkerThread-6:WorkerService$ScheduledWorkRequest@164] - Unexpected exception
java.lang.NullPointerException
at org.apache.zookeeper.server.quorum.LearnerZooKeeperServer.revalidateSession(LearnerZooKeeperServer.java:93)
at org.apache.zookeeper.server.ZooKeeperServer.reopenSession(ZooKeeperServer.java:692)
at org.apache.zookeeper.server.ZooKeeperServer.processConnectRequest(ZooKeeperServer.java:1039)
at org.apache.zookeeper.server.NIOServerCnxn.readConnectRequest(NIOServerCnxn.java:434)
at org.apache.zookeeper.server.NIOServerCnxn.readPayload(NIOServerCnxn.java:180)
at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:340)
at org.apache.zookeeper.server.NIOServerCnxnFactory$IOWorkRequest.doWork(NIOServerCnxnFactory.java:536)
at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:162)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)",[],Bug,ZOOKEEPER-2322,Minor,Neha Bathra,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper server throws NullPointerExceptions while revalidating the session to the client,2015-11-16T11:38:53.000+0000,[],2.0
,"[<JIRA Component: name='c client', id='12312380'>]",2015-11-14T00:12:07.000+0000,Hadriel Kaplan,"Invoking the C-client API function {{zoo_set_watcher()}} to remove/change a session event watcher is not a thread-safe operation. The IO thread accesses the session watcher (the one stored in the zhandle_t.watcher member) and copies its value into completion events, which are then later processed by the completion thread. This happens when it's processing session events, such as session connected/connecting/expired events. 

Meanwhile after the value has been copied by the IO thread, but before the completion thread has used it, the main thread could change the watcher to NULL using {{zoo_set_watcher()}} because the calling application may be free'ing it. The call to {{zoo_set_watcher()}} will return even though the IO and completion threads still have the old watcher pointer value, and the main application cannot safely free it. But since the function call returns, the main application thinks it can free it, and boom goes the dynamite.

So... either there needs to be a lockout while the IO/completion threads process session events, or the {{zoo_set_watcher()}} needs to become asynchronous itself by going through the same processing pipeline to the completion thread and having a completion callback to tell the calling application when it succeeded/failed.",[],Bug,ZOOKEEPER-2321,Major,Hadriel Kaplan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,C-client session watcher removal is not thread safe,2015-11-14T00:12:07.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",1.0
Abraham Fine,"[<JIRA Component: name='c client', id='12312380'>]",2015-11-12T23:37:38.000+0000,Hadriel Kaplan,"The C-client library will crash when invoking the asynchronous {{zoo_aremove_watchers()}} API function with the '{{local}}' argument set to 1.

The reason is: if the local argument is 1/true, then the code does '{{notify_sync_completion((struct sync_completion *)data);}}' But casting the '{{data}}' variable to a {{sync_completion}} struct pointer is bogus/invalid, and when it's later handles as that struct pointer it's accessing invalid memory.

As a side note: it will work ok when called _synchronously_ through {{zoo_remove_watchers()}}, because that function creates a {{sync_completion}} struct and passes it to the asynch {{zoo_aremove_watchers()}}, but it will not work ok when the asynch function is used directly for the reason stated perviously.

Another side note: the docs state that setting the 'local' flag makes the C-client remove the watcher ""even if there is no server connection"" - but really it makes the C-client remove the watcher without notifying the server at *all*, even if the connection to a server is up. (well... that's what it would do if it didn't just crash instead ;)",[],Bug,ZOOKEEPER-2320,Major,Hadriel Kaplan,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,"C-client crashes when removing watcher asynchronously in ""local"" mode",2018-12-16T15:01:43.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",5.0
Michael Han,[],2015-11-09T23:36:16.000+0000,Zhaohui Yu,"Given three nodes, the leader on 2, but some issue with this machine, so I shutdown this machine, and change the host name to another machine.
Then I start the node in the new machine, but the new node can not join.
I found the the 1 and 3's Listener thread exit.

With the code of Listener's run method:
I think we should catch UnresolvedAddressException to avoid the Listener exit.
{noformat}
@Override
        public void run() {
           
            while((!shutdown) && (numRetries < 3)){
                try {
                   // bind and accept
                        receiveConnection(client);
                  
                } catch (IOException e) {
                    
                }
            }
            //
        }
{noformat}
","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2319,Major,Zhaohui Yu,Fixed,2018-06-18T04:05:17.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,UnresolvedAddressException cause the QuorumCnxManager.Listener exit,2018-07-05T11:18:21.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",7.0
,"[<JIRA Component: name='c client', id='12312380'>]",2015-11-09T17:26:31.000+0000,Marshall McMullen,"We have seen some sporadic issues with unexplained segfaults inside auth_completion_func. The interesting thing is we are not using any auth mechanism at all. This happened against this version of the code:

svn.apache.org/repos/asf/zookeeper/trunk@1547702

Here's the stacktrace we are seeing:

{code}
Thread 1 (Thread 0x7f21d13ff700 ? (LWP 5230)):
#0  0x00007f21efff42f0 in auth_completion_func (rc=0, zh=0x7f21e7470800) at src/zookeeper.c:1696
#1  0x00007f21efff7898 in zookeeper_process (zh=0x7f21e7470800, events=2) at src/zookeeper.c:2708
#2  0x00007f21f0006583 in do_io (v=0x7f21e7470800) at src/mt_adaptor.c:440
#3  0x00007f21eeab7e9a in start_thread () from /lib/x86_64-linux-gnu/libpthread.so.0
#4  0x00007f21ed1803fd in clone () from /lib/x86_64-linux-gnu/libc.so.6
#5  0x0000000000000000 in ?? ()
{code}

The offending line in our case is:

1696            LOG_INFO(LOGCALLBACK(zh), ""Authentication scheme %s succeeded"", zh->auth_h.auth->scheme);

It must be the case that zh->auth_h.auth is NULL for this to happen since the code path returns if zh is NULL.

Interesting log messages around this time:

{code}
Socket [10.170.243.7:2181] zk retcode=-2, errno=115(Operation now in progress): unexpected server response: expected 0xfffffff9, but received 0xfffffff8
Priming connection to [10.170.243.4:2181]: last_zxid=0x370eb4d
initiated connection to server [10.170.243.4:2181]
Oct 13 12:03:21.273384 zookeeper - INFO  [NIOServerCxnFactory.AcceptThread:/10.170.243.4:2181:NIOServerCnxnFactory$AcceptThread@296] - Accepted socket connection from /10.170.243.4:48523
Oct 13 12:03:21.274321 zookeeper - WARN  [NIOWorkerThread-24:ZooKeeperServer@822] - Connection request from old client /10.170.243.4:48523; will be dropped if server is in r-o mode
Oct 13 12:03:21.274452 zookeeper - INFO  [NIOWorkerThread-24:ZooKeeperServer@869] - Client attempting to renew session 0x3000011596d004a at /10.170.243.4:48523; client last zxid is 0x30370eb4d; server last zxid is 0x30370eb4d
Oct 13 12:03:21.274584 zookeeper - INFO  [NIOWorkerThread-24:Learner@115] - Revalidating client: 0x3000011596d004a
session establishment complete on server [10.170.243.4:2181], sessionId=0x3000011596d004a, negotiated timeout=20000
Oct 13 12:03:21.275693 zookeeper - INFO  [QuorumPeer[myid=1]/10.170.243.4:2181:ZooKeeperServer@611] - Established session 0x3000011596d004a with negotiated timeout 20000 for client /10.170.243.4:48523
Oct 13 12:03:24.229590 zookeeper - WARN  [NIOWorkerThread-8:NIOServerCnxn@361] - Unable to read additional data from client sessionid 0x3000011596d004a, likely client has closed socket
Oct 13 12:03:24.230018 zookeeper - INFO  [NIOWorkerThread-8:NIOServerCnxn@999] - Closed socket connection for client /10.170.243.4:48523 which had sessionid 0x3000011596d004a
Oct 13 12:03:24.230257 zookeeper - WARN  [NIOWorkerThread-19:NIOServerCnxn@361] - Unable to read additional data from client sessionid 0x100002743aa0001, likely client has closed socket
{code}",[],Bug,ZOOKEEPER-2318,Major,Marshall McMullen,Duplicate,2016-05-27T05:27:37.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,segfault in auth_completion_func,2016-05-27T05:27:37.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
Sachin,"[<JIRA Component: name='build', id='12312383'>]",2015-11-09T10:59:30.000+0000,Markus Tippmann,"Bundle cannot be deployed to OSGi container.
Manifest version is not OSGi compatible.

Instead of using 3.5.1-alpha, manifest needs to contain 3.5.1.alpha
","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-2317,Blocker,Markus Tippmann,Fixed,2018-05-30T00:30:40.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Non-OSGi compatible version,2019-05-20T17:50:25.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",9.0
Umesh Panchaksharaiah,"[<JIRA Component: name='server', id='12312382'>]",2015-11-06T14:34:52.000+0000,sunhaitao,"when i read the code below, the comment is put in an incorrect,place.
"" // in order to be committed, a proposal must be accepted by a quorum ""
should be place on top of :
   if (!p.hasAllQuorums()) {
           return false;                 
        }

---------------------------------------------------------------------------------------          
3.5.1 Leader code  
// getting a quorum from all necessary configurations
        if (!p.hasAllQuorums()) {
           return false;                 
        }
        
        // commit proposals in order
        if (zxid != lastCommitted+1) {    
           LOG.warn(""Commiting zxid 0x"" + Long.toHexString(zxid)
                    + "" from "" + followerAddr + "" not first!"");
            LOG.warn(""First is ""
                    + (lastCommitted+1));
        }     
        
        // in order to be committed, a proposal must be accepted by a quorum              
        
        outstandingProposals.remove(zxid);
        ","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2316,Trivial,sunhaitao,Fixed,2017-04-27T21:21:06.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,comment does not match code logic,2017-07-13T03:07:16.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",7.0
Raju Bairishetti,"[<JIRA Component: name='documentation', id='12312422'>]",2015-11-04T03:53:16.000+0000,Raju Bairishetti,"https://zookeeper.apache.org/doc/r3.3.3/zookeeperInternals.html

Leader issues *commit request* to followers once the ack received from the followers. But the 2-phase commit diagram shows the direction of commit from Follower to Leader.

[2-phase-commit-image|https://github.com/apache/zookeeper/blob/trunk/src/docs/src/documentation/resources/images/2pc.jpg]
",[],Bug,ZOOKEEPER-2312,Minor,Raju Bairishetti,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Fix arrow direction in the 2-phase commit diagram in Zookeeper internal docs,2015-11-04T04:28:02.000+0000,[],2.0
Marshall McMullen,"[<JIRA Component: name='c client', id='12312380'>]",2015-11-02T20:24:54.000+0000,Marshall McMullen,"We've started seeing an assert failing inside setup_random at line 537:

{code}
 528 static void setup_random()
 529 {
 530 #ifndef _WIN32          // TODO: better seed
 531     int seed;
 532     int fd = open(""/dev/urandom"", O_RDONLY);
 533     if (fd == -1) {
 534         seed = getpid();
 535     } else {
 536         int rc = read(fd, &seed, sizeof(seed));
 537         assert(rc == sizeof(seed));
 538         close(fd);
 539     }
 540     srandom(seed);
 541     srand48(seed);
 542 #endif
{code}

The core files show:

Program terminated with signal 6, Aborted.
#0  0x00007f9ff665a0d5 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#0  0x00007f9ff665a0d5 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007f9ff665d83b in abort () from /lib/x86_64-linux-gnu/libc.so.6
#2  0x00007f9ff6652d9e in ?? () from /lib/x86_64-linux-gnu/libc.so.6
#3  0x00007f9ff6652e42 in __assert_fail () from /lib/x86_64-linux-gnu/libc.so.6
#4  0x00007f9ff8e4070a in setup_random () at src/zookeeper.c:476
#5  0x00007f9ff8e40d76 in resolve_hosts (zh=0x7f9fe14de400, hosts_in=0x7f9fd700f400 ""10.26.200.6:2181,10.26.200.7:2181,10.26.200.8:2181"", avec=0x7f9fd87fab60) at src/zookeeper.c:730
#6  0x00007f9ff8e40e87 in update_addrs (zh=0x7f9fe14de400) at src/zookeeper.c:801
#7  0x00007f9ff8e44176 in zookeeper_interest (zh=0x7f9fe14de400, fd=0x7f9fd87fac4c, interest=0x7f9fd87fac50, tv=0x7f9fd87fac80) at src/zookeeper.c:1980
#8  0x00007f9ff8e553f5 in do_io (v=0x7f9fe14de400) at src/mt_adaptor.c:379
#9  0x00007f9ff804de9a in start_thread () from /lib/x86_64-linux-gnu/libpthread.so.0
#10 0x00007f9ff671738d in clone () from /lib/x86_64-linux-gnu/libc.so.6
#11 0x0000000000000000 in ?? ()

I'm not sure what the underlying cause of this is... But POSIX always allows for a short read(2), and any program MUST check for short reads... 

Has anyone else encountered this issue? We are seeing it rather frequently which is concerning.","[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2311,Major,Marshall McMullen,Fixed,2015-12-06T19:34:42.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,assert in setup_random,2016-07-21T20:18:42.000+0000,"[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>]",4.0
Abhishek Rai,"[<JIRA Component: name='server', id='12312382'>]",2015-11-01T06:26:33.000+0000,Abhishek Rai,"Today, Zookeeper server syncs transaction log files to disk by default, but does not sync snapshot files.  Consequently, an untimely crash may result in a lost or incomplete snapshot file.  During recovery, if the server finds a valid older snapshot file, it will load it and replay subsequent log(s), skipping the incomplete snapshot file.  It's possible that the skipped file had some transactions which are not present in the replayed transaction logs.  Since quorum synchronization is based on last transaction ID of each server, this will never get noticed, resulting in inconsistency between servers and possible data loss.

Following sequence of events describes a sample scenario where this can happen:

# Server F is a follower in a Zookeeper ensemble.
# F's most recent valid snapshot file is named ""snapshot.10"" containing state up to zxid = 10.  F is currently writing to the transaction log file ""log.11"", with the most recent zxid = 20.
# Fresh round of election.
# F receives a few new transactions 21 to 30 from new leader L as the ""diff"".  Current server behavior is to dump current state plus diff to a new snapshot file, ""snapshot.30"".
# F finalizes the snapshot file, but file contents are still buffered in OS caches.  Zookeeper does not sync snapshot file contents to disk.
# F receives a new transaction 31 from the leader, which it appends to the existing transaction log file, ""log.11"" and syncs the file to disk.
# Server machine crashes or is cold rebooted.
# After recovery, snapshot file ""snapshot.30"" may not exist or may be empty.  See below for why that may happen.
# In either case, F looks for the last finalized snapshot file, finds and loads ""snapshot.10"".  It then replays transactions from ""log.11"".  Ultimately, its last seen zxid will be 31, but it would not have replayed transactions 21 to 30 received via the ""diff"" from the leader.
# Clients which are connected to F may see different data than clients connected to other members of the ensemble, violating single system image invariant.  Also, if F were to become a leader at some point, it could use its state to seed other servers, and they all could lose the writes in the missing interval above.

*Notes:*
- Reason why snapshot file may be missing or incomplete:
-- Zookeeper does not sync the data directory after creating a snapshot file.  Even if a newly created file is synced to disk, if the corresponding directory entry is not, then the file will not be visible in the namespace.
-- Zookeeper does not sync snapshot files.  So, they may be empty or incomplete during recovery from an untimely crash.
- In step (6) above, the server could also have written the new transaction 31 to a new log file, ""log.31"".  The final outcome would still be the same.

We are able to deterministically reproduce this problem using the following steps:

# Create a new Zookeeper ensemble on 3 hosts: A, B, and C.
# Ensured each server has at least one snapshot file in its data dir.
# Stop Zookeeper process on server A.
# Slow down disk syncs on server A (see example script below). This ensures that snapshot files written by Zookeeper don't make it to disk spontaneously.  Log files will be written to disk as Zookeeper explicitly issues a sync call on such files.
# Connect to server B and create a new znode /test1.
# Start Zookeeper process on A, wait for it to write a new snapshot to its datadir.  This snapshot would contain /test1 but it won’t be synced to disk yet.
# Connect to A and verify that /test1 is visible.
# Connect to B and create another znode /test2.  This will cause A’s transaction log to grow further to receive /test2.
# Cold reboot A.
# A’s last snapshot is a zero-sized file or is missing altogether since it did not get synced to disk before reboot.  We have seen both in different runs.
# Connect to A and verify that /test1 does not exist.  It exists on B and C.

Slowing down disk syncs:
{noformat}
echo 360000 | sudo tee /proc/sys/vm/dirty_writeback_centisecs
echo 360000 | sudo tee /proc/sys/vm/dirty_expire_centisecs
echo 99 | sudo tee /proc/sys/vm/dirty_background_ratio
echo 99 | sudo tee /proc/sys/vm/dirty_ratio
{noformat}
","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2310,Major,Abhishek Rai,Fixed,2021-02-11T20:37:27.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Snapshot files must be synced to prevent inconsistency or data loss,2021-02-11T20:37:27.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",6.0
Flavio Paiva Junqueira,[],2015-10-30T15:07:52.000+0000,Flavio Paiva Junqueira,"I'm getting this out of a fresh copy of branch-3.4.

{noformat}
tests/TestClient.cc:375: Assertion: equality assertion failed [Expected: -101, Actual  : -4]
tests/TestClient.cc:300: Assertion: assertion failed [Expression: ctx.waitForConnected(zk)]
Failures !!!
{noformat}",[],Bug,ZOOKEEPER-2309,Blocker,Flavio Paiva Junqueira,Not A Problem,2015-10-30T16:09:15.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,TestClient fails,2015-10-30T16:09:15.000+0000,[],1.0
,[],2015-10-29T16:57:37.000+0000,Ramesh Gopal,"telnet localhost 2181 gives the following warning messages.

2015-07-08 09:26:13,785 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@347] - caught end of stream exception
EndOfStreamException: Unable to read additional data from client sessionid 0x34e6a8473084e8d, likely client has closed socket
    at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:218)
    at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
    at java.lang.Thread.run(Thread.java:853)
2015-07-08 09:26:13,785 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@347] - caught end of stream exception
EndOfStreamException: Unable to read additional data from client sessionid 0x34e6a8473084e7f, likely client has closed socket
    at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:218)
    at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
    at java.lang.Thread.run(Thread.java:853)
2015-07-08 09:26:13,813 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@347] - caught end of stream exceptionEndOfStreamException: Unable to read additional data from client sessionid 0x34e6a8473084e8b, likely client has closed socket
    at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:218)
    at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
    at java.lang.Thread.run(Thread.java:853)
2015-07-08 09:26:13,963 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@347] - caught end of stream exceptionEndOfStreamException: Unable to read additional data from client sessionid 0x34e6a8473084e80, likely client has closed socket
    at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:218)
    at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
    at java.lang.Thread.run(Thread.java:853)
2015-07-08 09:26:13,980 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@347] - caught end of stream exceptionEndOfStreamException: Unable to read additional data from client sessionid 0x34e6a8473084e81, likely client has closed socket
    at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:218)
    at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
    at java.lang.Thread.run(Thread.java:853)
2015-07-08 09:26:13,982 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@347] - caught end of stream exceptionEndOfStreamException: Unable to read additional data from client sessionid 0x34e6a8473084e7c, likely client has closed socket
    at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:218)
    at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
    at java.lang.Thread.run(Thread.java:853)
2015-07-08 09:26:18,453 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@347] - caught end of stream exceptionEndOfStreamException: Unable to read additional data from client sessionid 0x34e6a8473084e84, likely client has closed socket
    at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:218)
    at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
    at java.lang.Thread.run(Thread.java:853)",[],Bug,ZOOKEEPER-2308,Major,Ramesh Gopal,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Warning messages in the zookeeper logs,2015-10-29T16:57:37.000+0000,[],3.0
Mohammad Arshad,"[<JIRA Component: name='server', id='12312382'>]",2015-10-29T15:12:25.000+0000,Mohammad Arshad,"This issue occurred in one of our test environment where disk was being changed to read only very frequently.
The the scenario is as follows:
# Configure three node ZooKeeper cluster, lets say nodes are A, B and C
# Start A and B. Both A and B start successfully, quorum is running.
# Start C, because of IO error C fails to update acceptedEpoch file. But C also starts successfully, joins the quorum as follower
# Stop C
# Start C, bellow exception with message ""The accepted epoch, 0 is less than the current epoch, 1"" is thrown
{code}
2015-10-29 16:52:32,942 [myid:3] - ERROR [main:QuorumPeer@784] - Unable to load database on disk
java.io.IOException: The accepted epoch, 0 is less than the current epoch, 1
	at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:781)
	at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:720)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:202)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:139)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:88)
2015-10-29 16:52:32,946 [myid:3] - ERROR [main:QuorumPeerMain@111] - Unexpected exception, exiting abnormally
java.lang.RuntimeException: Unable to run quorum server 
	at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:785)
	at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:720)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:202)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:139)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:88)
Caused by: java.io.IOException: The accepted epoch, 0 is less than the current epoch, 1
	at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:781)
{code}","[<JIRA Version: name='3.6.3', id='12348703'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.5.8', id='12346950'>]",Bug,ZOOKEEPER-2307,Major,Mohammad Arshad,Fixed,2019-12-17T12:48:46.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeper not starting because acceptedEpoch is less than the currentEpoch,2021-10-31T08:43:44.000+0000,[],14.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2015-10-27T11:48:05.000+0000,shengfeng,"deleteChild method in Pathtrie class, childNode.getChildren().length == 1, why not  childNode.getChildren().length == 0 ?",[],Bug,ZOOKEEPER-2305,Major,shengfeng,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,created,2015-10-28T05:45:18.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",1.0
Mohammad Arshad,"[<JIRA Component: name='jmx', id='12312451'>]",2015-10-26T11:47:15.000+0000,Mohammad Arshad,"The ""ClientPort"" property of {{org.apache.zookeeper.server.ZooKeeperServerBean}} returns incorrect value. It includes address also like 192.168.1.2:2183. It should return only port","[<JIRA Version: name='3.5.2', id='12331981'>]",Bug,ZOOKEEPER-2304,Major,Mohammad Arshad,Fixed,2015-12-06T20:39:33.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,JMX ClientPort from ZooKeeperServerBean incorrect,2016-07-21T20:18:30.000+0000,[],4.0
Steven Fisher,"[<JIRA Component: name='build', id='12312383'>]",2015-10-25T15:46:40.000+0000,Steven Fisher,"Trying to compile mesos on Raspberry Pi 2. Zookeeper builds ok with ant directly (Java only), but when using make for mesos it trys to compile the C code and this fails with the error:
libtool: compile:  gcc -DHAVE_CONFIG_H -I. -I./include -I./tests -I./generated -DTHREADED -g -O2 -D_GNU_SOURCE -MT libzkmt_la-mt_adaptor.lo -MD -MP -MF .deps/libzkmt_la-mt_adaptor.Tpo -c src/mt_adaptor.c  -fPIC -DPIC -o libzkmt_la-mt_adaptor.o
/tmp/ccw07Ju5.s: Assembler messages:
/tmp/ccw07Ju5.s:1515: Error: bad instruction `lock xaddl r1,[r0]'
Makefile:823: recipe for target 'libzkmt_la-mt_adaptor.lo' failed
make[5]: *** [libzkmt_la-mt_adaptor.lo] Error 1

The memos release comes with 3.4.5 but I have also tried 3.4.6",[],Bug,ZOOKEEPER-2303,Critical,Steven Fisher,Not A Problem,2015-10-27T16:17:45.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper fails to compile (mesos driver) on Raspberry Pi2,2015-10-27T16:18:14.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.4.6', id='12323310'>]",2.0
Mohammad Arshad,"[<JIRA Component: name='tests', id='12312427'>]",2015-10-25T10:52:43.000+0000,Mohammad Arshad,"When run ZooKeeper test cases following two test classes never run because wrong naming convention is followed.
{code}
org.apache.zookeeper.server.quorum.TestQuorumPeerConfig
org.apache.zookeeper.server.quorum.TestRemotePeerBean
{code}

Name of these test classes should be changed to 
{code}
org.apache.zookeeper.server.quorum.QuorumPeerConfigTest
org.apache.zookeeper.server.quorum.RemotePeerBeanTest
{code}

","[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2302,Major,Mohammad Arshad,Fixed,2015-10-28T21:58:48.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Some test cases are not running because wrongly named,2016-07-21T20:18:31.000+0000,[],5.0
Mohammad Arshad,"[<JIRA Component: name='server', id='12312382'>]",2015-10-25T09:44:22.000+0000,Mohammad Arshad,"QuorumPeer does not listen on passed client IP in the constructor, for client  connection. It always listens on all IP(0.0.0.0 or 0:0:0:0:0:0:0:0). This happens only when QuorumPeer is created using any of the bellow constructors
{code}
org.apache.zookeeper.server.quorum.QuorumPeer.QuorumPeer(Map<Long,QuorumServer> quorumPeers, File snapDir,
            File logDir, int clientPort, int electionAlg,
            long myid, int tickTime, int initLimit, int syncLimit)
{code}
{code}
org.apache.zookeeper.server.quorum.QuorumPeer.QuorumPeer(Map<Long,QuorumServer> quorumPeers, File snapDir,
            File logDir, int clientPort, int electionAlg,
            long myid, int tickTime, int initLimit, int syncLimit,
            QuorumVerifier quorumConfig)
{code}","[<JIRA Version: name='3.5.2', id='12331981'>]",Bug,ZOOKEEPER-2301,Major,Mohammad Arshad,Fixed,2015-12-07T05:01:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,QuorumPeer does not listen on passed client IP in the constructor,2016-07-21T20:18:28.000+0000,[],4.0
Mohammad Arshad,"[<JIRA Component: name='jmx', id='12312451'>]",2015-10-22T09:28:54.000+0000,Mohammad Arshad,"When clientPortAddress is not configured LocalPeerBean throws NullPointerException.

*Expected Behavior:*
# When only clientPort is configured ClientAddress value should be 0.0.0.0:clientPort or 0:0:0:0:0:0:0:0:clientPort
# When both clientPort clientPortAddress are configured then expected value is clientPortAddress:clientPort
","[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2299,Major,Mohammad Arshad,Fixed,2015-12-07T05:08:40.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,NullPointerException in LocalPeerBean for ClientAddress,2016-07-21T20:18:35.000+0000,[],6.0
,[],2015-10-22T04:31:22.000+0000,Neil Conway,"The zookeeper interface is designed to retry (once per second for up to ten minutes) if one or more of the Zookeeper hostnames can't be resolved (see [MESOS-1326] and [MESOS-1523]).

However, the current implementation assumes that a DNS resolution failure is indicated by zookeeper_init() returning NULL and errno being set to EINVAL (Zk translates getaddrinfo() failures into errno values). However, the current Zk code does:

{code}
static int getaddrinfo_errno(int rc) {
    switch(rc) {
    case EAI_NONAME:
// ZOOKEEPER-1323 EAI_NODATA and EAI_ADDRFAMILY are deprecated in FreeBSD.
#if defined EAI_NODATA && EAI_NODATA != EAI_NONAME
    case EAI_NODATA:
#endif
        return ENOENT;
    case EAI_MEMORY:
        return ENOMEM;
    default:
        return EINVAL;
    }
}
{code}

getaddrinfo() returns EAI_NONAME when ""the node or service is not known""; per discussion in [MESOS-2186], this seems to happen intermittently due to DNS failures.

Proposed fix: looking at errno is always going to be somewhat fragile, but if we're going to continue doing that, we should check for ENOENT as well as EINVAL.",[],Bug,ZOOKEEPER-2298,Minor,Neil Conway,Invalid,2015-10-22T04:33:17.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zookeeper: Should retry on EAI_NONAME return from getaddrinfo(),2015-10-22T04:33:17.000+0000,[],1.0
Mohammad Arshad,"[<JIRA Component: name='server', id='12312382'>]",2015-10-19T11:38:58.000+0000,Anushri,"NPE is thrown while creating ""key manager"" and ""trust manager"" , even though the zk setup is in non-secure mode

bq. 2015-10-19 12:54:12,278 [myid:2] - ERROR [ProcessThread(sid:2 cport:-1)::X509AuthenticationProvider@78] - Failed to create key manager

bq. org.apache.zookeeper.common.X509Exception$KeyManagerException: java.lang.NullPointerException

at org.apache.zookeeper.common.X509Util.createKeyManager(X509Util.java:129)

at org.apache.zookeeper.server.auth.X509AuthenticationProvider.<init>(X509AuthenticationProvider.java:75)

at org.apache.zookeeper.server.auth.ProviderRegistry.initialize(ProviderRegistry.java:42)

at org.apache.zookeeper.server.auth.ProviderRegistry.getProvider(ProviderRegistry.java:68)

at org.apache.zookeeper.server.PrepRequestProcessor.fixupACL(PrepRequestProcessor.java:952)

at org.apache.zookeeper.server.PrepRequestProcessor.pRequest2Txn(PrepRequestProcessor.java:379)

at org.apache.zookeeper.server.PrepRequestProcessor.pRequest(PrepRequestProcessor.java:716)

at org.apache.zookeeper.server.PrepRequestProcessor.run(PrepRequestProcessor.java:144)

Caused by: java.lang.NullPointerException

at org.apache.zookeeper.common.X509Util.createKeyManager(X509Util.java:113)

... 7 more

bq. 2015-10-19 12:54:12,279 [myid:2] - ERROR [ProcessThread(sid:2 cport:-1)::X509AuthenticationProvider@90] - Failed to create trust manager

bq.  org.apache.zookeeper.common.X509Exception$TrustManagerException: java.lang.NullPointerException

at org.apache.zookeeper.common.X509Util.createTrustManager(X509Util.java:158)

at org.apache.zookeeper.server.auth.X509AuthenticationProvider.<init>(X509AuthenticationProvider.java:87)

at org.apache.zookeeper.server.auth.ProviderRegistry.initialize(ProviderRegistry.java:42)

at org.apache.zookeeper.server.auth.ProviderRegistry.getProvider(ProviderRegistry.java:68)

at org.apache.zookeeper.server.PrepRequestProcessor.fixupACL(PrepRequestProcessor.java:952)

at org.apache.zookeeper.server.PrepRequestProcessor.pRequest2Txn(PrepRequestProcessor.java:379)

at org.apache.zookeeper.server.PrepRequestProcessor.pRequest(PrepRequestProcessor.java:716)

at org.apache.zookeeper.server.PrepRequestProcessor.run(PrepRequestProcessor.java:144)

Caused by: java.lang.NullPointerException

at org.apache.zookeeper.common.X509Util.createTrustManager(X509Util.java:143)

... 7 more

","[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2297,Blocker,Anushri,Fixed,2016-06-23T17:43:25.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"NPE is thrown while creating ""key manager"" and ""trust manager"" ",2016-07-21T20:18:32.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",10.0
Raúl Gutiérrez Segalés,[],2015-10-18T21:08:34.000+0000,Raúl Gutiérrez Segalés,"Apparently, ZOOKEEPER-2253 wasn't fully backported from trunk so it doesn't compile now. 

We should make sure jenkins runs for 3.4, to catch these issues in the future. ","[<JIRA Version: name='3.4.7', id='12325149'>]",Bug,ZOOKEEPER-2296,Blocker,Raúl Gutiérrez Segalés,Fixed,2015-10-21T22:12:37.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,compilation broken for 3.4,2015-10-21T22:12:37.000+0000,[],3.0
Mohammad Arshad,[],2015-10-17T06:32:53.000+0000,Mohammad Arshad,"When Kerberos is used as authentication mechanism some time TGT is getting expired because it is not refreshed timely.
The scenario is as follow:
suppose now=8 (the current milliseconds)
next refresh time= 10
TGT expire time= 9

*Current behaviour:* Error is logged and TGT refresh thread exits.
*Expected behaviour:* TGT should be refreshed immediately(now) instead of nextRefreshTime","[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2295,Major,Mohammad Arshad,Fixed,2015-12-09T04:11:33.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,TGT refresh time logic is wrong,2016-07-21T20:18:31.000+0000,"[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>]",4.0
Charlie Helin,"[<JIRA Component: name='build', id='12312383'>]",2015-10-14T19:50:48.000+0000,Charlie Helin,"It appears that the current implementation of 'generate-clover-reports' is broken. 

# It doesn't define the  clover-report task 
# The clover-report element is missing the proper clover db reference ","[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2294,Major,Charlie Helin,Fixed,2016-03-03T17:27:41.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Ant target generate-clover-reports is broken,2016-07-21T20:18:38.000+0000,[],4.0
Rabi Kumar K C,"[<JIRA Component: name='server', id='12312382'>]",2015-10-14T07:56:57.000+0000,Praveena Manvi,"In https://svn.apache.org/repos/asf/zookeeper/trunk/src/java/main/org/apache/zookeeper/server/ZooKeeperServer.java
If readOnly flag is not being sent it gets logged as warning. Since we have enabled warning, the server gets filled up with messages like

Btw, readonly is optional and introduced later (http://wiki.apache.org/hadoop/ZooKeeper/GSoCReadOnlyMode),

{code}
015-08-14T11:03:11+00:00 Connection request from old client /192.168.24.16:14479; will be dropped if server is in r-o mode
...
2015-08-14T11:21:56+00:00 Connection request from old client 
2015-08-14T11:18:40+00:00 Connection request from old client /192.168.24.14:12135; will be dropped if server is in r-o mode
2015-08-14T11:19:40+00:00 Connection request from old client /192.168.24.14:12310; will be dropped if server is in r-o mode
{code}

we are just forced to send read-only flag which is optional to avoid wrong logging level chosen by zookeeper.
{code}
       boolean readOnly = false;
        try {
            readOnly = bia.readBool(""readOnly"");
            cnxn.isOldClient = false;
        } catch (IOException e) {
            // this is ok -- just a packet from an old client which
            // doesn't contain readOnly field
            LOG.warn(""Connection request from old client ""
                    + cnxn.getRemoteSocketAddress()
                    + ""; will be dropped if server is in r-o mode"");
        }
{code}

Suggest to demote the same to DEBUG as its not intended to warn in anyway.

",[],Bug,ZOOKEEPER-2293,Minor,Praveena Manvi,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Incorrect log level 'warn'  is resulting in clutering of logs, Suggest to demote it to DEBUG from WARN",2020-02-01T17:19:06.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",4.0
Neha Bathra,[],2015-10-13T15:41:05.000+0000,Neha Bathra,"While stopping zookeeper prints below message :

""ZooKeeper JMX enabled by default
ZooKeeper remote JMX Port set to 2022
ZooKeeper remote JMX authenticate set to false
ZooKeeper remote JMX ssl set to false
ZooKeeper remote JMX log4j set to true
Stopping zookeeper ...STOPPED""

JMX message should only be printed while starting ZK.",[],Bug,ZOOKEEPER-2291,Minor,Neha Bathra,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,JMX log should not be printed while stopping ZK Server,2015-10-14T06:37:05.000+0000,[],2.0
Chris Nauroth,"[<JIRA Component: name='server', id='12312382'>]",2015-10-10T22:16:24.000+0000,Chris Nauroth,"During shutdown, requests may still be in flight in the request processing pipeline.  Some of these requests have reached a state where the transaction has executed and committed, but has not yet been acknowledged back to the client.  It's possible that these transactions will not ack to the client before the shutdown sequence completes.",[],Bug,ZOOKEEPER-2288,Major,Chris Nauroth,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,"During shutdown, server may fail to ack completed transactions to clients.",2015-10-10T22:55:32.000+0000,[],2.0
Mohammad Arshad,"[<JIRA Component: name='tests', id='12312427'>]",2015-09-28T21:25:05.000+0000,Mohammad Arshad,"# test case {{org.apache.zookeeper.test.QuorumTest.testSessionMove()}} is marked ignored by ZOOKEEPER-907
# Most of the CI pre-commit feedback is -1 because of above ignored test case.
# Test case is locally passing

The ignore tag should be removed from testSessionMove test case.","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-2285,Major,Mohammad Arshad,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,QuorumTest's ignored test case causes wrong CI pre-commit feedback,2022-02-03T08:36:23.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Ling Mao,[],2015-09-28T18:27:56.000+0000,Mohammad Arshad,"{{LogFormatter}} and {{SnapshotFormatter}} does not handle FileNotFoundException gracefully. If file no exist then these classes propagate the exception to console.
{code}
Exception in thread ""main"" java.io.FileNotFoundException: log.1 (The system cannot find the file specified)
        at java.io.FileInputStream.open(Native Method)
        at java.io.FileInputStream.<init>(FileInputStream.java:146)
        at java.io.FileInputStream.<init>(FileInputStream.java:101)
        at org.apache.zookeeper.server.LogFormatter.main(LogFormatter.java:49)
{code}

 File existence should be validated and appropriate message should be displayed on console if file does not exist","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-2284,Minor,Mohammad Arshad,Fixed,2019-02-22T09:45:08.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,LogFormatter and SnapshotFormatter does not handle FileNotFoundException gracefully,2019-05-20T17:51:06.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",6.0
Mohammad Arshad,"[<JIRA Component: name='documentation', id='12312422'>]",2015-09-28T17:47:19.000+0000,Mohammad Arshad,"zookeeperAdmin guide have following description for traceFile property
{noformat}
traceFile
(Java system property: requestTraceFile)
If this option is defined, requests will be will logged to a trace file named traceFile.year.month.day.
Use of this option provides useful debugging information, but will impact performance. (Note: The system property has no zookeeper prefix, and the configuration variable name is different from the system property. Yes - it's not consistent, and it's annoying.)
{noformat}
But this property is used no where  in the whole ZooKeeper code.    it should be removed from documentation

","[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2283,Major,Mohammad Arshad,Fixed,2016-03-14T07:00:38.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"traceFile property is not used in the ZooKeeper,  it should be removed from documentation",2019-05-09T18:59:52.000+0000,"[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.0', id='12316644'>]",4.0
Andrew Grasso,"[<JIRA Component: name='c client', id='12312380'>]",2015-09-28T17:19:48.000+0000,Andrew Grasso,"Callbacks passed to [zoo_acreate], [zoo_async], and [zoo_amulti] (for create ops) are called on paths that include the chroot. This is analagous to issue 1027, which fixed this bug for synchronous calls.
I've created a patch to fix this in trunk","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.7', id='12346098'>]",Bug,ZOOKEEPER-2282,Critical,Andrew Grasso,Fixed,2019-09-27T13:43:56.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,chroot not stripped from path in asynchronous callbacks,2020-02-14T15:23:48.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",5.0
Neha Bathra,"[<JIRA Component: name='scripts', id='12312384'>]",2015-09-23T14:43:57.000+0000,Neha Bathra,"Zookeeper startup fails if there are spaces in the %JAVA_HOME% variable. 
{code}
if not exist %JAVA_HOME%\bin\java.exe (
  echo Error: JAVA_HOME is incorrectly set.
  goto :eof
)

set JAVA=%JAVA_HOME%\bin\java
{code}","[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2281,Minor,Neha Bathra,Fixed,2015-12-09T23:27:24.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZK Server startup fails if there are spaces in the JAVA_HOME path,2016-07-21T20:18:15.000+0000,[],8.0
,"[<JIRA Component: name='server', id='12312382'>]",2015-09-22T01:13:24.000+0000,Edward Ribeiro,Even though NettyServerCnxnFactory has maxClientCnxns (default to 60) it doesn't enforce this limit in the code.,[],Bug,ZOOKEEPER-2280,Major,Edward Ribeiro,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,NettyServerCnxnFactory doesn't honor maxClientCnxns param,2022-02-03T08:50:14.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.5.1', id='12326786'>]",7.0
Mohammad Arshad,"[<JIRA Component: name='quorum', id='12312379'>]",2015-09-21T15:10:25.000+0000,sunhaitao,"in  loadDataBase() method, the below info is incorrect.
 if (acceptedEpoch < currentEpoch) {
                throw new IOException(""The current epoch, "" + ZxidUtils.zxidToString(currentEpoch) + "" is less than the accepted epoch, "" + ZxidUtils.zxidToString(acceptedEpoch));
            }

It should print:
Change the message to (""The accepted epoch, "" + ZxidUtils.zxidToString(acceptedEpoch) + "" is less than the current epoch, "" + ZxidUtils.zxidToString(currentEpoch)
","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2279,Major,sunhaitao,Fixed,2015-09-25T06:35:59.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,QuorumPeer  loadDataBase() error message is incorrect,2016-07-21T20:18:41.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.5.1', id='12326786'>]",5.0
,[],2015-09-18T12:50:21.000+0000,Neha Bathra,run echo stmk 123 | netcat <hostname> <port> fails with NullPointerException,[],Bug,ZOOKEEPER-2278,Major,Neha Bathra,Duplicate,2015-09-18T16:51:35.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,4lw command stmk throws NullPointerException,2015-09-18T17:59:49.000+0000,[],4.0
Mohammad Arshad,"[<JIRA Component: name='java client', id='12312381'>]",2015-09-15T05:54:38.000+0000,Neha Bathra,"With normal create operation, the path of the failed node is displayed in KeeperException but this is not the case when create operation is through multi api

","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-2276,Minor,Neha Bathra,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Multi operation failure  does not include path in KeeperException,2022-02-03T08:36:22.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
Hannu Valtonen,"[<JIRA Component: name='build', id='12312383'>]",2015-09-14T20:21:34.000+0000,Hannu Valtonen,"Three issues with RPM package building, 

The install stage was removing BUILDROOT content:

[rpm] + rm -rf /tmp/zkpython_build_rpm/BUILD

Since BUILD and BUILDROOT are actually the same folder, everything is
removed before being used.

The original fix for this problem  was submitted by Cédric Lejeune

http://mail-archives.apache.org/mod_mbox/zookeeper-user/201212.mbox/%3C50D2D481.8010507@pt-consulting.eu%3E

The other two issues that need to be fixed are an invalid argument given to popd and a reference to old redhat RPM packaging scripts.
",[],Bug,ZOOKEEPER-2275,Major,Hannu Valtonen,Won't Fix,2016-03-03T16:24:57.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Fix RPM package creation on recent distros,2016-03-03T16:24:57.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",2.0
,[],2015-09-10T18:45:40.000+0000,Benjamin Jaton,"Scenario:
- Install a Zookeeper on machine A
- Install a Zookeeper on machine B, joining A to form an ensemble
- Reinstall ZooKeeper on A (but with standaloneEnabled=false)
-> B automatically joins A to form an ensemble again

I think the work needed is discussed and addressed in ZOOKEEPER-832.",[],Bug,ZOOKEEPER-2273,Major,Benjamin Jaton,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Uninvited ZK joins the ensemble,2015-09-16T23:14:03.000+0000,[],2.0
Mohammad Arshad,"[<JIRA Component: name='jmx', id='12312451'>]",2015-09-09T12:16:27.000+0000,Mohammad Arshad,"{code}org.apache.zookeeper.server.quorum.RemotePeerBean.getClientAddress(){code} throws NullPointerException when clientPort is not part of dynamic configuration.

","[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2269,Major,Mohammad Arshad,Fixed,2015-09-10T04:50:31.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,NullPointerException  in RemotePeerBean,2016-07-21T20:18:27.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Mohammad Arshad,"[<JIRA Component: name='build', id='12312383'>]",2015-09-08T14:08:00.000+0000,Mohammad Arshad,"Zookeeper doc creation fails on windows with following error
{code}
D:\gitHome\zookeeper-trunk\build.xml:484: Execute failed: java.io.IOException: Cannot run program ""C:\non-install\apache-forrest-0.9\bin\forrest""
y ""D:\gitHome\zookeeper-trunk\src\docs""): CreateProcess error=193, %1 is not a valid Win32 application

{code}","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.2', id='12331981'>]",Bug,ZOOKEEPER-2268,Major,Mohammad Arshad,Fixed,2015-10-03T21:04:47.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Zookeeper doc creation fails on windows,2016-07-21T20:18:34.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Mohammad Arshad,"[<JIRA Component: name='build', id='12312383'>]",2015-09-03T14:32:07.000+0000,Mohammad Arshad,"running  {color:red}ant tar{color}  gives following error
{code}
D:\gitHome\zookeeper-trunk\build.xml:1432: Execute failed: java.io.IOException: Cannot run program ""autoreconf"" (in directory ""D:\gitHome\zookeeper-trunk\src\c""):
{code}
This error is purely environment error and it can be fixed by installing appropriate software package. 
But does it really required to configure the cpp unit as  {color:red}ant tar{color} target flow does not run cppunit test cases. Then why to configure?
There should be no cppunit configurations for  {color:red}ant tar{color} target flow.",[],Bug,ZOOKEEPER-2265,Minor,Mohammad Arshad,Invalid,2021-01-07T11:09:42.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zookeeper build fails while doing configuration for cppunit test,2021-01-07T11:09:42.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",2.0
Mohammad Arshad,"[<JIRA Component: name='server', id='12312382'>]",2015-08-31T13:22:07.000+0000,Mohammad Arshad,"# Wrong error message when secureClientPortAddress is configured but secureClientPort is not configured.
zookeeper throws IllegalArgumentException with error message {{clientPortAddress is set but clientPort is not set}} but should be {{secureClientPortAddress is set but secureClientPort is not set}}
# There is another problem with the same code.
value is assigned to local variable but null check is done on instance variable so we will never get error message for this scenario.
{code}if (this.secureClientPortAddress != null) {{code}
should be replaced with 
{code}if (secureClientPortAddress != null) {{code}
# Above problem is there for clientPort scenario also. So we should replace
{code}if (this.clientPortAddress != null) {{code}
with 
{code}if (clientPortAddress != null) {{code}
","[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2264,Minor,Mohammad Arshad,Fixed,2015-09-06T18:06:00.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Wrong error message when secureClientPortAddress is configured but secureClientPort is not configured ,2016-07-21T20:18:22.000+0000,[],4.0
Mohammad Arshad,[],2015-08-28T15:14:58.000+0000,Mohammad Arshad,"ZooKeeper server should not start when neither clientPort no secureClientPort is configured.
Without any client port ZooKeeper server can not server any purpose. It should simply return with proper error message","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-2263,Minor,Mohammad Arshad,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,ZooKeeper server should not start when neither clientPort no secureClientPort is configured,2022-02-03T08:36:23.000+0000,[],3.0
Mohammad Arshad,"[<JIRA Component: name='server', id='12312382'>]",2015-08-28T14:56:05.000+0000,Mohammad Arshad,"Admin commands  do not include secure client information
connections, configuration, connection_stat_reset and stats admin should include secure client informations
1) configuration should include the secure client port also
2) connections should include secure connections also
3) connection_stat_reset should also reset secure connection
4) stats command should accumulate both secure and non secure information",[],Bug,ZOOKEEPER-2262,Major,Mohammad Arshad,Won't Fix,2021-01-07T11:05:36.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Admin commands do not include secure client information,2021-01-07T11:05:36.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",1.0
Andor Molnar,[],2015-08-28T14:32:27.000+0000,Mohammad Arshad,"When only secureClientPort is configured connections, configuration, connection_stat_reset and stats admin commands throw NullPointerException. Here is stack trace one of the connections command.
{code}
java.lang.NullPointerException
	at org.apache.zookeeper.server.admin.Commands$ConsCommand.run(Commands.java:177)
	at org.apache.zookeeper.server.admin.Commands.runCommand(Commands.java:92)
	at org.apache.zookeeper.server.admin.JettyAdminServer$CommandServlet.doGet(JettyAdminServer.java:166)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
{code}","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-2261,Major,Mohammad Arshad,Fixed,2018-09-10T22:18:09.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"When only secureClientPort is configured connections, configuration, connection_stat_reset, and stats admin commands throw NullPointerException",2019-05-20T17:50:27.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
,[],2015-08-27T14:46:41.000+0000,Mohammad Arshad,"4 letter word commands are slow because these commands unnecessarily go through Sasl authentication. 
{code}
ZooKeeperSaslServer.<init>(Login) line: 48	
	NettyServerCnxn.<init>(Channel, ZooKeeperServer, NettyServerCnxnFactory) line: 88	
	NettyServerCnxnFactory$CnxnChannelHandler.channelConnected(ChannelHandlerContext, ChannelStateEvent) line: 89	
	NettyServerCnxnFactory$CnxnChannelHandler(SimpleChannelHandler).handleUpstream(ChannelHandlerContext, ChannelEvent) line: 118	
	DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline$DefaultChannelHandlerContext, ChannelEvent) line: 564
{code}
as per the document 4lw commands are executed as bellow
{{$ echo mntr | nc localhost 2185}} .
Even without passing any authentication information it works fine.

So  4lw command either should do authentication properly or it should not go through Sasl authentication flow.



",[],Bug,ZOOKEEPER-2259,Major,Mohammad Arshad,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,4 letter word commands are slow because these unnecessarily go through Sasl authentication,2015-08-28T06:29:45.000+0000,[],2.0
Mohammad Arshad,"[<JIRA Component: name='scripts', id='12312384'>]",2015-08-20T12:57:42.000+0000,Mohammad Arshad,"Zookeeper is not using specified JMX port.
I put bellow entry in zkEnv.sh
{{export JMXPORT=12345}}
But zookeeper still uses random port for jmx.","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2256,Minor,Mohammad Arshad,Fixed,2015-09-06T17:51:20.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Zookeeper is not using specified JMX port in zkEnv.sh,2016-07-21T20:18:33.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
,[],2015-08-19T22:32:24.000+0000,JUAN TORRES,"Hi,

I need support. I have tried to troubleshoot this problem, but anything I do, is not working. I do not have a problem with the NN, ZK or any network limitation, but the phoenix still not able to connect the zookeeper servers.

[root@svrdn174 bin]# /opt/phoenix/bin/sqlline.py svrzj001,svrzj002,svrzj003= :2181/hbase Setting property: [isolation, TRANSACTION_READ_COMMITTED]
issuing: !connect jdbc:phoenix:svrzj001,svrzj002,svrzj003:2181/hbase none n= one org.apache.phoenix.jdbc.PhoenixDriver
Connecting to jdbc:phoenix:svrzj001,svrzj002,svrzj003:2181/hbase
15/08/19 18:15:30 WARN util.NativeCodeLoader: Unable to load native-hadoop = library for your platform... using builtin-java classes where applicable
Error: ERROR 103 (08004): Unable to establish connection. (state=3D08004,co=
de=3D103)
java.sql.SQLException: ERROR 103 (08004): Unable to establish connection.
        at org.apache.phoenix.exception.SQLExceptionCode$Factory$1.newExcep=
tion(SQLExceptionCode.java:388)
        at org.apache.phoenix.exception.SQLExceptionInfo.buildException(SQL=
ExceptionInfo.java:145)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl.openConnect=
ion(ConnectionQueryServicesImpl.java:297)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl.access$300(=
ConnectionQueryServicesImpl.java:180)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl$12.call(Con=
nectionQueryServicesImpl.java:1901)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl$12.call(Con=
nectionQueryServicesImpl.java:1880)
        at org.apache.phoenix.util.PhoenixContextExecutor.call(PhoenixConte=
xtExecutor.java:77)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl.init(Connec=
tionQueryServicesImpl.java:1880)
        at org.apache.phoenix.jdbc.PhoenixDriver.getConnectionQueryServices=
(PhoenixDriver.java:180)
        at org.apache.phoenix.jdbc.PhoenixEmbeddedDriver.connect(PhoenixEmb=
eddedDriver.java:132)
        at org.apache.phoenix.jdbc.PhoenixDriver.connect(PhoenixDriver.java=
:151)
        at sqlline.DatabaseConnection.connect(DatabaseConnection.java:157)
        at sqlline.DatabaseConnection.getConnection(DatabaseConnection.java=
:203)
        at sqlline.Commands.connect(Commands.java:1064)
        at sqlline.Commands.connect(Commands.java:996)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessor=
Impl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethod=
AccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sqlline.ReflectiveCommandHandler.execute(ReflectiveCommandHandle=
r.java:36)
        at sqlline.SqlLine.dispatch(SqlLine.java:804)
        at sqlline.SqlLine.initArgs(SqlLine.java:588)
        at sqlline.SqlLine.begin(SqlLine.java:656)
        at sqlline.SqlLine.start(SqlLine.java:398)
        at sqlline.SqlLine.main(SqlLine.java:292)
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnectio=
n(ConnectionFactory.java:240)
        at org.apache.hadoop.hbase.client.ConnectionManager.createConnectio=
n(ConnectionManager.java:410)
        at org.apache.hadoop.hbase.client.ConnectionManager.createConnectio=
nInternal(ConnectionManager.java:319)
        at org.apache.hadoop.hbase.client.HConnectionManager.createConnecti=
on(HConnectionManager.java:144)
        at org.apache.phoenix.query.HConnectionFactory$HConnectionFactoryIm=
pl.createConnection(HConnectionFactory.java:47)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl.openConnect=
ion(ConnectionQueryServicesImpl.java:295)
        ... 22 more
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Me=
thod)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeCons=
tructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Delega=
tingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnectio=
n(ConnectionFactory.java:238)
        ... 27 more
Caused by: java.lang.ExceptionInInitializerError
        at org.apache.hadoop.hbase.ClusterId.parseFrom(ClusterId.java:64)
        at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode=
(ZKClusterId.java:75)
        at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(Zo=
oKeeperRegistry.java:86)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImpl=
ementation.retrieveClusterId(ConnectionManager.java:833)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImpl=
ementation.<init>(ConnectionManager.java:623)
        ... 32 more
Caused by: java.lang.IllegalArgumentException: java.net.UnknownHostExceptio=
n: svrhdfscluster
        at org.apache.hadoop.security.SecurityUtil.buildTokenService(Securi=
tyUtil.java:373)
        at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNode=
Proxies.java:258)
        at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxi=
es.java:153)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:602)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:547)     =
   at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFi=
leSystem.java:139)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java=
:2591)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.jav=
a:2625)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2607)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)
        at org.apache.hadoop.hbase.util.DynamicClassLoader.<init>(DynamicCl=
assLoader.java:104)
        at org.apache.hadoop.hbase.protobuf.ProtobufUtil.<clinit>(ProtobufU=
til.java:229)
        ... 37 more
Caused by: java.net.UnknownHostException: svrhdfscluster
        ... 51 more
sqlline version 1.1.8
0: jdbc:phoenix:svrzj001,svrzj002,svrzj003:21>

Thanks,


",[],Bug,ZOOKEEPER-2254,Major,JUAN TORRES,Won't Fix,2015-08-20T11:47:57.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,CDH 5.4.4 HBASE 1.0.0 with PHX 4.5.0,2015-08-20T11:47:57.000+0000,[],1.0
Chris Chen,"[<JIRA Component: name='c client', id='12312380'>]",2015-08-19T21:58:45.000+0000,Chris Chen,"Affects C clients from 3.3 to trunk.

The Java client does not enforce ordering on ping requests. It merely updates fields when a ping reply is received and schedules a new ping request when necessary.

The C client actually enqueues the void response in the completion data structure and pulls it off when it gets a response.

This sounds like an implementation detail (and it is, sort of), but if a future server were to, say, send unsolicited ping replies to a client to assert liveness, it would work fine against a Java client but would cause a C client to fail the assertion in zookeeper_process, ""assert(cptr)"", line 2912, zookeeper.c.",[],Bug,ZOOKEEPER-2253,Major,Chris Chen,Fixed,2015-09-26T20:30:04.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"C asserts ordering of ping requests, while Java client does not",2016-03-03T01:29:40.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Timothy James Ward,[],2015-08-19T20:09:07.000+0000,Mohammad Arshad,"Test {{org.apache.zookeeper.test.StaticHostProviderTest.testTwoInvalidHostAddresses()}} fails randomly.

Refer bellow test ci buils:
https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/2827/testReport/

https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/2828/testReport/

https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/2830/testReport/




","[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2252,Minor,Mohammad Arshad,Fixed,2015-12-10T20:44:43.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Random test case failure in org.apache.zookeeper.test.StaticHostProviderTest,2016-07-21T20:18:21.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",7.0
Mohammad Arshad,"[<JIRA Component: name='java client', id='12312381'>]",2015-08-19T02:47:03.000+0000,nijel,"I came across one issue related to Client side packet response timeout In my cluster many packet drops happened for some time.

One observation is the zookeeper client got hanged. As per the thread dump it is waiting for the response/ACK for the operation performed (synchronous API used here).
I am using zookeeper.serverCnxnFactory=org.apache.zookeeper.server.NIOServerCnxnFactory

Since only few packets missed there is no DISCONNECTED event occurred.

Need add a ""response time out"" for the operations or packets.

*Comments from [~rakeshr]*
My observation about the problem:-

* Can use tools like 'Wireshark' to simulate the artificial packet loss.
* Assume there is only one packet in the 'outgoingQueue' and unfortunately the server response packet lost. Now, client will enter into infinite waiting. https://github.com/apache/zookeeper/blob/trunk/src/java/main/org/apache/zookeeper/ClientCnxn.java#L1515
* Probably we can discuss more about this problem and possible solutions(add packet ACK timeout or another better approach) in the jira.
","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-2251,Critical,nijel,Fixed,2018-07-27T03:16:40.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Add Client side packet response timeout to avoid infinite wait.,2019-05-20T17:50:59.000+0000,"[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.4.11', id='12339207'>]",17.0
Abraham Fine,[],2015-08-17T16:24:12.000+0000,Benjamin Jaton,"Unexpected exception, exiting abnormally 
java.io.IOException: CRC check failed 
org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:612) 
org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:157) 
org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:223) 
org.apache.zookeeper.server.ZooKeeperServer.loadData(ZooKeeperServer.java:272) 
org.apache.zookeeper.server.ZooKeeperServer.startdata(ZooKeeperServer.java:399)

To reproduce, set the preAllocSize to 8MB, the jute.maxbuffer to 20MB and try saving a 15MB node several times.
In my case the erroneous CRC appears after the second save. I use the LogFormatter class to detect it.
I suspect that the CRC error happens when the new transaction log is created, the code probably expects to have enough room to save the transaction when creating a new file, but it's too small.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.12', id='12342040'>]",Bug,ZOOKEEPER-2249,Major,Benjamin Jaton,Fixed,2018-01-18T23:48:44.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,CRC check failed when preAllocSize smaller than node data,2018-03-08T17:40:03.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.6.0', id='12326518'>]",5.0
Rakesh Radhakrishnan,[],2015-08-14T13:13:18.000+0000,Mohammad Arshad,"Zookeeper service becomes unavailable when leader fails to write transaction log. Bellow are the exceptions
{code}
2015-08-14 15:41:18,556 [myid:100] - ERROR [SyncThread:100:ZooKeeperCriticalThread@48] - Severe unrecoverable error, from thread : SyncThread:100
java.io.IOException: Input/output error
	at sun.nio.ch.FileDispatcherImpl.force0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.force(FileDispatcherImpl.java:76)
	at sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:376)
	at org.apache.zookeeper.server.persistence.FileTxnLog.commit(FileTxnLog.java:331)
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.commit(FileTxnSnapLog.java:380)
	at org.apache.zookeeper.server.ZKDatabase.commit(ZKDatabase.java:563)
	at org.apache.zookeeper.server.SyncRequestProcessor.flush(SyncRequestProcessor.java:178)
	at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:113)
2015-08-14 15:41:18,559 [myid:100] - INFO  [SyncThread:100:ZooKeeperServer$ZooKeeperServerListenerImpl@500] - Thread SyncThread:100 exits, error code 1
2015-08-14 15:41:18,559 [myid:100] - INFO  [SyncThread:100:ZooKeeperServer@523] - shutting down
2015-08-14 15:41:18,560 [myid:100] - INFO  [SyncThread:100:SessionTrackerImpl@232] - Shutting down
2015-08-14 15:41:18,560 [myid:100] - INFO  [SyncThread:100:LeaderRequestProcessor@77] - Shutting down
2015-08-14 15:41:18,560 [myid:100] - INFO  [SyncThread:100:PrepRequestProcessor@1035] - Shutting down
2015-08-14 15:41:18,560 [myid:100] - INFO  [SyncThread:100:ProposalRequestProcessor@88] - Shutting down
2015-08-14 15:41:18,561 [myid:100] - INFO  [SyncThread:100:CommitProcessor@356] - Shutting down
2015-08-14 15:41:18,561 [myid:100] - INFO  [CommitProcessor:100:CommitProcessor@191] - CommitProcessor exited loop!
2015-08-14 15:41:18,562 [myid:100] - INFO  [SyncThread:100:Leader$ToBeAppliedRequestProcessor@915] - Shutting down
2015-08-14 15:41:18,562 [myid:100] - INFO  [SyncThread:100:FinalRequestProcessor@646] - shutdown of request processor complete
2015-08-14 15:41:18,562 [myid:100] - INFO  [SyncThread:100:SyncRequestProcessor@191] - Shutting down
2015-08-14 15:41:18,563 [myid:100] - INFO  [ProcessThread(sid:100 cport:-1)::PrepRequestProcessor@159] - PrepRequestProcessor exited loop!
{code}
After this exception Leader server still remains leader. After this non recoverable exception the leader should go down and let other followers become leader.
","[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2247,Critical,Mohammad Arshad,Fixed,2016-08-13T13:59:04.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Zookeeper service becomes unavailable when leader fails to write transaction log,2019-01-30T14:31:35.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",12.0
Michael Han,"[<JIRA Component: name='quorum', id='12312379'>]",2015-08-13T02:44:54.000+0000,Michi Mutsuzaki,"Receive worker can take a long time to shut down because the socket timeout is set to zero: http://s.apache.org/TfI

There was a discussion on the mailing list a while back: http://s.apache.org/cYG",[],Bug,ZOOKEEPER-2246,Major,Michi Mutsuzaki,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,quorum connection manager takes a long time to shut down,2022-02-03T08:50:12.000+0000,[],8.0
Mohammad Arshad,[],2015-08-11T13:48:18.000+0000,Mohammad Arshad,"When {{org.apache.zookeeper.test.system.SimpleSysTest}} is run for in-memory Zookeeper Servers, by specifying baseSysTest.fakeMachines=yes, it fails. Its displays following errors
1:{code}
java.io.IOException: org.apache.zookeeper.server.quorum.QuorumPeerConfig$ConfigException: Address unresolved: 127.0.0.1:participant
	at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:474)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1077)
Caused by: org.apache.zookeeper.server.quorum.QuorumPeerConfig$ConfigException: Address unresolved: 127.0.0.1:participant
	at org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer.<init>(QuorumPeer.java:221)
{code}
2:
{code}
java.lang.NullPointerException
	at org.apache.zookeeper.test.system.BaseSysTest.tearDown(BaseSysTest.java:66)
{code}","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2245,Major,Mohammad Arshad,Fixed,2015-09-17T07:14:36.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,SimpleSysTest test cases fails,2016-07-21T20:18:17.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
Mohammad Arshad,[],2015-08-11T11:26:53.000+0000,Mohammad Arshad,"This issue occurs in following scenario
1) configure server properties in  zookeeper configuration file(zoo.cfg)
example:
{code}
server.1=localhost:43222:43225:participant;0.0.0.0:43228
server.2=localhost:43223:43226:participant;0.0.0.0:43229
server.3=localhost:43224:43227:participant;0.0.0.0:43230
{code}
2)  start the servers on windows. All the servers started successfully
3) stop any of the server
4)  try to start the stopped server. It fails with following error
{code}
org.apache.zookeeper.server.quorum.QuorumPeerConfig$ConfigException: Error processing D:SystemTestCasesZKServer1confzoo.cfg.dynamic.100000000
{code}





","[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2244,Critical,Mohammad Arshad,Fixed,2015-09-29T02:12:36.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,On Windows zookeeper fails to restart,2016-07-21T20:18:35.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",7.0
Chris Nauroth,[],2015-08-11T08:16:17.000+0000,Ivan Kelly,"http://zookeeper.apache.org/doc/r3.4.6/zookeeperAdmin.html#sc_supportedPlatforms

It refers to Solaris as Sun Solaris so it's at least 5 years out of date.

We should ""support"" the platforms that we are running zookeeper on regularly, so I suggest paring it down to linux and windows (mac os doesn't really count because people don't run it on servers anymore). Everything else should be ""may work, not supported, but will fix obvious bugs"".
","[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2243,Major,Ivan Kelly,Fixed,2016-02-08T21:33:20.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Supported platforms is completely out of date,2016-07-21T20:18:29.000+0000,[],8.0
,"[<JIRA Component: name='build', id='12312383'>]",2015-08-06T17:41:20.000+0000,Chris Nauroth,"The ZooKeeper build injects OSGi headers into the manifest, but the {{Import-Package}} header does not include {{org.ietf.jgss}}, which is used by the ZooKeeper code.  For applications using ZooKeeper inside an OSGi container, this can cause {{ClassNotFoundException}} unless the application adds the missing import to its own OSGi bundle.",[],Bug,ZOOKEEPER-2242,Major,Chris Nauroth,Duplicate,2015-08-21T16:27:47.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZooKeeper OSGi bundle is missing package import for org.ietf.jgss.,2015-08-21T16:27:47.000+0000,[],1.0
,[],2015-08-05T20:18:31.000+0000,Reza Farivar,"In the Login.java code, if a TGT with a small expiration date (e.g. 5 minutes) is passed in, the refresh date is set at a value less than the MIN_TIME_BEFORE_RELOGIN, which is a minute by default. As a result, the condition in line 153 evaluates to true, setting nextRefresh to now. Then right after, in line 176, it checks the nextRefresh againt now, and will jump to line 186 and just exit (without throwing an exception), exiting the refresh thread.

https://github.com/apache/zookeeper/blob/trunk/src/java/main/org/apache/zookeeper/Login.java#L186 

Possible Solution: changing line 176 to 
if (now <= nextRefresh) 

",[],Bug,ZOOKEEPER-2241,Minor,Reza Farivar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"In Login.java, a tgt with a small expiry time will break the code",2015-08-05T20:18:31.000+0000,[],1.0
Kevin Lee,"[<JIRA Component: name='jmx', id='12312451'>]",2015-08-04T16:33:11.000+0000,Kevin Lee,"The ""State"" property of LocalPeerBean in package org.apache.zookeeper.server.quorum is returning the incorrect value.  It is performing peer.getState() which is calling the getState() method on java.lang.Thread instead of getting the server state from org.apache.zookeeper.server.quorum.QuorumPeer.  The Javadoc within LocalPeerMXBean.java states that it should be returning the server state as well.  The fix is to call peer.getServerState() in the getState() method of LocalPeerBean instead of peer.getState().toString().  This will return the states defined in QuorumStats.Provider (unknown, leaderelection, leading, following, and observing).  This issue prevents JMX monitoring of the Zookeeper server state.","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2239,Major,Kevin Lee,Fixed,2015-10-26T07:15:51.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,JMX State from LocalPeerBean incorrect,2016-07-21T20:18:35.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.5.1', id='12326786'>]",6.0
,"[<JIRA Component: name='contrib-zkfuse', id='12312644'>, <JIRA Component: name='server', id='12312382'>, <JIRA Component: name='tests', id='12312427'>]",2015-07-23T15:46:35.000+0000,Bharat Singh,"I am facing a rename issue with Zkfuse. 
I am trying to test file atomic updates. After some iterations the file size becomes 0.
This is easily reproducible, just running the below script for ~5mins.

Setup:
zookeeper-3.4.6 with Zkfuse mounted, size of testFile = 1k

while [ 1 ]
do
        cp /root/testFile /mnt/zk/testFile.tmp
        mv /mnt/zk/testFile.tmp /mnt/zk/testFile
        ls -larth /mnt/zk/
        sleep 1
done

Zkfuse debug logs doesn't show any suspicious activity. Looks like zookeeper/zkfuse RENAME is not atomic.

But code browsing and log messages show that update have issues:
1) update is not able to pull data from zookeeper due to the _refCnt > 1,
so rename get an empty ZkfuseFile object.
2) I also hit an assert in update, 
assert(newFile == false || _isOnlyRegOpen());

Now I have suspicion on the refcount logic. Have any one faced similar issues or have used Zkfuse in production environment.

",[],Bug,ZOOKEEPER-2236,Major,Bharat Singh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper truncates file to 0bytes,2016-12-12T08:36:12.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",3.0
Flavio Paiva Junqueira,[],2015-07-17T16:30:19.000+0000,Flavio Paiva Junqueira,"Updating license files and notice.txt as needed. Here is a list of the jars we are currently bundling with the release artifact with the corresponding license:

# commons-cli-1.2.jar -- ASF
# javacc.jar -- BSD license
# jline-2.11.jar -- BSD license
# servlet-api-2.5-20081211.jar - CDDL
# jackson-core-asl-1.9.11.jar -- ALv2 
# jetty-6.1.26.jar -- ALv2       
# log4j-1.2.16.jar -- ALv2       
# jackson-mapper-asl-1.9.11.jar -- ALv2
# jetty-util-6.1.26.jar -- ALv2
# netty-3.7.0.Final.jar -- ALv2
# slf4j-log4j12-1.7.5.jar -- MIT ","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2235,Blocker,Flavio Paiva Junqueira,Fixed,2016-05-03T18:30:02.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,License update,2016-07-21T20:18:24.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",6.0
,[],2015-07-16T10:30:17.000+0000,Adam Milne-Smith,"This issue can be reproduced by creating a node with a new ACL during data tree serialization after ACL cache serialization. When restoring from this snapshot without the tranlog, the state will include a node with no corresponding ACL in the ACL cache. This node will then be impossible to operate on as it will cause a MarshallingError.

If the tranlog is played over a server in this erroneous state, it does appear to correct itself. This bug means that to reliably restore from a snapshot, you must also have backed up the subsequent tranlog covering at least the transactions that were partially written to the snapshot.

Issue first described here:
http://mail-archives.apache.org/mod_mbox/zookeeper-user/201507.mbox/%3C0LzCmv-1YtgSd0Dqb-014Qqf@mrelayeu.kundenserver.de%3E

It also appears possible for a snapshot to be missing a session yet contain an ephemeral node created by that session; fortunately ZooKeeperServer.loadData() should clean these ephemerals up.",[],Bug,ZOOKEEPER-2234,Minor,Adam Milne-Smith,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Snapshot serialization race condition can lead to partial transaction and inoperable data node,2015-07-16T10:30:17.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2015-07-03T07:55:25.000+0000,Gábor Lipták,"I think reporting [this stackoverflow question|http://stackoverflow.com/q/31163513/337621] to the ZooKeeper team is important.

org.apache.zookeeper.server.NIOServerCnxnFactory.configure(InetSocketAddress, int) has the following code:
{code:java}
@Override
public void configure(InetSocketAddress addr, int maxcc) throws IOException {
    configureSaslLogin();

    thread = new Thread(this, ""NIOServerCxn.Factory:"" + addr);
    thread.setDaemon(true);
    maxClientCnxns = maxcc;
    this.ss = ServerSocketChannel.open();
    ss.socket().setReuseAddress(true);
    LOG.info(""binding to port "" + addr);
    ss.socket().bind(addr);
    ss.configureBlocking(false);
    ss.register(selector, SelectionKey.OP_ACCEPT);
}
{code}

So the intention is to use SO_REUSEADDR. This does not work under linux (at least with the java version I use). The reason is that sun.nio.ch.ServerSocketChannelImpl.setOption(SocketOption<T>, T) used by ZooKeeper has this code:

{code:java}
public <T> ServerSocketChannel setOption(SocketOption<T> paramSocketOption, T paramT) throws IOException
{
    if (paramSocketOption == null)
        throw new NullPointerException();
    if (!(supportedOptions().contains(paramSocketOption)))
        throw new UnsupportedOperationException(""'"" + paramSocketOption + ""' not supported"");
    synchronized (this.stateLock) {
        if (!(isOpen()))
            throw new ClosedChannelException();
        if ((paramSocketOption == StandardSocketOptions.SO_REUSEADDR) && (Net.useExclusiveBind()))
        {
            this.isReuseAddress = ((Boolean)paramT).booleanValue();
        }
        else {
            Net.setSocketOption(this.fd, Net.UNSPEC, paramSocketOption, paramT);
        }
        return this;
    }
}
{code}

""Net.useExclusiveBind()"" seems to give back always false under linux no matter what value is set for [sun.net.useExclusiveBind|http://www.oracle.com/technetwork/java/javase/7u25-relnotes-1955741.html#napi-win] environment entry.

If someone wants to stop and start an embedded ZooKeeper server, it can result in BindExceptions. If there would be some workaround under Linux, it would be really good.

Also under windows the sun.net.useExclusiveBind env entry seems to be important to have the SO_REUSEADDR option. Maybe it would worth to document this network setting.

I have a [test code|http://pastebin.com/Hhyfiz3Y] which can reproduce the BindException under Linux.",[],Bug,ZOOKEEPER-2231,Major,Gábor Lipták,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ServerSocket opened by ZooKeeperServer cannot use SO_REUSEADDR under Linux,2015-07-10T15:59:46.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",3.0
Enis Soztutar,"[<JIRA Component: name='server', id='12312382'>]",2015-07-02T06:13:20.000+0000,Deepesh Reja,"ZooKeeper server becomes slow over time when native GSSAPI is used. The connection to the server starts taking upto 10 seconds.
This is happening with ZooKeeper-3.4.6 and is fairly reproducible.

Debug logs:
{noformat}
2015-07-02 00:58:49,318 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:NIOServerCnxnFactory@197] - Accepted socket connection from /<client_ip>:47942
2015-07-02 00:58:49,318 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperSaslServer@78] - serviceHostname is '<zookeeper-server>'
2015-07-02 00:58:49,318 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperSaslServer@79] - servicePrincipalName is 'zookeeper'
2015-07-02 00:58:49,318 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperSaslServer@80] - SASL mechanism(mech) is 'GSSAPI'
2015-07-02 00:58:49,324 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperSaslServer@106] - Added private credential to subject: [GSSCredential: 
zookeeper@<zookeeper-server> 1.2.840.113554.1.2.2 Accept [class sun.security.jgss.wrapper.GSSCredElement]]
2015-07-02 00:58:59,441 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@810] - Session establishment request from client /<client_ip>:47942 client's lastZxid is 0x0
2015-07-02 00:58:59,441 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@868] - Client attempting to establish new session at /<client_ip>:47942
2015-07-02 00:58:59,448 [myid:] - DEBUG [SyncThread:0:FinalRequestProcessor@88] - Processing request:: sessionid:0x14e486028785c81 type:createSession cxid:0x0 zxid:0x110e79 txntype:-10 reqpath:n/a
2015-07-02 00:58:59,448 [myid:] - DEBUG [SyncThread:0:FinalRequestProcessor@160] - sessionid:0x14e486028785c81 type:createSession cxid:0x0 zxid:0x110e79 txntype:-10 reqpath:n/a
2015-07-02 00:58:59,448 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@617] - Established session 0x14e486028785c81 with negotiated timeout 10000 for client /<client_ip>:47942
2015-07-02 00:58:59,452 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@949] - Responding to client SASL token.
2015-07-02 00:58:59,452 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@953] - Size of client SASL token: 706
2015-07-02 00:58:59,460 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@984] - Size of server SASL response: 161
2015-07-02 00:58:59,462 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@949] - Responding to client SASL token.
2015-07-02 00:58:59,462 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@953] - Size of client SASL token: 0
2015-07-02 00:58:59,462 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@984] - Size of server SASL response: 32
2015-07-02 00:58:59,463 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@949] - Responding to client SASL token.
2015-07-02 00:58:59,463 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@953] - Size of client SASL token: 32
2015-07-02 00:58:59,464 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:SaslServerCallbackHandler@118] - Successfully authenticated client: authenticationID=<user_principal>;  authorizationID=<user_principal>.
2015-07-02 00:58:59,464 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:ZooKeeperServer@964] - adding SASL authorization for authorizationID: <user_principal>
2015-07-02 00:58:59,465 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@494] - Processed session termination for sessionid: 0x14e486028785c81
2015-07-02 00:58:59,467 [myid:] - DEBUG [SyncThread:0:FinalRequestProcessor@88] - Processing request:: sessionid:0x14e486028785c81 type:closeSession cxid:0x1 zxid:0x110e7a txntype:-11 reqpath:n/a
2015-07-02 00:58:59,467 [myid:] - DEBUG [SyncThread:0:FinalRequestProcessor@160] - sessionid:0x14e486028785c81 type:closeSession cxid:0x1 zxid:0x110e7a txntype:-11 reqpath:n/a
2015-07-02 00:58:59,467 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:42405:NIOServerCnxn@1007] - Closed socket connection for client /<client_ip>:47942 which had sessionid 0x14e486028785c81
{noformat}

If you see, after adding the credentials to privateCredential set, it takes roughly 10 seconds to reach to session establishment request. From the code it looks like Subject.doAs() is taking a lot of time.

I connected it to jdb while it was waiting and got following stacktrace:
{noformat}
NIOServerCxn.Factory:0.0.0.0/0.0.0.0:58909:
  [1] java.util.HashMap$TreeNode.find (HashMap.java:1,865)
  [2] java.util.HashMap$TreeNode.find (HashMap.java:1,861)
  [3] java.util.HashMap$TreeNode.find (HashMap.java:1,861)
  [4] java.util.HashMap$TreeNode.find (HashMap.java:1,861)
  [5] java.util.HashMap$TreeNode.find (HashMap.java:1,861)
  [6] java.util.HashMap$TreeNode.find (HashMap.java:1,861)
  [7] java.util.HashMap$TreeNode.find (HashMap.java:1,861)
  [8] java.util.HashMap$TreeNode.putTreeVal (HashMap.java:1,981)
  [9] java.util.HashMap.putVal (HashMap.java:637)
  [10] java.util.HashMap.put (HashMap.java:611)
  [11] java.util.HashSet.add (HashSet.java:219)
  [12] javax.security.auth.Subject$ClassSet.populateSet (Subject.java:1,418)
  [13] javax.security.auth.Subject$ClassSet.<init> (Subject.java:1,372)
  [14] javax.security.auth.Subject.getPrivateCredentials (Subject.java:767)
  [15] sun.security.jgss.GSSUtil$1.run (GSSUtil.java:340)
  [16] sun.security.jgss.GSSUtil$1.run (GSSUtil.java:332)
  [17] java.security.AccessController.doPrivileged (native method)
  [18] sun.security.jgss.GSSUtil.searchSubject (GSSUtil.java:332)
  [19] sun.security.jgss.wrapper.NativeGSSFactory.getCredFromSubject (NativeGSSFactory.java:53)
  [20] sun.security.jgss.wrapper.NativeGSSFactory.getCredentialElement (NativeGSSFactory.java:116)
  [21] sun.security.jgss.GSSManagerImpl.getCredentialElement (GSSManagerImpl.java:193)
  [22] sun.security.jgss.GSSCredentialImpl.add (GSSCredentialImpl.java:427)
  [23] sun.security.jgss.GSSCredentialImpl.<init> (GSSCredentialImpl.java:62)
  [24] sun.security.jgss.GSSManagerImpl.createCredential (GSSManagerImpl.java:154)
  [25] com.sun.security.sasl.gsskerb.GssKrb5Server.<init> (GssKrb5Server.java:108)
  [26] com.sun.security.sasl.gsskerb.FactoryImpl.createSaslServer (FactoryImpl.java:85)
  [27] javax.security.sasl.Sasl.createSaslServer (Sasl.java:524)
  [28] org.apache.zookeeper.server.ZooKeeperSaslServer$1.run (ZooKeeperSaslServer.java:118)
  [29] org.apache.zookeeper.server.ZooKeeperSaslServer$1.run (ZooKeeperSaslServer.java:114)
  [30] java.security.AccessController.doPrivileged (native method)
  [31] javax.security.auth.Subject.doAs (Subject.java:422)
  [32] org.apache.zookeeper.server.ZooKeeperSaslServer.createSaslServer (ZooKeeperSaslServer.java:114)
  [33] org.apache.zookeeper.server.ZooKeeperSaslServer.<init> (ZooKeeperSaslServer.java:48)
  [34] org.apache.zookeeper.server.NIOServerCnxn.<init> (NIOServerCnxn.java:100)
  [35] org.apache.zookeeper.server.NIOServerCnxnFactory.createConnection (NIOServerCnxnFactory.java:161)
  [36] org.apache.zookeeper.server.NIOServerCnxnFactory.run (NIOServerCnxnFactory.java:202)
  [37] java.lang.Thread.run (Thread.java:745)
{noformat}

This doesn't happen when we use JGSS, I think because adding credential to privateCredential set for every connection is causing Subject.doAS() to take much longer time.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.2', id='12331981'>]",Bug,ZOOKEEPER-2230,Major,Deepesh Reja,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Connections fo ZooKeeper server becomes slow over time with native GSSAPI,2021-05-12T08:33:33.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.0', id='12316644'>]",13.0
Chris Nauroth,"[<JIRA Component: name='documentation', id='12312422'>]",2015-07-02T00:18:14.000+0000,Chris Nauroth,"The {{isro}}, {{gtmk}} and {{stmk}} commands are not covered in the four-letter word documentation.","[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2229,Major,Chris Nauroth,Fixed,2015-12-10T06:16:53.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Several four-letter words are undocumented.,2016-07-21T20:18:23.000+0000,[],5.0
,"[<JIRA Component: name='server', id='12312382'>]",2015-07-01T23:46:30.000+0000,Raúl Gutiérrez Segalés,"It seems like in FastLeaderElection#Messenger#WorkerReceiver the main loop should be left immediately after this path \[0\] is taken:

{code}
                                       if (!rqv.equals(curQV)) {
                                           LOG.info(""restarting leader election"");
                                           self.shuttingDownLE = true;
                                           self.getElectionAlg().shutdown();
                                       }
{code}

Instead, it keeps going which means the received message would still be applied and a new message might be send out. Should there be a break statement right after self.getElectionAlg().shutdown()?

Any ideas [~shralex]?

\[0\]: https://github.com/apache/zookeeper/blob/trunk/src/java/main/org/apache/zookeeper/server/quorum/FastLeaderElection.java#L300",[],Bug,ZOOKEEPER-2228,Major,Raúl Gutiérrez Segalés,Not A Problem,2015-07-03T01:40:02.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,WorkerReceiver's main loop  (in FastLeaderElection's) should break loop upon restart,2015-07-03T01:40:02.000+0000,[],3.0
Chris Nauroth,"[<JIRA Component: name='server', id='12312382'>]",2015-07-01T19:22:10.000+0000,Chris Nauroth,"When the server handles the {{stmk}} four-letter word, it attempts to read an 8-byte Java {{long}} from the request as the trace mask argument.  The read fails, because the destination buffer's capacity is only 4 bytes.","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2227,Major,Chris Nauroth,Fixed,2015-11-08T22:31:16.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,stmk four-letter word fails execution at server while reading trace mask argument.,2016-07-21T20:18:18.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",7.0
,[],2015-07-01T18:22:31.000+0000,David Capwell,"I have the following code (in curator):

{code}
int id = extractId(client.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT_SEQUENTIAL).forPath(prefix,
data));
{code}

and

{code}
client.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(path(id),
data);
{code}

The first part joins our cluster and gets a id from zookeeper.  The
second call will create a znode that looks like a znode above.

The reason I do this is that I would like for ops to be able to define
the ids when they want and not always have to (other code will
""setData"" one of the paths defined above, leaving out since thats not
having issues).

I created a test case and the error thrown was not what I was expecing: Node Exists

Here is the test:

create 4 PERSISTENT znodes with ids 1, 2, 3, 4
create 1 PERSISTENT_SEQUENTIAL znode (change id = 4, so conflicts with above)

Here is the error I saw

INFO 2015-07-01 10:46:46,349 [ProcessThread(sid:0 cport:-1):]
[PrepRequestProcessor] [line 627] Got user-level KeeperException when
processing sessionid:0x14e4aba4d490000 type:create cxid:0x25 zxid:0xe
txntype:-1 reqpath:n/a Error
Path:/test/MembershipTest/replaceFourRegisterOne/member-0000000004
Error:KeeperErrorCode = NodeExists for
/test/MembershipTest/replaceFourRegisterOne/member-0000000004

org.apache.zookeeper.KeeperException$NodeExistsException:
KeeperErrorCode = NodeExists for
/test/MembershipTest/replaceFourRegisterOne/member-
...
Caused by: org.apache.zookeeper.KeeperException$NodeExistsException:
KeeperErrorCode = NodeExists for
/test/MembershipTest/replaceFourRegisterOne/member-
at org.apache.zookeeper.KeeperException.create(KeeperException.java:119)
at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)
at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)
at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)
at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)
at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)
at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)
at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)


When using sequential nodes, its unexpected that they can fail because a node already exists.",[],Bug,ZOOKEEPER-2226,Major,David Capwell,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Mixing sequential and non-sequential can throw NodeExists for sequential nodes,2019-02-23T08:53:02.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",2.0
Mohammad Arshad,"[<JIRA Component: name='java client', id='12312381'>]",2015-06-30T10:00:04.000+0000,Mohammad Arshad,"Four letter command hangs when network is slow or network goes down in between the operation, and the application also, which calling this four letter command,  hangs.","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2224,Minor,Mohammad Arshad,Fixed,2015-07-06T15:50:47.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Four letter command hangs when network is slow,2015-08-26T03:40:37.000+0000,[],9.0
Surendra Singh Lilhore,"[<JIRA Component: name='server', id='12312382'>]",2015-06-25T14:07:10.000+0000,Surendra Singh Lilhore,"Currently JettyAdminServer starting on ""0.0.0.0"" IP. ""0.0.0.0"" means ""all IP addresses on the local machine"". So, if your webserver machine has two ip addresses, 192.168.1.1(private) and 10.1.2.1(public), and you allow a webserver daemon like apache to listen on 0.0.0.0, it will be reachable at both of those IPs.

This is security issue. webserver should be accessible from only configured IP","[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2221,Major,Surendra Singh Lilhore,Fixed,2015-06-30T18:51:36.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper JettyAdminServer server should start on configured IP.,2015-07-28T06:48:34.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",7.0
,"[<JIRA Component: name='c client', id='12312380'>]",2015-06-24T17:57:52.000+0000,rupa mogali,"I am trying to test SSL connectivity between client and server following the instructions in the following page:
https://cwiki.apache.org/confluence/display/ZOOKEEPER/ZooKeeper+SSL+User+Guide
But, I get the following when trying to connect to server from client..
2015-06-24 12:14:36,589 [myid:] - INFO [main:ZooKeeper@709] - Initiating client connection, connectString=localhost:2282 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@f2a0b8e
Exception in thread ""main"" java.io.IOException: Couldn't instantiate org.apache.zookeeper.ClientCnxnSocketNetty
Can you tell me what I am doing wrong here?
Very new to Zookeeper. 
Thanks!
Reply",[],Bug,ZOOKEEPER-2220,Major,rupa mogali,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Couldn't instantiate org.apache.zookeeper.ClientCnxnSocketNetty,2017-10-10T05:50:48.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
,[],2015-06-24T06:39:28.000+0000,Zhihai Xu,"ZooKeeper server should better handle SessionMovedException.
We hit the SessionMovedException. the following is the reason for the SessionMovedException we find:
1. ZK client tried to connect to Leader L. Network was very slow, so before leader processed the request, client disconnected.
2. Client then re-connected to Follower F reusing the same session ID. It was successful.
3. The request in step 1 went into leader. Leader processed it and invalidated the connection created in step 2. But client didn't know the connection it used is invalidated.
4. Client got SessionMovedException when it used the connection invalidated by leader for any ZooKeeper operation.

The following are logs: c045dkh is the Leader, c470udy is the Follower and the sessionID is 0x14be28f50f4419d.
1. ZK client try to initiate session to Leader at 015-03-16 10:59:40,735 and timeout after 10/3 seconds.
logs from ZK client 
{code}
2015-03-16 10:59:40,078 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 6670ms for sessionid 0x14be28f50f4419d, closing socket connection and attempting reconnect
015-03-16 10:59:40,735 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server c045dkh/?.?.?.67:2181. Will not attempt to authenticate using SASL (unknown error)
2015-03-16 10:59:40,735 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to c045dkh/?.?.?.67:2181, initiating session
2015-03-16 10:59:44,071 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 3336ms for sessionid 0x14be28f50f4419d, closing socket connection and attempting reconnect
{code}

2. ZK client initiated session to Follower successfully at 2015-03-16 10:59:44,688
logs from ZK client
{code}
2015-03-16 10:59:44,673 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server c470udy/?.?.?.65:2181. Will not attempt to authenticate using SASL (unknown error)
2015-03-16 10:59:44,673 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to c470udy/?.?.?.65:2181, initiating session
2015-03-16 10:59:44,688 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server c470udy/?.?.?.65:2181, sessionid = 0x14be28f50f4419d, negotiated timeout = 10000
{code}
logs from ZK Follower server
{code}
2015-03-16 10:59:44,673 INFO org.apache.zookeeper.server.NIOServerCnxnFactory: Accepted socket connection from /?.?.?.65:42777
2015-03-16 10:59:44,674 INFO org.apache.zookeeper.server.ZooKeeperServer: Client attempting to renew session 0x14be28f50f4419d at /?.?.?.65:42777
2015-03-16 10:59:44,674 INFO org.apache.zookeeper.server.quorum.Learner: Revalidating client: 0x14be28f50f4419d
2015-03-16 10:59:44,675 INFO org.apache.zookeeper.server.ZooKeeperServer: Established session 0x14be28f50f4419d with negotiated timeout 10000 for client /?.?.?.65:42777
{code}

3. At 2015-03-16 10:59:45,668, Leader processed the delayed request which is sent by Client at 2015-03-16 10:59:40,735, right after session was established, it close the socket connection/session because client was already disconnected due to timeout.
logs from ZK Leader server
{code}
2015-03-16 10:59:45,668 INFO org.apache.zookeeper.server.ZooKeeperServer: Client attempting to renew session 0x14be28f50f4419d at /?.?.?.65:50271
2015-03-16 10:59:45,668 INFO org.apache.zookeeper.server.ZooKeeperServer: Established session 0x14be28f50f4419d with negotiated timeout 10000 for client /?.?.?.65:50271
2015-03-16 10:59:45,670 WARN org.apache.zookeeper.server.NIOServerCnxn: Exception causing close of session 0x14be28f50f4419d due to java.io.IOException: Broken pipe
2015-03-16 10:59:45,671 INFO org.apache.zookeeper.server.NIOServerCnxn: Closed socket connection for client /?.?.?.65:50271 which had sessionid 0x14be28f50f4419d
{code}

4. Client got SessionMovedException at 2015-03-16 10:59:45,693
logs from ZK Leader server
{code}
2015-03-16 10:59:45,693 INFO org.apache.zookeeper.server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x14be28f50f4419d type:multi cxid:0x86e3 zxid:0x1c002a4e53 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:null Error:KeeperErrorCode = Session moved
2015-03-16 10:59:45,695 INFO org.apache.zookeeper.server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x14be28f50f4419d type:multi cxid:0x86e5 zxid:0x1c002a4e56 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:null Error:KeeperErrorCode = Session moved
2015-03-16 10:59:45,700 INFO org.apache.zookeeper.server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x14be28f50f4419d type:multi cxid:0x86e7 zxid:0x1c002a4e57 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:null Error:KeeperErrorCode = Session moved
{code}

5. At 2015-03-16 10:59:45,710, we close the session 0x14be28f50f4419d but the socket connection between ZK client and ZK Follower is closed at 2015-03-16 10:59:45,715 after session termination.
logs from ZK Leader server:
{code}
2015-03-16 10:59:45,710 INFO org.apache.zookeeper.server.PrepRequestProcessor: Processed session termination for sessionid: 0x14be28f50f4419d
{code}
logs from ZK Follower server:
{code}
2015-03-16 10:59:45,715 INFO org.apache.zookeeper.server.NIOServerCnxn: Closed socket connection for client /?.?.?.65:42777 which had sessionid 0x14be28f50f4419d
{code}

It looks like Zk client is out-of-sync with ZK server.
My question is how ZK client can recover from this error. It looks like the ZK Client won't be disconnected from Follower until session is closed and any ZooKeeper operation will fail with SessionMovedException before session is closed.
Also since ZK Leader already closed the socket connection/session to ZK Client at step 3, why it still reject the ZooKeeper operation from client with SessionMovedException. Will it be better to endorse the session/connection between ZK client and ZK Follower? This seems like a bug to me. ",[],Bug,ZOOKEEPER-2219,Major,Zhihai Xu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper server should better handle SessionMovedException.,2018-11-22T02:09:49.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",6.0
,[],2015-06-12T12:09:49.000+0000,Surendra Singh Lilhore, echo dump | netcat <IP> <port>,[],Bug,ZOOKEEPER-2215,Major,Surendra Singh Lilhore,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Four letter command don't have kerberos authentication ,2015-06-13T05:34:00.000+0000,[],2.0
Hongchao Deng,"[<JIRA Component: name='server', id='12312382'>]",2015-06-10T15:29:23.000+0000,Brian Brazil,"See https://github.com/samuel/go-zookeeper/issues/62

I've reproduced this on 3.4.5 with the code:
        c, _, _ := zk.Connect([]string{""127.0.0.1""}, time.Second)
        c.Set("""", []byte{}, 0)

This crashes a local zookeeper 3.4.5 server:
2015-06-10 16:21:10,862 [myid:] - ERROR [SyncThread:0:SyncRequestProcessor@151] - Severe unrecoverable error, exiting  
java.lang.IllegalArgumentException: Invalid path
        at org.apache.zookeeper.common.PathTrie.findMaxPrefix(PathTrie.java:259)
        at org.apache.zookeeper.server.DataTree.getMaxPrefixWithQuota(DataTree.java:634)
        at org.apache.zookeeper.server.DataTree.setData(DataTree.java:616)
        at org.apache.zookeeper.server.DataTree.processTxn(DataTree.java:807)
        at org.apache.zookeeper.server.ZKDatabase.processTxn(ZKDatabase.java:329)
        at org.apache.zookeeper.server.ZooKeeperServer.processTxn(ZooKeeperServer.java:965)
        at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:116)
        at org.apache.zookeeper.server.SyncRequestProcessor.flush(SyncRequestProcessor.java:167)
        at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:101)

On restart the zookeeper server crashes out:
2015-06-10 16:22:21,352 [myid:] - ERROR [main:ZooKeeperServerMain@54] - Invalid arguments, exiting abnormally
java.lang.IllegalArgumentException: Invalid path
        at org.apache.zookeeper.common.PathTrie.findMaxPrefix(PathTrie.java:259)
        at org.apache.zookeeper.server.DataTree.getMaxPrefixWithQuota(DataTree.java:634)
        at org.apache.zookeeper.server.DataTree.setData(DataTree.java:616)
        at org.apache.zookeeper.server.DataTree.processTxn(DataTree.java:807)
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.processTransaction(FileTxnSnapLog.java:198)
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:151)
        at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:223)
        at org.apache.zookeeper.server.ZooKeeperServer.loadData(ZooKeeperServer.java:250)
        at org.apache.zookeeper.server.ZooKeeperServer.startdata(ZooKeeperServer.java:377)
        at org.apache.zookeeper.server.NIOServerCnxnFactory.startup(NIOServerCnxnFactory.java:122)
        at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:112)
        at org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:86)
        at org.apache.zookeeper.server.ZooKeeperServerMain.main(ZooKeeperServerMain.java:52)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:116)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2213,Blocker,Brian Brazil,Fixed,2015-06-11T18:16:00.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Empty path in Set crashes server and prevents restart,2015-06-29T17:43:47.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",7.0
Akihiro Suda,"[<JIRA Component: name='quorum', id='12312379'>]",2015-06-10T06:45:23.000+0000,Akihiro Suda,"When a joiner is listed as an observer in an initial config,
the joiner should become a non-voting follower (not an observer) until reconfig is triggered. [(Link)|http://zookeeper.apache.org/doc/trunk/zookeeperReconfig.html#sc_reconfig_general]

I found a distributed race-condition situation where an observer keeps being an observer and cannot become a non-voting follower.

This race condition happens when an observer receives an UPTODATE Quorum Packet from the leader:2888/tcp *after* receiving a Notification FLE Packet of which n.config version is larger than the observer's one from leader:3888/tcp.

h4. Detail
 * Problem: An observer cannot become a non-voting follower
 * Cause: Cannot restart FLE
 * Cause: In {{QuorumPeer.run()}}, cannot shutdown {{Observer}} [(Link)|https://github.com/apache/zookeeper/blob/98a3cabfa279833b81908d72f1c10ee9f598a045/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeer.java#L1014]
 * Cause: In {{QuorumPeer.run()}}, cannot return from {{Observer.observeLeader()}} [(Link)|https://github.com/apache/zookeeper/blob/98a3cabfa279833b81908d72f1c10ee9f598a045/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeer.java#L1010]
 * Cause: In {{Observer.observeLeader()}}, {{Learner.syncWithLeader()}} does not throw an exception of ""changes proposed in reconfig"" [(Link)|https://github.com/apache/zookeeper/blob/98a3cabfa279833b81908d72f1c10ee9f598a045/src/java/main/org/apache/zookeeper/server/quorum/Observer.java#L79]
 * Cause: In {{switch(qp.getType()) case UPTODATE}} of {{Learner.syncWithLeader()}} [(Link)|https://github.com/apache/zookeeper/blob/98a3cabfa279833b81908d72f1c10ee9f598a045/src/java/main/org/apache/zookeeper/server/quorum/Learner.java#L492-507], {{QuorumPeer.processReconfig()}} [(Link)|https://github.com/apache/zookeeper/blob/98a3cabfa279833b81908d72f1c10ee9f598a045/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeer.java#L1644]returns false with a log message like [""2 setQuorumVerifier called with known or old config 4294967296. Current version: 4294967296""|https://github.com/osrg/earthquake/blob/v0.1/example/zk-found-bug.ether/example-output/3.REPRODUCED/zk2.log]. [(Link)|https://github.com/apache/zookeeper/blob/98a3cabfa279833b81908d72f1c10ee9f598a045/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeer.java#L1369]
,
 * Cause: The observer have already received a Notification Packet({{n.config.version=4294967296}}) and invoked {{QuorumPeer.processReconfig()}} [(Link)|https://github.com/apache/zookeeper/blob/98a3cabfa279833b81908d72f1c10ee9f598a045/src/java/main/org/apache/zookeeper/server/quorum/FastLeaderElection.java#L291-304]
   
h4. How I found this bug
I found this bug using [Earthquake|http://osrg.github.io/earthquake/], our open-source dynamic model checker for real implementations of distributed systems.

Earthquakes permutes C/Java function calls, Ethernet packets, and injected fault events in various orders so as to find implementation-level bugs of the distributed system.

When Earthquake finds a bug, Earthquake automatically records [the event history|https://github.com/osrg/earthquake/blob/v0.1/example/zk-found-bug.ether/example-output/3.REPRODUCED/json] and helps the user to analyze which permutation of events triggers the bug.

I analyzed Earthquake's event histories and found that the bug is triggered when an observer receives an UPTODATE *after* receiving a specific kind of FLE packet.

h4. How to reproduce this bug
You can also easily reproduce the bug using Earthquake.
I made a Docker container [osrg/earthquake-zookeeper-2212|https://registry.hub.docker.com/u/osrg/earthquake-zookeeper-2212/] on Docker hub:
{code}
    host$ sudo modprobe openvswitch
    host$ docker run --privileged -t -i --rm osrg/earthquake-zookeeper-2212
    guest$ ./000-prepare.sh
    [INFO] Starting Earthquake Ethernet Switch
    [INFO] Starting Earthquake Orchestrator
    [INFO] Starting Earthquake Ethernet Inspector
    [IMPORTANT] Please kill the processes (switch=1234, orchestrator=1235, and inspector=1236) after you finished all of the experiments
    [IMPORTANT] Please continue to 100-run-experiment.sh..
    guest$ ./100-run-experiment.sh
    [IMPORTANT] THE BUG WAS REPRODUCED!
    guest$ kill -9 1234 1235 1236
{code}

Note that {{--privileged}} is needed, as this container uses Docker-in-Docker.

For further information about reproducing this bug, please refer to https://github.com/osrg/earthquake/blob/v0.1/example/zk-found-bug.ether
","[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2212,Critical,Akihiro Suda,Fixed,2015-06-15T23:08:30.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,distributed race condition related to QV version,2015-08-14T04:12:35.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",8.0
Mohammad Arshad,"[<JIRA Component: name='scripts', id='12312384'>]",2015-06-09T20:45:57.000+0000,Wesley Chow,"PurgeTxnLog does not work when snapshots and transaction logs are at different file paths. The argument handling is buggy and only works when both snap and datalog dirs are given, and datalog dir contains both logs and snaps (snap is ignored).","[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2211,Major,Wesley Chow,Fixed,2015-12-04T04:20:38.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,PurgeTxnLog does not correctly purge when snapshots and logs are at different locations,2016-07-21T20:18:40.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",8.0
Michi Mutsuzaki,"[<JIRA Component: name='c client', id='12312380'>]",2015-06-09T04:47:12.000+0000,Michi Mutsuzaki,"{noformat}
src/zookeeper.c:286:9: warning: implicit declaration of function 'clock_gettime' is invalid in C99 [-Wimplicit-function-declaration]
  ret = clock_gettime(CLOCK_MONOTONIC, &ts);
        ^
src/zookeeper.c:286:23: error: use of undeclared identifier 'CLOCK_MONOTONIC'
  ret = clock_gettime(CLOCK_MONOTONIC, &ts);
{noformat}","[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2210,Major,Michi Mutsuzaki,Fixed,2015-06-22T00:20:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,clock_gettime is not available in os x,2015-06-22T10:46:29.000+0000,[],6.0
,"[<JIRA Component: name='leaderElection', id='12312378'>]",2015-06-03T03:36:16.000+0000,Akihiro Suda,"This sequence leads the ensemble to a split-brain state:
 * Start server 1 (config=1:participant, 2:participant, 3:participant)
 * Start server 2 (config=1:participant, 2:participant, 3:participant)
 * 1 and 2 believe 2 is the leader
 * Start server 3 (config=1:observer, 2:observer, 3:participant)
 * 3 believes 3 is the leader, although 1 and 2 still believe 2 is the leader

Such a split-brain ensemble is very unstable.
Znodes can be lost easily:
 * Create some znodes on 2
 * Restart 1 and 2
 * 1, 2 and 3 can think 3 is the leader
 * znodes created on 2 are lost, as 1 and 2 sync with 3


I consider this behavior as a bug and that ZK should fail gracefully if a participant is listed as an observer in the config.

In current implementation, ZK cannot detect such an invalid config, as FastLeaderElection.sendNotification() sends notifications to only voting members and hence there is no message from observers(1 and 2) to the new voter (3).
I think FastLeaderElection.sendNotification() should send notifications to all the members and FastLeaderElection.Messenger.WorkerReceiver.run() should verify acks.

Any thoughts?",[],Bug,ZOOKEEPER-2203,Major,Akihiro Suda,Not A Problem,2015-06-16T18:17:18.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,multiple leaders can be elected when configs conflict,2015-06-16T18:17:18.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",2.0
Raúl Gutiérrez Segalés,[],2015-06-02T21:44:45.000+0000,Raúl Gutiérrez Segalés,"While adding support for reconfig() in Kazoo (https://github.com/python-zk/kazoo/pull/333) I found that the cluster can be crashed if you add an observer whose election port isn't reachable (i.e.: packets for that destination are dropped, not rejected). This will raise a SocketTimeoutException which will bring down the PrepRequestProcessor:

{code}
2015-06-02 14:37:16,473 [myid:3] - WARN  [ProcessThread(sid:3 cport:-1)::QuorumCnxManager@384] - Cannot open channel to 100 at election address /8.8.8.8:38703
java.net.SocketTimeoutException: connect timed out
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:345)
        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:589)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:369)
        at org.apache.zookeeper.server.quorum.QuorumPeer.connectNewPeers(QuorumPeer.java:1288)
        at org.apache.zookeeper.server.quorum.QuorumPeer.setLastSeenQuorumVerifier(QuorumPeer.java:1315)
        at org.apache.zookeeper.server.quorum.Leader.propose(Leader.java:1056)
        at org.apache.zookeeper.server.quorum.ProposalRequestProcessor.processRequest(ProposalRequestProcessor.java:78)
        at org.apache.zookeeper.server.PrepRequestProcessor.pRequest(PrepRequestProcessor.java:877)
        at org.apache.zookeeper.server.PrepRequestProcessor.run(PrepRequestProcessor.java:143)
{code}

A simple repro can be obtained by using the code in the referenced pull request above and using 8.8.8.8:3888 (for example) instead of a free (but closed) port in the loopback. 

I think that adding an Observer (or a Participant) that isn't currently reachable is a valid use case (i.e.: you are provisioning the machine and it's not currently needed) so I think we could handle this with lower connect timeouts, not sure. 
","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-2202,Major,Raúl Gutiérrez Segalés,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Cluster crashes when reconfig adds an unreachable observer,2022-02-03T08:36:23.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.6.0', id='12326518'>]",7.0
Donny Nadolny,[],2015-06-02T01:30:25.000+0000,Donny Nadolny,"{{DataTree.serializeNode}} synchronizes on the {{DataNode}} it is about to serialize then writes it out via {{OutputArchive.writeRecord}}, potentially to a network connection. Under default linux TCP settings, a network connection where the other side completely disappears will hang (blocking on the {{java.net.SocketOutputStream.socketWrite0}} call) for over 15 minutes. During this time, any attempt to create/delete/modify the {{DataNode}} will cause the leader to hang at the beginning of the request processor chain:

{noformat}
""ProcessThread(sid:5 cport:-1):"" prio=10 tid=0x00000000026f1800 nid=0x379c waiting for monitor entry [0x00007fe6c2a8c000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.zookeeper.server.PrepRequestProcessor.getRecordForPath(PrepRequestProcessor.java:163)
        - waiting to lock <0x00000000d4cd9e28> (a org.apache.zookeeper.server.DataNode)
        - locked <0x00000000d2ef81d0> (a java.util.ArrayList)
        at org.apache.zookeeper.server.PrepRequestProcessor.pRequest2Txn(PrepRequestProcessor.java:345)
        at org.apache.zookeeper.server.PrepRequestProcessor.pRequest(PrepRequestProcessor.java:534)
        at org.apache.zookeeper.server.PrepRequestProcessor.run(PrepRequestProcessor.java:131)
{noformat}

Additionally, any attempt to send a snapshot to a follower or to disk will hang.

Because the ping packets are sent by another thread which is unaffected, followers never time out and become leader, even though the cluster will make no progress until either the leader is killed or the TCP connection times out. This isn't exactly a deadlock since it will resolve itself eventually, but as mentioned above this will take > 15 minutes with the default TCP retry settings in linux.

A simple solution to this is: in {{DataTree.serializeNode}} we can take a copy of the contents of the {{DataNode}} (as is done with its children) in the synchronized block, then call {{writeRecord}} with the copy of the {{DataNode}} outside of the original {{DataNode}} synchronized block.","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2201,Critical,Donny Nadolny,Fixed,2015-06-06T16:54:09.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Network issues can cause cluster to hang due to near-deadlock,2016-07-21T20:18:45.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",10.0
,"[<JIRA Component: name='c client', id='12312380'>]",2015-06-01T19:02:52.000+0000,KARA VAN HORN,"We are using Perl Net::ZooKeeper (0.38) and Net::ZooKeeper::Lock (0.03) libraries.  Deadlock appears to occur at the end during lock cleanup activity.  Here is a stack dump (sensitive names changed):

Thread 2 (Thread 0x2ac6fbfa3940 (LWP 13292)):
#0  0x00002ac6f5aed654 in __lll_lock_wait () from /lib64/libpthread.so.0
#1  0x00002ac6f5aeb47b in pthread_cond_signal@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#2  0x00002ac6f835539c in _zk_watcher (handle=<value optimized out>, type=2, state=3, path=<value optimized out>, context=0x33f3ce0) at ZooKeeper.xs:179
#3  0x00002ac6f856d942 in do_foreach_watcher (zh=0x33e4fb0, type=2, state=3, path=0x33f3f50 ""/lock/cmts/cisco_device1.net-0001851215"", list=0x33ed290)
    at /home/myhome/rpm/BUILD/zookeeper-3.4.6/src/c/src/zk_hashtable.c:279
#4  deliverWatchers (zh=0x33e4fb0, type=2, state=3, path=0x33f3f50 ""/lock/cmts/cisco_device1.net-0001851215"", list=0x33ed290)
    at /home/myhome/rpm/BUILD/zookeeper-3.4.6/src/c/src/zk_hashtable.c:321
#5  0x00002ac6f8564966 in process_completions (zh=0x33e4fb0) at /home/myhome/rpm/BUILD/zookeeper-3.4.6/src/c/src/zookeeper.c:2114
#6  0x00002ac6f856e101 in do_completion (v=<value optimized out>) at /home/myhome/rpm/BUILD/zookeeper-3.4.6/src/c/src/mt_adaptor.c:466
#7  0x00002ac6f5ae683d in start_thread (arg=<value optimized out>) at pthread_create.c:301
#8  0x00002ac6f5dd1fcd in clone () from /lib64/libc.so.6

Thread 1 (Thread 0x2ac6f6056af0 (LWP 12972)):
#0  0x00002ac6f5ae7c65 in pthread_join (threadid=47034119371072, thread_return=0x0) at pthread_join.c:89
#1  0x00002ac6f856e7de in adaptor_finish (zh=0x33e4fb0) at /home/myhome/rpm/BUILD/zookeeper-3.4.6/src/c/src/mt_adaptor.c:293
#2  0x00002ac6f8566cdc in zookeeper_close (zh=0x33e4fb0) at /home/myhome/rpm/BUILD/zookeeper-3.4.6/src/c/src/zookeeper.c:2536
#3  0x00002ac6f8357222 in XS_Net__ZooKeeper_DESTROY (my_perl=0x20df010, cv=<value optimized out>) at ZooKeeper.xs:885
#4  0x00002ac6f4b38af6 in Perl_pp_entersub () from /usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE/libperl.so
#5  0x00002ac6f4adb8d7 in ?? () from /usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE/libperl.so
#6  0x00002ac6f4adf720 in Perl_call_sv () from /usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE/libperl.so
#7  0x00002ac6f4b3d3c6 in Perl_sv_clear () from /usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE/libperl.so
#8  0x00002ac6f4b3db70 in Perl_sv_free () from /usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE/libperl.so
#9  0x00002ac6f4b6025c in Perl_free_tmps () from /usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE/libperl.so
#10 0x00002ac6f4adf78a in Perl_call_sv () from /usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE/libperl.so
#11 0x00002ac6f4b3d3c6 in Perl_sv_clear () from /usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE/libperl.so
#12 0x00002ac6f4b3db70 in Perl_sv_free () from /usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE/libperl.so
#13 0x00002ac6f4b3b0e5 in ?? () from /usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE/libperl.so
#14 0x00002ac6f4b3b141 in Perl_sv_clean_objs () from /usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE/libperl.so
#15 0x00002ac6f4ae185e in perl_destruct () from /usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE/libperl.so
#16 0x0000000000401773 in main ()

There are about 4 out of 10,000 processes that end up in deadlock, and according to our web searches, the only reason pthread_cond_signal would lock is due to it waiting on an already destroyed condition.
",[],Bug,ZOOKEEPER-2200,Major,KARA VAN HORN,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Perl ZooKeeper locks up during heavy load,2016-03-30T16:42:22.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",2.0
Michi Mutsuzaki,"[<JIRA Component: name='c client', id='12312380'>]",2015-05-31T23:28:24.000+0000,Michi Mutsuzaki,"Windows doesn't have unistd.h.

https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper-trunk-WinVS2008/","[<JIRA Version: name='3.5.1', id='12326786'>]",Bug,ZOOKEEPER-2199,Major,Michi Mutsuzaki,Duplicate,2015-06-01T06:36:58.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Don't include unistd.h in windows,2019-12-19T23:02:00.000+0000,[],2.0
Chris Nauroth,"[<JIRA Component: name='build', id='12312383'>]",2015-05-30T21:22:23.000+0000,Chris Nauroth,"Some systems are seeing test failures under concurrent execution.  This issue proposes to change the default {{test.junit.threads}} to 1 so that those environments continue to get consistent test runs.  Jenkins and individual developer environments can set multiple threads with a command line argument, so most environments will still get the benefit of faster test runs.","[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2198,Minor,Chris Nauroth,Fixed,2015-05-31T09:23:51.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Set default test.junit.threads to 1.,2015-05-31T18:55:56.000+0000,[],4.0
Michi Mutsuzaki,[],2015-05-30T20:25:05.000+0000,Michi Mutsuzaki,"src/java/main/org/apache/zookeeper/server/FinalRequestProcessor.java:134: error: unmappable character for encoding ASCII
    [javac]         // was not being queued ??? ZOOKEEPER-558) properly. This happens, for example,","[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2197,Minor,Michi Mutsuzaki,Fixed,2015-06-02T05:25:02.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,non-ascii character in FinalRequestProcessor.java,2015-06-02T10:57:25.000+0000,[],7.0
,[],2015-05-22T21:29:11.000+0000,Rob Eden,"The link for ""Web Access"" on the [SVN site page|https://zookeeper.apache.org/svn.html] is broken.

This link:

http://svn.apache.org/viewcvs.cgi/zookeeper/

Should be replaced with this:

https://svn.apache.org/viewvc/zookeeper

(in ""site/trunk/content/svn.textile"")",[],Bug,ZOOKEEPER-2196,Major,Rob Eden,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Web Access link broken,2015-05-22T21:29:11.000+0000,[],1.0
Biju Nair,"[<JIRA Component: name='quorum', id='12312379'>]",2015-05-21T07:27:55.000+0000,David Fan,"Config fsync.warningthresholdms in zoo.cfg does not work.
I find QuorumPeerConfig.parseProperties give fsync.warningthresholdms a prefix like ""zookeeper.fsync.warningthresholdms"".  But in class FileTxnLog where fsync.warningthresholdms is used, code is :Long.getLong(""fsync.warningthresholdms"", 1000),without prefix ""zookeeper."", therefore can not get fsync.warningthresholdms's value.

I wonder the speed of fsync, need this config to see whether the speed is good enough.","[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2195,Trivial,David Fan,Fixed,2016-03-20T18:37:52.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,fsync.warningthresholdms in zoo.cfg not working,2016-07-21T20:18:24.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",5.0
Yasuhito Fukuda,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='server', id='12312382'>]",2015-05-20T09:38:23.000+0000,Yasuhito Fukuda,"Even if reconfig parameter is wrong, it was confirmed to complete.
refer to the following.

- Ensemble consists of four nodes

{noformat}
[zk: vm-101:2181(CONNECTED) 0] config
server.1=192.168.100.101:2888:3888:participant
server.2=192.168.100.102:2888:3888:participant
server.3=192.168.100.103:2888:3888:participant
server.4=192.168.100.104:2888:3888:participant
version=100000000
{noformat}

- add node by reconfig command

{noformat}
[zk: vm-101:2181(CONNECTED) 9] reconfig -add server.5=192.168.100.104:2888:3888:participant;0.0.0.0:2181
Committed new configuration:
server.1=192.168.100.101:2888:3888:participant
server.2=192.168.100.102:2888:3888:participant
server.3=192.168.100.103:2888:3888:participant
server.4=192.168.100.104:2888:3888:participant
server.5=192.168.100.104:2888:3888:participant;0.0.0.0:2181
version=300000007
{noformat}

server.4 and server.5 of the IP address is a duplicate.

In this state, reader election will not work properly.
Besides, it is assumed an ensemble will be undesirable state.
I think that need a parameter validation when reconfig.","[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2193,Major,Yasuhito Fukuda,Fixed,2015-06-27T00:06:00.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,reconfig command completes even if parameter is wrong obviously,2015-07-28T06:49:10.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",9.0
Hongchao Deng,"[<JIRA Component: name='tests', id='12312427'>]",2015-05-14T02:59:35.000+0000,Hongchao Deng,,"[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2190,Major,Hongchao Deng,Fixed,2015-05-14T19:57:27.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"In StandaloneDisabledTest, testReconfig() shouldn't take leaving servers as joining servers",2015-05-16T04:20:49.000+0000,[],6.0
,"[<JIRA Component: name='leaderElection', id='12312378'>]",2015-05-13T06:20:59.000+0000,Akihiro Suda,"This was original JIRA of ZOOKEEPER-2203. For project management reason, all the issues and related discussion are moved to ZOOKEEPER-2203. This JIRA is linked to ZOOKEEPER-2098.

==============

This sequence leads the ensemble to a split-brain state:
 * Start server 1 (config=1:participant, 2:participant, 3:participant)
 * Start server 2 (config=1:participant, 2:participant, 3:participant)
 * 1 and 2 believe 2 is the leader
 * Start server 3 (config=1:observer, 2:observer, 3:participant)
 * 3 believes 3 is the leader, although 1 and 2 still believe 2 is the leader

Such a split-brain ensemble is very unstable.
Znodes can be lost easily:
 * Create some znodes on 2
 * Restart 1 and 2
 * 1, 2 and 3 can think 3 is the leader
 * znodes created on 2 are lost, as 1 and 2 sync with 3


I consider this behavior as a bug and that ZK should fail gracefully if a participant is listed as an observer in the config.

In current implementation, ZK cannot detect such an invalid config, as FastLeaderElection.sendNotification() sends notifications to only voting members and hence there is no message from observers(1 and 2) to the new voter (3).
I think FastLeaderElection.sendNotification() should send notifications to all the members and FastLeaderElection.Messenger.WorkerReceiver.run() should verify acks.

Any thoughts?",[],Bug,ZOOKEEPER-2189,Major,Akihiro Suda,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,QuorumCnxManager: use BufferedOutputStream for initial msg,2015-06-03T16:46:17.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
,"[<JIRA Component: name='java client', id='12312381'>]",2015-05-11T11:07:00.000+0000,sunhaitao,"There is something wrong with the client code ClientCnxn.java, it will keep trying to connect to server in a dead loop.
This is my test step, shut down zookeeper cluster, exectue zkCli.sh script to connect to zookeeper cluster, it will keep trying to connect to zookeeper server without stop.

public void run() {
            clientCnxnSocket.introduce(this, sessionId, outgoingQueue);
            clientCnxnSocket.updateNow();
            clientCnxnSocket.updateLastSendAndHeard();
            int to;
            long lastPingRwServer = Time.currentElapsedTime();
            final int MAX_SEND_PING_INTERVAL = 10000; //10 seconds
            while (state.isAlive()) {
                try {
                    if (!clientCnxnSocket.isConnected()) {
                        // don't re-establish connection if we are closing
                        if (closing) {
                            break;
                        }
                        startConnect();
                        clientCnxnSocket.updateLastSendAndHeard();
                    }

public boolean isAlive() {
            return this != CLOSED && this != AUTH_FAILED;
        }

because at the beginning it is CONNECTING so isAlive always returns true, which leads to dead loop.
we should add some retry limit to stop this",[],Bug,ZOOKEEPER-2188,Major,sunhaitao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,client connection hung up because of  dead loop,2015-07-30T05:31:44.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",3.0
Raúl Gutiérrez Segalés,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2015-05-09T03:53:00.000+0000,Raúl Gutiérrez Segalés,"To avoid cargo culting and reducing duplicated code we can merge most of CreateRequest & CreateRequest2 given that only the Response object is actually different.

This will improve readability of the code plus make it less confusing for people adding new opcodes in the future (i.e.: copying a request definition vs reusing what's already there, etc.). ","[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2187,Minor,Raúl Gutiérrez Segalés,Fixed,2015-05-29T17:47:22.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"remove duplicated code between CreateRequest{,2}",2015-05-30T10:40:24.000+0000,[],6.0
Raúl Gutiérrez Segalés,"[<JIRA Component: name='server', id='12312382'>]",2015-05-08T19:36:43.000+0000,Raúl Gutiérrez Segalés,"This will allocate an arbitrarily large byte buffer (and try to read it!):

{code}
    public boolean receiveConnection(Socket sock) {
        Long sid = null;
...
                sid = din.readLong();
                // next comes the #bytes in the remainder of the message                                                                             
                int num_remaining_bytes = din.readInt();
                byte[] b = new byte[num_remaining_bytes];
                // remove the remainder of the message from din                                                                                      
                int num_read = din.read(b);
{code}

This will crash the QuorumCnxManager thread, so the cluster will keep going but future elections might fail to converge (ditto for leaving/joining members). 

Patch coming up in a bit.","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2186,Major,Raúl Gutiérrez Segalés,Fixed,2015-05-24T06:34:14.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,QuorumCnxManager#receiveConnection may crash with random input,2018-04-09T10:11:58.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",13.0
Andor Molnar,"[<JIRA Component: name='java client', id='12312381'>]",2015-05-07T23:46:12.000+0000,Robert P. Thille,"Testing in a Docker environment with a single Kafka instance using a single Zookeeper instance. Restarting the Zookeeper container will cause it to receive a new IP address. Kafka will never be able to reconnect to Zookeeper and will hang indefinitely. Updating DNS or /etc/hosts with the new IP address will not help the client to reconnect as the zookeeper/client/StaticHostProvider resolves the connection string hosts at creation time and never re-resolves.

A solution would be for the client to notice that connection attempts fail and attempt to re-resolve the hostnames in the connectString.","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.4.13', id='12342973'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-2184,Blocker,Robert P. Thille,Fixed,2018-06-22T22:11:10.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Zookeeper Client should re-resolve hosts when connection attempts fail,2020-04-03T22:45:57.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.4.10', id='12338036'>, <JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.4.11', id='12339207'>]",41.0
Chris Nauroth,"[<JIRA Component: name='tests', id='12312427'>]",2015-05-07T18:23:42.000+0000,Chris Nauroth,"In build.xml, the {{<junit>}} task definition uses an include pattern of {{\*\*/\*$\{test.category\}Test.java}}.  This is important so that we don't accidentally try to run utility classes like {{PortAssignment}} or {{TestableZooKeeper}} as if they were JUnit suites.  However, several test suites are misnamed so that they don't satisfy this pattern, and therefore pre-commit hasn't been running them.

{{ClientRetry}}
{{ReconfigFailureCases}}
{{WatchEventWhenAutoReset}}
","[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2182,Major,Chris Nauroth,Fixed,2015-05-12T05:37:41.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"Several test suites are not running during pre-commit, because their names do not end with ""Test"".",2015-05-12T05:47:38.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",6.0
,[],2015-05-05T11:16:57.000+0000,Ayappan,"TestZKIntegration testcase in slider fails with zookeeper-3.5.0-alpha version. From the logs, it came to know the state change went to LOST rather than CONNECTED while creating ZK path. The above testcase passes with zookeeper-3.4.6. A slider jira SLIDER-862 is already opened for this. But the problem seems to be with zookeeper-3.5.0-alpha.

Running org.apache.slider.common.tools.TestZKIntegration
2015-04-24 06:56:52,118 [Thread-2] INFO  services.MicroZookeeperService (MicroZookeeperService.java:serviceStart(235)) - Starting Local Zookeeper service
2015-04-24 06:56:52,299 [Thread-2] INFO  services.MicroZookeeperService (MicroZookeeperService.java:serviceStart(241)) - In memory ZK started at localhost:50577

2015-04-24 06:56:52,300 [Thread-2] INFO  test.MicroZKCluster (MicroZKCluster.groovy:createCluster(53)) - Created Micro ZK cluster as localhost:50577
2015-04-24 06:56:52,492 [Thread-2] INFO  imps.CuratorFrameworkImpl (CuratorFrameworkImpl.java:start(223)) - Starting
2015-04-24 06:56:52,513 [Thread-2] DEBUG zk.ZKIntegration (ZKIntegration.java:init(96)) - Binding ZK client to localhost:50577
2015-04-24 06:56:52,513 [Thread-2] INFO  zk.BlockingZKWatcher (BlockingZKWatcher.java:waitForZKConnection(57)) - waiting for ZK event
2015-04-24 06:56:52,543 [Thread-2-EventThread] DEBUG zk.ZKIntegration (ZKIntegration.java:process(178)) - WatchedEvent state:Expired type:None path:null
2015-04-24 06:56:52,544 [Thread-2-EventThread] DEBUG zk.ZKIntegration (ZKIntegration.java:maybeInit(191)) - initing
2015-04-24 06:56:52,544 [Thread-2-EventThread] DEBUG zk.ZKIntegration (ZKIntegration.java:createPath(222)) - Creating ZK path /services
2015-04-24 06:56:52,545 [Thread-2-EventThread] INFO  state.ConnectionStateManager (ConnectionStateManager.java:postState(194)) - State change: LOST
2015-04-24 06:56:52,546 [Thread-2-EventThread] WARN  curator.ConnectionState (ConnectionState.java:handleExpiredSession(289)) - Session expired event received
2015-04-24 06:56:52,548 [ConnectionStateManager-0] WARN  state.ConnectionStateManager (ConnectionStateManager.java:processEvents(212)) - There are no ConnectionStateListeners registered.
2015-04-24 06:56:52,549 [NIOWorkerThread-1] WARN  server.NIOServerCnxn (NIOServerCnxn.java:doIO(368)) - Unable to read additional data from client sessionid 0x14ceb499c750000, likely client has closed socket
2015-04-24 06:56:52,550 [Thread-2-EventThread] ERROR zk.ZKIntegration (ZKIntegration.java:process(182)) - Failed to init
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /services
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:131)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1067)
        at org.apache.slider.core.zk.ZKIntegration.createPath(ZKIntegration.java:223)
        at org.apache.slider.core.zk.ZKIntegration.mkPath(ZKIntegration.java:242)
        at org.apache.slider.core.zk.ZKIntegration.maybeInit(ZKIntegration.java:193)
        at org.apache.slider.core.zk.ZKIntegration.process(ZKIntegration.java:180)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:539)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:515)
2015-04-24 06:56:52,550 [NIOWorkerThread-3] WARN  server.NIOServerCnxn (NIOServerCnxn.java:doIO(368)) - Unable to read additional data from client sessionid 0x14ceb499c750001, likely client has closed socket
2015-04-24 06:56:52,551 [Thread-2-EventThread] INFO  zk.BlockingZKWatcher (BlockingZKWatcher.java:process(37)) - ZK binding callback received",[],Bug,ZOOKEEPER-2181,Major,Ayappan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Slider-Zookeeper integration testcase fails with Zookeeper-3.5.0-alpha version,2015-06-03T11:07:36.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",1.0
,[],2015-05-05T02:04:33.000+0000,seekerak,"[zk: localhost:2181(CONNECTED) 18] listquota /mynode
absolute path is /zookeeper/quota/mynode/zookeeper_limits
Output quota for /mynode count=-1,bytes=100
Output stat for /mynode count=6,bytes=484



[zk: localhost:2181(CONNECTED) 19] listquota /mynode_n
absolute path is /zookeeper/quota/mynode_n/zookeeper_limits
Output quota for /mynode_n count=2,bytes=-1
Output stat for /mynode_n count=5,bytes=5",[],Bug,ZOOKEEPER-2180,Major,seekerak,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,quota do not take effect in version 3.4.6,2019-03-23T08:44:09.000+0000,[],3.0
Chris Nauroth,"[<JIRA Component: name='c client', id='12312380'>]",2015-05-02T20:29:14.000+0000,Chris Nauroth,"Due to several recent changes, the native client fails to compile on Windows:
# ZOOKEEPER-827 (read-only mode) mismatched a function return type between the declaration and definition.
# ZOOKEEPER-1626 (monotonic clock for tolerance to time adjustments) added an include of unistd.h, which does not exist on Windows.
# Additionally, ZOOKEEPER-1626 did not implement a code path for accessing the Windows monotonic clock.
","[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2178,Major,Chris Nauroth,Fixed,2015-06-01T06:48:26.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Native client fails compilation on Windows.,2017-03-14T04:32:31.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",6.0
,[],2015-04-24T02:17:57.000+0000,Brahma Reddy Battula," *Session Id from ZK :* 

2015-04-15 21:24:54,257 | INFO  | CommitProcessor:22 | Established session 0x164cb2b3e4b36ae4 with negotiated timeout 45000 for client /160.149.0.117:44586 | org.apache.zookeeper.server.ZooKeeperServer.finishSessionInit(ZooKeeperServer.java:623)
2015-04-15 21:24:54,261 | INFO  | NIOServerCxn.Factory:160-149-0-114/160.149.0.114:24002 | Successfully authenticated client: authenticationID=hdfs/hadoop@HADOOP.COM;  authorizationID=hdfs/hadoop@HADOOP.COM. | org.apache.zookeeper.server.auth.SaslServerCallbackHandler.handleAuthorizeCallback(SaslServerCallbackHandler.java:118)
2015-04-15 21:24:54,261 | INFO  | NIOServerCxn.Factory:160-149-0-114/160.149.0.114:24002 | Setting authorizedID: hdfs/hadoop@HADOOP.COM | org.apache.zookeeper.server.auth.SaslServerCallbackHandler.handleAuthorizeCallback(SaslServerCallbackHandler.java:134)
2015-04-15 21:24:54,261 | INFO  | NIOServerCxn.Factory:160-149-0-114/160.149.0.114:24002 | adding SASL authorization for authorizationID: hdfs/hadoop@HADOOP.COM | org.apache.zookeeper.server.ZooKeeperServer.processSasl(ZooKeeperServer.java:1009)
2015-04-15 21:24:54,262 | INFO  | ProcessThread(sid:22 cport:-1): | Got user-level KeeperException when processing  *{color:red}sessionid:0x164cb2b3e4b36ae4{color}*  type:create cxid:0x3 zxid:0x20009fafc txntype:-1 reqpath:n/a Error Path:/hadoop-ha/hacluster/ActiveStandbyElectorLock Error:KeeperErrorCode = NodeExists for /hadoop-ha/hacluster/ActiveStandbyElectorLock | org.apache.zookeeper.server.PrepRequestProcessor.pRequest(PrepRequestProcessor.java:648)

 *ZKFC Received :*  ZK client

2015-04-15 21:24:54,237 | INFO  | main-SendThread(160-149-0-114:24002) | Socket connection established to 160-149-0-114/160.149.0.114:24002, initiating session | org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:854)
2015-04-15 21:24:54,257 | INFO  | main-SendThread(160-149-0-114:24002) | Session establishment complete on server 160-149-0-114/160.149.0.114:24002,  *{color:blue}sessionid = 0x144cb2b3e4b36ae4 {color}* , negotiated timeout = 45000 | org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1259)
2015-04-15 21:24:54,260 | INFO  | main-EventThread | EventThread shut down | org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:512)
2015-04-15 21:24:54,262 | INFO  | main-EventThread | Session connected. | org.apache.hadoop.ha.ActiveStandbyElector.processWatchEvent(ActiveStandbyElector.java:547)
2015-04-15 21:24:54,264 | INFO  | main-EventThread | Successfully authenticated to ZooKeeper using SASL. | org.apache.hadoop.ha.ActiveStandbyElector.processWatchEvent(ActiveStandbyElector.java:573)

one bit corrupted..please check the following for same..

144cb2b3e4b36ae4=1010001001100101100101011001111100100101100110110101011100100
164cb2b3e4b36ae4=1011001001100101100101011001111100100101100110110101011100100",[],Bug,ZOOKEEPER-2175,Major,Brahma Reddy Battula,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Checksum validation for malformed packets needs to handle.,2016-04-07T07:45:22.000+0000,[],11.0
Chris Nauroth,"[<JIRA Component: name='tests', id='12312427'>]",2015-04-21T23:43:37.000+0000,Chris Nauroth,"{{JUnit4ZKTestRunner}} wraps JUnit test method execution, and if any exception is thrown, it logs a message stating that the test failed.  However, some ZooKeeper tests are annotated with {{@Test(expected=...)}} to indicate that an exception is the expected result, and thus the test passes.  The runner should be aware of expected exceptions and only log if an unexpected exception occurs.","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2174,Minor,Chris Nauroth,Fixed,2015-05-03T18:04:45.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,JUnit4ZKTestRunner logs test failure for all exceptions even if the test method is annotated with an expected exception.,2016-07-21T20:18:43.000+0000,[],6.0
J.Andreina,[],2015-04-21T10:34:55.000+0000,J.Andreina,"If any failure during zk Startup (myid file does not exist), then still zk startup returns as successful (STARTED).

ZK startup failure should be handled with proper error message","[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2173,Major,J.Andreina,Fixed,2015-04-28T00:39:09.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZK startup failure should be handled with proper error message,2015-04-28T04:30:15.000+0000,[],4.0
Mohammad Arshad,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2015-04-21T04:14:24.000+0000,Ziyou Wang,"The operations are quite simple: start three zk servers one by one, then reconfig the cluster to add the new one as a participant. When I add the  third one, the zk cluster may enter a weird state and cannot recover.
 
      I found “2015-04-20 12:53:48,236 [myid:1] - INFO  [ProcessThread(sid:1 cport:-1)::PrepRequestProcessor@547] - Incremental reconfig” in node-1 log. So the first node received the reconfig cmd at 12:53:48. Latter, it logged “2015-04-20  12:53:52,230 [myid:1] - ERROR [LearnerHandler-/10.0.0.2:55890:LearnerHandler@580] - Unexpected exception causing shutdown while sock still open” and “2015-04-20 12:53:52,231 [myid:1] - WARN  [LearnerHandler-/10.0.0.2:55890:LearnerHandler@595] - ******* GOODBYE  /10.0.0.2:55890 ********”. From then on, the first node and second node rejected all client connections and the third node didn’t join the cluster as a participant. The whole cluster was done.
 
     When the problem happened, all three nodes just used the same dynamic config file zoo.cfg.dynamic.10000005d which only contained the first two nodes. But there was another unused dynamic config file in node-1 directory zoo.cfg.dynamic.next  which already contained three nodes.
 
     When I extended the waiting time between starting the third node and reconfiguring the cluster, the problem didn’t show again. So it should be a race condition problem.","[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2172,Critical,Ziyou Wang,Fixed,2016-09-08T21:01:46.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Cluster crashes when reconfig a new node as a participant,2018-03-07T14:37:08.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",16.0
Raúl Gutiérrez Segalés,"[<JIRA Component: name='quorum', id='12312379'>]",2015-04-21T00:22:38.000+0000,Raúl Gutiérrez Segalés,"Apparently, ZOOKEEPER-107 (via a quick git-blame look) introduced a bunch of getHostName() calls in QCM. Besides the overhead, these can cause problems when mixed with failing/mis-configured DNS servers.

It would be nice to reduce them, if that doesn't affect operational correctness. ","[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2171,Major,Raúl Gutiérrez Segalés,Fixed,2015-05-09T22:31:13.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,avoid reverse lookups in QuorumCnxManager,2016-02-15T11:59:12.000+0000,[],7.0
Mohammad Arshad,[],2015-04-20T11:19:36.000+0000,Mohammad Arshad,"In conf/log4j.properties default root logger is 
{code}
zookeeper.root.logger=INFO, CONSOLE
{code}

Changing root logger to bellow value or any other value does not change logging effect
{code}
zookeeper.root.logger=DEBUG, ROLLINGFILE
{code}
","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-2170,Major,Mohammad Arshad,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Zookeeper is not logging as per the configuration in log4j.properties,2022-02-03T08:36:24.000+0000,[],8.0
,[],2015-04-15T00:01:24.000+0000,Mike Lundy,"I'm seeing an issue where a restart of the current leader node results in a long-term / permanent loss of quorum (I've only waited 30 minutes, but it doesn't look like it's making any progress). Restarting the same instance _again_ seems to resolve the problem.

To me, this looks a lot like the issue described in https://issues.apache.org/jira/browse/ZOOKEEPER-1026, but I'm filing this separately for the moment in case I am wrong.

Notes on the attached log:
1) If you search for XXX in the log, you'll see where I've annotated it to include where the process was told to terminate, when it is reported to have completed that, and then the same for the start
2) To save you the trouble of figuring it out, here's the zkid <=> ip mapping:
zid=1, ip=10.20.0.19
zid=2, ip=10.20.0.18
zid=3, ip=10.20.0.20
zid=4, ip=10.20.0.21
zid=5, ip=10.20.0.22
3) It's important to note that this is log is during the process of a rolling service restart to remove an instance; in this case, zid #2 / 10.20.0.18 is the one being removed, so if you see a conspicuous silence from that service, that's why. 
4) I've been unable to reproduce this problem _except_ during cluster size changes, so I suspect that may be related; it's also important to note that this test is going from 5 -> 4 (which means, since we remove one and then do a rolling restart, we are actually temporarily dropping to 3). I know this is not a recommended thing (this is more of a stress test). We have seen this same problem on larger cluster sizes, it just seems easier to reproduce it on smaller sizes.
5) The log starts roughly at the point 10.20.0.21 / zid=4 wins the election during the final quorum; zid=4 is the one whose shutdown triggers the problem.",[],Bug,ZOOKEEPER-2167,Major,Mike Lundy,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Restarting current leader node sometimes results in a permanent loss of quorum,2015-04-16T03:50:06.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2015-04-14T21:00:25.000+0000,Jordan Zimmerman,QuorumPeerConfig.backupOldConfig() should check if configFileStr is null or not and do nothing if it is null. This is currently breaking Apache Curator's TestingCluster.,[],Bug,ZOOKEEPER-2166,Major,Jordan Zimmerman,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,backupOldConfig() doesn't check for null,2015-04-14T21:07:33.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",2.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2015-04-14T11:42:28.000+0000,Simon Kitching,"Class QuoromPeer has a constructor which takes a QuorumVerifier value as a parameter. This class is defined in package ""org.apache.zookeeper.server.quorum.flexible"" but that package is not exported.",[],Bug,ZOOKEEPER-2165,Minor,Simon Kitching,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"OSGi requires package ""server.quorom.flexible"" be exported",2015-04-23T08:42:10.000+0000,[],3.0
Mate Szalay-Beko,"[<JIRA Component: name='leaderElection', id='12312378'>]",2015-04-14T07:36:12.000+0000,Michi Mutsuzaki,"I have a 3-node cluster with sids 1, 2 and 3. Originally 2 is the leader. When I shut down 2, 1 and 3 keep going back to leader election. Here is what seems to be happening.

- Both 1 and 3 elect 3 as the leader.
- 1 receives votes from 3 and itself, and starts trying to connect to 3 as a follower.
- 3 doesn't receive votes for 5 seconds because connectOne() to 2 doesn't timeout for 5 seconds: https://github.com/apache/zookeeper/blob/41c9fcb3ca09cd3d05e59fe47f08ecf0b85532c8/src/java/main/org/apache/zookeeper/server/quorum/QuorumCnxManager.java#L346
- By the time 3 receives votes, 1 has given up trying to connect to 3: https://github.com/apache/zookeeper/blob/41c9fcb3ca09cd3d05e59fe47f08ecf0b85532c8/src/java/main/org/apache/zookeeper/server/quorum/Learner.java#L247

I'm using 3.4.5, but it looks like this part of the code hasn't changed for a while, so I'm guessing later versions have the same issue.","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",Bug,ZOOKEEPER-2164,Major,Michi Mutsuzaki,Fixed,2020-03-12T13:52:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,fast leader election keeps failing,2020-05-11T15:41:11.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",29.0
Akihiro Suda,"[<JIRA Component: name='server', id='12312382'>]",2015-04-13T04:33:50.000+0000,Akihiro Suda,"This sequence leads server.1 and server.2 to infinite exception loop.

 * Start server.1 and server.2 with the initial ensemble server.1=participant, server.2=observer.
   In this time, acceptedEpoch\[i\] == currentEpoch\[i\] == 1 for i = 1, 2.
 * Invoke reconfig so that acceptedEpoch\[i\] and currentEpoch\[i\] grows up to 2.
 * Kill server.2
 * Remove dataDir of server.2 excluding the myid file.
   (In real production environments, both of confDir and dataDir can be lost due to reprovisioning)
 * Start server.2
 * server.1 and server.2 enters infinite exception loop.
   The log (threshold is set to INFO in log4j.properties) size can reach > 100MB in 30 seconds.

AFAIK, the bug can be reproduced with ZooKeeper@f5fb50ed2591ba9a24685a227bb5374759516828 (Apr 7, 2015).

I made a Docker container so that people who are interested can reproduce the bug easily. (Sorry for no JUnit test right now)
{noformat}
$ docker run -i -t --rm akihirosuda/zookeeper-bug01
Reproducing the bug: infinite exception loop occurs when dataDir is lost
* Resetting
* Starting [1,2] with the initial ensemble [1]
* Sleeping for 3 seconds
* Invoking Reconfig [1]->[2]
* Sleeping for 3 seconds
* Killing server.2 (pid=10542)
* Sleeping for 3 seconds
* Resetting /zk02_data
* Starting server.2
* Sleeping for 30 seconds
/zk01_log: 81665114 bytes
The log dir is extremely large. Perhaps the bug was REPRODUCED!
/zk02_log: 23949367 bytes
The log dir is extremely large. Perhaps the bug was REPRODUCED!
* Exiting
{noformat}

h2. Log
h3. server.1
{noformat}
    2015-04-13 03:48:17,624 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):QuorumPeer@1022] - FOLLOWING
    2015-04-13 03:48:17,624 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):ZooKeeperServer@825] - minSessionTimeout set to 4000
    2015-04-13 03:48:17,624 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):ZooKeeperServer@834] - maxSessionTimeout set to 40000
    2015-04-13 03:48:17,624 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):ZooKeeperServer@156] - Created server with tickTime 2000 minSession
    Timeout 4000 maxSessionTimeout 40000 datadir /zk01_data/version-2 snapdir /zk01_data/version-2
    2015-04-13 03:48:17,624 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Follower@66] - FOLLOWING - LEADER ELECTION TOOK - 0
    2015-04-13 03:48:17,625 [myid:1] - WARN  [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Follower@93] - Exception when following the leader
    java.io.IOException: Leaders epoch, 1 is less than accepted epoch, 2
            at org.apache.zookeeper.server.quorum.Learner.registerWithLeader(Learner.java:331)
            at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:75)
            at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1024)
    2015-04-13 03:48:17,626 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):MBeanRegistry@119] - Unregister MBean [org.apache.ZooKeeperService:
    name0=ReplicatedServer_id1,name1=replica.1,name2=Follower]
    2015-04-13 03:48:17,626 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):Follower@198] - shutdown called
    java.lang.Exception: shutdown Follower
            at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:198)
            at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:1028)
    2015-04-13 03:48:17,626 [myid:1] - DEBUG [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):LearnerZooKeeperServer@162] - ZooKeeper server is not running, so n
    ot proceeding to shutdown!
    2015-04-13 03:48:17,626 [myid:1] - WARN  [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):QuorumPeer@1071] - PeerState set to LOOKING
    2015-04-13 03:48:17,626 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):QuorumPeer@946] - LOOKING
    2015-04-13 03:48:17,626 [myid:1] - DEBUG [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):QuorumPeer@875] - Initializing leader election protocol...
    2015-04-13 03:48:17,626 [myid:1] - DEBUG [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):FastLeaderElection@790] - Updating proposal: -9223372036854775808 (
    newleader), 0x100000002 (newzxid), -9223372036854775808 (oldleader), 0x100000002 (oldzxid)
    2015-04-13 03:48:17,626 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):FastLeaderElection@889] - New election. My id =  1, proposed zxid=0
    x100000002
    2015-04-13 03:48:17,626 [myid:1] - DEBUG [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):FastLeaderElection@673] - Sending Notification: -922337203685477580
    8 (n.leader), 0x100000002 (n.zxid), 0x2 (n.round), 2 (recipient), 1 (myid), 0x2 (n.peerEpoch)
    2015-04-13 03:48:17,626 [myid:1] - DEBUG [WorkerSender[myid=1]:QuorumCnxManager@400] - There is a connection already for server 2
    2015-04-13 03:48:17,627 [myid:1] - DEBUG [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@336] - Receive new notification message. My id = 1
    2015-04-13 03:48:17,627 [myid:1] - INFO  [WorkerReceiver[myid=1]:FastLeaderElection@683] - Notification: 2 (message format version), 2 (n.leader), 0x0 (n.zxid), 0x1 (n.round)
    , LEADING (n.state), 2 (n.sid), 0x1 (n.peerEPoch), LOOKING (my state)100000002 (n.config version)
    2015-04-13 03:48:17,627 [myid:1] - DEBUG [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):FastLeaderElection@812] - I'm a participant: 1
    2015-04-13 03:48:17,627 [myid:1] - DEBUG [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):FastLeaderElection@637] - About to leave FLE instance: leader=2, zx
    id=0x0, my id=1, my state=FOLLOWING
    2015-04-13 03:48:17,627 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):MBeanRegistry@119] - Unregister MBean [org.apache.ZooKeeperService:
    name0=ReplicatedServer_id1,name1=replica.1,name2=LeaderElection]
    2015-04-13 03:48:17,627 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0:0:0:0:0:0:0:0:2181)(secure=disabled):QuorumPeer@1022] - FOLLOWING
    ..
{noformat}    

h3. server.2
{noformat}
    2015-04-13 03:48:17,672 [myid:2] - ERROR [LearnerHandler-/127.0.0.1:36337:LearnerHandler@580] - Unexpected exception causing shutdown while sock still open
    java.io.EOFException
            at java.io.DataInputStream.readInt(DataInputStream.java:392)
            at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
            at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:83)
            at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:99)
            at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:392)
    2015-04-13 03:48:17,672 [myid:2] - WARN  [LearnerHandler-/127.0.0.1:36337:LearnerHandler@595] - ******* GOODBYE /127.0.0.1:36337 ********
    2015-04-13 03:48:17,674 [myid:2] - DEBUG [WorkerSender[myid=2]:QuorumCnxManager@400] - There is a connection already for server 1
    2015-04-13 03:48:17,676 [myid:2] - INFO  [LearnerHandler-/127.0.0.1:36338:LearnerHandler@364] - Follower sid: 1 not in the current config 100000002
    2015-04-13 03:48:17,676 [myid:2] - ERROR [LearnerHandler-/127.0.0.1:36338:LearnerHandler@580] - Unexpected exception causing shutdown while sock still open
    java.io.EOFException
            at java.io.DataInputStream.readInt(DataInputStream.java:392)
            at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
            at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:83)
            at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:99)
            at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:392)
    2015-04-13 03:48:17,677 [myid:2] - WARN  [LearnerHandler-/127.0.0.1:36338:LearnerHandler@595] - ******* GOODBYE /127.0.0.1:36338 ********
    ..
{noformat}",[],Bug,ZOOKEEPER-2162,Major,Akihiro Suda,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,infinite exception loop occurs when dataDir is lost,2017-09-20T08:03:59.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",7.0
,[],2015-04-08T16:10:12.000+0000,Michael Chiocca,"The cleanup task fails with the following stack trace. This is happening repeatedly every time the cleanup task runs. Even the command line invocation of cleanup fails with the same stack trace.

zookeeper@zoo91-node-5dw4yocu7bvj-fpjhrmhvgyhz-mnjsb4zltcy5-7588:~$ java -cp ./zookeeper-3.4.6.jar:./lib/log4j-1.2.16.jar:./lib/slf4j-log4j12-1.6.1.jar:./lib/slf4j-api-1.6.1.jar:/etc/zookeeper/conf org.apache.zookeeper.server.PurgeTxnLog /var/log/zookeeper /var/lib/zookeeper 5
log4j:ERROR setFile(null,true) call failed.
java.io.FileNotFoundException: /zookeeper.log (Permission denied)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:142)
	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294)
	at org.apache.log4j.RollingFileAppender.setFile(RollingFileAppender.java:207)
	at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)
	at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)
	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)
	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:809)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:735)
	at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:615)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:502)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:547)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:483)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:73)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:242)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:254)
	at org.apache.zookeeper.server.PurgeTxnLog.<clinit>(PurgeTxnLog.java:45)

The data log dir is set to /var/log/zookeeper in the /etc/zookeeper/conf/zoo.cfg config file. But as you can see, specifying the config directory in the Java classpath doesn't help eliminate the problem.",[],Bug,ZOOKEEPER-2161,Major,Michael Chiocca,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Cleanup task fails - java.io.FileNotFoundException: /zookeeper.log (Permission Denied),2015-04-08T16:10:12.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",1.0
J.Andreina,[],2015-04-03T06:21:17.000+0000,J.Andreina,"Upgrade option should be removed from zkServer.sh usage from trunk code

Currently upgrade option is available in zkServer.sh usage , while upgrade feature is already been removed from trunk.

{noformat}
#:~/March_1/zookeeper/bin> ./zkServer.sh upgrade
ZooKeeper JMX enabled by default
Using config: /home/REX/March_1/zookeeper/bin/../conf/zoo.cfg
Usage: ./zkServer.sh [--config <conf-dir>] {start|start-foreground|stop|restart|status|upgrade|print-cmd}
{noformat}","[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2157,Minor,J.Andreina,Fixed,2015-04-07T17:31:14.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Upgrade option should be removed from zkServer.sh usage,2015-04-08T11:19:48.000+0000,[],5.0
J.Andreina,"[<JIRA Component: name='scripts', id='12312384'>]",2015-04-03T05:03:39.000+0000,J.Andreina,"If JAVA_HOME is not set,  zk startup and fetching status command execution result misleads user.

1. Eventhough zk startup has failed since JAVA_HOME is not set , on CLI it displays that zk STARTED.
{noformat}
#:~/Apr3rd/zookeeper-3.4.6/bin> ./zkServer.sh start
JMX enabled by default
Using config: /home/REX/Apr3rd/zookeeper-3.4.6/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
{noformat}

2.  Fetching zk status when JAVA_HOME is not set displays that process not running .
{noformat}
#:~/Apr3rd/zookeeper-3.4.6/bin> ./zkServer.sh status
JMX enabled by default
Using config: /home/REX/Apr3rd/zookeeper-3.4.6/bin/../conf/zoo.cfg
Error contacting service. It is probably not running.
{noformat}","[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2156,Major,J.Andreina,Fixed,2015-05-20T07:36:49.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,If JAVA_HOME is not set zk startup and fetching status command execution result misleads user.,2016-07-21T20:18:24.000+0000,[],6.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2015-04-02T06:35:53.000+0000,linking12,"When I set up a ZooKeeper ensemble that uses Observers, The network is not very good.
I find all of the watcher disappear.

I read the source code and find:
  When the observer connect to leader, will dump the DataTree from leader and rebuild in observer.
But the datawachers and childWatches is cleared for this operation.

after i change code like:
WatchManager dataWatchers = zk.getZKDatabase().getDataTree()
                   .getDataWatches();
WatchManager childWatchers = zk.getZKDatabase().getDataTree()
                   .getChildWatches();
zk.getZKDatabase().clear();
zk.getZKDatabase().deserializeSnapshot(leaderIs);
zk.getZKDatabase().getDataTree().setDataWatches(dataWatchers);
zk.getZKDatabase().getDataTree().setChildWatches(childWatchers);

The watcher do not disappear","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-2155,Critical,linking12,Invalid,2015-08-26T20:45:18.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"network is not good, the watcher in observer env will clear",2015-08-26T20:45:18.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",4.0
,"[<JIRA Component: name='java client', id='12312381'>]",2015-03-31T05:55:08.000+0000,Surendra Singh Lilhore,KeeperException should handle exception is code is null...,[],Bug,ZOOKEEPER-2154,Major,Surendra Singh Lilhore,Duplicate,2021-06-24T07:50:02.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,NPE in KeeperException,2021-06-24T07:50:02.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",4.0
,"[<JIRA Component: name='server', id='12312382'>]",2015-03-24T19:20:22.000+0000,Jared Cantwell,"We are seeing one follower server in our quorum stuck with thousands of outstanding requests:
---------------------------------------------
node04:~$ telnet 10.10.10.6 2181
Trying 10.10.10.6...
Connected to 10.10.10.6.
Escape character is '^]'.
*stat*
Zookeeper version: 3.5.0-1547702, built on 05/15/2014 03:06 GMT
Clients:
 /10.10.10.6:60646\[0\](queued=0,recved=1,sent=0)
 /10.10.10.6:60648\[0\](queued=0,recved=1,sent=0)
 /10.10.10.6:41786\[0\](queued=1,recved=3,sent=1)

Latency min/avg/max: 0/0/1887
Received: 3064156900
Sent: 3064134581
Connections: 3
*Outstanding: 24395*
Zxid: 0x11050f7e4b
Mode: follower
Node count: 6969
Connection closed by foreign host.
---------------------------------------------

When this happens, our c client is able to establish an initial connection to the server, but any request then times out.  It re-establishes a connection, then times out, rinse, repeat.  We are noticing this because we set up this particular client to connect directly to only one server in the quorum, so any problem with that server will be noticed.  Our other clients are just connecting to the next server in the list, which is why only this client notices a problem.

We were able to capture a heap dump in one instance.  This is what we observed:

- FollowerZookeeperServer.requestsInProcess has count ~24K
- CommitProcessor.queuedRequest list has the 24K items in it, so the FinalRequestProcessor's processRequest function isn't ever getting called to complete the requests.
- CommitProcessor.isWaitingForCommit()==true
- CommitProcessor.committedRequests.isEmpty()==true
- CommitProcessor.nextPending is a create request
- CommitProcessor.currentlyCommitting is null
- CommitProcessor.numRequestsProcessing is 0
- FollowerZookeeperServer, who should be calling commit() on the CommitProcessor, has no elements in its pendingTxns list, which indicates that it thinks it has already passed a COMMIT message to the CommitProcessor for every request that is stuck in the queuedRequests list and nextPending member of CommitProcessor.

The CommitProcessor's run() is doing this:
{quote}
Thread 23510: (state = BLOCKED)
   java.lang.Object.wait(long) @bci=0 (Compiled frame; information may be imprecise)
   org.apache.zookeeper.server.quorum.CommitProcessor.run() @bci=165, line=182 (Compiled frame)
{quote}

When we attached via gdb to get the dump, sockets closed that caused a new round of leader election.  When this happened, the issued corrected itself since the whole FollowerZookeeperServer got restarted.

I've confirmed that no time changing was happening before things got stuck 2 days before we noticed it.

","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-2151,Major,Jared Cantwell,Duplicate,2015-03-25T20:35:02.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,FollowerZookeeperServer has thousands of outstanding requests stuck in CommitProcessor,2015-03-25T20:35:02.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",2.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2015-03-24T16:06:34.000+0000,parin jogani,"We have pool of zookeeper machines (contains both active and observer) in version 3.4.3. 
We recently undated our exhibitor from 1.2.x to 1.5.4. 
We are seeing a strange behavior in our observers: they keep losing connection with the active ensemble and do not recover. The connection goes into CLOSE_WAIT state. Dont think there is any relation to exhibitor.",[],Bug,ZOOKEEPER-2150,Major,parin jogani,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Observers losing connection with active ensemble and dont recover,2015-03-24T16:06:34.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",1.0
Hongchao Deng,[],2015-03-20T23:07:54.000+0000,Hongchao Deng,"I recently observed a problem caused by malformed packets. ZK server crashed because of OutOfMemoryError.

The reason is BinaryInputArchive didn't check the length before allocating memory in readString():
{code}
  public String readString(String tag) throws IOException {
    	int len = in.readInt();
    	if (len == -1) return null;
    	byte b[] = new byte[len];
        ...
{code}

I suggest to add the same check as in readBuffer.","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2146,Major,Hongchao Deng,Fixed,2015-03-25T07:40:52.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,BinaryInputArchive readString should check length before allocating memory,2015-04-18T00:48:54.000+0000,[],6.0
,[],2015-03-18T20:27:23.000+0000,Frans Lawaetz,"I have a three-server ensemble that appears to be working fine in every respect but for the fact that I can ls or get a znode but can not rmr it.

>[zk: localhost:2181(CONNECTED) 0] get /accumulo/9354e975-7e2a-4207-8c7b-5d36c0e7765d/masters/goal_state
CLEAN_STOP
cZxid = 0x15
ctime = Fri Feb 20 13:37:59 CST 2015
mZxid = 0x72
mtime = Fri Feb 20 13:38:05 CST 2015
pZxid = 0x15
cversion = 0
dataVersion = 2
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 10
numChildren = 0
[zk: localhost:2181(CONNECTED) 1] rmr /accumulo/9354e975-7e2a-4207-8c7b-5d36c0e7765d/masters/goal_state
Node does not exist: /accumulo/9354e975-7e2a-4207-8c7b-5d36c0e7765d/masters/goal_state

I have run a 'stat' against all three servers and they seem properly structured with a leader and two followers.  An md5sum of all zoo.cfg shows them to be identical.  

The problem seems localized to the accumulo/935.... directory as I can create and delete znodes outside of that path fine but not inside of it.

For example:

[zk: localhost:2181(CONNECTED) 12] create /accumulo/9354e975-7e2a-4207-8c7b-5d36c0e7765d/fubar asdf
Node does not exist: /accumulo/9354e975-7e2a-4207-8c7b-5d36c0e7765d/fubar
[zk: localhost:2181(CONNECTED) 13] create /accumulo/fubar asdf
Created /accumulo/fubar
[zk: localhost:2181(CONNECTED) 14] ls /accumulo/fubar
[]
[zk: localhost:2181(CONNECTED) 15] rmr /accumulo/fubar
[zk: localhost:2181(CONNECTED) 16]

Here is my zoo.cfg:
tickTime=2000
initLimit=10
syncLimit=15
dataDir=/data/extera/zkeeper/data
clientPort=2181

 maxClientCnxns=300
autopurge.snapRetainCount=10
autopurge.purgeInterval=1

server.1=cdf61:2888:3888
server.2=cdf62:2888:3888
server.3=cdf63:2888:3888

",[],Bug,ZOOKEEPER-2145,Major,Frans Lawaetz,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Node can be seen but not deleted,2016-01-19T00:40:10.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",6.0
Edward Ribeiro,[],2015-03-16T10:30:46.000+0000,Karol Dudzinski,Observers show up in JMX as StandaloneServer rather than Observer.,"[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2142,Trivial,Karol Dudzinski,Fixed,2015-10-31T22:22:37.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,JMX ObjectName is incorrect for observers,2016-07-21T20:18:37.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.1', id='12326786'>]",8.0
Adam Milne-Smith,[],2015-03-16T10:28:31.000+0000,Karol Dudzinski,"The problem and potential solutions are discussed in http://mail-archives.apache.org/mod_mbox/zookeeper-user/201502.mbox/browser

I will attach a proposed patch in due course.","[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>]",Bug,ZOOKEEPER-2141,Blocker,Karol Dudzinski,Fixed,2016-04-06T18:09:26.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ACL cache in DataTree never removes entries,2016-07-21T20:18:31.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",9.0
Michael Han,[],2015-03-08T21:16:51.000+0000,Michi Mutsuzaki,,[],Bug,ZOOKEEPER-2135,Major,Michi Mutsuzaki,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,fix trunk build,2022-02-03T08:50:15.000+0000,[],2.0
Botond Hejj,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2015-03-05T16:49:28.000+0000,Botond Hejj,"If Node content is null:

[zk: (CONNECTED) 0] get /apps
null
cZxid = 0x10000000d

than

my $data = $zk->{zkh}->get('/apps');

causing a core dump with Segmentation fault","[<JIRA Version: name='3.4.9', id='12334700'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2133,Major,Botond Hejj,Fixed,2016-03-11T06:36:23.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkperl: Segmentation fault if getting a node with null value,2016-07-21T20:18:32.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2015-02-26T18:49:02.000+0000,Asad Saeed,"If multiple leader elections occur before SyncRequestProcessor takes a snapshot and rolls logs (at least 50000 transactions by default). PurgeTxnLog may inadvertently delete the current transaction log file.

Follower::syncWithLeader currently takes a snapshot after it is synced with the leader but does not roll logs. If a zookeeper restart of a quorum of nodes occurs, the cluster may silently revert back to the last snapshot, loosing all transactions in the log!",[],Bug,ZOOKEEPER-2132,Major,Asad Saeed,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Follower::syncWithLeader should roll logs before taking snapshot,2016-02-04T12:17:24.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",4.0
,[],2015-02-24T17:25:41.000+0000,dirtdiver512,"getting error below:

[2015-02-24 11:11:25,226] INFO Got user-level KeeperException when processing sessionid:0x14bbc922a2b0002 type:create cxid:0x1a zxid:0xe8 txntype:-1 reqpath:n/a Error Path:/consumers/console-consumer-67319/owners Error:KeeperErrorCode = NoNode for /consumers/console-consumer-67319/owners (org.apache.zookeeper.server.PrepRequestProcessor)
",[],Bug,ZOOKEEPER-2131,Major,dirtdiver512,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Error:KeeperErrorCode = NoNode,2015-02-24T17:25:41.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",1.0
,[],2015-02-24T10:15:58.000+0000,Mohammad Arshad,"ruok commands prints the output in the same line unlike the other four letter commands which print output in next line. Even though output is correct it is difficult to notice the output specially for a first time user. Its output should contain new line character as other four letter command's output 

ruok command output:
{code}
HOST1:/home # echo ruok | netcat 10.x.x.x 2181
imokHOST1:/home #
{code}

conf Command output:
{code}
HOST1:/home # echo conf | netcat 10.x.x.x 2181
clientPort=2181
dataDir=/tmp/zookeeper/data/version-2
dataLogDir=/tmp/zookeeper/data/version-2
tickTime=2000
.......
HOST1:/home #
{code}",[],Bug,ZOOKEEPER-2129,Trivial,Mohammad Arshad,Not A Problem,2015-03-05T10:30:53.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ruok command is not consistent with other four letter commands,2019-12-19T22:59:49.000+0000,[],2.0
Dave Gosselin,[],2015-02-24T03:46:25.000+0000,Dave Gosselin,The C API for zoo_aremove_watchers incorrectly specifies the seventh argument as a pointer to a function pointer.  It should be simply a function pointer only.,[],Bug,ZOOKEEPER-2128,Major,Dave Gosselin,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zoo_aremove_watchers API is incorrect,2022-02-03T08:50:23.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",4.0
,[],2015-02-23T11:48:34.000+0000,Joe Gamache,"According to the answer provided to this stack overflow question:

http://stackoverflow.com/questions/28589703/zookeeper-zkcli-sh-create-switches-documentation/28594057#28594057

the zkCli.sh script is not documented in terms of what all the switches mean.  Such documentation should be provided.",[],Bug,ZOOKEEPER-2127,Major,Joe Gamache,Done,2019-08-04T12:45:00.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Document zkCli.sh,2019-08-04T12:45:00.000+0000,[],2.0
Chris Nauroth,[],2015-02-20T18:58:43.000+0000,Jerry He,"Using Bigtop or other RPM build for Zookeeper, there is a problem with using the hyphen '-' character in the version string:
{noformat}
[bigdata@bdvs1166 bigtop]$ gradle zookeeper-rpm
:buildSrc:compileJava UP-TO-DATE
:buildSrc:compileGroovy UP-TO-DATE
:buildSrc:processResources UP-TO-DATE
:buildSrc:classes UP-TO-DATE
:buildSrc:jar UP-TO-DATE
:buildSrc:assemble UP-TO-DATE
:buildSrc:compileTestJava UP-TO-DATE
:buildSrc:compileTestGroovy UP-TO-DATE
:buildSrc:processTestResources UP-TO-DATE
:buildSrc:testClasses UP-TO-DATE
:buildSrc:test UP-TO-DATE
:buildSrc:check UP-TO-DATE
:buildSrc:build UP-TO-DATE
:zookeeper_vardefines
:zookeeper-download
:zookeeper-tar
Copy /home/bigdata/bigtop/dl/zookeeper-3.4.6-IBM-1.tar.gz to /home/bigdata/bigtop/build/zookeeper/tar/zookeeper-3.4.6-IBM-1.tar.gz
:zookeeper-srpm
error: line 64: Illegal char '-' in: Version: 3.4.6-IBM-1
:zookeeper-srpm FAILED

FAILURE: Build failed with an exception.

* Where:
Script '/home/bigdata/bigtop/packages.gradle' line: 462

* What went wrong:
Execution failed for task ':zookeeper-srpm'.
> Process 'command 'rpmbuild'' finished with non-zero exit value 1

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED
{noformat}

Also, according to the [rpm-maven-plugin|http://mojo.codehaus.org/rpm-maven-plugin/ident-params.html] documentation:
{noformat}

version
The version number to use for the RPM package. By default, this is the project version. This value cannot contain a dash (-) due to contraints in the RPM file naming convention. Any specified value will be truncated at the first dash

release
The release number of the RPM.
Beginning with release 2.0-beta-2, this is an optional parameter. By default, the release will be generated from the modifier portion of the project version using the following rules:
If no modifier exists, the release will be 1.
If the modifier ends with SNAPSHOT, the timestamp (in UTC) of the build will be appended to end.
All instances of '-' in the modifier will be replaced with '_'.
If a modifier exists and does not end with SNAPSHOT, ""_1"" will be appended to end.

{noformat}

We should allow underscore '_' as part of the version string. e.g. 
3.4.6_abc_1","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2124,Major,Jerry He,Fixed,2015-05-24T06:38:14.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Allow Zookeeper version string to have underscore '_',2015-09-14T20:35:34.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",7.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2015-02-18T17:08:43.000+0000,Joe Gamache,"If you go to the create method documentation here:
http://zookeeper.apache.org/doc/r3.4.6/api/index.html

Then you see:
{code}
public String create(String path,
            byte[] data,
            List<ACL> acl,
            CreateMode createMode)
              throws KeeperException,
                     InterruptedException
Create a node with the given path. The node data will be the given data, and node acl will be the given acl.
The flags argument specifies whether the created node will be ephemeral or not.

An ephemeral node will be removed by the ZooKeeper automatically when the session associated with the creation of the node expires.

The flags argument can also specify to create a sequential node. The actual path name of a sequential node will be the given path plus a suffix ""i"" where i is the current sequential number of the node. The sequence number is always fixed length of 10 digits, 0 padded. Once such a node is created, the sequential number will be incremented by one.
{code}

While there are 'path', 'data', 'acl', and 'createMode' arguments, there is no ""flags argument"".  This documentation needs to be corrected to be clear, unambiguous, and perhaps provide and example.",[],Bug,ZOOKEEPER-2121,Major,Joe Gamache,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Documentation of create method is unclear, incorrect, or both",2015-08-16T08:04:06.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",3.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2015-02-15T06:32:53.000+0000,ysl871308,"We need Netty in quorum communication to make use of SSL/auth feature in Netty.

This might need more thoughts like ZOOKEEPER-901. This issue would be a good place to start discussing thoughts.",[],Bug,ZOOKEEPER-2118,Major,ysl871308,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,CLONE - Netty for quorum communication,2015-04-01T05:52:12.000+0000,[],2.0
,"[<JIRA Component: name='c client', id='12312380'>]",2015-02-13T15:13:45.000+0000,Bruno Gauthier,"Hi All,

Under WIndows 8.1 and 2012, using the ZooKeeper C client 3.5.0, when running my ZooKeeper client, just after the ZooKeeper Client is connecting with the ZooKeeper server, the ZooKeeper server is generating a “caught end of stream” exception and deciding my is ZooKeeper client is not responsive: Zookeeper.c::check_events, line 2298: ESTALE. (see log below).

This problem systematically appears if the ZooKeeper DLL is NOT link statically with the Visual Studio debug version of the threaded runtime library.
This is reproducible 10/10

In clear, Windows ZooKeeper C client will works only if you link your ZooKeeper DLL with the switch ""/MTd"" (see VS Studio->Project->Configuration properties->C/C++->Code generation->runtime library)
 
Thanks
 
Bruno
 
========================================
ZooKeeper server log
========================================
 
2015-02-06 13:19:57,552 [myid:vgcclustermgr] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:31000:NIOServerCnxnFactory@197] - Accepted socket connection from /10.1.200.237:63499
2015-02-06 13:19:57,553 [myid:vgcclustermgr] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:31000:ZooKeeperServer@868] - Client attempting to establish new session at /10.1.200.237:63499
2015-02-06 13:19:57,554 [myid:vgcclustermgr] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:31000:NIOServerCnxnFactory@197] - Accepted socket connection from /10.1.200.237:63500
2015-02-06 13:19:57,554 [myid:vgcclustermgr] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:31000:ZooKeeperServer@868] - Client attempting to establish new session at /10.1.200.237:63500
2015-02-06 13:19:57,555 [myid:vgcclustermgr] - INFO  [SyncThread:0:ZooKeeperServer@617] - Established session 0x14b5bfcba7b0409 with negotiated timeout 80000 for client /10.1.200.237:63499
2015-02-06 13:19:57,555 [myid:vgcclustermgr] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:31000:NIOServerCnxn@357] - caught end of stream exception
EndOfStreamException: Unable to read additional data from client sessionid 0x14b5bfcba7b0409, likely client has closed socket
        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)
        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
        at java.lang.Thread.run(Thread.java:744)
2015-02-06 13:19:57,555 [myid:vgcclustermgr] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:31000:NIOServerCnxn@1007] - Closed socket connection for client /10.1.200.237:63499 which had sessionid 0x14b5bfcba7b0409
2015-02-06 13:19:57,559 [myid:vgcclustermgr] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:31000:NIOServerCnxnFactory@197] - Accepted socket connection from /10.1.200.237:63501
",[],Bug,ZOOKEEPER-2117,Critical,Bruno Gauthier,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"""caught end of stream"", server: ""Stale state"" of a Zk client just after connecting",2016-03-03T05:59:04.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",2.0
Surendra Singh Lilhore,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='scripts', id='12312384'>]",2015-02-11T23:48:44.000+0000,Maxim Novikov,"This doc http://zookeeper.apache.org/doc/r3.1.2/zookeeperStarted.html (""Connecting to ZooKeeper"" section) says:

Once ZooKeeper is running, you have several options for connection to it:

Java: Use
bin/zkCli.sh 127.0.0.1:2181

In fact, it doesn't work that way. I am running ZooKeeper with a different port to listen to client connections (2888), and this command

{code}
bin/zkCli.sh 127.0.0.1:2888
{code}

is still trying to connect to 2181.

{code:title=output|borderStyle=solid}
Connecting to localhost:2181
2015-02-11 15:38:14,415 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2015-02-11 15:38:14,421 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=localhost
2015-02-11 15:38:14,421 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.7.0_17
2015-02-11 15:38:14,424 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation
2015-02-11 15:38:14,424 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/usr/java/jdk1.7.0_17/jre
2015-02-11 15:38:14,424 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/opt/zookeeper-3.4.6/bin/../build/classes:/opt/zookeeper-3.4.6/bin/../build/lib/*.jar:/opt/zookeeper-3.4.6/bin/../lib/slf4j-log4j12-1.6.1.jar:/opt/zookeeper-3.4.6/bin/../lib/slf4j-api-1.6.1.jar:/opt/zookeeper-3.4.6/bin/../lib/netty-3.7.0.Final.jar:/opt/zookeeper-3.4.6/bin/../lib/log4j-1.2.16.jar:/opt/zookeeper-3.4.6/bin/../lib/jline-0.9.94.jar:/opt/zookeeper-3.4.6/bin/../zookeeper-3.4.6.jar:/opt/zookeeper-3.4.6/bin/../src/java/lib/*.jar:../conf::/usr/share/antlr3/lib/antlr-3.5-complete-no-st3.jar
2015-02-11 15:38:14,425 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2015-02-11 15:38:14,425 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/tmp
2015-02-11 15:38:14,425 [myid:] - INFO  [main:Environment@100] - Client environment:java.compiler=<NA>
2015-02-11 15:38:14,425 [myid:] - INFO  [main:Environment@100] - Client environment:os.name=Linux
2015-02-11 15:38:14,425 [myid:] - INFO  [main:Environment@100] - Client environment:os.arch=amd64
2015-02-11 15:38:14,426 [myid:] - INFO  [main:Environment@100] - Client environment:os.version=3.8.0-41-generic
2015-02-11 15:38:14,426 [myid:] - INFO  [main:Environment@100] - Client environment:user.name=mnovikov
2015-02-11 15:38:14,426 [myid:] - INFO  [main:Environment@100] - Client environment:user.home=/home/mnovikov
2015-02-11 15:38:14,426 [myid:] - INFO  [main:Environment@100] - Client environment:user.dir=/opt/zookeeper-3.4.6/bin
2015-02-11 15:38:14,428 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@3107eafc
Welcome to ZooKeeper!
2015-02-11 15:38:14,471 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@975] - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2015-02-11 15:38:14,479 [myid:] - WARN  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1102] - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
{code}

PS1 I can connect to ZK at 2888 using ZK Java client from code specifying the correct port with no issues. But CLI seems just to ignore the provided host:port parameter.

PS2 Tried to run it with the pre-defined ZOOCFGDIR environment variable (to point to the path with the config file where the client port is set to 2888). No luck, same results as shown above.","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2116,Critical,Maxim Novikov,Implemented,2015-05-28T09:50:59.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zkCli.sh doesn't honor host:port parameter,2015-05-29T03:51:06.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",4.0
Biju Nair,"[<JIRA Component: name='scripts', id='12312384'>]",2015-02-06T00:44:40.000+0000,Manikandan Narayanaswamy,"while testing single user mode that the initialize command succeeds even though it didn't in case of permission errors on the data directory:
{code}
....
+ exec /usr/lib/zookeeper/bin/zkServer-initialize.sh --myid=1
mkdir: cannot create directory `/var/lib/zookeeper/version-2': Permission denied
mkdir: cannot create directory `/var/lib/zookeeper/version-2': Permission denied
/usr/lib/zookeeper/bin/zkServer-initialize.sh: line 112: /var/lib/zookeeper/myid: Permission denied
{code}","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-2115,Trivial,Manikandan Narayanaswamy,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Initialize command succeeds even though it didn't in case of permission errors on the data directory,2022-02-03T08:36:25.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Tim Crowder,"[<JIRA Component: name='c client', id='12312380'>]",2015-02-05T08:06:56.000+0000,Tim Crowder,"Some jute generated functions (e.g. allocate_ACL_vector) that should be publicly exported are given local (vs global) linkage. This is due to an incomplete regex for EXPORT_SYMBOLS  in the C Makefile.am.

Without allocate_ACL_vector it's not possible to set ACL lists from C.
The regex should include ""allocate_"" :

EXPORT_SYMBOLS = '(zoo_|zookeeper_|zhandle|Z|format_log_message|log_message|logLevel|deallocate_|allocate_|zerror|is_unrecoverable)'
","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2114,Major,Tim Crowder,Fixed,2015-02-22T21:12:35.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,jute generated allocate_* functions are not externally visible,2015-02-22T22:52:14.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Hongchao Deng,[],2015-01-29T21:38:03.000+0000,Hongchao Deng,This is a SSL feature on netty client.,[],Bug,ZOOKEEPER-2113,Major,Hongchao Deng,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,SSL support for ClientCnxnSocketNetty,2015-01-29T21:55:51.000+0000,[],2.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2015-01-28T01:00:32.000+0000,Hongchao Deng,Add Netty option to replace NIO for quorum communication.,[],Bug,ZOOKEEPER-2112,Major,Hongchao Deng,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Netty for quorum communication,2015-04-01T05:52:12.000+0000,[],2.0
Hongchao Deng,"[<JIRA Component: name='java client', id='12312381'>]",2015-01-27T19:21:08.000+0000,Hongchao Deng,"In ClientCnxn.queuePacket, it checks variables of state and closing and then make decisions. There is toctou race in queuePacket():
{code}
        if (!state.isAlive() || closing) {
            conLossPacket(packet);
        } else {
            ...
        }
{code}

A possible race:
in SendThread.run():
{code}
  while (state.isAlive()) {
    ...
  }
  cleanup();
{code}

When it checks in queuePacket(), state is still alive. Then state isn't alive, SendThread.run() cleans up outgoingQueue. Then queuePacket adds packet to outgoingQueue. The packet should be waken up with exception. But it won't at this case.","[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2111,Major,Hongchao Deng,Fixed,2015-01-31T07:11:54.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Not isAlive states should be synchronized in ClientCnxn,2020-02-22T02:55:02.000+0000,[],5.0
Surendra Singh Lilhore,[],2015-01-26T12:29:40.000+0000,Emmanuel Bourg,"There is a minor typo in {{src/c/src/load_gen.c}}, ""Succesfully"" should be spelled ""Successfully""","[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2109,Trivial,Emmanuel Bourg,Fixed,2015-03-16T05:28:24.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Typo in src/c/src/load_gen.c,2015-03-16T11:24:05.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
,[],2015-01-26T12:19:14.000+0000,Emmanuel Bourg,"Hi,

Debian and Fedora have a patch fixing a compilation failure in ZkAdaptor.cc but it doesn't appear to be fixed in the upcoming version 3.5.0. This issue is similar to ZOOKEEPER-470 and ZOOKEEPER-1795.

The error is :
{code}
g++ -DHAVE_CONFIG_H -I. -I..   -D_FORTIFY_SOURCE=2 -I/home/ebourg/packaging/zookeeper/src/contrib/zktreeutil/../../c/include -I/home/ebourg/packaging/zookeeper/src/contrib/zktreeutil/../../c/generated -I../include -I/usr/local/include -I/usr/include -I/usr/include/libxml2 -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -MT ZkAdaptor.o -MD -MP -MF .deps/ZkAdaptor.Tpo -c -o ZkAdaptor.o ZkAdaptor.cc
ZkAdaptor.cc: In member function ‘void zktreeutil::ZooKeeperAdapter::reconnect()’:
ZkAdaptor.cc:220:21: error: ‘sleep’ was not declared in this scope
             sleep (1);
{code}

This is fixed by including unistd.h in ZkAdaptor.cc or  ZkAdaptor.h

The Debian patch:
https://sources.debian.net/src/zookeeper/3.4.5%2Bdfsg-2/debian/patches/ftbfs-gcc-4.7.diff/

and the Fedora patch:
http://pkgs.fedoraproject.org/cgit/zookeeper.git/tree/zookeeper-3.4.5-zktreeutil-gcc.patch
",[],Bug,ZOOKEEPER-2108,Minor,Emmanuel Bourg,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Compilation error in ZkAdaptor.cc with GCC 4.7 or later,2019-10-11T15:02:40.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",1.0
,[],2015-01-13T16:28:14.000+0000,Robert Joseph Evans,"I tried looking through existing JIRA for something like this, but the closest I came was ZOOKEEPER-2104.  It looks very similar, but I don't know if it really is the same thing.  Essentially we had a 5 node ensemble for a large storm cluster.  For a few of the nodes at the same time they get an error that looks like.

{code}
WARN  [RecvWorker:2:QuorumCnxManager$RecvWorker@762] - Connection broken for id 2, my id = 4, error = 
java.io.EOFException
      at java.io.DataInputStream.readInt(DataInputStream.java:392)
      at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:747)
WARN  [RecvWorker:2:QuorumCnxManager$RecvWorker@765] - Interrupting SendWorker
WARN  [SendWorker:2:QuorumCnxManager$SendWorker@679] - Interrupted while waiting for message on queue
java.lang.InterruptedException
     at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2017)
      at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2095)
      at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:389)
      at org.apache.zookeeper.server.quorum.QuorumCnxManager.pollSendQueue(QuorumCnxManager.java:831)
      at org.apache.zookeeper.server.quorum.QuorumCnxManager.access$500(QuorumCnxManager.java:62)
     at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:667)
WARN  [SendWorker:2:QuorumCnxManager$SendWorker@688] - Send worker leaving thread
WARN  [QuorumPeer[myid=4]/0.0.0.0:50512:Follower@89] - Exception when following the leader
java.net.SocketException: Connection reset
     at java.net.SocketInputStream.read(SocketInputStream.java:189)
     at java.net.SocketInputStream.read(SocketInputStream.java:121)
     at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
     at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
     at java.io.DataInputStream.readInt(DataInputStream.java:387)
     at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
     at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:83)
     at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:108)
     at org.apache.zookeeper.server.quorum.Learner.readPacket(Learner.java:152)
     at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:85)
    at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:740)
INFO  [QuorumPeer[myid=4]/0.0.0.0:50512:Follower@166] - shutdown called
java.lang.Exception: shutdown Follower
      at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:166)
     at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:744)
{code}

After that all of the connections are shut down
{code}
INFO  [QuorumPeer[myid=4]/0.0.0.0:50512:NIOServerCnxn@1001] - Closed socket connection for client ...
{code}

but it does not manage to have the JVM shut down

{code}
INFO  [QuorumPeer[myid=4]/0.0.0.0:50512:FollowerZooKeeperServer@139] - Shutting down
INFO  [QuorumPeer[myid=4]/0.0.0.0:50512:ZooKeeperServer@419] - shutting down
INFO  [QuorumPeer[myid=4]/0.0.0.0:50512:FollowerRequestProcessor@105] - Shutting down
INFO  [QuorumPeer[myid=4]/0.0.0.0:50512:CommitProcessor@181] - Shutting down
INFO  [FollowerRequestProcessor:4:FollowerRequestProcessor@95] - FollowerRequestProcessor exited loop!
INFO  [QuorumPeer[myid=4]/0.0.0.0:50512:FinalRequestProcessor@415] - shutdown of request processor complete
INFO  [CommitProcessor:4:CommitProcessor@150] - CommitProcessor exited loop!
WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:50512:NIOServerCnxn@354] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:50512:NIOServerCnxn@1001] - Closed socket connection for client /... (no session established for client)
INFO  [QuorumPeer[myid=4]/0.0.0.0:50512:SyncRequestProcessor@175] - Shutting down
INFO  [SyncThread:4:SyncRequestProcessor@155] - SyncRequestProcessor exited!
INFO  [QuorumPeer[myid=4]/0.0.0.0:50512:QuorumPeer@670] - LOOKING
{code}

after that all connections to that node initiate, and then are shut down with ZooKeeperServer not running.  It seems to stay in this state indefinitely until the process is manually restarted.  After that it recovers.

We have seen this happen on multiple servers at the same time resulting in the entire ensemble being unusable.",[],Bug,ZOOKEEPER-2106,Critical,Robert Joseph Evans,Invalid,2015-01-13T16:54:12.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Error when reading from leader causes JVM to hang,2015-01-13T16:54:12.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",1.0
,[],2015-01-10T01:11:15.000+0000,Ted Yu,"{code}
        final PrintWriter pwriter = new PrintWriter(
                new BufferedWriter(new SendBufferWriter()));
{code}
pwriter should be closed upon return from the method.",[],Bug,ZOOKEEPER-2105,Minor,Ted Yu,Not A Problem,2015-01-19T05:22:10.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,PrintWriter left unclosed in NIOServerCnxn#checkFourLetterWord,2015-01-19T05:22:10.000+0000,[],2.0
,"[<JIRA Component: name='server', id='12312382'>]",2015-01-10T00:48:51.000+0000,Benjamin Jaton,"In a 3 nodes ensemble, suddenly all the nodes seem to fail, displaying ""ZooKeeper is not running"" messages.
Not retry seems to be happening after that.

This a request to understand what happened and probably to improve the logs when it does.

See logs below:

NODE1:

-- no log for several days before this --
2015-01-04 16:18:22,259 [myid:1] - WARN  [SyncThread:1:FileTxnLog@321] - fsync-ing the write ahead log in SyncThread:1 took 11024ms which will adversely effect operation latency. See the ZooKeeper troubleshooting guide
2015-01-04 16:18:22,380 [myid:1] - WARN  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:Follower@89] - Exception when following the leader
java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:392)
        at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
        at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:83)
        at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:103)
        at org.apache.zookeeper.server.quorum.Learner.readPacket(Learner.java:153)
        at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:85)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:786)
2015-01-04 16:18:23,384 [myid:1] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@362] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
2015-01-04 16:18:23,492 [myid:1] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@362] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
2015-01-04 16:18:24,060 [myid:1] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@362] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running


NODE2:

-- no log for several days before this --
2015-01-04 16:18:21,899 [myid:3] - WARN  [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:2181:Follower@89] - Exception when following the leader
java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:392)
        at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
        at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:83)
        at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:103)
        at org.apache.zookeeper.server.quorum.Learner.readPacket(Learner.java:153)
        at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:85)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:786)
2015-01-04 16:18:22,760 [myid:3] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@362] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
2015-01-04 16:18:22,801 [myid:3] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@362] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
2015-01-04 16:18:22,886 [myid:3] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@362] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running


NODE3 (leader):

-- no log for several days before this --
2015-01-04 16:18:21,897 [myid:2] - WARN  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:2181:LearnerHandler@687] - Closing connection to peer due to transaction timeout.
2015-01-04 16:18:21,898 [myid:2] - WARN  [LearnerHandler-/204.53.107.249:43402:LearnerHandler@646] - ******* GOODBYE /204.53.107.249:43402 ********
2015-01-04 16:18:21,905 [myid:2] - WARN  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:2181:LearnerHandler@687] - Closing connection to peer due to transaction timeout.
2015-01-04 16:18:21,907 [myid:2] - WARN  [LearnerHandler-/204.53.107.247:45953:LearnerHandler@646] - ******* GOODBYE /204.53.107.247:45953 ********
2015-01-04 16:18:21,918 [myid:2] - WARN  [LearnerHandler-/204.53.107.247:45953:LearnerHandler@658] - Ignoring unexpected exception
java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)
        at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)
        at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)
        at org.apache.zookeeper.server.quorum.LearnerHandler.shutdown(LearnerHandler.java:656)
        at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:649)
2015-01-04 16:18:23,003 [myid:2] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@362] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
2015-01-04 16:18:23,007 [myid:2] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@362] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
2015-01-04 16:18:23,115 [myid:2] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@362] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running",[],Bug,ZOOKEEPER-2104,Major,Benjamin Jaton,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Sudden crash of all nodes in the cluster,2021-11-25T07:01:56.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",26.0
,"[<JIRA Component: name='server', id='12312382'>]",2015-01-07T19:02:50.000+0000,Raúl Gutiérrez Segalés,"Using zab-dump (https://github.com/twitter/zktraffic/pull/11), I am seeing this in a prod cluster running 3.5.0 + patches:

{noformat}
QuorumPacket(
     timestamp=18:45:35:962873,
     src=10.0.1.1:2889,
     type=commitandactivate,
     zxid=292104572694,
     length=114
)
QuorumPacket(
     timestamp=18:45:35:962876,
     src=10.0.1.1:2889,
     type=commitandactivate,
     zxid=292104572694,
     length=114
)
QuorumPacket(
     timestamp=18:45:35:962893,
     src=10.0.1.1:2889,
     type=commitandactivate,
     zxid=292104572694,
     length=114
)
....
{noformat}

From a ~5min dump, I see ~80k QuorumPackets of which ~50k are commitandactivate packets! Sounds like some sort of loop.

Any ideas [~shralex]?

cc: [~hdeng], [~fpj]",[],Bug,ZOOKEEPER-2102,Major,Raúl Gutiérrez Segalés,Invalid,2015-01-07T23:07:28.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,commitandactivate messages spamming the quorum,2015-01-07T23:07:28.000+0000,[],2.0
Andor Molnar,"[<JIRA Component: name='jute', id='12312385'>]",2015-01-04T10:13:32.000+0000,Shaohui Liu,"*Problem*
For multi operation, PrepRequestProcessor may produce a large transaction whose size may be larger than the max buffer size of jute. There is check of buffer size in readBuffer method  of BinaryInputArchive, but no check in writeBuffer method  of BinaryOutputArchive, which will cause that 

1, Leader can sync transaction to txn log and send the large transaction to the followers, but the followers failed to read the transaction and can't sync with leader.
{code}
2015-01-04,12:42:26,474 WARN org.apache.zookeeper.server.quorum.Learner: [myid:2] Exception when following the leader
java.io.IOException: Unreasonable length = 2054758
        at org.apache.jute.BinaryInputArchive.readBuffer(BinaryInputArchive.java:100)
        at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:85)
        at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:108)
        at org.apache.zookeeper.server.quorum.Learner.readPacket(Learner.java:152)
        at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:85)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:740)
2015-01-04,12:42:26,475 INFO org.apache.zookeeper.server.quorum.Learner: [myid:2] shutdown called
java.lang.Exception: shutdown Follower
        at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:166)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:744)
{code}

2, The leader lose all followers, which trigger the leader election. The old leader will become leader again for it has up-to-date data.
{code}
2015-01-04,12:42:28,502 INFO org.apache.zookeeper.server.quorum.Leader: [myid:3] Shutting down
2015-01-04,12:42:28,502 INFO org.apache.zookeeper.server.quorum.Leader: [myid:3] Shutdown called
java.lang.Exception: shutdown Leader! reason: Only 1 followers, need 2
        at org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:496)
        at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:471)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:753)
{code}
3, The leader can not load the transaction from the txn log for the length of data is larger than the max buffer of jute.
{code}

2015-01-04,12:42:31,282 ERROR org.apache.zookeeper.server.quorum.QuorumPeer: [myid:3] Unable to load database on disk
java.io.IOException: Unreasonable length = 2054758
        at org.apache.jute.BinaryInputArchive.readBuffer(BinaryInputArchive.java:100)
        at org.apache.zookeeper.server.persistence.Util.readTxnBytes(Util.java:233)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:602)
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:157)
        at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:223)
        at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:417)
        at org.apache.zookeeper.server.quorum.QuorumPeer.getLastLoggedZxid(QuorumPeer.java:546)
        at org.apache.zookeeper.server.quorum.FastLeaderElection.getInitLastLoggedZxid(FastLeaderElection.java:690)
        at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:737)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:716)
{code}

The zookeeper service will be unavailable until we enlarge the jute.maxbuffer and restart zookeeper hbase cluster.

*Solution*
Add buffer size check in BinaryOutputArchive to avoid large transaction be written to log and sent to followers.

But I am not sure if there are side-effects of throwing an IOException in BinaryOutputArchive  and RequestProcessors
","[<JIRA Version: name='3.5.4', id='12340141'>]",Bug,ZOOKEEPER-2101,Major,Shaohui Liu,Cannot Reproduce,2018-03-26T13:30:59.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Transaction larger than max buffer of jute makes zookeeper unavailable,2019-12-19T23:01:58.000+0000,"[<JIRA Version: name='3.4.4', id='12319841'>]",16.0
,"[<JIRA Component: name='java client', id='12312381'>]",2014-12-29T23:59:11.000+0000,Gregory Chanan,"I found this in 3.4.5, but a quick perusal of the code suggests this exists in later versions as well.

Setup: I'm running some ZooKeeper SASL tests with Hadoop's MiniKDC under Solr's test framework, which checks for things like thread leaks.  The thread leak checker is complaining about the Login thread, which is created but never shut down.  It's started here:
https://github.com/apache/zookeeper/blob/6ebd23b32d2cf606e01906bee4460bf79eb7f3fa/src/java/main/org/apache/zookeeper/client/ZooKeeperSaslClient.java#L227 and you can verify via reading the code that it is never shut down.

This may be intentional, because the Login object is static, so it is probably supposed to stick around for the lifetime of the application.  This is not great for a test setup, where the idea is that a cluster and all associated clients are started/stopped for each test suite.  You wouldn't want either:
1) a thread stick around doing nothing, or
2) sticking around doing something (because it makes the first suite that happens to run behave differently than subsequents suites).

in addition, this only happens with SASL clients, so we'd want to only turn off the leak checker if we are running under SASL (so we don't miss other leaked threads), which is a bit more complexity than I would like.

I'd be happy with a function I could call to say ""I'm really done, close down  everything, even in the Login thread"" or some automatic way of doing it.",[],Bug,ZOOKEEPER-2100,Major,Gregory Chanan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeperSaslClient doesn't shut down Login thread,2015-01-11T16:54:58.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",3.0
Martin Kuchta,"[<JIRA Component: name='server', id='12312382'>]",2014-12-29T21:35:58.000+0000,Santeri (Santtu) Voutilainen,"When a learner sync's with the leader, it is possible for the Leader to send the learner a DIFF that does NOT contain all the transactions between the learner's zxid and that of the leader's zxid thus resulting in a corruption datatree on the learner.
For this to occur, the leader must have sync'd with a previous leader using a SNAP and the zxid requested by the learner must still exist in the current leader's txnlog files.
This issue was introduced by ZOOKEEPER-1413.

*Scenario*
A sample sequence in which this issue occurs:
# Hosts H1 and H2 disconnect from the current leader H3 (crash, network partition, etc).  The last zxid on these hosts is Z1.
# Additional transactions occur on the cluster resulting in the latest zxid being Z2.
# Host H1 recovers and connects to H3 to sync and sends Z1 as part of its FOLLOWERINFO or OBSERVERINFO packet.
# The leader, H3, decides to send a SNAP because a) it does not have the necessary records in the in-mem committed log, AND b) the size of the required txnlog to send it larger than the limit.
# Host H1 successfully sync's with the leader (H3). At this point H1's txnlogs have records up to and including Z1 as well as Z2 and up.  It does NOT have records between Z1 and Z2.
# Host H3 fails; a leader election occurs and H1 is chosen as the leader
# Host H2 recovers and connects to H1 to sync and sends Z1 in its FOLLOWERINFO/OBSERVERINFO packet
# The leader, H1, determines it can send a DIFF.  It concludes this because although it does not have the necessary records in its in-memory commit log, it does have Z1 in its txnlog and the size of the log is less than the limit.  H1 ends up with a different size calculation than H3 because H1 is missing all the records between Z1 and Z2 so it has less log to send.
# H2 receives the DIFF and applies the records to its data tree. Depending on the type of transactions that occurred between Z1 and Z2 it may not hit any errors when applying these records.

H2 now has a corrupted view of the data tree because it is missing all the changes made by the transactions between Z1 and Z2.

*Recovery*
The way to recover from this situation is to delete the data/snap directory contents from the affected hosts and have them resync with the leader at which point they will receive a SNAP since they will appear as empty hosts.

*Workaround*
A quick workaround for anyone concerned about this issue is to disable sync from the txnlog by changing the database size limit to 0.  This is a code change as it is not a configurable setting.

*Potential fixes*
There are several ways of fixing this.  A few of options:
* Delete all snaps and txnlog files on a host when it receives a SNAP from the leader
* Invalidate sync from txnlog after receiving a SNAP. This state must also be persisted on-disk so that the txnlogs with the gap cannot be used to provide a DIFF even after restart.  A couple ways in which the state could be persisted:
** Write a file (for example: loggap.<zxid>) in the data dir indicating that the host was sync'd with a SNAP and thus txnlogs might be missing. Presence of these files would be checked when reading txnlogs.
** Write a new record into the txnlog file as ""sync'd-by-snap-from-leader"" marker. Readers of the txnlog would then check for presence of this record when iterating through it and act appropriately.",[],Bug,ZOOKEEPER-2099,Major,Santeri (Santtu) Voutilainen,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Using txnlog to sync a learner can corrupt the learner's datatree,2016-10-25T20:47:05.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.6.0', id='12326518'>]",7.0
Vitaly Stakhovsky,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='c client', id='12312380'>]",2014-12-19T17:41:29.000+0000,Vitaly Stakhovsky,"It reports:
warning C4005: 'EWOULDBLOCK' : macro redefinition
warning C4005: 'EINPROGRESS' : macro redefinition

In MSVS 2010+, these constants are now in <errno.h>.
What's worse, they have different numeric values.

Possible fix:
In ""src/c/include/winconfig.h"" :
#if _MSC_VER < 1600
#define EWOULDBLOCK WSAEWOULDBLOCK
#define EINPROGRESS WSAEINPROGRESS
#endif
","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2096,Major,Vitaly Stakhovsky,Fixed,2015-06-02T20:44:24.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,C client builds with incorrect error codes in VisualStudio 2010+,2016-03-03T01:30:17.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",5.0
,[],2014-11-27T13:03:29.000+0000,Shaohui Liu,"In our 5 node zk cluster, we found a zk node always can not be connected. From the stack we found the ZooKeeperServer hung at waiting the server to be running. But the node is running normally and synced with the leader.

{code}
$ ./zkCli.sh -server 10.101.10.67:11000 ls /
2014-11-27 20:57:11,843 [myid:] - WARN  [main-SendThread(lg-com-master02.bj:11000):ClientCnxn$SendThread@1089] - Session 0x0 for server lg-com-master02.bj/10.101.10.67:11000, unexpected error, closing socket connection and attempting reconnect
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:68)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:353)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068)
Exception in thread ""main"" org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1469)
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1497)
	at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:726)
	at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:594)
	at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:355)
	at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:283)
{code}

ZooKeeperServer stack
{code}
""NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11000"" daemon prio=10 tid=0x00007f60143f7800 nid=0x31fd in Object.wait() [0x00007f5fd4678000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at org.apache.zookeeper.server.ZooKeeperServer.submitRequest(ZooKeeperServer.java:634)
        - locked <0x00000007602756a0> (a org.apache.zookeeper.server.quorum.FollowerZooKeeperServer)
        at org.apache.zookeeper.server.ZooKeeperServer.submitRequest(ZooKeeperServer.java:626)
        at org.apache.zookeeper.server.ZooKeeperServer.createSession(ZooKeeperServer.java:525)
        at org.apache.zookeeper.server.ZooKeeperServer.processConnectRequest(ZooKeeperServer.java:841)
        at org.apache.zookeeper.server.NIOServerCnxn.readConnectRequest(NIOServerCnxn.java:410)
        at org.apache.zookeeper.server.NIOServerCnxn.readPayload(NIOServerCnxn.java:200)
        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:236)
        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
        at java.lang.Thread.run(Thread.java:662)
{code}

Any suggestions about this problem? Thanks.
",[],Bug,ZOOKEEPER-2092,Major,Shaohui Liu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,A zk instance can not be connected for ZooKeeperServer is not running,2014-11-28T11:29:26.000+0000,"[<JIRA Version: name='3.4.4', id='12319841'>]",3.0
Rakesh Radhakrishnan,"[<JIRA Component: name='java client', id='12312381'>]",2014-11-26T01:06:21.000+0000,Cheng,"When SASL authentication is enabled, the ZooKeeper client will finally call ClientCnxnSocketNIO#sendPacket(Packet p) to send a packet to server:
@Override
void sendPacket(Packet p) throws IOException {
    SocketChannel sock = (SocketChannel) sockKey.channel();
    if (sock == null) {
        throw new IOException(""Socket is null!"");
    }
    p.createBB();
    ByteBuffer pbb = p.bb;
    sock.write(pbb);
}

One problem I can see is that the sock is non-blocking, so when the sock's output buffer is full(theoretically), only part of the Packet is sent out and the communication will break.","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-2091,Major,Cheng,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Possible logic error in ClientCnxnSocketNIO,2022-02-03T08:36:22.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",5.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2014-11-25T14:54:18.000+0000,Hector,"The docs on the website are full of TBDs since a long time ago. While there is not a full lack of docs, and you can get going with what there is, the main general-purpose entry points are not too polished and they give the impression ZK is not very well maintained to newcomers and anyone who just wants to see how ZK is progressing and refresh concepts.

The ZK overview doc (http://zookeeper.apache.org/doc/trunk/zookeeperOver.html) is supposed to be a first entry point for new Zookeeper users and it is full of _\[tbd\]s_:

{quote}
When the session ends the znode is deleted. Ephemeral nodes are useful when you want to implement \[tbd\].
{quote}
{quote}
And if the connection between the client and one of the Zoo Keeper servers is broken, the client will receive a local notification. These can be used to \[tbd\].
{quote}
{quote}
    Timeliness - The clients view of the system is guaranteed to be up-to-date within a certain time bound.

For more information on these, and how they can be used, see \[tbd\] 
{quote}
{quote}
For a more in-depth discussion on these, and how they can be used to implement higher level operations, please refer to \[tbd\]
{quote}

{quote}
Some distributed applications have used it to: \[tbd: add uses from white paper and video presentation.\] For more information, see \[tbd\]
{quote}

{quote}
These znodes exists as long as the session that created the znode is active. When the session ends the znode is deleted. Ephemeral nodes are useful when you want to implement \[tbd\].
{quote}

The second entry point, ""Getting Started"" (http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html)
{quote}
\[tbd: what is the other config param?\]
{quote}

Programmers guide (http://zookeeper.apache.org/doc/trunk/zookeeperProgrammers.html)
{quote}
""If the version it supplies doesn't match the actual version of the data, the update will fail. (This behavior can be overridden. For more information see... )\[tbd...\]""
{quote}
{quote}
Connecting to ZooKeeper

Read Operations

Write Operations

Handling Watches

Miscelleaneous ZooKeeper Operations

Program Structure, with Simple Example

\[tbd\]
{quote}

{quote}
 ZooKeeper Whitepaper \[tbd: find url\]

    The definitive discussion of ZooKeeper design and performance, by Yahoo! Research
API Reference \[tbd: find url\]

    The complete reference to the ZooKeeper API
{quote}


Administration guide (http://zookeeper.apache.org/doc/trunk/zookeeperAdmin.html)
{quote}
Provisioning

Things to Consider: ZooKeeper Strengths and Limitations

Administering

{quote}
{quote}
 TBD - tuning options for netty - currently there are none that are netty specific but we should add some. Esp around max bound on the number of reader worker threads netty creates.

TBD - how to manage encryption

TBD - how to manage certificates 
{quote}

Since it is not a big deal to fix these, I think it is worth it to spend some hours doing it.",[],Bug,ZOOKEEPER-2090,Trivial,Hector,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Fix Zookeeper docs ""To be done"" notices",2016-09-13T18:32:06.000+0000,[],2.0
Saurabh Chhajed,"[<JIRA Component: name='java client', id='12312381'>]",2014-11-21T08:37:20.000+0000,Cheng,"In org.apache.zookeeper.ZooKeeper.ZKWatchManager#materialize(), even if the defaultWatcher is null, it is still be added into the Set and returned. This would cause a lot of annoying error log at org.apache.zookeeper.ClientCnxn.EventThread#processEvent as below:
       2014-11-21 15:21:23,279 - ERROR - [main-EventThread:ClientCnxn$EventThread@524] - Error while calling watcher
       java.lang.NullPointerException
           at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
           at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)

It can be simply fixed by having a null check in ZKWatchManager.",[],Bug,ZOOKEEPER-2086,Minor,Cheng,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Unnecessary error log when defaultWatcher  is not set for ZooKeeper client,2014-12-16T20:50:54.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",4.0
,"[<JIRA Component: name='leaderElection', id='12312378'>]",2014-11-14T22:42:30.000+0000,Tianyin Xu,"The parameter, electionAlgo, is supposed to be 0--3. However, when I mistyped the value in my zoo.cfg (I'm stupid), ZK falls into a dead loop and starts filling up the entire disk which millions of the follow 2 lines...

2014-11-14 14:28:44,588 \[myid:3\] - INFO  \[QuorumPeer\[myid=3\]/0:0:0:0:0:0:0:0:2183:QuorumPeer@714\] - LOOKING
2014-11-14 14:28:44,588 \[myid:3\] - WARN  \[QuorumPeer\[myid=3\]/0:0:0:0:0:0:0:0:2183:QuorumPeer@764\] - Unexpected exception
java.lang.NullPointerException
    at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:762)

The error rooted in createElectionAlgorithm() where an invalid setting leads to null for the Election object. Then, in the while look in run(), it causes null-pointer de-referencing which is captured but is not handled well.

I think our should check the setting of electionAlg in the very beginning to make sure it's a valid setting, instead of using it at runtime and cause the unfortunate things.

Let me know if you wanna a patch. I'd like to check it in the parseProperties() function in QuorumPeerConfig.java.

Thanks!",[],Bug,ZOOKEEPER-2082,Minor,Tianyin Xu,Fixed,2018-10-21T07:28:25.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Mistype of electionAlgo can fill out your disk in minutes,2018-10-21T07:28:25.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",6.0
,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='quorum', id='12312379'>]",2014-11-14T01:27:51.000+0000,Matan,"I noticed a situation when one of our 3-node clusters on RHEL lost a machine due to PSU failure. The remaining two nodes failed to complete leader election and would continually restart the leader election process.
Restarting the nodes would not help and they would reach the same exact state.

This was curious so I spent some time and managed to reproduce this on my local machine and found what looks like the main factor:
When a node is unreachable (timeouts), this somehow causes the election process to get out of sync.  Once a leader is decided, the follower tries to connect to the leader only when the leader is not listening.
Then the follower gives up and the process starts again ad infinitum.

How to reproduce on a local machine:

1. Setup up a 3 node cluster of ZK.  Note we only need to set up 2 boxes since we'll just make the third unreachable:

MyId 1:

server.1=MyMachine:2881:3881
server.2=<Put any IP that we can block>:2882:3882
server.3=MyMachine:2883:3883

MyId 3:

server.1=MyMachine:2881:3881
server.2=<Put any IP that we can block>:2882:3882
server.3=MyMachine:2883:3883

Now set up a blackhole route for the IP you choose (Mac OSX, Linux is similar):
> route add -host <IP you selected> 127.0.0.1 -blackhole

Start your 2 nodes.  They will never reach quorum.

However, if I remove the blackhole route and just not start the 3rd instance (but the host is still reachable), it will work fine and quorum will be reached almost immediately.

It seems the difference between the “timeout” and a ""connection refused” makes all the difference somehow in the election process.

I verified this behavior on 3.4.6 and 3.3.6.
",[],Bug,ZOOKEEPER-2081,Major,Matan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Leader election cannot complete when a node is blackholed (unreachable) even when quorum is possible.,2015-10-29T18:33:26.000+0000,"[<JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.4.6', id='12323310'>]",13.0
Chris Nauroth,"[<JIRA Component: name='scripts', id='12312384'>]",2014-11-12T12:09:38.000+0000,metatech,"The script ""zkServer.sh"" contains a pattern (POSIX ""character class syntax"") which is not supported by ""grep"" on Solaris (both versions 10 and 11).

{code}
ZOO_DATADIR=""$(grep ""^[[:space:]]*dataDir"" ""$ZOOCFG"" | sed -e 's/.*=//')""
{code}

This results into the environment variable being set with an empty value, which later gives the following error : 

{code}
Starting zookeeper ... bin/zkServer.sh: line 114: /zookeeper_server.pid: Permission denied
{code}


The workaround is to simplify the pattern used by ""grep"" :

{code}
ZOO_DATADIR=""$(grep ""^dataDir"" ""$ZOOCFG"" | sed -e 's/.*=//')""
{code}

The same pattern is also used in the ""status"" command, which fails to read the ""clientPort"", which results into the following error :
{code}
Error contacting service. It is probably not running.
{code}
",[],Bug,ZOOKEEPER-2078,Minor,metatech,Duplicate,2015-05-01T05:57:46.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"zkServer.sh uses pattern unsupported by ""grep"" on Solaris",2015-05-01T05:57:47.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",2.0
,"[<JIRA Component: name='java client', id='12312381'>]",2014-11-07T20:22:03.000+0000,Charlie Helin,"Just happened to notice Utils.bufEquals(byte[], byte[]) as a rather large outlier (~7% CPU time) when running with an attached profiler.

By just simply switching the implementation to delegate directly to Arrays.equals(byte[], byte[]) the invocation disappears from the profile. The reason for this is that this is one of the methods which the JIT (not the interpreter) will generate an intrinsic for, using the builtin support of the CPU to do the check.

The fix is trivial

{code}
    public static boolean bufEquals(byte onearray[], byte twoarray[] ) {
       return Arrays.equals(onearray, twoarray);
    }
{code}",[],Bug,ZOOKEEPER-2075,Major,Charlie Helin,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Utils.bufEquals,2014-11-07T20:22:03.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",1.0
Abraham Fine,[],2014-11-07T09:20:00.000+0000,Surendra Singh Lilhore,"Linux@hghoulaslx406:/> $ZOOKEEPER_HOME/bin/zkCli.sh create /test ""test""
Created /test1
Linux@hghoulaslx406:/> echo $?
0
Linux@hghoulaslx406:/> $ZOOKEEPER_HOME/bin/zkCli.sh create /test ""test""
Node already exists: /test1
Linux@hghoulaslx406:/> echo $?
0
Linux@hghoulaslx406:/> $ZOOKEEPER_HOME/bin/zkCli.sh delete /test
Linux@hghoulaslx406:/> echo $?
0
Linux@hghoulaslx406:/> $ZOOKEEPER_HOME/bin/zkCli.sh delete /test
Node does not exist: /test1
Linux@hghoulaslx406:/> echo $?
0


Here for failed command it should return exit code 1","[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2074,Minor,Surendra Singh Lilhore,Fixed,2016-08-10T17:33:02.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Incorrect exit codes for ""./zkCli.sh cmd arg""",2017-05-18T03:44:03.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",10.0
Dave Gosselin,"[<JIRA Component: name='c client', id='12312380'>]",2014-11-04T17:10:49.000+0000,Dave Gosselin,"When running valgrind against a zookeeper client using the C API, I noticed an occasional memory leak on zookeeper_close.  I traced the issue to a regression added by fix ZOOKEEPER-804.  The attached patch fixes the regression and the associated memory leak.","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2073,Critical,Dave Gosselin,Fixed,2015-02-22T22:09:41.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Memory leak on zookeeper_close,2015-02-22T22:52:14.000+0000,"[<JIRA Version: name='3.6.0', id='12326518'>]",5.0
Hongchao Deng,"[<JIRA Component: name='server', id='12312382'>]",2014-10-28T22:19:53.000+0000,Hongchao Deng,"Currently, netty server is setting up child channel in this way:
{code}
 bootstrap.getPipeline().addLast(""servercnxnfactory"", channelHandler);
{code}

According to the [netty doc|http://netty.io/3.9/api/org/jboss/netty/bootstrap/ServerBootstrap.html],
bq. you cannot use this approach if you are going to open more than one Channels or run a server that accepts incoming connections to create its child channels.","[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2072,Major,Hongchao Deng,Fixed,2015-01-30T02:26:34.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Netty Server Should Configure Child Channel Pipeline By Specifying ChannelPipelineFactory,2015-01-30T11:13:59.000+0000,[],6.0
Wendy Smoak,[],2014-10-26T23:38:08.000+0000,Wendy Smoak,"The documentation needs to follow the Apache branding and trademark requirements:  http://www.apache.org/foundation/marks/pmcs.html

",[],Bug,ZOOKEEPER-2071,Major,Wendy Smoak,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Update docs for Apache branding and trademark requirements,2014-11-08T11:18:27.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",2.0
,[],2014-10-26T03:38:04.000+0000,Hongchao Deng,"If using Netty server (setting ""zookeeper.serverCnxnFactory"" to ""NettyServerCnxnFactory""), ServerCnxnTest.testServerCnxnExpiry always failed",[],Bug,ZOOKEEPER-2068,Major,Hongchao Deng,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ServerCnxnTest.testServerCnxnExpiry failed when using Netty server option,2014-11-01T15:57:09.000+0000,[],1.0
Jeremy Carroll,[],2014-10-25T01:40:51.000+0000,Jeremy Carroll,"When setting a dataDir in zoo.cfg that does not match /var/lib/zookeeper, the supplied init.d script failed to track the PID file. This change moves the logic that is present in zkServer.sh to determine the PID location into zkEnv.sh. Also removed the hard coded path for the zookeeper dataDir.",[],Bug,ZOOKEEPER-2067,Minor,Jeremy Carroll,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Init script fails to track PID file when using a non-standard dataDir,2016-03-03T06:14:32.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",3.0
Ted Yu,[],2014-10-21T19:57:54.000+0000,Ted Yu,"In various classes, there is potential resource leak.
e.g. LogIterator / RandomAccessFileReader is not closed upon return from the method.

Corresponding close() should be called to prevent resource leak.","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2064,Critical,Ted Yu,Fixed,2014-11-29T15:57:09.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Prevent resource leak in various classes,2014-11-30T11:23:33.000+0000,[],4.0
Chris Nauroth,"[<JIRA Component: name='tests', id='12312427'>]",2014-10-14T23:23:44.000+0000,Flavio Paiva Junqueira,"[junit] Running org.apache.zookeeper.RemoveWatchesTest
    [junit] Tests run: 46, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 306.188 sec","[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2062,Major,Flavio Paiva Junqueira,Fixed,2015-05-05T02:44:05.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,RemoveWatchesTest takes forever to run,2018-12-16T14:27:51.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",7.0
Ian Dimayuga,"[<JIRA Component: name='server', id='12312382'>]",2014-10-14T17:58:38.000+0000,Ian Dimayuga,"In NettyServerCnxnFactory, high throughput triggers a deadlock.

This is caused by a channel-buffer-dumping debug statement in NettyServerCnxnFactory.java that is executed regardless of log level.

This code path only executes when the server is throttling, but when it does it encounters a race and occasional deadlock between the channel buffer and NettyServerCnxn (jstack attached).

The proposed fix adds the debug logging guard to this statement, similar to other existing statements.","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2060,Major,Ian Dimayuga,Fixed,2014-11-19T22:40:30.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Trace bug in NettyServerCnxnFactory,2014-11-30T11:23:34.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",5.0
,[],2014-10-09T09:03:08.000+0000,huanghaijun,"Use command like this [./zkCli.sh -server host:port cmd args], such as [./zkCli.sh -server localhost:2181 create /test """"]  to create a node, 3.4.5 is work fine, but 3.4.6 it doesn't work.

for 3.4.5 it is ok
zookeeper-3.4.5/bin> ./zkCli.sh -server localhost:34096 create /test """"
Connecting to localhost:34096

WATCHER::

WatchedEvent state:SyncConnected type:None path:null
Created /test

for 3.4.6 it's not ok
zookeeper-3.4.6/bin> ./zkCli.sh -server localhost:43096 crate /test1 """"
Connecting to localhost:43096
....
2014-10-10 01:24:44,517 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=localhost:43096 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@48b8f82d

",[],Bug,ZOOKEEPER-2059,Major,huanghaijun,Cannot Reproduce,2015-08-26T20:48:33.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"Use command like this ""./zkCli.sh -server host:port cmd args"" but it doesn't work, 3.4.5 version is work fine",2015-08-26T20:48:34.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",3.0
Michi Mutsuzaki,"[<JIRA Component: name='build', id='12312383'>]",2014-10-09T05:04:28.000+0000,Michi Mutsuzaki,Somehow the release audit started complaining about *.cer files.,"[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2058,Major,Michi Mutsuzaki,Fixed,2014-10-13T03:20:42.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,rat: exclude *.cer files,2014-10-13T04:22:49.000+0000,[],5.0
Deiwin Sarjas,[],2014-10-08T15:51:41.000+0000,Keren Dong,"Similar to this issue https://issues.apache.org/jira/browse/ZOOKEEPER-1334, the MANIFEST.MF is still flawed. When using in OSGi, I got this exception:

java.lang.NoClassDefFoundError: org/ietf/jgss/GSSException
        at org.apache.zookeeper.ClientCnxn$SendThread.startConnect(ClientCnxn.java:1063)[168:org.apache.hadoop.zookeeper:3.5.01]
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1114)[168:org.apache.hadoop.zookeeper:3.5.01]
Caused by: java.lang.ClassNotFoundException: org.ietf.jgss.GSSException not found by org.apache.hadoop.zookeeper [168]
        at org.apache.felix.framework.BundleWiringImpl.findClassOrResourceByDelegation(BundleWiringImpl.java:1532)[org.apache.felix.framework-4.2.1.jar:]
        at org.apache.felix.framework.BundleWiringImpl.access$400(BundleWiringImpl.java:75)[org.apache.felix.framework-4.2.1.jar:]
        at org.apache.felix.framework.BundleWiringImpl$BundleClassLoader.loadClass(BundleWiringImpl.java:1955)[org.apache.felix.framework-4.2.1.jar:]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:356)[:1.7.0_15]
        ... 2 more

Looking at the bundle headers, it doesn't have the package org.ietf.jgss imported:

Import-Package =
        javax.management;resolution:=optional,
        javax.security.auth.callback,
        javax.security.auth.login,
        javax.security.sasl,
        org.slf4j;version=""[1.6,2)"",
        org.jboss.netty.buffer;resolution:=optional;version=""[3.2,4)"",
        org.jboss.netty.channel;resolution:=optional;version=""[3.2,4)"",
        org.jboss.netty.channel.group;resolution:=optional;version=""[3.2,4)"",
        org.jboss.netty.channel.socket.nio;resolution:=optional;version=""[3.2,4)"",
        org.osgi.framework;resolution:=optional;version=""[1.5,2)"",
        org.osgi.util.tracker;resolution:=optional;version=""[1.4,2)""
Export-Package =
        org.apache.zookeeper;version=3.5.01,
        org.apache.zookeeper.client;version=3.5.01,
        org.apache.zookeeper.data;version=3.5.01,
        org.apache.zookeeper.version;version=3.5.01,
        org.apache.zookeeper.server;version=3.5.01,
        org.apache.zookeeper.server.auth;version=3.5.01,
        org.apache.zookeeper.server.persistence;version=3.5.01,
        org.apache.zookeeper.server.quorum;version=3.5.01,
        org.apache.zookeeper.common;version=3.5.01","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2056,Major,Keren Dong,Fixed,2015-04-07T01:55:59.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper 3.4.x and 3.5.0-alpha is not OSGi compliant,2015-08-21T16:27:42.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",9.0
Steve R,[],2014-10-07T11:56:49.000+0000,Steve R,"When using SASLAuthenticationProvider and the jaas.conf file doesn't have a username and/or password for either the server or client configuration, when the client tries to connect via zkCli, an ArrayIndexOutOfBoundsException is thrown

Example conf file:
Server {
       org.apache.zookeeper.server.auth.DigestLoginModule required;
};
Client {
       org.apache.zookeeper.server.auth.DigestLoginModule required
       username=""bob""
       password=""bob123"";
};

Shows the resuting information:
INFO [main-SendThread(127.0.0.1:2181)] Client will use DIGEST-MD5 as SASL mechanism.
ERROR[main-SendThread(127.0.0.1:2181)] Exception while trying to create SASL client: java.lang.ArrayIndexOutOfBoundsException: Array index out of range: 0",[],Bug,ZOOKEEPER-2055,Minor,Steve R,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Don't throw ArrayIndexOutOfBoundsException when SASL username/password isn't specified,2016-03-03T06:06:58.000+0000,[],1.0
Michi Mutsuzaki,[],2014-10-07T01:52:47.000+0000,Michi Mutsuzaki,It seems to be causing NioNettySuiteHammerTest failure.,"[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2054,Major,Michi Mutsuzaki,Fixed,2014-10-08T08:18:46.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,test-patch.sh: don't set ulimit -n,2014-10-08T10:30:58.000+0000,[],4.0
,[],2014-10-02T20:39:51.000+0000,Owen O'Malley,"Currently the scripts will determine the root of the Zookeeper installation based on the location of the script. However, it would be convenient if the scripts honored the ZOOKEEPER_HOME environment variable like the other Hadoop-related projects.",[],Bug,ZOOKEEPER-2053,Major,Owen O'Malley,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Zookeeper scripts should honor ZOOKEEPER_HOME,2021-12-03T09:48:19.000+0000,[],2.0
Hongchao Deng,"[<JIRA Component: name='server', id='12312382'>]",2014-10-02T07:08:40.000+0000,Yip Ng,"We stumbled upon a ZooKeeper bug where a node with no children cannot be removed on our 3 node ZooKeeper ensemble or standalone ZooKeeper on Red Hat Enterprise Linux x86_64 environment.  Here is an example scenario/setup:

o Standalone ZooKeeper or 3 node ensemble (v3.4.6)
o 2 Java clients (v3.4.6)
  - Client A creates a persistent node (e.g.:  /metadata/resources)
  - Client B creates ephemeral nodes under this persistent node 

o Client A attempts to remove the /metadata/resources node via multi op  
   delete but fails since there are children
o Client B's session expired, all the ephemeral nodes are removed
o Client A attempts to recursively remove /metadata/resources node via 
   multi op, this is expected to succeed but got the following exception:
      org.apache.zookeeper.KeeperException$NotEmptyException:     
         KeeperErrorCode = Directory not empty

   (Note that Client B is the only client that creates these ephemeral nodes)

o After this, we use zkCli.sh to inspect the problematic node but the zkCli.sh shows the /metadata/resources node indeed have no children but it will not allow /metadata/resources node to get deleted.  (shown below)

[zk: localhost:2181(CONNECTED) 0] ls /
[zookeeper, metadata]
[zk: localhost:2181(CONNECTED) 1] ls /metadata
[resources]
[zk: localhost:2181(CONNECTED) 2] get /metadata/resources
null
cZxid = 0x3
ctime = Wed Oct 01 22:04:11 PDT 2014
mZxid = 0x3
mtime = Wed Oct 01 22:04:11 PDT 2014
pZxid = 0x9
cversion = 2
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 0
numChildren = 0
[zk: localhost:2181(CONNECTED) 3] delete /metadata/resources
Node not empty: /metadata/resources
[zk: localhost:2181(CONNECTED) 4] get /metadata/resources   
null
cZxid = 0x3
ctime = Wed Oct 01 22:04:11 PDT 2014
mZxid = 0x3
mtime = Wed Oct 01 22:04:11 PDT 2014
pZxid = 0x9
cversion = 2
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 0
numChildren = 0

o The only ways to remove this node is to either:
   a) Restart the ZooKeeper server
   b) set data to /metadata/resources then followed by a subsequent delete.
","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2052,Major,Yip Ng,Fixed,2014-10-28T04:41:16.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Unable to delete a node when the node has no children,2014-10-28T11:10:52.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",8.0
Raúl Gutiérrez Segalés,"[<JIRA Component: name='server', id='12312382'>]",2014-09-30T20:08:42.000+0000,Raúl Gutiérrez Segalés,"With local sessions enabled, the premise is that as soon as you try to create an ephemeral znode your session will be upgraded to global. The problem is that the session upgrade logic doesn't intercept transactions. So creating an ephemeral znode from within a transaction fails with SessionExpired.

A small example with Kazoo:

{noformat}
from kazoo.client import KazooClient

k = KazooClient(""localhost:2181"")
k.start()

t = k.transaction()
t.create(""/hello_"", """", ephemeral=True)
t.commit()
[kazoo.exceptions.SessionExpiredError((), {})]
{noformat}

A workaround, for now, is to create an ephemeral before your transaction which forces your session to be upgraded.

Possible solutions could be:

* extending zookeeper_init() so that you can request global=True
*  and/or, providing an upgradeSession() API

Thoughts?

cc: [~thawan], [~phunt], [~fpj]


",[],Bug,ZOOKEEPER-2051,Major,Raúl Gutiérrez Segalés,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Creating ephemeral znodes from within a transaction fail with local sessions,2022-02-03T08:50:17.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
,[],2014-09-30T13:35:59.000+0000,samraj,"When i add the latest zookeeper jar in the dependency it throws the error and say following jars are missing.If i added those in exclusion its working fine.

<exclusions>
				<exclusion>
					<groupId>com.sun.jmx</groupId>
					<artifactId>jmxri</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.sun.jdmk</groupId>
					<artifactId>jmxtools</artifactId>
				</exclusion>
				<exclusion>
					<groupId>javax.jms</groupId>
					<artifactId>jms</artifactId>
				</exclusion>
			</exclusions>
",[],Bug,ZOOKEEPER-2050,Minor,samraj,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Maven dependency should remove the 3 dependencies ,2014-09-30T13:35:59.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",2.0
Till Toenshoff,[],2014-09-29T18:08:50.000+0000,Till Toenshoff,"recordio.h defines {{htonll}} which conflicts with Apple's equally named implementation from within arpa/inet.h.


{noformat}
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O2 -D_GNU_SOURCE -MT zk_log.lo -MD -MP -MF .deps/zk_log.Tpo -c src/zk_log.c  -fno-common -DPIC -o zk_log.o
In file included from src/recordio.c:19:
./include/recordio.h:76:9: error: expected ')'
int64_t htonll(int64_t v);
        ^
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:30: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
                             ^
./include/recordio.h:76:9: note: to match this '('
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:5: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
    ^
In file included from src/recordio.c:19:
./include/recordio.h:76:9: error: conflicting types for '__builtin_constant_p'
int64_t htonll(int64_t v);
        ^
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:6: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
     ^
./include/recordio.h:76:9: note: '__builtin_constant_p' is a builtin with type 'int ()'
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:6: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
     ^
In file included from generated/zookeeper.jute.c:20:
In file included from ./generated/zookeeper.jute.h:21:
./include/recordio.h:76:9: error: expected ')'
int64_t htonll(int64_t v);
        ^
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:30: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
                             ^
./include/recordio.h:76:9: note: to match this '('
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:5: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
    ^
In file included from generated/zookeeper.jute.c:20:
In file included from ./generated/zookeeper.jute.h:21:
./include/recordio.h:76:9: error: conflicting types for '__builtin_constant_p'
int64_t htonll(int64_t v);
        ^
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:6: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
     ^
./include/recordio.h:76:9: note: '__builtin_constant_p' is a builtin with type 'int ()'
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:6: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
     ^
In file included from src/zookeeper.c:27:
In file included from ./include/zookeeper.h:34:
./include/recordio.h:76:9: error: expected ')'
int64_t htonll(int64_t v);
        ^
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:30: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
                             ^
./include/recordio.h:76:9: note: to match this '('
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:5: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
    ^
In file included from src/zookeeper.c:27:
In file included from ./include/zookeeper.h:34:
./include/recordio.h:76:9: error: conflicting types for '__builtin_constant_p'
int64_t htonll(int64_t v);
        ^
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:6: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
     ^
./include/recordio.h:76:9: note: '__builtin_constant_p' is a builtin with type 'int ()'
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:6: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
     ^
In file included from src/zk_hashtable.c:19:
In file included from src/zk_hashtable.h:22:
In file included from ./include/zookeeper.h:34:
./include/recordio.h:76:9: error: expected ')'
int64_t htonll(int64_t v);
        ^
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:30: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
                             ^
./include/recordio.h:76:9: note: to match this '('
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:5: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
    ^
In file included from src/zk_hashtable.c:19:
In file included from src/zk_hashtable.h:22:
In file included from ./include/zookeeper.h:34:
./include/recordio.h:76:9: error: conflicting types for '__builtin_constant_p'
int64_t htonll(int64_t v);
        ^
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:6: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
     ^
./include/recordio.h:76:9: note: '__builtin_constant_p' is a builtin with type 'int ()'
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:6: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
     ^
src/recordio.c:83:9: error: expected ')'
int64_t htonll(int64_t v)
        ^
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:30: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
                             ^
src/recordio.c:83:9: note: to match this '('
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:5: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
    ^
src/recordio.c:83:9: error: conflicting types for '__builtin_constant_p'
int64_t htonll(int64_t v)
        ^
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:6: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
     ^
./include/recordio.h:76:9: note: '__builtin_constant_p' is a builtin with type 'int ()'
int64_t htonll(int64_t v);
        ^
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:6: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
     ^
src/recordio.c:83:9: error: definition of builtin function '__builtin_constant_p'
int64_t htonll(int64_t v)
        ^
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:6: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
     ^
In file included from src/zk_log.c:23:
In file included from ./include/zookeeper_log.h:22:
In file included from ./include/zookeeper.h:34:
./include/recordio.h:76:9: error: expected ')'
int64_t htonll(int64_t v);
        ^
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:30: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
                             ^
./include/recordio.h:76:9: note: to match this '('
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:5: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
    ^
In file included from src/zk_log.c:23:
In file included from ./include/zookeeper_log.h:22:
In file included from ./include/zookeeper.h:34:
./include/recordio.h:76:9: error: conflicting types for '__builtin_constant_p'
int64_t htonll(int64_t v);
        ^
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:6: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
     ^
./include/recordio.h:76:9: note: '__builtin_constant_p' is a builtin with type 'int ()'
/usr/include/sys/_endian.h:141:25: note: expanded from macro 'htonll'
#define htonll(x)       __DARWIN_OSSwapInt64(x)
                        ^
/usr/include/libkern/_OSByteOrder.h:78:6: note: expanded from macro '__DARWIN_OSSwapInt64'
    (__builtin_constant_p(x) ? __DARWIN_OSSwapConstInt64(x) : _OSSwapInt64(x))
     ^
2 errors generated.
5 errors generated.
2 errors generated.
make[5]: *** [recordio.lo] Error 1
make[5]: *** Waiting for unfinished jobs....
2 errors generated.
make[5]: *** [zookeeper.jute.lo] Error 1
make[5]: *** [zk_hashtable.lo] Error 1
make[5]: *** [zk_log.lo] Error 1
2 errors generated.
make[5]: *** [zookeeper.lo] Error 1
make[4]: *** [all] Error 2
make[3]: *** [zookeeper-3.4.5/src/c/libzookeeper_mt.la] Error 2
make[3]: *** Waiting for unfinished jobs....
ln -fs libleveldb.dylib.1.4 libleveldb.dylib
ln -fs libleveldb.dylib.1.4 libleveldb.dylib.1
make[2]: *** [all-recursive] Error 1
make[1]: *** [all] Error 2
make: *** [all-recursive] Error 1
{noformat}","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2049,Major,Till Toenshoff,Fixed,2014-10-16T04:55:28.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Yosemite build failure: htonll conflict,2018-08-19T17:24:07.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",8.0
,"[<JIRA Component: name='build', id='12312383'>]",2014-09-23T06:43:14.000+0000,Guo Ruijing,"Currently, zookeeper is compiled by JDK 5 in default as

<property name=""javac.target"" value=""1.5"" />
<property name=""javac.source"" value=""1.5"" />

we may change it to JDK 7 in default",[],Bug,ZOOKEEPER-2046,Major,Guo Ruijing,Duplicate,2014-12-02T21:22:47.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Compile zookeeper by JDK 7 in default,2014-12-02T21:22:47.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",7.0
Hongchao Deng,[],2014-09-22T23:17:41.000+0000,Hongchao Deng,"{code}
public final class ConnectStringParser {
...
   public ConnectStringParser(String connectString) {
   ...
{code}
ConnectStringParser is a public api. Besides that, both ZooKeeper constructor and ZooKeeper#updateServerList used it.

However, it doesn't handle a null connectString. It doesn't help that much to see a NPE showing up. So I add a check to the constructor.",[],Bug,ZOOKEEPER-2045,Minor,Hongchao Deng,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,ConnectStringParser public api didn't handle null connect string,2014-09-23T00:22:21.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",1.0
Michael Han,"[<JIRA Component: name='server', id='12312382'>]",2014-09-22T10:17:01.000+0000,shamjith antholi,"I am getting cancelled key exception in zookeeper (version 3.4.5). Please see the log below. When this error is thrown, the connected solr shard is going down by giving the error ""Failed to index metadata in Solr,StackTrace=SolrError: HTTP status 503.Reason: {""responseHeader"":{""status"":503,""QTime"":204},""error"":{""msg"":""ClusterState says we are the leader, but locally we don't think so"",""code"":503""  and ultimately the current activity is going down. Could you please give a solution for this ?


Zookeper log 
----------------------------------------------------------
2014-09-16 02:58:47,799 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@832] - Client attempting to renew session 0x24868e7ca980003 at /172.22.0.5:58587
2014-09-16 02:58:47,800 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:Learner@107] - Revalidating client: 0x24868e7ca980003
2014-09-16 02:58:47,802 [myid:1] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:ZooKeeperServer@588] - Invalid session 0x24868e7ca980003 for client /172.22.0.5:58587, probably expired
2014-09-16 02:58:47,803 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1001] - Closed socket connection for client /172.22.0.5:58587 which had sessionid 0x24868e7ca980003
2014-09-16 02:58:47,810 [myid:1] - ERROR [CommitProcessor:1:NIOServerCnxn@180] - Unexpected Exception:
java.nio.channels.CancelledKeyException
        at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:55)
        at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:59)
        at org.apache.zookeeper.server.NIOServerCnxn.sendBuffer(NIOServerCnxn.java:153)
        at org.apache.zookeeper.server.NIOServerCnxn.sendResponse(NIOServerCnxn.java:1076)
        at org.apache.zookeeper.server.NIOServerCnxn.process(NIOServerCnxn.java:1113)
        at org.apache.zookeeper.server.DataTree.setWatches(DataTree.java:1327)
        at org.apache.zookeeper.server.ZKDatabase.setWatches(ZKDatabase.java:384)
        at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:304)
        at org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:74)


 ","[<JIRA Version: name='3.4.10', id='12338036'>]",Bug,ZOOKEEPER-2044,Minor,shamjith antholi,Fixed,2017-01-31T16:40:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,CancelledKeyException in zookeeper branch-3.4,2020-03-27T13:10:24.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",21.0
,[],2014-09-22T09:15:09.000+0000,hikin,"org.apache.zookeeper.server.NIOServerCnxn
void doIO(SelectionKey k) throws InterruptedException {
        try {
            if (isSocketOpen() == false) {
                LOG.warn(""trying to do i/o on a null socket for session:0x""
                         + Long.toHexString(sessionId));

                return;
            }
 public void close() {
        if (!factory.removeCnxn(this)) {
            return;
        }
If the socket suddenly broken, do not have the right to clean up the connection, this one line of code that caused a lot of links, eventually exceed the maximum maxClientCnxns, cause the client end connections do not go up.



",[],Bug,ZOOKEEPER-2043,Major,hikin,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Too many connections,maxClientCnxns  don't close ",2019-09-17T12:54:08.000+0000,[],3.0
Chris Nauroth,"[<JIRA Component: name='scripts', id='12312384'>]",2014-09-17T17:44:45.000+0000,John Lindwall,"There are two issues in the zkServer.sh script that make it not work properly out of the box on Solaris.

1. The bin/zkServer.sh script uses plain ""echo"" in all instances but one: when writing the pid to the pid file.  In that instance it uses ""/bin/echo"".

The ""/bin/echo"" command on Solaris does not understand the ""-n"" parameter and interprets it as a literal string, so the ""-n"" gets written into the pid file along with the pid.  This causes the ""stop"" command to fail.

2. The /bin/grep command in Solaris does not understand special character classes like ""[[:space:]]"".  You must use the alternate posix version of grep as found in /usr/xpg4/bin/grep for this to work.  If the script cannot be made completely generic then at least we should document the need to use the posix grep implementation on Solaris.
",[],Bug,ZOOKEEPER-2042,Minor,John Lindwall,Duplicate,2015-05-01T05:56:59.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zkServer.sh does not work properly on Solaris,2015-05-01T05:56:59.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",3.0
Ian Dimayuga,"[<JIRA Component: name='jute', id='12312385'>]",2014-09-16T02:34:57.000+0000,Ian Dimayuga,"The Jute utility's naïve byte-array comparison compares b1[off1+i] with b2[off2+1]. (A literal 1, not the variable i)

It should be off2+i, in parallel with the other operand.","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>]",Bug,ZOOKEEPER-2039,Minor,Ian Dimayuga,Fixed,2014-09-18T15:51:43.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Jute compareBytes incorrect comparison index,2014-09-28T09:48:58.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",5.0
,"[<JIRA Component: name='recipes', id='12313246'>]",2014-09-16T02:23:04.000+0000,YuQing,"In fuction child_floor(), strcmp() is used to compare the whole string.
But there exists conditions a sorted_data looks like (""x-000-00"", ""x-222-01"", ""x-111-02""), and now ""x-222-01"" is calling child_floor() to get a predecessor for watching, so the current logic will return ""x-111-02"" instead of the correct ""x-000-00"".
Use a strcmp() == 0 and a break statement should solve this problem.
",[],Bug,ZOOKEEPER-2038,Minor,YuQing,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Coding error in recipes/lock/src/c/src/zoo_lock.c,2014-09-16T02:23:04.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2014-09-15T08:08:27.000+0000,Brahma Reddy Battula," *{color:blue}Scenario:{color}* 

Started the Secure ZK Cluster.
Logged with Secure ZK Client(by passing valid jaas.conf) and created the Znodes

Now logged in to same secure cluster using unsecure ZKClient (without jaas.conf) to same Cluster and able to access the data which is created by the Secured Client..

 *{color:blue}Secured Client{color}:(which is created the Znodes)* 

2014-09-15 13:40:56,288 [myid:] - INFO  [main-SendThread(localhost:2181):ZooKeeperSaslClient$1@285] - Client will use GSSAPI as SASL mechanism.
2014-09-15 13:40:56,296 [myid:] - INFO  [Thread-1:Login@301] - TGT valid starting at:        Mon Sep 15 13:40:56 IST 2014
2014-09-15 13:40:56,296 [myid:] - INFO  [Thread-1:Login@302] - TGT expires:                  Tue Sep 16 13:40:56 IST 2014
2014-09-15 13:40:56,296 [myid:] - INFO  [Thread-1:Login$1@181] - TGT refresh sleeping until: Tue Sep 16 09:36:04 IST 2014
2014-09-15 13:40:56,302 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1000] - Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will attempt to SASL-authenticate using Login Context section 'Client'
2014-09-15 13:40:56,308 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@855] - Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session
2014-09-15 13:40:56,344 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1260] - Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x1486856657e0016, negotiated timeout = 30000

WATCHER::

WatchedEvent state:SyncConnected type:None path:null

WATCHER::

WatchedEvent state: *{color:red}SaslAuthenticated{color}*  type:None path:null

[zk: localhost:2181(CONNECTED) 1] create -s /tmp-seq 'sd:er:'
Created /tmp-seq0000000003
[zk: localhost:2181(CONNECTED) 2] create -s /tmp-seq 'sd:er:'
Created /tmp-seq0000000004
[zk: localhost:2181(CONNECTED) 0] ls /
[tmp-seq0000000004, tmp-seq0000000003, hadoop, hadoop-ha, tmp-seq0000000002, zookeeper]

 *{color:blue}UnSecured Client{color}:(which is Accesing Znodes)* 
Welcome to ZooKeeper!
2014-09-15 13:00:30,440 [myid:] - WARN  [main-SendThread(localhost:2181):ClientCnxn$SendThread@982] - SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/home/****/zookeeper/conf/jaas.conf'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it.
014-09-15 13:00:30,441 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1000] - Opening socket connection to server localhost/127.0.0.1:2181
 WatchedEvent state: *{color:red}AuthFailed{color}*  type:None path:null
JLine support is enabled
2014-09-15 13:00:30,451 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@855] - Socket connection established to localhost/127.0.0.1:2181, initiating session
[zk: localhost:2181(CONNECTING) 0] 2014-09-15 13:00:30,488 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1260] - Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x348685662250005, negotiated timeout = 30000

WATCHER::

WatchedEvent state:SyncConnected type:None path:null

[zk: localhost:2181(CONNECTED) 0] ls /
[tmp-seq0000000004, tmp-seq0000000003, hadoop, hadoop-ha, tmp-seq0000000002, zookeeper]
[zk: localhost:2181(CONNECTED) 1] get /tmp-seq000000000

tmp-seq0000000004   tmp-seq0000000003   tmp-seq0000000002
[zk: localhost:2181(CONNECTED) 1] get /tmp-seq0000000002
''
cZxid = 0x100000040
ctime = Mon Sep 15 12:51:50 IST 2014
mZxid = 0x100000040
mtime = Mon Sep 15 12:51:50 IST 2014
pZxid = 0x100000040
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 2
numChildren = 0


",[],Bug,ZOOKEEPER-2036,Blocker,Brahma Reddy Battula,Not A Problem,2014-10-28T13:24:55.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Client which is not authorized able to access the Secure Data which is created by the Secure Client,2014-10-28T13:24:55.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",2.0
,[],2014-09-13T17:33:14.000+0000,Steve Loughran,"The diagnostics code in {{ZooKeeperSaslClient.createSaslToken()}} which looks for a {{""(""Mechanism level: Server not found in Kerberos database (7) - UNKNOWN_SERVER)""}} error string isn't finding a match ... the text now appears to be {{(Mechanism level: Server not found in Kerberos database (7) - Server not found in Kerberos database)}}",[],Bug,ZOOKEEPER-2035,Minor,Steve Loughran,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,diagnostics on SASL connection problems doesn't match error strings sent back,2015-12-15T17:47:47.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",3.0
,[],2014-09-13T17:09:48.000+0000,Steve Loughran,"I'm seeing {{StringIndexOutOfBoundsException}} in {{createSaslServer}}, where my test kerberos code is (presumably) is not correctly set up. 

Looking at the comments, it hints that the problem is my principals are called {{zookeeper@EXAMPLE.COM}}, which doesn't match the pattern {{principal/host@realm}}",[],Bug,ZOOKEEPER-2034,Minor,Steve Loughran,Not A Problem,2014-09-13T17:11:54.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,StringIndexOutOfBoundsException in createSaslServer ,2014-09-13T17:11:54.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",1.0
Asad Saeed,"[<JIRA Component: name='quorum', id='12312379'>]",2014-09-10T18:14:12.000+0000,Asad Saeed,"The following issue was seen when adding a new node to a zookeeper cluster.

Reproduction steps
1. Create a 2 node ensemble. Write some keys.
2. Add another node to the ensemble, by modifying the config. Restarting entire cluster.
3. Restart the new node before writing any new keys.

What occurs is that the new node gets a SNAP from the newly elected leader, since it is too far behind. The zxid for this snapshot is from the new epoch but that is not in the committed log cache.

On restart of this new node. The follower sends the new epoch zxid. The leader looks at it's maxCommitted logs, and sees that it is not the newest epoch, and therefore sends a TRUNC.

The follower sees the TRUNC but it only has a snapshot, so it cannot truncate!","[<JIRA Version: name='3.4.7', id='12325149'>]",Bug,ZOOKEEPER-2033,Major,Asad Saeed,Fixed,2015-09-03T23:00:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zookeeper follower fails to start after a restart immediately following a new epoch,2015-09-04T00:06:02.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",5.0
Alexander Shraer,"[<JIRA Component: name='server', id='12312382'>]",2014-09-07T04:42:42.000+0000,Alexander Shraer,a relative path doesn't seem like a good idea since it will work only if we start the server from the same directory as we did previously.,"[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2030,Minor,Alexander Shraer,Fixed,2014-09-17T05:36:53.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"dynamicConfigFile should have an absolute path, not a relative path, to the dynamic configuration file",2014-09-17T11:11:25.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
Rakesh Radhakrishnan,"[<JIRA Component: name='quorum', id='12312379'>]",2014-09-05T23:21:43.000+0000,Asad Saeed,Leader.LearnerCnxAcceptor swallows exceptions and shuts itself down. It should instead crash the Leader.,"[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2029,Minor,Asad Saeed,Fixed,2015-04-11T18:49:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Leader.LearnerCnxAcceptor should handle exceptions in run(),2021-02-06T21:27:29.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",5.0
Qiang Tian,"[<JIRA Component: name='tests', id='12312427'>]",2014-09-05T07:20:52.000+0000,Qiang Tian,"Hi Guys,
the testcase consistently fails if debug is turned on(set zoo_set_debug_level(ZOO_LOG_LEVEL_DEBUG) in TestDriver.cc); if debug is OFF, it fails for the first time, subsequent runs succeed.

can someone help take a look?
thanks!

below is related info: 

1. screen output
{quote}
     [exec] Zookeeper_simpleSystem::testPing : elapsed 17200 : OK
     [exec] Zookeeper_simpleSystem::testAcl : elapsed 1014 : OK
     [exec] Zookeeper_simpleSystem::testChroot : elapsed 3041 : OK
     [exec] terminate called after throwing an instance of 'CppUnit::Exception'
     [exec]   what():  equality assertion failed
     [exec] - Expected: 0
     [exec] - Actual  : -116
     [exec] 
     [exec] make: *** [run-check] Aborted (core dumped)
     [exec] Zookeeper_simpleSystem::testAuth
{quote}

2. last lines in zk server log:
{quote}
2014-09-04 21:13:57,711 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:22181:ZooKeeperServer@868] - Client attempting to establish new session at /127.0.0.1:34992
2014-09-04 21:13:57,714 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@617] - Established session 0x14844044d96000a with negotiated timeout 10000 for client /127.0.0.1:34992
2014-09-04 21:14:01,039 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:22181:ZooKeeperServer@892] - got auth packet /127.0.0.1:34992
2014-09-04 21:14:01,747 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:22181:ZooKeeperServer@926] - auth success /127.0.0.1:34992
2014-09-04 21:14:01,912 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:22181:NIOServerCnxn@362] - Exception causing close of session 0x14844044d96000a due to java.io.IOException: Connection reset by peer
2014-09-04 21:14:01,914 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:22181:NIOServerCnxn@1007] - Closed socket connection for client /127.0.0.1:34992 which had sessionid 0x14844044d96000a
2014-09-04 21:14:12,000 [myid:] - INFO  [SessionTracker:ZooKeeperServer@347] - Expiring session 0x14844044d96000a, timeout of 10000ms exceeded
2014-09-04 21:14:12,001 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@494] - Processed session termination for sessionid: 0x14844044d96000a
{quote}

3. last lines in TEST-Zookeeper_simpleSystem-mt.txt:
{quote}
2014-09-04 21:13:57,703:383481(0x7f8866c4b720):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.6
2014-09-04 21:13:57,703:383481(0x7f8866c4b720):ZOO_INFO@log_env@716: Client environment:host.name=localhost
2014-09-04 21:13:57,703:383481(0x7f8866c4b720):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-09-04 21:13:57,703:383481(0x7f8866c4b720):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-358.el6.x86_64
2014-09-04 21:13:57,703:383481(0x7f8866c4b720):ZOO_INFO@log_env@725: Client environment:os.version=#1 SMP Tue Jan 29 11:47:41 EST 2013
2014-09-04 21:13:57,703:383481(0x7f8866c4b720):ZOO_INFO@log_env@733: Client environment:user.name=tianq
2014-09-04 21:13:57,703:383481(0x7f8866c4b720):ZOO_INFO@log_env@741: Client environment:user.home=/home/tianq
2014-09-04 21:13:57,703:383481(0x7f8866c4b720):ZOO_INFO@log_env@753: Client environment:user.dir=/home/tianq/zookeeper/build/test/test-cppunit
2014-09-04 21:13:57,703:383481(0x7f8866c4b720):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:22181 sessionTimeout=10000 watcher=0x42e590 sessionId=0 sessionPasswd=<null> context=0x7fff695ea9a0 flags=0
2014-09-04 21:13:57,703:383481(0x7f8866c4b720):ZOO_DEBUG@start_threads@221: starting threads...
2014-09-04 21:13:57,704:383481(0x7f8857fff700):ZOO_DEBUG@do_io@367: started IO thread
2014-09-04 21:13:57,704:383481(0x7f8857fff700):ZOO_INFO@check_events@1705: initiated connection to server [127.0.0.1:22181]
2014-09-04 21:13:57,704:383481(0x7f88667f9700):ZOO_DEBUG@do_completion@459: started completion thread
2014-09-04 21:13:57,714:383481(0x7f8857fff700):ZOO_INFO@check_events@1752: session establishment complete on server [127.0.0.1:22181], sessionId=0x14844044d96000a, negotiated timeout=10000
2014-09-04 21:13:57,714:383481(0x7f8857fff700):ZOO_DEBUG@check_events@1758: Calling a watcher for a ZOO_SESSION_EVENT and the state=ZOO_CONNECTED_STATE
2014-09-04 21:13:57,714:383481(0x7f88667f9700):ZOO_DEBUG@process_completions@2113: Calling a watcher for node [], type = -1 event=ZOO_SESSION_EVENT
2014-09-04 21:13:58,704:383481(0x7f8866c4b720):ZOO_DEBUG@send_last_auth_info@1353: Sending auth info request to 127.0.0.1:22181  
{quote}

If I understand correctly, it fails because assert expected 0, but looking at the testcase log, ""Sending auth info request to .."" appears for the first time, so it should correspond to the first zoo_add_auth call in testAuth. but its expected value is ZBADARGUMENTS...?
",[],Bug,ZOOKEEPER-2028,Minor,Qiang Tian,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,TestClient#testAuth aborts because ASSERT throws exception again in destructor when there is active exception already,2017-07-19T20:25:20.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",3.0
Stevo Slavić,"[<JIRA Component: name='jmx', id='12312451'>, <JIRA Component: name='server', id='12312382'>]",2014-09-02T07:21:54.000+0000,Stevo Slavić,"{{NIOServerCnxnFactory}} and {{NettyServerCnxnFactory}} {{startup}} method implementations are binding {{ZooKeeperServer}} too late, so in {{ZooKeeperServer}} in its startup can fail to register appropriate JMX MBean.

See [this|http://mail-archives.apache.org/mod_mbox/zookeeper-user/201409.mbox/%3CCAAUywg9-ad3oWfqRWahB9PyBEbg6%2Bd%3DDyj5PAUU7A%3Dm9wRncaw%40mail.gmail.com%3E] post on ZK user mailing list for more details.","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2026,Minor,Stevo Slavić,Fixed,2014-09-28T17:23:10.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Startup order in ServerCnxnFactory-ies is wrong,2014-09-29T10:31:10.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",5.0
,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='server', id='12312382'>]",2014-08-29T20:21:33.000+0000,Stephen Tyree,"Description will be included in an attached PDF.

The two main questions we have are:
1: What would be the cause of the ""Unreasonable Length"" error in our context, and how might we prevent it from occurring?
2: What can we do to prevent the reconnection storm that led to the cluster becoming unresponsive?
",[],Bug,ZOOKEEPER-2025,Major,Stephen Tyree,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Single-node ejection caused apparent reconnection storm, leading to cluster unresponsiveness",2014-10-16T17:40:22.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",7.0
,"[<JIRA Component: name='server', id='12312382'>]",2014-08-26T13:22:40.000+0000,Alvaro Gareppe,"When usgin the command like this:

zkServer.cmd zoo.cfg
we get this error:
ERROR: Invalid arguments, exiting abnormally
java.lang.NumberFormatException: For input string :""C:\Development\zookeeperserver-3.4.6\bin\..\conf\zoo.cfg"" 

Patch (Workaround):
change the code in zkServer.cfg to:

setlocal
call ""%~dp0zkEnv.cmd""

set ZOOCFG=%ZOOCFGDIR%\%1

set ZOOMAIN=org.apache.zookeeper.server.quorum.QuorumPeerMain
echo on
java ""-Dzookeeper.log.dir=%ZOO_LOG_DIR%"" ""-Dzookeeper.root.logger=%ZOO_LOG4J_PROP%"" -cp ""%CLASSPATH%"" %ZOOMAIN% ""%ZOOCFG%"" 

endlocal




",[],Bug,ZOOKEEPER-2021,Major,Alvaro Gareppe,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZKserver.cmd fails usng the config param ,2014-08-26T13:22:40.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",1.0
Raúl Gutiérrez Segalés,"[<JIRA Component: name='server', id='12312382'>]",2014-08-22T23:23:31.000+0000,Raúl Gutiérrez Segalés,"If you have quotas properly set for a given path, i.e.:

{noformat}
create /zookeeper/quota/test/zookeeper_limits 'count=1,bytes=100'
create /zookeeper/quota/test/zookeeper_stats 'count=1,bytes=100'
{noformat}

and then you update the limits znode with bogus data, i.e.:

{noformat}
set /zookeeper/quota/test/zookeeper_limits ''
{noformat}

you'll crash the cluster because IllegalArgumentException isn't handled when dealing with quotas znodes:

https://github.com/apache/zookeeper/blob/ZOOKEEPER-823/src/java/main/org/apache/zookeeper/server/DataTree.java#L379
https://github.com/apache/zookeeper/blob/ZOOKEEPER-823/src/java/main/org/apache/zookeeper/server/DataTree.java#L425

We should handle IllegalArgumentException. Optionally, we should also throw BadArgumentsException from PrepRequestProcessor. 

Review Board: https://reviews.apache.org/r/25968/","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-2019,Major,Raúl Gutiérrez Segalés,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Unhandled exception when setting invalid limits data in /zookeeper/quota/some/path/zookeeper_limits ,2022-02-03T08:36:23.000+0000,[],5.0
,[],2014-08-22T18:17:15.000+0000,Samer Al-Kiswany,"After studying the steps ZooKeeper takes to update the logs we found the following bug. The bug may manifest in file systems with writeback buffering. 

If you run the zookeeper client script (zkCli.sh) with the following commands:
VALUE=”8KB value”  # 8KB in size
create /dir1 $VALUE
create /dir1/dir2 $VALUE

the strace generated at the zookeeprer node is: 
mkdir(v)
create(v/log)
append(v/log)
trunk(v/log)
…
fdatasync(v/log)
write(v/log)    ……. 1
write(v/log)    ……. 2
write(v/log)    ……. 3
fdatasync(v/log)

The last four calls are related to the second create of dir2.

If the last write (#3) goes to disk before the second write (#2) and the system crashes before #2 reaches the disk, the zookeeper node will not boot.",[],Bug,ZOOKEEPER-2018,Major,Samer Al-Kiswany,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeper node fails to boot if writes are reordered,2014-08-23T01:43:15.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",2.0
,"[<JIRA Component: name='c client', id='12312380'>]",2014-08-21T10:00:57.000+0000,gao fengpu,"==15070== 895,632 bytes in 57,640 blocks are indirectly lost in loss record 370 of 371
==15070==    at 0x4C2677B: calloc (vg_replace_malloc.c:593)
==15070==    by 0x4C59BB: deserialize_String_vector (zookeeper.jute.c:245)
==15070==    by 0x4C5AE7: deserialize_GetChildrenResponse (zookeeper.jute.c:874)
==15070==    by 0x4BEE7E: zookeeper_process (zookeeper.c:1906)
==15070==    by 0x4BFF8E: do_io (mt_adaptor.c:439)
==15070==    by 0x4E36850: start_thread (in /lib64/libpthread-2.12.so)
==15070==    by 0x58D367C: clone (in /lib64/libc-2.12.so)
==15070==
==15070== 1,946,648 (1,051,016 direct, 895,632 indirect) bytes in 64,035 blocks are definitely lost in loss record 371 of 371
==15070==    at 0x4C2677B: calloc (vg_replace_malloc.c:593)
==15070==    by 0x4C59BB: deserialize_String_vector (zookeeper.jute.c:245)
==15070==    by 0x4C5AE7: deserialize_GetChildrenResponse (zookeeper.jute.c:874)
==15070==    by 0x4BEE7E: zookeeper_process (zookeeper.c:1906)
==15070==    by 0x4BFF8E: do_io (mt_adaptor.c:439)
==15070==    by 0x4E36850: start_thread (in /lib64/libpthread-2.12.so)
==15070==    by 0x58D367C: clone (in /lib64/libc-2.12.so)","[<JIRA Version: name='3.4.6', id='12323310'>]",Bug,ZOOKEEPER-2015,Minor,gao fengpu,Not A Problem,2014-08-25T05:55:15.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,I found memory leak in zk client for c++,2015-11-12T23:11:05.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",5.0
Michael Han,"[<JIRA Component: name='server', id='12312382'>]",2014-08-21T02:57:37.000+0000,Raúl Gutiérrez Segalés,"ZOOKEEPER-107 introduces reconfiguration support via the reconfig() call. We should, at the very least, ensure that only the Admin can reconfigure a cluster. Perhaps restricting access to /zookeeper/config as well, though this is debatable. Surely one could ensure Admin only access via an ACL, but that would leave everyone who doesn't use ACLs unprotected. We could also force a default ACL to make it a bit more consistent (maybe).

Finally, making reconfig() only available to Admins means they have to run with zookeeper.DigestAuthenticationProvider.superDigest (which I am not sure if everyone does, or how would it work with other authentication providers). 

Review board https://reviews.apache.org/r/51546/","[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2014,Blocker,Raúl Gutiérrez Segalés,Fixed,2016-11-13T20:10:56.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Only admin should be allowed to reconfig a cluster,2017-05-18T03:44:01.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",15.0
Tim Chambers,"[<JIRA Component: name='documentation', id='12312422'>]",2014-08-19T22:12:17.000+0000,Tim Chambers,I noticed a couple typos. See patch.,"[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2013,Trivial,Tim Chambers,Fixed,2014-08-20T17:21:08.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,typos in zookeeperProgrammers,2014-08-30T18:48:01.000+0000,[],6.0
Qiang Tian,[],2014-08-15T04:08:46.000+0000,Qiang Tian,"please see http://apache-hbase.679495.n3.nabble.com/HBase-client-hangs-after-client-side-OOM-td4062675.html.

it looks the send thread caught the error successfully, as it is finally running fine.. but the cleanup fail to notify the main thread...so I suspect it is a very small timing hole that the packet is not on the 2 queues at the same time..it looks it could happen in the latest code ClientCnxnSocketNIO#doIO as well..

potential fixes:
1)add timeout during wait
2)try/catch for the possible timing hole:
{code}
                  if (!p.bb.hasRemaining()) {
                        sentCount++;
                        outgoingQueue.removeFirstOccurrence(p);
                        if (p.requestHeader != null
                                && p.requestHeader.getType() != OpCode.ping
                                && p.requestHeader.getType() != OpCode.auth) {
                            synchronized (pendingQueue) {
                                pendingQueue.add(p);
                            }
                        }
                    }
{code}

thoughts?
thanks.
",[],Bug,ZOOKEEPER-2012,Minor,Qiang Tian,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,HBase client hangs after client-side OOM,2014-08-24T04:13:04.000+0000,[],3.0
,[],2014-08-14T09:39:53.000+0000,Simon Cooper,"In 3.4.5, zkCli executed commands passed on the command line. This command would create the {{/test}} znode and exit, with a non-zero exit code if the command failed:

{code}
$ ./zkCli.sh create /test null
{code}

This is no longer the case in 3.4.6 - the command is not executed, but zkCli still runs & exits with a zero exit code.

The interim workaround in bash is to use here documents:

{code}
$ ./zkCli.sh <<EOF
create /test null
EOF
{code}",[],Bug,ZOOKEEPER-2009,Minor,Simon Cooper,Duplicate,2016-08-16T23:45:31.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zkCli does not execute command passed as arguments,2016-08-16T23:45:32.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",4.0
Kfir Lev-Ari,"[<JIRA Component: name='contrib-fatjar', id='12312645'>]",2014-08-12T09:20:19.000+0000,Kfir Lev-Ari,Leader election and client ports are not initialized when creating a QuorumServer during system tests.,"[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2008,Minor,Kfir Lev-Ari,Fixed,2014-08-14T07:37:12.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,System test fails due to missing leader election port,2014-08-30T18:47:20.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Eric Yang,[],2014-08-08T21:37:58.000+0000,Eric Yang,"RPM package init.d startup script is not relocatable, and there is some bugs in contrib directory build structure where property is not passed from main project to contrib, hence some of the contrib projects generate ${dist.dir} directory instead of building in the top level build directory.  The usage of BUILD directory is not exactly correct from ZOOKEEPER-1210.  RPM build procedure should use BUILDROOT as install destination to properly support RPM 4.6+ while package is building.",[],Bug,ZOOKEEPER-2007,Major,Eric Yang,Won't Fix,2016-03-03T16:23:58.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Update RPM package to be relocatable and contrib packaging bugfix,2016-03-03T16:23:59.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",1.0
Hongchao Deng,"[<JIRA Component: name='server', id='12312382'>]",2014-08-07T22:26:52.000+0000,Hongchao Deng,"When clientPort is specified in the new format, using ""server.x=host:port1:port2;clientPort"" in either static or dynamic file and without a ""clientPort = xxxx"" statement, a standalone mode server doesn't set up client port. 

A second problem is that zkServer.sh looks for the client port in both static and dynamic files, but when looking in the static files it only looks for the ""clientPort"" statement, so if its specified in the new format the port will be missed and commands such as ""zkServer.sh status"" will not work. This is a problem for standalone mode, but also in distributed mode when the server is still LOOKING (once a leader is established and the server is LEADING/FOLLOWING/OBSERVING, a dynamic file is created and the client port will be found by the script).

Review Board:
https://reviews.apache.org/r/24786/","[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-2006,Major,Hongchao Deng,Fixed,2014-08-19T20:53:12.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Standalone mode won't take client port from dynamic config,2014-08-30T18:29:27.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
,"[<JIRA Component: name='leaderElection', id='12312378'>]",2014-08-07T14:10:23.000+0000,Ioannis Canellos,"We are embedding the zookeeper server in our container and every now and then I see the exception below when running our integration tests suite.

This is something that have never bother us before when using 3.4.5 but we do see in 3.4.6. 

When this occurs, the ensemble is not formed.


java.io.IOException: Could not rename temporary file /data/zookeeper/0001/version-2/currentEpoch.tmp to /data/zookeeper/0001/version-2/currentEpoch
        at org.apache.zookeeper.common.AtomicFileOutputStream.close(AtomicFileOutputStream.java:82)
        at org.apache.zookeeper.server.quorum.QuorumPeer.writeLongToFile(QuorumPeer.java:1202)
        at org.apache.zookeeper.server.quorum.QuorumPeer.setCurrentEpoch(QuorumPeer.java:1223)
        at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:395)",[],Bug,ZOOKEEPER-2005,Major,Ioannis Canellos,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Failure to setCurrentEpoch on lead,2015-08-26T20:46:58.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",3.0
,"[<JIRA Component: name='scripts', id='12312384'>]",2014-08-05T07:47:59.000+0000,Nelson,"Hi,

With zookeeper 3.3.6, output is as expected (cf last line which returns the result of ls /
{code}
nelson@nelson-laptop (0) $ ./zookeeper-3.3.6/bin/zkCli.sh -server 127.0.0.1:2181 ls /
Connecting to 127.0.0.1:2181
.... LOGS ....
2014-08-04 16:22:53,032 - INFO  [main:Environment@97] - Client environment:user.name=nelson
2014-08-04 16:22:53,032 - INFO  [main:Environment@97] - Client environment:user.home=/home/nelson
2014-08-04 16:22:53,033 - INFO  [main:Environment@97] - Client environment:user.dir=/home/nelson/git/
2014-08-04 16:22:53,035 - INFO  [main:ZooKeeper@379] - Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@75af8109
2014-08-04 16:22:53,056 - INFO  [main-SendThread():ClientCnxn$SendThread@1058] - Opening socket connection to server /127.0.0.1:2181
2014-08-04 16:22:53,158 - INFO  [main-SendThread(127.0.0.1:2181):ClientCnxn$SendThread@947] - Socket connection established to127.0.0.1:2181, initiating session
2014-08-04 16:22:53,216 - INFO  [main-SendThread(127.0.0.1:2181):ClientCnxn$SendThread@736] - Session establishment complete on server 127.0.0.1:2181, sessionid = 0x147a10f7d02005a, negotiated timeout = 30000

WATCHER::

WatchedEvent state:SyncConnected type:None path:null
[kafka, zookeeper, mesos, marathon, chronos]
nelson@nelson-laptop (0) $ 
{code}

With zookeeper 3.4.6 no output
{code}
nelson@nelson-laptop (0) $ ./zookeeper-3.4.6/bin/zkCli.sh -server 127.0.0.1:2181 ls /
Connecting to 127.0.0.1:2181
.... LOGS ....
2014-08-04 16:22:56,480 [myid:] - INFO  [main:Environment@100] - Client environment:user.name=nelson
2014-08-04 16:22:56,480 [myid:] - INFO  [main:Environment@100] - Client environment:user.home=/home/nelson
2014-08-04 16:22:56,480 [myid:] - INFO  [main:Environment@100] - Client environment:user.dir=/home/nelson/git/
2014-08-04 16:22:56,481 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@87d53d8

nelson@nelson-laptop (0) $ 
{code}
",[],Bug,ZOOKEEPER-2004,Major,Nelson,Duplicate,2016-08-10T19:32:19.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zkCli doesn't output command,2016-08-10T19:32:19.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",6.0
,[],2014-08-05T07:04:17.000+0000,Samer Al-Kiswany,"After studying the steps ZooKeeper takes to update the logs we found the following bug. The bug may not manifest in the current file system implementations, but it violates the POSIX recommendations and may be an issue in some file systems.

Looking at the strace of zookeeper we see the following:
mkdir(v)
create(v/log)
append(v/log)
trunk(v/log)
write(v/log) 
fdatasync(v/log)

Although the data is fdatasynced to the log, the parent directory was never fsynced, consequently in case of a crash, the parent directory or the log file may be lost, as the parent directory and file metadata were never persisted on disk.
To be safe, both the log directory, and parent directory of the log directory should be fsynced as well.",[],Bug,ZOOKEEPER-2003,Major,Samer Al-Kiswany,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Missing fsync() on the logs parent directory,2014-08-05T17:49:35.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",4.0
,[],2014-08-03T08:06:45.000+0000,Ruel Balabis,,[],Bug,ZOOKEEPER-2002,Major,Ruel Balabis,Invalid,2014-08-03T08:19:00.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Host,2014-08-03T08:19:00.000+0000,[],1.0
Hongchao Deng,[],2014-07-30T18:09:48.000+0000,Hongchao Deng,"The gitattributes set java files line ending be LF.
The DynamicConfigBackwardCompatibilityTest.java uses CRLF and should be converted to LF.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1999,Major,Hongchao Deng,Fixed,2014-08-01T14:59:23.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Converting CRLF to LF in DynamicConfigBackwardCompatibilityTest,2014-08-01T21:03:15.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",3.0
Damien Diederen,"[<JIRA Component: name='c client', id='12312380'>]",2014-07-29T19:27:17.000+0000,Raúl Gutiérrez Segalés,"(commented this on ZOOKEEPER-338)

I've just noticed that we call getaddrinfo from zookeeper_interest... on every call. So from zookeeper_interest we always call update_addrs:

https://github.com/apache/zookeeper/blob/trunk/src/c/src/zookeeper.c#L2082

which in turns unconditionally calls resolve_hosts:

https://github.com/apache/zookeeper/blob/trunk/src/c/src/zookeeper.c#L787

which does the unconditional calls to getaddrinfo:

https://github.com/apache/zookeeper/blob/trunk/src/c/src/zookeeper.c#L648

We should fix this since it'll make 3.5.0 slower for people relying on DNS. I think this is happened as part of ZOOKEEPER-107 in which the list of servers can be updated. 

cc: [~shralex], [~phunt], [~fpj]","[<JIRA Version: name='3.7.0', id='12346617'>]",Bug,ZOOKEEPER-1998,Major,Raúl Gutiérrez Segalés,Fixed,2020-05-17T13:17:42.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,C library calls getaddrinfo unconditionally from zookeeper_interest,2021-03-28T08:55:32.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",14.0
,[],2014-07-28T20:22:38.000+0000,Hongchao Deng,"A server goes to standalone mode if there is only a single server line in server list description.

In fact, the server line was ignored.

The test [testStandaloneQuorum|https://github.com/apache/zookeeper/blob/fa6d21fa8a8acba812237538e4f7172faf969d37/src/java/test/org/apache/zookeeper/test/StandaloneTest.java#L64] was incorrectly successful before -- the client port was ignored and the server was responding through the jetty port.

When I do a client port check, it failed.

This is caused by the logic in [checkvalidity|https://github.com/apache/zookeeper/blob/4bb76bd22916de8dcfe0c40f649d02d61737e871/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeerConfig.java#L439]:
{code}
 if (numMembers > 1  || (!standaloneEnabled && numMembers > 0)) {
...
{code}
This would assume it's standaloneEnabled mode and won't take anything in server list where the client port is defined as introduced in 3.5 dynamic config format.

This is undesired after introducing reconfig because a cluster could set up one server and then add more later.",[],Bug,ZOOKEEPER-1997,Major,Hongchao Deng,Duplicate,2014-08-14T22:48:37.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,server with a single line server list shouldn't be StandaloneEnabled,2014-08-14T22:48:37.000+0000,[],4.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2014-07-28T13:17:52.000+0000,Dmitry Sivachenko,"Documentation contains the following warning about FreeBSD:
http://zookeeper.apache.org/doc/r3.4.6/zookeeperAdmin.html#sc_systemReq
-------
FreeBSD is supported as a development and production platform for clients only. Java NIO selector support in the FreeBSD JVM is broken.
-------

I believe it is outdated info from pre-OpenJDK time.  With recent OpenJDK-7
I am running Zookeeper in production without any problems and I asked other people who run it on FreeBSD, they also experience no trouble.

I propose to remove this information and list FreeBSD as supported platform unless you know something bad in particular.",[],Bug,ZOOKEEPER-1996,Major,Dmitry Sivachenko,Duplicate,2015-11-18T23:27:41.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Incorrect statement in documentation,2015-11-18T23:27:41.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",2.0
Hongchao Deng,[],2014-07-28T02:05:18.000+0000,Hongchao Deng,"This issue supersedes our discussion in ZOOKEEPER-1989.

To summarize, ZK users can seamlessly upgrade 3.4 to 3.5. But two things will happen:
1. the server list will be separated out as a dynamic file (the original should be backup automatically).
2. Client port is mandatory on reconfig. So when reconfig the server itself (its id), the client port in config file will be removed and replaced by the one in reconfig (written in dynamic file).","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1992,Blocker,Hongchao Deng,Fixed,2014-08-01T05:43:13.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,backward compatibility of zoo.cfg,2014-08-02T19:00:12.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Biju Nair,"[<JIRA Component: name='scripts', id='12312384'>]",2014-07-26T01:29:49.000+0000,Reed Wanderman-Milne,"If ZooKeeper is started with zkServer.sh, and an error is shown that a ZooKeeper process is already running, the command returns with an exit status of 0, while it should end with a non-zero exit status.

Example:
$ bin/zkServer.sh start
JMX enabled by default
Using config: /home/reed/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... already running as process 25063.
$ echo $?
0

This can make it difficult for automated scripts to check if creating a new ZooKeeper process was successful, as it won't catch if a user accidentally started it before. 
","[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-1991,Minor,Reed Wanderman-Milne,Fixed,2016-03-02T18:33:51.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkServer.sh returns with a zero exit status when a ZooKeeper process is already running,2016-07-21T20:18:42.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",5.0
Norbert Kalmár,[],2014-07-25T23:33:44.000+0000,Patrick D. Hunt,"It's not clear to me why we are doing this, but it looks very suspicious. Why aren't we just calling ""new Random()"" in these cases? (even for the tests I don't really see it - typically that would just be for repeatability)

{noformat}
ag ""new Random[ \t]*\("" .
src/java/main/org/apache/zookeeper/ClientCnxn.java
817:        private Random r = new Random(System.nanoTime());        

src/java/main/org/apache/zookeeper/client/StaticHostProvider.java
75:       sourceOfRandomness = new Random(System.currentTimeMillis() ^ this.hashCode());
98:        sourceOfRandomness = new Random(randomnessSeed);

src/java/main/org/apache/zookeeper/server/quorum/AuthFastLeaderElection.java
420:                rand = new Random(java.lang.Thread.currentThread().getId()

src/java/main/org/apache/zookeeper/server/SyncRequestProcessor.java
64:    private final Random r = new Random(System.nanoTime());

src/java/main/org/apache/zookeeper/server/ZooKeeperServer.java
537:        Random r = new Random(id ^ superSecret);
554:        Random r = new Random(sessionId ^ superSecret);

src/java/test/org/apache/zookeeper/server/quorum/WatchLeakTest.java
271:        Random r = new Random(SESSION_ID ^ superSecret);

src/java/test/org/apache/zookeeper/server/quorum/CommitProcessorTest.java
151:            Random rand = new Random(Thread.currentThread().getId());
258:            Random rand = new Random(Thread.currentThread().getId());
288:        Random rand = new Random(Thread.currentThread().getId());

src/java/test/org/apache/zookeeper/test/StaticHostProviderTest.java
40:    private Random r = new Random(1);

{noformat}
","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-1990,Critical,Patrick D. Hunt,Fixed,2018-09-10T09:45:39.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,suspicious instantiation of java Random instances,2019-05-20T17:51:03.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",8.0
Alexander Shraer,"[<JIRA Component: name='tests', id='12312427'>]",2014-07-24T00:02:02.000+0000,Patrick D. Hunt,"I tried a fairly simple test, start a three node cluster, bring it down, then restart it. On restart the servers elect the leader and send updates, however the negotiation never completes - the client ports are never bound for example.","[<JIRA Version: name='3.5.1', id='12326786'>]",Bug,ZOOKEEPER-1987,Blocker,Patrick D. Hunt,Fixed,2014-11-07T17:25:50.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,unable to restart 3 node cluster,2014-11-07T17:25:50.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",6.0
desmondhe,"[<JIRA Component: name='c client', id='12312380'>]",2014-07-23T02:22:37.000+0000,desmondhe,"in the file zookeeper.c, most function call of ""close_buffer_oarchive(&oa, 0)"" shoud been instead by 
close_buffer_oarchive(&oa, rc < 0 ? 1 : 0); ",[],Bug,ZOOKEEPER-1985,Major,desmondhe,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Memory leak in C client,2022-02-03T08:50:17.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",4.0
Alexander Shraer,"[<JIRA Component: name='tests', id='12312427'>]",2014-07-22T21:37:26.000+0000,Patrick D. Hunt,"I'm seeing intermittent failures in testLeaderTimesoutOnNewQuorum

 It's failing both on jdk6 and jdk7. (this is my personal
jenkins, I haven't see any other failures than this during the past
few days).

{noformat}
junit.framework.AssertionFailedError
at org.apache.zookeeper.test.ReconfigTest.testServerHasConfig(ReconfigTest.java:127)
at org.apache.zookeeper.test.ReconfigTest.testLeaderTimesoutOnNewQuorum(ReconfigTest.java:450)
at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
{noformat}
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1984,Major,Patrick D. Hunt,Fixed,2014-07-23T16:37:02.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,testLeaderTimesoutOnNewQuorum is a flakey test ,2014-07-24T11:22:03.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",3.0
Shyamal Prasad,"[<JIRA Component: name='server', id='12312382'>]",2014-07-22T21:18:38.000+0000,Shyamal Prasad,"Currently zkServer.sh will redirect output to zookeeper.out using a simple shell redirect. 

When logrotate (and similar tools) are used to rotate the zookeeper.out file with the 'copytruncate' semantics (copy the file, truncate it to zero bytes) the next write results in a sparse file with the write at the offset of the last file. Effectively the log file is now full a null bytes and it is hard to read/use the file (and the rotated copies). 

Even worse, the result is zookeeper.out file only gets ""larger"" (though sparse) and after a while on a chatty system it takes significant CPU resources to compress the file (which is all nulls!)

The simple fix is to append to the file (>>) instead of a simple redirection (>)

This issue was found in a 3.3.5 production system, however code in trunk has the same issue.","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-1983,Major,Shyamal Prasad,,,This issue is being actively worked on at the moment by the assignee.,In Progress,0.0,Append to zookeeper.out (not overwrite) to support logrotation,2022-02-03T08:36:24.000+0000,"[<JIRA Version: name='3.3.5', id='12319081'>, <JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.4.6', id='12323310'>]",4.0
,"[<JIRA Component: name='server', id='12312382'>]",2014-07-20T03:01:01.000+0000,Patrick D. Hunt,"Findbugs 2.0.3 found a number of internationalization issues with the code, we ignored these for the time being in ZOOKEEPER-1975, however we should address them one by one eventually.",[],Bug,ZOOKEEPER-1976,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,address internationalization issues identified by findbugs 2.0.3,2017-06-19T10:33:43.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.4.11', id='12339207'>]",2.0
Flavio Paiva Junqueira,"[<JIRA Component: name='c client', id='12312380'>]",2014-07-19T16:57:27.000+0000,Patrick D. Hunt,"The winvs2008 build is failing with

bq. unresolved external symbol __imp__ZOO_READONLY_STATE

see: https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper-trunk-WinVS2008/1445/console","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1974,Blocker,Patrick D. Hunt,Fixed,2014-07-25T16:24:35.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"winvs2008 jenkins job failing with ""unresolved external symbol""",2014-07-25T17:39:13.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Bill Havanki,"[<JIRA Component: name='server', id='12312382'>]",2014-07-19T16:50:31.000+0000,Patrick D. Hunt,"The recent Jetty Server additions ZOOKEEPER-1346 have broken ibm6 support, can someone take a look? (we've had issues like this in the past - typically due to using specialized/sun classes that don't exist in that jdk)

https://builds.apache.org/job/ZooKeeper-trunk-ibm6/556/","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1973,Major,Patrick D. Hunt,Fixed,2014-07-21T17:59:05.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Jetty Server changes broke ibm6 support,2014-07-21T18:57:47.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Hongchao Deng,[],2014-07-18T16:47:12.000+0000,Hongchao Deng,"The test is failiing:
{code}
failed SocketConnector@0.0.0.0:8080: java.net.BindException: Address already in use
{code}

I tried to assign a unique port in test and Jenkins build comes normal again.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1969,Major,Hongchao Deng,Fixed,2014-07-18T17:05:11.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Fix Port Already In Use for JettyAdminServerTest,2014-07-18T17:56:44.000+0000,[],4.0
Orion Hodson,"[<JIRA Component: name='c client', id='12312380'>]",2014-07-15T13:52:11.000+0000,Orion Hodson,"The patch attached to https://issues.apache.org/jira/browse/ZOOKEEPER-1953 has caused problems for git users when committed from SVN.

The attached patch simply changes the line endings of the offending files from CRLF to LF in the hope that when they are committed to SVN, the LF line endings end up as the canonical representation in the Apache ZooKeeper git repo.

An interpretation of what's happened here is that svn has stored the CRLF line endings and these have been pushed into git by git-svn as described here: http://blog.subgit.com/line-endings-handling-in-svn-git-and-subgit/. Git clients are then confused as the text files have an unexpected representation in the repo.

Experimentally VS is indifferent to line endings – ran dos2unix on the vcxproj and sln files and VS opened and closed the files without modifying them. This page seems to advertise the indifference to line endings and discusses selecting custom options: http://msdn.microsoft.com/en-us/library/dd409797.aspx.

","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1966,Major,Orion Hodson,Fixed,2014-07-16T16:01:35.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,VS and line breaks,2014-07-17T17:23:27.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
,"[<JIRA Component: name='c client', id='12312380'>]",2014-07-15T02:36:07.000+0000,Sonny Yang,"after (ant compile_jute) and(cd src/c/ ./configure), make can not be done!
It contains error:
/tmp/ccyJ6new.s: Assembler messages:
/tmp/ccyJ6new.s:67: Error: Unrecognized opcode: `lock'
/tmp/ccyJ6new.s:102: Error: Unrecognized opcode: `lock'
/tmp/ccyJ6new.s:431: Error: Unrecognized opcode: `lock'
/tmp/ccyJ6new.s:464: Error: Unrecognized opcode: `lock'
make[1]: *** [libzkmt_la-mt_adaptor.lo] Error 1
make[1]: Leaving directory `/gpfs/ibmu/sjtupower/rawdep/zookeeper-3.4.5/src/c'
make: *** [all] Error 2

I don't know how to fix it",[],Bug,ZOOKEEPER-1965,Major,Sonny Yang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Install could not be done on powerpc Error: Unrecognized opcode: `lock',2014-07-15T02:36:07.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",1.0
Hongchao Deng,[],2014-07-11T23:20:58.000+0000,Hongchao Deng,"There is flaky tests in ReconfigTest showing something like:
{code}
junit.framework.AssertionFailedError: Mismatches ElectionAddress! expected:<[127.0.0.1]:12369> but was:<[localhost]:12369>
	at org.apache.zookeeper.test.ReconfigTest.assertRemotePeerMXBeanAttributes(ReconfigTest.java:967)
	at org.apache.zookeeper.test.ReconfigTest.testJMXBeanAfterRemoveAddOne(ReconfigTest.java:809)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
{code}

Basically, the problem is that there might be inconsistency between numerical ip and literal ip. Converting both sides to one (numerical IP) will fix it.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1964,Minor,Hongchao Deng,Fixed,2014-07-14T17:46:29.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Fix Flaky Test in ReconfigTest.java,2014-07-15T11:20:22.000+0000,[],5.0
,"[<JIRA Component: name='c client', id='12312380'>]",2014-07-08T00:32:36.000+0000,Erik Anderson,"When attempting to connect to a zookeeper server that is not currently running, the connection will return the ""connection refused"" message through the timeout logic.  This is because Windows is returning the error code through select->error rather than select->write (which is what the logic is apparently expecting)

Patch is pending",[],Bug,ZOOKEEPER-1958,Minor,Erik Anderson,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Client does not detect rejected connections,2014-07-08T06:33:43.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",1.0
,[],2014-07-07T10:04:25.000+0000,nijel,"The script zkCleanup.sh support cleaning the zk data in linux system.
The same function needs to be supported in windows also",[],Bug,ZOOKEEPER-1956,Minor,nijel,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Support Cleanup script in windows,2016-03-03T01:51:55.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",2.0
,[],2014-07-04T22:30:02.000+0000,Aaron Zimmerman,"We have a 5 node zookeeper cluster that has been operating normally for several months.  Starting a few days ago, the entire cluster crashes a few times per day, all nodes at the exact same time.  We can't track down the exact issue, but deleting the snapshots and logs and restarting allows the cluster to come back up.  

We are running exhibitor to monitor the cluster.  

It appears that something bad gets into the logs, causing an EOFException and this cascades through the entire cluster:



2014-07-04 12:55:26,328 [myid:1] - WARN  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:Follower@89] - Exception when following the leader
java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:375)
        at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
        at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:83)
        at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:108)
        at org.apache.zookeeper.server.quorum.Learner.readPacket(Learner.java:152)
        at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:85)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:740)
2014-07-04 12:55:26,328 [myid:1] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:Follower@166] - shutdown called
java.lang.Exception: shutdown Follower
        at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:166)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:744)


Then the server dies, exhibitor tries to restart each node, and they all get stuck trying to replay the bad transaction, logging things like:
 

2014-07-04 12:58:52,734 [myid:1] - INFO  [main:FileSnap@83] - Reading snapshot /var/lib/zookeeper/version-2/snapshot.300011fc0
2014-07-04 12:58:52,896 [myid:1] - DEBUG [main:FileTxnLog$FileTxnIterator@575] - Created new input stream /var/lib/zookeeper/version-2/log.300000021
2014-07-04 12:58:52,915 [myid:1] - DEBUG [main:FileTxnLog$FileTxnIterator@578] - Created new input archive /var/lib/zookeeper/version-2/log.300000021
2014-07-04 12:59:25,870 [myid:1] - DEBUG [main:FileTxnLog$FileTxnIterator@618] - EOF excepton java.io.EOFException: Failed to read /var/lib/zookeeper/version-2/log.300000021
2014-07-04 12:59:25,871 [myid:1] - DEBUG [main:FileTxnLog$FileTxnIterator@575] - Created new input stream /var/lib/zookeeper/version-2/log.300011fc2
2014-07-04 12:59:25,872 [myid:1] - DEBUG [main:FileTxnLog$FileTxnIterator@578] - Created new input archive /var/lib/zookeeper/version-2/log.300011fc2
2014-07-04 12:59:48,722 [myid:1] - DEBUG [main:FileTxnLog$FileTxnIterator@618] - EOF excepton java.io.EOFException: Failed to read /var/lib/zookeeper/version-2/log.300011fc2

And the cluster is dead.  The only way we have found to recover is to delete all of the data and restart.

[~fournc] Appreciate any assistance you can offer.  ",[],Bug,ZOOKEEPER-1955,Major,Aaron Zimmerman,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,EOFException on Reading Snapshot,2015-11-30T15:13:43.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",8.0
,"[<JIRA Component: name='java client', id='12312381'>]",2014-07-03T20:45:09.000+0000,Bill Havanki,"I have been getting constant failures of the {{ClientPortBindTest}} unit test (see ZOOKEEPER-1256) on my Macbook. I traced the problem to loss of the IPv6 scope ID on the address chosen for the loopback address in the unit test.

The address chosen is: fe80:0:0:0:0:0:0:1%1. The scope ID here is 1, after the percent sign.

The scope ID is lost in the {{resolveAndShuffle()}} method of {{StaticHostProvider}}. The method uses {{InetAddress.getByAddress()}} which apparently does not preserve the scope ID in the host string it is passed. {{Inet6Address.getByAddress()}} can, although you have to parse the scope ID out of the host string yourself and pass it as its own parameter.",[],Bug,ZOOKEEPER-1954,Minor,Bill Havanki,,,This issue is being actively worked on at the moment by the assignee.,In Progress,0.0,StaticHostProvider loses IPv6 scope ID when resolving server addresses,2015-06-18T12:58:18.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",3.0
nijel,[],2014-07-02T12:26:43.000+0000,nijel,"The log folder and log file name is configurable now.
The default log folder is ""."" in distribution. So the log file (zookeeper.out) will be placed in bin folder

Can this be changed to <zk_home>/logs/zookeeperserver-<hostname>.log ?
","[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-1952,Minor,nijel,Fixed,2015-03-01T16:50:00.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Default log directory and file name can be changed,2015-03-02T11:14:56.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",7.0
,[],2014-06-30T21:43:37.000+0000,Hongchao Deng,"The current implementation divide information of servers of legacy config into two separate dynamic config files. There is a problem.

When we set ""clientPort"" variable in config file, it gets automatically erased and later on there is no information about ""clientPort"" in either the old or new (.dynamic) config file.

It becomes a serious problem when users of *3.4* jump to *3.5* directly without changing their config: when a server crashes and restarts, there is no client port serving.

For example,
a legacy config might look like:

```zoo.cfg
dataDir=/root/zookeeper/groupconfig/conf1/data
syncLimit=5
initLimit=10
tickTime=2000
clientPort=2181
server.1=127.0.0.1:2222:2223
server.2=127.0.0.1:3333:3334
server.3=127.0.0.1:4444:4445
```

After dynamic reconfig, it might look like

```zoo.cfg
dataDir=/root/zookeeper/groupconfig/conf1/data
syncLimit=5
tickTime=2000
initLimit=10
dynamicConfigFile=./zoo.cfg.dynamic
```

and
```zoo.cfg.dynamic
server.1=127.0.0.1:2222:2223:participant
server.2=127.0.0.1:3333:3334:participant
server.3=127.0.0.1:4444:4445:participant
version=e00000000
```

This could be successfully started at first time. But when server restarts from crash, it never serve client port again.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1950,Major,Hongchao Deng,Duplicate,2014-07-24T15:20:20.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,configBackwardCompatibilityMode breaks compatibility,2014-07-24T16:06:06.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",1.0
Rakesh Radhakrishnan,"[<JIRA Component: name='recipes', id='12313246'>]",2014-06-27T08:01:23.000+0000,Rakesh Radhakrishnan,"Following recipe jars doesn't exists in the distribution ""zookeeper-3.4.6.tar.gz""

recipes/election/zookeeper-3.4.6-recipes-election.jar
recipes/lock/zookeeper-3.4.6-recipes-lock.jar
recipes/queue/zookeeper-3.4.6-recipes-queue.jar","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-1949,Major,Rakesh Radhakrishnan,Fixed,2015-02-11T07:02:48.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,recipes jar not included in the distribution package,2015-02-11T11:16:03.000+0000,[],5.0
Mark Flickinger,[],2014-06-24T03:51:18.000+0000,Mark Flickinger,"This is the same issue as ZOOKEEPER-1719, where the shebang for etc/init.d/zookeeper is set to /bin/sh, but needs to be fixed for deb packages(in src/packages/deb/init.d/zookeeper)","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1945,Major,Mark Flickinger,Fixed,2014-06-25T13:42:11.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"deb - zkCli.sh, zkServer.sh and zkEnv.sh regression caused by ZOOKEEPER-1663",2014-06-26T11:19:07.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",4.0
Hongchao Deng,"[<JIRA Component: name='contrib', id='12312700'>]",2014-06-19T17:36:06.000+0000,Hongchao Deng,"Using the git repo and checkout ""branch-3.4"", the ""src/contrib/zooinspector/NOTICE.txt"" file always shows up for changes but cannot be checkout or reset.

This is caused by "".gitattributes"" line ""text=auto"" where git automatically sets the line ending stuff.

To solve this, I am gonna commit the git auto change and submit it as a patch for ""branch-3.4"".","[<JIRA Version: name='3.4.7', id='12325149'>]",Bug,ZOOKEEPER-1943,Major,Hongchao Deng,Fixed,2014-06-23T00:07:29.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"""src/contrib/zooinspector/NOTICE.txt"" isn't complying to "".gitattributes"" in branch-3.4",2014-06-23T00:08:59.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",3.0
,[],2014-06-18T14:48:33.000+0000,Kalvin Misquith,"For OSGI applications, the zookeeper manifest file should have org.ietf.jgss in its Import-Package statement. org.apache.zookeeper.client.ZooKeeperSaslClient imports org.ietf.jgss.*. 

The following ClassDefNotFoundError occurs without it. 

java.lang.NoClassDefFoundError: org.ietf.jgss.GSSException
    at java.lang.J9VMInternals.verifyImpl(Native Method)
    at java.lang.J9VMInternals.verify(J9VMInternals.java:94)
    at java.lang.J9VMInternals.initialize(J9VMInternals.java:171)
    at org.apache.zookeeper.ClientCnxn$SendThread.startConnect(ClientCnxn.java:945)
    at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1003)
Caused by: java.lang.ClassNotFoundException: org.ietf.jgss.GSSException
    at org.eclipse.osgi.internal.loader.BundleLoader.findClassInternal(BundleLoader.java:501)
    at org.eclipse.osgi.internal.loader.BundleLoader.findClass(BundleLoader.java:421)
    at org.eclipse.osgi.internal.loader.BundleLoader.findClass(BundleLoader.java:412)
    at org.eclipse.osgi.internal.baseadaptor.DefaultClassLoader.loadClass(DefaultClassLoader.java:107)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:707)
    ... 5 more
",[],Bug,ZOOKEEPER-1942,Major,Kalvin Misquith,Duplicate,2015-08-21T13:24:52.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZooKeeper OSGi package imports: org.ietf.jgss dependency missing from manifest,2015-11-12T20:06:39.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",11.0
Rakesh Radhakrishnan,"[<JIRA Component: name='tests', id='12312427'>]",2014-06-12T06:39:20.000+0000,Rakesh Radhakrishnan,"Following is the failure log message:

2014-06-11 23:53:22,538 [myid:] - INFO  [main:JUnit4ZKTestRunner$LoggedInvokeMethod@62] - TEST METHOD FAILED testNextConfigUnreachable
java.lang.AssertionError: QP failed to shutdown in 30 seconds: QuorumPeer[myid=0]/127.0.0.1:11251
	at org.junit.Assert.fail(Assert.java:93)
	at org.apache.zookeeper.test.QuorumBase.shutdown(QuorumBase.java:393)
	at org.apache.zookeeper.server.quorum.QuorumPeerTestBase$TestQPMain.shutdown(QuorumPeerTestBase.java:52)
	at org.apache.zookeeper.server.quorum.QuorumPeerTestBase$MainThread.shutdown(QuorumPeerTestBase.java:161)
	at org.apache.zookeeper.server.quorum.ReconfigRecoveryTest.testNextConfigUnreachable(ReconfigRecoveryTest.java:268)","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1939,Major,Rakesh Radhakrishnan,Fixed,2014-06-25T17:29:59.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ReconfigRecoveryTest.testNextConfigUnreachable is failing,2014-06-26T11:19:10.000+0000,[],4.0
Marshall McMullen,[],2014-06-11T03:28:06.000+0000,Nathan Sullivan,"ZOOKEEPER-1719 changed the interpreter to bash for zkCli.sh, zkServer.sh and zkEnv.sh, but did not change src/packages/deb/init.d/zookeeper 

This causes the following failure using /bin/sh

[...] root@hostname:~# service zookeeper stop
/etc/init.d/zookeeper: 81: /usr/libexec/zkEnv.sh: Syntax error: ""("" unexpected (expecting ""fi"")

Simple fix, change the shebang to #!/bin/bash - tested and works fine.",[],Bug,ZOOKEEPER-1937,Major,Nathan Sullivan,Fixed,2014-07-24T23:17:25.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,init script needs fixing for ZOOKEEPER-1719,2014-07-25T11:25:04.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",7.0
Ted Yu,"[<JIRA Component: name='server', id='12312382'>]",2014-06-10T07:25:37.000+0000,Harald Musum,"We sometime see issues with ZooKeeper server not starting and seeing this error in the log:


[2014-05-27 09:29:48.248] ERROR   : -               
.org.apache.zookeeper.server.ZooKeeperServerMain    Unexpected exception,
exiting abnormally\nexception=\njava.io.IOException: Unable to create data
directory /home/y/var/zookeeper/version-2\n\tat
org.apache.zookeeper.server.persistence.FileTxnSnapLog.<init>(FileTxnSnapLog.java:85)\n\tat
org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:103)\n\tat
org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:86)\n\tat
org.apache.zookeeper.server.ZooKeeperServerMain.main(ZooKeeperServerMain.java:52)\n\tat
org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:116)\n\tat
org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)\n\t
[...]

Stack trace from JVM gives this:

""PurgeTask"" daemon prio=10 tid=0x000000000201d000 nid=0x1727 runnable
[0x00007f55d7dc7000]
   java.lang.Thread.State: RUNNABLE
    at java.io.UnixFileSystem.createDirectory(Native Method)
    at java.io.File.mkdir(File.java:1310)
    at java.io.File.mkdirs(File.java:1337)
    at
org.apache.zookeeper.server.persistence.FileTxnSnapLog.<init>(FileTxnSnapLog.java:84)
    at org.apache.zookeeper.server.PurgeTxnLog.purge(PurgeTxnLog.java:68)
    at
org.apache.zookeeper.server.DatadirCleanupManager$PurgeTask.run(DatadirCleanupManager.java:140)
    at java.util.TimerThread.mainLoop(Timer.java:555)
    at java.util.TimerThread.run(Timer.java:505)

""zookeeper server"" prio=10 tid=0x00000000027df800 nid=0x1715 runnable
[0x00007f55d7ed8000]
   java.lang.Thread.State: RUNNABLE
    at java.io.UnixFileSystem.createDirectory(Native Method)
    at java.io.File.mkdir(File.java:1310)
    at java.io.File.mkdirs(File.java:1337)
    at
org.apache.zookeeper.server.persistence.FileTxnSnapLog.<init>(FileTxnSnapLog.java:84)
    at
org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:103)
    at
org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:86)
    at
org.apache.zookeeper.server.ZooKeeperServerMain.main(ZooKeeperServerMain.java:52)
    at
org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:116)
    at
org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)
[...]

So it seems that when autopurge is used (as it is in our case), it might happen at the same time as starting the server itself. In FileTxnSnapLog() it will check if the directory exists and create it if not. These two tasks do this at the same time, and mkdir fails and server exits the JVM.

","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-1936,Minor,Harald Musum,Fixed,2020-01-23T15:47:09.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Server exits when unable to create data directory due to race ,2020-01-24T14:20:39.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",16.0
,[],2014-06-05T23:01:06.000+0000,Marshall McMullen,"In our regression testing we encountered an error wherein we were caching a value we read from zookeeper and then experienced session loss. We subsequently got reconnected to a different zookeeper server. When we tried to read the same path from this new zookeeper server we are getting a stale value.

Specifically, we are reading ""/binchanges"" and originally got back a value of ""3"" from the first server. After we lost connection and reconnected before the session timeout, we then read ""/binchanges"" from the new server and got back a value of ""2"". In our code path we never set this value from 3 to 2. We throw an assertion if the value ever goes backwards. Which is how we caught this error. 

It's my understanding of the single system image guarantee that this should never be allowed. I realize that the single system image guarantee is still quorum based and it's certainly possible that a minority of the ensemble may have stale data. However, I also believe that each client has to send the highest zxid it's seen as part of its connection request to the server. And if the server it's connecting to has a smaller zxid than the value the client sends, then the connection request should be refused.

Assuming I have all of that correct, then I'm at a loss for how this happened. 

The failure happened around Jun  4 08:13:44. Just before that, at June  4 08:13:30 there was a round of leader election. During that round of leader election we voted server with id=4 and zxid=0x300001c4c. This then led to a new zxid=0x400000001. The new leader sends a diff to all the servers including the one we will soon read the stale data from (id=2). Server with ID=2's log files also reflect that as of 08:13:43 it was up to date and current with an UPTODATE message.

I'm going to attach log files from all 5 ensemble nodes. I also used zktreeutil to dump the database out for the 5 ensemble nodes. I diff'd those, and compared them all for correctness. 1 of the nodes (id=2) has a massively divergent zktreeutil dump than the other 4 nodes even though it received the diff from the new leader.

In the attachments there are 5 nodes. I will number each log file by it's zookeeper id, e.g. node4.log.



",[],Bug,ZOOKEEPER-1934,Major,Marshall McMullen,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Stale data received from sync'd ensemble peer,2014-09-16T16:04:42.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
Orion Hodson,"[<JIRA Component: name='c client', id='12312380'>]",2014-06-03T16:59:07.000+0000,Norris Lee,"When building zookeeper in Visual Studio in debug mode, the client can connect to the server without error. When building in release mode, I get a continuous error message:
{code}
2014-06-02 11:25:20,070:7144(0xc84):ZOO_INFO@zookeeper_init_internal@1008: Initiating client connection, host=192.168.39.43:5181 sessionTimeout=30000 watcher=10049C90 sessionId=0 sessionPasswd=<null> context=001FC0F0 flags=0
2014-06-02 11:25:20,072:7144(0xc84):ZOO_DEBUG@start_threads@221: starting threads...
2014-06-02 11:25:20,072:7144(0x1ea0):ZOO_DEBUG@do_completion@460: started completion thread
2014-06-02 11:25:20,072:7144(0x1e08):ZOO_DEBUG@do_io@403: started IO thread
2014-06-02 11:25:20,072:7144(0x1e08):ZOO_DEBUG@get_next_server_in_reconfig@1148: [OLD] count=0 capacity=0 next=0 hasnext=0
2014-06-02 11:25:20,072:7144(0x1e08):ZOO_DEBUG@get_next_server_in_reconfig@1151: [NEW] count=1 capacity=16 next=0 hasnext=1
2014-06-02 11:25:20,072:7144(0x1e08):ZOO_DEBUG@get_next_server_in_reconfig@1160: Using next from NEW=192.168.39.43:5181
2014-06-02 11:25:20,072:7144(0x1e08):ZOO_DEBUG@zookeeper_interest@1992: [zk] connect()

2014-06-02 11:25:20,158:7144(0x1e08):ZOO_ERROR@handle_socket_error_msg@1847: Socket [192.168.39.43:5181] zk retcode=-4, errno=10035(Unknown error): failed to send a handshake packet: Unknown error
2014-06-02 11:25:20,158:7144(0x1e08):ZOO_DEBUG@handle_error@1595: Previous connection=[192.168.39.43:5181] delay=0
2014-06-02 11:25:20,158:7144(0x1e08):ZOO_DEBUG@get_next_server_in_reconfig@1148: [OLD] count=0 capacity=0 next=0 hasnext=0
2014-06-02 11:25:20,158:7144(0x1e08):ZOO_DEBUG@get_next_server_in_reconfig@1151: [NEW] count=1 capacity=16 next=0 hasnext=1
2014-06-02 11:25:20,158:7144(0x1e08):ZOO_DEBUG@get_next_server_in_reconfig@1160: Using next from NEW=192.168.39.43:5181
2014-06-02 11:25:20,158:7144(0x1e08):ZOO_DEBUG@zookeeper_interest@1992: [zk] connect()

2014-06-02 11:25:20,159:7144(0x1e08):ZOO_ERROR@handle_socket_error_msg@1847: Socket [192.168.39.43:5181] zk retcode=-4, errno=10035(Unknown error): failed to send a handshake packet: Unknown error
2014-06-02 11:25:20,159:7144(0x1e08):ZOO_DEBUG@handle_error@1595: Previous connection=[192.168.39.43:5181] delay=0
2014-06-02 11:25:20,159:7144(0x1e08):ZOO_DEBUG@get_next_server_in_reconfig@1148: [OLD] count=0 capacity=0 next=0 hasnext=0
2014-06-02 11:25:20,159:7144(0x1e08):ZOO_DEBUG@get_next_server_in_reconfig@1151: [NEW] count=1 capacity=16 next=0 hasnext=1
2014-06-02 11:25:20,159:7144(0x1e08):ZOO_DEBUG@get_next_server_in_reconfig@1160: Using next from NEW=192.168.39.43:5181
2014-06-02 11:25:20,159:7144(0x1e08):ZOO_DEBUG@zookeeper_interest@1992: [zk] connect()

2014-06-02 11:25:20,159:7144(0x1e08):ZOO_ERROR@handle_socket_error_msg@1847: Socket [192.168.39.43:5181] zk retcode=-4, errno=10035(Unknown error): failed to send a handshake packet: Unknown error
2014-06-02 11:25:20,159:7144(0x1e08):ZOO_DEBUG@handle_error@1595: Previous connection=[192.168.39.43:5181] delay=0
2014-06-02 11:25:20,159:7144(0x1e08):ZOO_DEBUG@get_next_server_in_reconfig@1148: [OLD] count=0 capacity=0 next=0 hasnext=0
2014-06-02 11:25:20,159:7144(0x1e08):ZOO_DEBUG@get_next_server_in_reconfig@1151: [NEW] count=1 capacity=16 next=0 hasnext=1
2014-06-02 11:25:20,159:7144(0x1e08):ZOO_DEBUG@get_next_server_in_reconfig@1160: Using next from NEW=192.168.39.43:5181
2014-06-02 11:25:20,159:7144(0x1e08):ZOO_DEBUG@zookeeper_interest@1992: [zk] connect()
{code}","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1933,Major,Norris Lee,Fixed,2014-07-25T16:58:40.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Windows release build of zk client cannot connect to zk server,2014-07-25T17:39:11.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",7.0
Michael Han,"[<JIRA Component: name='leaderElection', id='12312378'>]",2014-06-02T00:58:01.000+0000,Michi Mutsuzaki,"org.apache.zookeeper.test.LETest.testLE is failing on trunk once in a while. I'm not able to reproduce the failure on my box. I looked at the log, but I couldn't quite figure out what's going on. 

https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper-trunk/2315/testReport/

Update:
======
Because LE is deprecated there is not much points on spending effort fixing it, as discussed in the JIRA. Updated JIRA title to reflect the state of the issue.","[<JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-1932,Major,Michi Mutsuzaki,Fixed,2017-05-11T15:01:21.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Remove deprecated LeaderElection class,2017-05-11T16:42:03.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",7.0
Chengwei Yang,"[<JIRA Component: name='documentation', id='12312422'>]",2014-05-28T05:47:51.000+0000,Chengwei Yang,"With the sequence flag, ZooKeeper automatically appends a sequence number that is *greater that* any one previously appended to a child of ""/election"".

*greater that* above should be *greater than*",[],Bug,ZOOKEEPER-1930,Minor,Chengwei Yang,Fixed,2014-05-28T17:58:17.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,A typo in zookeeper recipes.html,2014-05-29T11:35:48.000+0000,[],4.0
Charles Strahan,"[<JIRA Component: name='contrib-zkfuse', id='12312644'>]",2014-05-24T04:13:02.000+0000,Eduard White,"Trying to open zk root directory:

./zkfuse -z localhost:2181 -m /CLOUD/zookeeper -d
1    [0x7f89362d0780] INFO  zkfuse null - Starting zkfuse
cacheSize = 256, debug = 1, forceDirSuffix = ""._dir_"", mount = ""/CLOUD/zookeeper"", name = ""_data_"", zookeeper = ""localhost:2181"", optind = 6, argc = 6, current arg = ""NULL""
1    [0x7f89362d0780] INFO  zkfuse null - Create ZK adapter
1    [0x7f89362d0780] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::ZooKeeperAdapter(zk::ZooKeeperConfig, zk::ZKEventListener*, bool)::Trace::Trace(const void*) 0x434ecd Enter
1    [0x7f89362d0780] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::ZooKeeperAdapter(zk::ZooKeeperConfig, zk::ZKEventListener*, bool)::Trace::~Trace() 0x434ecd Exit
1    [0x7f89362bd700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::processEvents()::Trace::Trace(const void*) 0x434df4 Enter
1    [0x7f89327bb700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::processUserEvents()::Trace::Trace(const void*) 0x434e60 Enter
1    [0x7f89362d0780] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::reconnect()::Trace::Trace(const void*) 0x434c71 Enter
1    [0x7f89362d0780] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::disconnect()::Trace::Trace(const void*) 0x434c4c Enter
1    [0x7f89362d0780] TRACE zookeeper.adapter null - mp_zkHandle: (nil), state 0
1    [0x7f89362d0780] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::disconnect()::Trace::~Trace() 0x434c4c Exit
2014-05-24 08:07:44,860:20540(0x7f89362d0780):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.6
2014-05-24 08:07:44,860:20540(0x7f89362d0780):ZOO_INFO@log_env@716: Client environment:host.name=nanoha
2014-05-24 08:07:44,860:20540(0x7f89362d0780):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-05-24 08:07:44,860:20540(0x7f89362d0780):ZOO_INFO@log_env@724: Client environment:os.arch=3.2.0-4-amd64
2014-05-24 08:07:44,860:20540(0x7f89362d0780):ZOO_INFO@log_env@725: Client environment:os.version=#1 SMP Debian 3.2.54-2
2014-05-24 08:07:44,860:20540(0x7f89362d0780):ZOO_INFO@log_env@733: Client environment:user.name=root
2014-05-24 08:07:44,860:20540(0x7f89362d0780):ZOO_INFO@log_env@741: Client environment:user.home=/root
2014-05-24 08:07:44,860:20540(0x7f89362d0780):ZOO_INFO@log_env@753: Client environment:user.dir=/opt/zoo/3.4.6/build/contrib/zkfuse/src
2014-05-24 08:07:44,860:20540(0x7f89362d0780):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=localhost:2181 sessionTimeout=1000 watcher=0x429780 sessionId=0 sessionPasswd=<null> context=0x9b5e30 flags=0
2014-05-24 08:07:44,861:20540(0x7f89362d0780):ZOO_DEBUG@start_threads@221: starting threads...
2    [0x7f89362d0780] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::setState(zk::ZooKeeperAdapter::AdapterState)::Trace::Trace(const void*) 0x434c43 Enter
2    [0x7f89362d0780] INFO  zookeeper.adapter null - Adapter state transition: 0 -> 1
2014-05-24 08:07:44,861:20540(0x7f8931899700):ZOO_DEBUG@do_completion@459: started completion thread
2    [0x7f89362d0780] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::setState(zk::ZooKeeperAdapter::AdapterState)::Trace::~Trace() 0x434c43 Exit
3    [0x7f89362d0780] DEBUG zookeeper.adapter null - mp_zkHandle: 0x9bbba0, state 1
2014-05-24 08:07:44,861:20540(0x7f893209a700):ZOO_DEBUG@do_io@367: started IO thread
3    [0x7f89362d0780] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::reconnect()::Trace::~Trace() 0x434c71 Exit
ZOOKEEPER_ROOT_CHILDREN_WATCH_BUG enabled
3    [0x7f89362d0780] INFO  zkfuse null - Initialize fuse
2014-05-24 08:07:44,861:20540(0x7f893209a700):ZOO_INFO@check_events@1705: initiated connection to server [127.0.0.1:2181]
FUSE library version: 2.9.3
nullpath_ok: 0
nopath: 0
utime_omit_ok: 0
unique: 1, opcode: INIT (26), nodeid: 0, insize: 56, pid: 0
INIT: 7.17
flags=0x0000047b
max_readahead=0x00020000
   INIT: 7.19
   flags=0x00000013
   max_readahead=0x00020000
   max_write=0x00020000
   max_background=0
   congestion_threshold=0
   unique: 1, success, outsize: 40
2014-05-24 08:07:44,909:20540(0x7f893209a700):ZOO_INFO@check_events@1752: session establishment complete on server [127.0.0.1:2181], sessionId=0x1461f2be1b10025, negotiated timeout=4000
2014-05-24 08:07:44,909:20540(0x7f893209a700):ZOO_DEBUG@check_events@1758: Calling a watcher for a ZOO_SESSION_EVENT and the state=ZOO_CONNECTED_STATE
2014-05-24 08:07:44,910:20540(0x7f8931899700):ZOO_DEBUG@process_completions@2113: Calling a watcher for node [], type = -1 event=ZOO_SESSION_EVENT
51   [0x7f8931899700] TRACE zookeeper.adapter null - zk::zkWatcher(zhandle_t*, int, int, const char*, void*)::Trace::Trace(const void*) 0x434e10 Enter
51   [0x7f8931899700] INFO  zookeeper.adapter null - Received a ZK event - type: -1, state: 3, path: ''
51   [0x7f8931899700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::enqueueEvent(int, int, const string&)::Trace::Trace(const void*) 0x434e02 Enter
51   [0x7f8931899700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::enqueueEvent(int, int, const string&)::Trace::~Trace() 0x434e02 Exit
51   [0x7f8931899700] TRACE zookeeper.adapter null - zk::zkWatcher(zhandle_t*, int, int, const char*, void*)::Trace::~Trace() 0x434e10 Exit
51   [0x7f89362bd700] INFO  zookeeper.adapter null - Received SESSION event, state: 3. Adapter state: 1
51   [0x7f89362bd700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::setState(zk::ZooKeeperAdapter::AdapterState)::Trace::Trace(const void*) 0x434c43 Enter
51   [0x7f89362bd700] INFO  zookeeper.adapter null - Adapter state transition: 1 -> 2
51   [0x7f89362bd700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::setState(zk::ZooKeeperAdapter::AdapterState)::Trace::~Trace() 0x434c43 Exit
52   [0x7f89327bb700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::handleEvent(int, int, const string&)::Trace::Trace(const void*) 0x434e37 Enter
52   [0x7f89327bb700] TRACE zookeeper.adapter null - type: -1, state 3, path: 
52   [0x7f89327bb700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::handleEvent(int, int, const string&, const Listener2Context&)::Trace::Trace(const void*) 0x434de7 Enter
52   [0x7f89327bb700] DEBUG zkfuse null - eventReceived() eventType -1, eventState 3, path 
52   [0x7f89327bb700] TRACE zkfuse null - *** CONNECTED ***
52   [0x7f89327bb700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::handleEvent(int, int, const string&, const Listener2Context&)::Trace::~Trace() 0x434de7 Exit
52   [0x7f89327bb700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::handleEvent(int, int, const string&)::Trace::~Trace() 0x434e37 Exit
2014-05-24 08:07:46,196:20540(0x7f893209a700):ZOO_DEBUG@zookeeper_process@2264: Got ping response in 0 ms
2014-05-24 08:07:47,531:20540(0x7f893209a700):ZOO_DEBUG@zookeeper_process@2264: Got ping response in 0 ms
2014-05-24 08:07:48,865:20540(0x7f893209a700):ZOO_DEBUG@zookeeper_process@2264: Got ping response in 0 ms
2014-05-24 08:07:50,200:20540(0x7f893209a700):ZOO_DEBUG@zookeeper_process@2264: Got ping response in 0 ms
2014-05-24 08:07:51,535:20540(0x7f893209a700):ZOO_DEBUG@zookeeper_process@2264: Got ping response in 0 ms
2014-05-24 08:07:52,869:20540(0x7f893209a700):ZOO_DEBUG@zookeeper_process@2264: Got ping response in 0 ms
unique: 2, opcode: ACCESS (34), nodeid: 1, insize: 48, pid: 16822
   unique: 2, error: -38 (Function not implemented), outsize: 16
unique: 3, opcode: GETATTR (3), nodeid: 1, insize: 56, pid: 16822
getattr /
8606 [0x7f8931098700] DEBUG zkfuse null - zkfuse_getattr(path /)
8606 [0x7f8931098700] DEBUG zkfuse null - getattr(path /)
8606 [0x7f8931098700] DEBUG zkfuse null - getZkPath(path /)
8606 [0x7f8931098700] DEBUG zkfuse null - getZkPath returns /, nameType 2
8606 [0x7f8931098700] DEBUG zkfuse null - open(path /, justCreated 0)
8606 [0x7f8931098700] DEBUG zkfuse null - allocate(path /)
8606 [0x7f8931098700] DEBUG zkfuse null - not found
8606 [0x7f8931098700] DEBUG zkfuse null - free list empty, resize handle 1
8606 [0x7f8931098700] DEBUG zkfuse null - constructor() path /
8606 [0x7f8931098700] DEBUG zkfuse null - incRefCount(count 0) path /
8606 [0x7f8931098700] DEBUG zkfuse null - incRefCount returns 1
8606 [0x7f8931098700] DEBUG zkfuse null - numInUse 1
8606 [0x7f8931098700] DEBUG zkfuse null - allocate returns 1, newFile 1
8606 [0x7f8931098700] DEBUG zkfuse null - update(newFile 1) path /
8606 [0x7f8931098700] DEBUG zkfuse null - initialized children 0, data 0
8606 [0x7f8931098700] DEBUG zkfuse null - has children watch 0, data watch 0
8606 [0x7f8931098700] DEBUG zkfuse null - update children
8606 [0x7f8931098700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::getNodeChildren(std::vector<std::basic_string<char> >&, const string&, zk::ZKEventListener*, void*)::Trace::Trace(const void*) 0x434ede Enter
8607 [0x7f8931098700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::validatePath(const string&)::Trace::Trace(const void*) 0x434cc4 Enter
8607 [0x7f8931098700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::validatePath(const string&)::Trace::~Trace() 0x434cc4 Exit
8607 [0x7f8931098700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::verifyConnection()::Trace::Trace(const void*) 0x434cb3 Enter
8607 [0x7f8931098700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::verifyConnection()::Trace::~Trace() 0x434cb3 Exit
2014-05-24 08:07:53,465:20540(0x7f8931098700):ZOO_DEBUG@zoo_awget_children_@2874: Sending request xid=0x53801b11 for path [/] to 127.0.0.1:2181
2014-05-24 08:07:53,465:20540(0x7f893209a700):ZOO_DEBUG@process_sync_completion@1870: Processing sync_completion with type=3 xid=0x53801b11 rc=0
8607 [0x7f8931098700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::getNodeChildren(std::vector<std::basic_string<char> >&, const string&, zk::ZKEventListener*, void*)::Trace::~Trace() 0x434ede Exit
8607 [0x7f8931098700] DEBUG zkfuse null - update children done
8607 [0x7f8931098700] DEBUG zkfuse null - node first use or reuse
8607 [0x7f8931098700] DEBUG zkfuse null - update data
8607 [0x7f8931098700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::getNodeData(const string&, zk::ZKEventListener*, void*, Stat*)::Trace::Trace(const void*) 0x434e82 Enter
8607 [0x7f8931098700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::validatePath(const string&)::Trace::Trace(const void*) 0x434cc4 Enter
8607 [0x7f8931098700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::validatePath(const string&)::Trace::~Trace() 0x434cc4 Exit
8607 [0x7f8931098700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::verifyConnection()::Trace::Trace(const void*) 0x434cb3 Enter
8607 [0x7f8931098700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::verifyConnection()::Trace::~Trace() 0x434cb3 Exit
2014-05-24 08:07:53,466:20540(0x7f8931098700):ZOO_DEBUG@zoo_awget@2661: Sending request xid=0x53801b12 for path [/] to 127.0.0.1:2181
2014-05-24 08:07:53,466:20540(0x7f893209a700):ZOO_DEBUG@process_sync_completion@1870: Processing sync_completion with type=2 xid=0x53801b12 rc=0
8608 [0x7f8931098700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::getNodeData(const string&, zk::ZKEventListener*, void*, Stat*)::Trace::~Trace() 0x434e82 Exit
8608 [0x7f8931098700] DEBUG zkfuse null - update data done, latest version 0
8608 [0x7f8931098700] DEBUG zkfuse null - update set active version 0
8608 [0x7f8931098700] DEBUG zkfuse null - update returns 0
8608 [0x7f8931098700] DEBUG zkfuse null - open returns 1
8608 [0x7f8931098700] DEBUG zkfuse null - getattr(nameType 2) path /
8608 [0x7f8931098700] DEBUG zkfuse null - isRegNameType(nameType 2) returns 0
8608 [0x7f8931098700] DEBUG zkfuse null - directory
8608 [0x7f8931098700] DEBUG zkfuse null - hasChild(childPath /.zkfuse.dir) returns 0
8608 [0x7f8931098700] DEBUG zkfuse null - getattr returns 0
8608 [0x7f8931098700] DEBUG zkfuse null - close() path /
8608 [0x7f8931098700] DEBUG zkfuse null - flush() path /
8608 [0x7f8931098700] DEBUG zkfuse null - not dirty
8608 [0x7f8931098700] DEBUG zkfuse null - flush returns 0
8608 [0x7f8931098700] DEBUG zkfuse null - deallocate(handle 1)
8608 [0x7f8931098700] DEBUG zkfuse null - incRefCount(count -1) path /
8608 [0x7f8931098700] DEBUG zkfuse null - incRefCount returns 0
8608 [0x7f8931098700] DEBUG zkfuse null - path / ref count 0
8608 [0x7f8931098700] DEBUG zkfuse null - deallocate done
8608 [0x7f8931098700] DEBUG zkfuse null - close returns 0
8608 [0x7f8931098700] DEBUG zkfuse null - getattr returns 0
8608 [0x7f8931098700] DEBUG zkfuse null - zkfuse_getattr returns 0
   unique: 3, success, outsize: 120
unique: 4, opcode: OPENDIR (27), nodeid: 1, insize: 48, pid: 16822
opendir flags: 0x98800 /
unique: 5, opcode: INTERRUPT (36), nodeid: 0, insize: 48, pid: 0
8610 [0x7f8930897700] DEBUG zkfuse null - zkfuse_opendir(path /)
INTERRUPT: 4
8610 [0x7f8930897700] DEBUG zkfuse null - getZkPath(path /)
8610 [0x7f8930897700] DEBUG zkfuse null - getZkPath returns /, nameType 2
8610 [0x7f8930897700] DEBUG zkfuse null - open(path /, justCreated 0)
8610 [0x7f8930897700] DEBUG zkfuse null - allocate(path /)
8610 [0x7f8930897700] DEBUG zkfuse null - found
8610 [0x7f8930897700] DEBUG zkfuse null - incRefCount(count 1) path /
8610 [0x7f8930897700] DEBUG zkfuse null - incRefCount returns 1
8610 [0x7f8930897700] DEBUG zkfuse null - resurrecting zombie, numInUse 1
8610 [0x7f8930897700] DEBUG zkfuse null - allocate returns 1, newFile 0
8611 [0x7f8930897700] DEBUG zkfuse null - update(newFile 0) path /
8611 [0x7f8930897700] DEBUG zkfuse null - initialized children 1, data 1
8611 [0x7f8930897700] DEBUG zkfuse null - has children watch 1, data watch 1
8611 [0x7f8930897700] DEBUG zkfuse null - update children
8611 [0x7f8930897700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::getNodeChildren(std::vector<std::basic_string<char> >&, const string&, zk::ZKEventListener*, void*)::Trace::Trace(const void*) 0x434ede Enter
8611 [0x7f8930897700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::validatePath(const string&)::Trace::Trace(const void*) 0x434cc4 Enter
8611 [0x7f8930897700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::validatePath(const string&)::Trace::~Trace() 0x434cc4 Exit
8611 [0x7f8930897700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::verifyConnection()::Trace::Trace(const void*) 0x434cb3 Enter
8611 [0x7f8930897700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::verifyConnection()::Trace::~Trace() 0x434cb3 Exit
2014-05-24 08:07:53,469:20540(0x7f8930897700):ZOO_DEBUG@zoo_awget_children_@2874: Sending request xid=0x53801b13 for path [/] to 127.0.0.1:2181
2014-05-24 08:07:53,469:20540(0x7f893209a700):ZOO_DEBUG@process_sync_completion@1870: Processing sync_completion with type=3 xid=0x53801b13 rc=0
8611 [0x7f8930897700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::getNodeChildren(std::vector<std::basic_string<char> >&, const string&, zk::ZKEventListener*, void*)::Trace::~Trace() 0x434ede Exit
8611 [0x7f8930897700] DEBUG zkfuse null - update children done
8611 [0x7f8930897700] DEBUG zkfuse null - node first use or reuse
8611 [0x7f8930897700] DEBUG zkfuse null - update set active version 0
8611 [0x7f8930897700] DEBUG zkfuse null - update returns 0
8611 [0x7f8930897700] DEBUG zkfuse null - open returns 1
8611 [0x7f8930897700] DEBUG zkfuse null - incOpenDirCount(count 1) path /
8611 [0x7f8930897700] DEBUG zkfuse null - incOpenDirCount returns 1
8611 [0x7f8930897700] DEBUG zkfuse null - zkfuse_opendir returns 0
   opendir[1] flags: 0x98800 /
   unique: 4, success, outsize: 32
unique: 6, opcode: READDIR (28), nodeid: 1, insize: 80, pid: 16822
readdir[1] from 0
8612 [0x7f8913fff700] DEBUG zkfuse null - zkfuse_readdir(path /, offset 0)
8612 [0x7f8913fff700] DEBUG zkfuse null - readdir(offset 0) path /
8612 [0x7f8913fff700] DEBUG zkfuse null - isMeta(childName /aliases.json) returns 0
8612 [0x7f8913fff700] DEBUG zkfuse null - isMeta(childName /clusterstate.json) returns 0
8612 [0x7f8913fff700] DEBUG zkfuse null - isMeta(childName /collections) returns 0
8612 [0x7f8913fff700] DEBUG zkfuse null - isMeta(childName /configs) returns 0
8612 [0x7f8913fff700] DEBUG zkfuse null - isMeta(childName /live_nodes) returns 0
8612 [0x7f8913fff700] DEBUG zkfuse null - isMeta(childName /overseer) returns 0
8612 [0x7f8913fff700] DEBUG zkfuse null - isMeta(childName /overseer_elect) returns 0
8612 [0x7f8913fff700] DEBUG zkfuse null - isMeta(childName /zookeeper) returns 0
8612 [0x7f8913fff700] DEBUG zkfuse null - open(path /aliases.json, justCreated 0)
8612 [0x7f8913fff700] DEBUG zkfuse null - allocate(path /aliases.json)
8612 [0x7f8913fff700] DEBUG zkfuse null - not found
8612 [0x7f8913fff700] DEBUG zkfuse null - free list empty, resize handle 2
8612 [0x7f8913fff700] DEBUG zkfuse null - constructor() path /aliases.json
8612 [0x7f8913fff700] DEBUG zkfuse null - incRefCount(count 0) path /aliases.json
8612 [0x7f8913fff700] DEBUG zkfuse null - incRefCount returns 1
8612 [0x7f8913fff700] DEBUG zkfuse null - numInUse 2
8612 [0x7f8913fff700] DEBUG zkfuse null - allocate returns 2, newFile 1
8612 [0x7f8913fff700] DEBUG zkfuse null - update(newFile 1) path /aliases.json
8612 [0x7f8913fff700] DEBUG zkfuse null - initialized children 0, data 0
8612 [0x7f8913fff700] DEBUG zkfuse null - has children watch 0, data watch 0
8613 [0x7f8913fff700] DEBUG zkfuse null - update children
8613 [0x7f8913fff700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::getNodeChildren(std::vector<std::basic_string<char> >&, const string&, zk::ZKEventListener*, void*)::Trace::Trace(const void*) 0x434ede Enter
8613 [0x7f8913fff700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::validatePath(const string&)::Trace::Trace(const void*) 0x434cc4 Enter
8613 [0x7f8913fff700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::validatePath(const string&)::Trace::~Trace() 0x434cc4 Exit
8613 [0x7f8913fff700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::verifyConnection()::Trace::Trace(const void*) 0x434cb3 Enter
8613 [0x7f8913fff700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::verifyConnection()::Trace::~Trace() 0x434cb3 Exit
2014-05-24 08:07:53,471:20540(0x7f8913fff700):ZOO_DEBUG@zoo_awget_children_@2874: Sending request xid=0x53801b14 for path [/aliases.json] to 127.0.0.1:2181
2014-05-24 08:07:53,471:20540(0x7f893209a700):ZOO_DEBUG@process_sync_completion@1870: Processing sync_completion with type=3 xid=0x53801b14 rc=0
8613 [0x7f8913fff700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::getNodeChildren(std::vector<std::basic_string<char> >&, const string&, zk::ZKEventListener*, void*)::Trace::~Trace() 0x434ede Exit
8613 [0x7f8913fff700] DEBUG zkfuse null - update children done
8613 [0x7f8913fff700] DEBUG zkfuse null - node first use or reuse
8613 [0x7f8913fff700] DEBUG zkfuse null - update data
8613 [0x7f8913fff700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::getNodeData(const string&, zk::ZKEventListener*, void*, Stat*)::Trace::Trace(const void*) 0x434e82 Enter
8613 [0x7f8913fff700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::validatePath(const string&)::Trace::Trace(const void*) 0x434cc4 Enter
8613 [0x7f8913fff700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::validatePath(const string&)::Trace::~Trace() 0x434cc4 Exit
8614 [0x7f8913fff700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::verifyConnection()::Trace::Trace(const void*) 0x434cb3 Enter
8614 [0x7f8913fff700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::verifyConnection()::Trace::~Trace() 0x434cb3 Exit
2014-05-24 08:07:53,472:20540(0x7f8913fff700):ZOO_DEBUG@zoo_awget@2661: Sending request xid=0x53801b15 for path [/aliases.json] to 127.0.0.1:2181
2014-05-24 08:07:53,472:20540(0x7f893209a700):ZOO_DEBUG@process_sync_completion@1870: Processing sync_completion with type=2 xid=0x53801b15 rc=0
8614 [0x7f8913fff700] TRACE zookeeper.adapter null - zk::ZooKeeperAdapter::getNodeData(const string&, zk::ZKEventListener*, void*, Stat*)::Trace::~Trace() 0x434e82 Exit
terminate called after throwing an instance of 'std::length_error'
  what():  basic_string::_S_create","[<JIRA Version: name='3.4.8', id='12326517'>, <JIRA Version: name='3.5.2', id='12331981'>]",Bug,ZOOKEEPER-1929,Major,Eduard White,Fixed,2015-11-21T20:15:47.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,std::length_error on update children,2016-07-21T20:18:25.000+0000,"[<JIRA Version: name='3.3.5', id='12319081'>, <JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.1', id='12326786'>]",5.0
Chris Nauroth,"[<JIRA Component: name='scripts', id='12312384'>]",2014-05-10T18:06:37.000+0000,Ed Schmed,"Fails to write PID file with a permissions error, because the startup script fails to read the dataDir variable from zoo.cfg, and then tries to use the drive root ( / ) as the data dir.

Tracked the problem down to line 84 of zkServer.sh:

ZOO_DATADIR=""$(grep ""^[[:space:]]*dataDir"" ""$ZOOCFG"" | sed -e 's/.*=//')""

If i run just that line and point it right at the config file, ZOO_DATADIR is empty.

If I remove [[:space:]]* from the grep:

ZOO_DATADIR=""$(grep ""^dataDir"" ""$ZOOCFG"" | sed -e 's/.*=//')""

Then it works fine. (If I also make the same change on line 164 and 169)

My regex skills are pretty bad, so I'm afraid to comment on why [[space]]* needs to be in there?","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-1927,Major,Ed Schmed,Fixed,2015-08-25T05:14:41.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"zkServer.sh fails to read dataDir (and others) from zoo.cfg on Solaris 10 (grep issue, manifests as FAILED TO WRITE PID).  ",2016-09-08T03:58:37.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",8.0
Enis Soztutar,"[<JIRA Component: name='tests', id='12312427'>]",2014-05-08T22:34:57.000+0000,Enis Soztutar,"Some of the unit tests are creating temp files under system tmp dir (/tmp), and put data there.

We should encapsulate all temporary data from unit tests under build/test/data. ant clean will clean all data from previous runs. 

","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1926,Major,Enis Soztutar,Fixed,2014-05-09T21:40:21.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Unit tests should only use build/test/data for data,2014-05-20T11:09:13.000+0000,[],4.0
,[],2014-05-07T12:28:07.000+0000,Shwetha GS,,[],Bug,ZOOKEEPER-1924,Major,Shwetha GS,Invalid,2014-05-07T12:28:31.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"\,/ 'n;kj kkln. l,   ",2014-05-07T12:28:31.000+0000,[],1.0
Chengwei Yang,"[<JIRA Component: name='documentation', id='12312422'>]",2014-05-05T13:49:19.000+0000,Chengwei Yang,"There is a typo in the document zookeeperStarted.*, see http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html, in the section *Connecting to ZooKeeper*, where the *help* output *createpath* which should be *create path*.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1923,Minor,Chengwei Yang,Fixed,2014-05-08T21:47:12.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,A typo in zookeeperStarted document,2017-08-07T09:02:16.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",7.0
,"[<JIRA Component: name='server', id='12312382'>]",2014-05-05T12:21:40.000+0000,J Potter,"We're seeing the output of stat on one node return a negative value for min latency time:

stat 
Zookeeper version: 3.4.6-1569965, built on 02/20/2014 09:09 GMT
Clients:
...
Latency min/avg/max: -477/149/261002

(The max value seems suspicious, too.)

Figured I'd report this, as I don't see any mention of it online or in other bug reports. Maybe negative values shouldn't be recorded?",[],Bug,ZOOKEEPER-1922,Minor,J Potter,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Negative min latency value,2018-07-02T12:34:08.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",6.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2014-05-04T15:28:32.000+0000,Nathan Neulinger,"I did not see this behavior with 3.4.5. When I upgraded to 3.4.6, couldn't establish quorum any more - what I found was that the listener on :3888 was only running on 127.0.0.1.

I was able to work around it by forcing dns to get the external hostnames, or removing 'my' hostname from /etc/hosts.

Key point - this appears to be a significant change in behavior from 3.4.5 - which I did not have any problems with... 

I know you can specify the clientPortAddress - is there any way in the configuration to specify which address should be used for quorum connection listeners - or to force it to listen on 0.0.0.0 for quorum connections?",[],Bug,ZOOKEEPER-1921,Major,Nathan Neulinger,Not A Problem,2014-12-23T20:57:22.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zk binding to 127.0.0.1 for quorum connections,2014-12-23T20:57:22.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",3.0
Rakesh Radhakrishnan,"[<JIRA Component: name='java client', id='12312381'>]",2014-04-28T02:59:50.000+0000,liuyang,"A new ZooKeeper client will start three threads, the SendThread, EventThread and LoginThread. I belive that these threads will be shutdown after call the zk.close. I test that the SendThread and EventThread will be die,  but LoginThread is still alive. The stack is:

""Thread-0"" daemon prio=10 tid=0x00007ffcf0020000 nid=0x69c8 waiting on condition [0x00007ffd3cc25000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.zookeeper.Login$1.run(Login.java:183)
	at java.lang.Thread.run(Thread.java:744)",[],Bug,ZOOKEEPER-1920,Minor,liuyang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Login thread is not shutdown when close the ClientCnxn,2015-04-30T11:26:50.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",7.0
Raúl Gutiérrez Segalés,"[<JIRA Component: name='c client', id='12312380'>]",2014-04-25T02:44:52.000+0000,Raúl Gutiérrez Segalés,See https://issues.apache.org/jira/browse/ZOOKEEPER-1910,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-1919,Blocker,Raúl Gutiérrez Segalés,Fixed,2018-05-29T23:29:12.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Update the C implementation of removeWatches to have it match ZOOKEEPER-1910,2019-05-20T17:50:28.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
Flavio Paiva Junqueira,[],2014-04-22T18:02:04.000+0000,Flavio Paiva Junqueira,"Check the CVE entry for a description:

http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2014-0085","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-1917,Blocker,Flavio Paiva Junqueira,Fixed,2014-10-13T03:16:16.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Apache Zookeeper logs cleartext admin passwords,2014-10-13T04:22:49.000+0000,[],6.0
Michi Mutsuzaki,"[<JIRA Component: name='c client', id='12312380'>]",2014-04-18T22:51:56.000+0000,Michi Mutsuzaki,"https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/2051/console

     [exec]      [exec] /home/jenkins/jenkins-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/src/c/tests/TestWatchers.cc:667: Assertion: assertion failed [Expression: ensureCondition( deliveryTracker.deliveryCounterEquals(2),1000)<1000]
",[],Bug,ZOOKEEPER-1914,Major,Michi Mutsuzaki,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,TestWatchers.cc failure,2022-02-03T08:50:18.000+0000,[],4.0
Raúl Gutiérrez Segalés,"[<JIRA Component: name='build', id='12312383'>]",2014-04-18T20:42:22.000+0000,Raúl Gutiérrez Segalés,"Without the proposed patch, I get invalid manifests because stderr is added to the revision property. I think this might be something specific to my setup though:

{noformat}
    $ java -version
    Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=utf8
    java version ""1.7.0_51""
    OpenJDK Runtime Environment (fedora-2.4.5.1.fc20-x86_64 u51-b31)
    OpenJDK 64-Bit Server VM (build 24.51-b03, mixed mode)
{noformat}

since it doesn't seem happen with older java/ant combinations.

Nonetheless, it seems like the right thing is to explicitly ignore stderr.","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1913,Major,Raúl Gutiérrez Segalés,Fixed,2014-04-18T22:35:19.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Invalid manifest files due to bogus revision property value,2014-04-19T11:06:11.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",6.0
Flavio Paiva Junqueira,"[<JIRA Component: name='leaderElection', id='12312378'>]",2014-04-14T20:14:50.000+0000,Tanakorn Leesatapornwongsa,"In 3-node cluster, when there are 2 nodes die and reboot during leader election, it might lead to the case that there are 2 leaders happen in the system. Eventually, a leader that does not has follower supports and quit being leader, but it makes us lose some availability.

I am building a tools that can reorder messages and disk write, and also inject node crash to the system and found this bug.
These are the step of events that my tools execute in sequence that lead to 2 leaders at the end.
My zookeeper nodes have id = 0,1,2

packetsend from=0 to=1 state=0 leader=0 zxid=0 electionEpoch=1 peerEpoch=0
packetsend from=0 to=2 state=0 leader=0 zxid=0 electionEpoch=1 peerEpoch=0
packetsend from=2 to=0 state=0 leader=2 zxid=0 electionEpoch=1 peerEpoch=0
packetsend from=2 to=1 state=0 leader=2 zxid=0 electionEpoch=1 peerEpoch=0
packetsend from=1 to=0 state=0 leader=1 zxid=0 electionEpoch=1 peerEpoch=0
packetsend from=1 to=2 state=0 leader=1 zxid=0 electionEpoch=1 peerEpoch=0
packetsend from=1 to=0 state=0 leader=2 zxid=0 electionEpoch=1 peerEpoch=0
packetsend from=0 to=1 state=0 leader=2 zxid=0 electionEpoch=1 peerEpoch=0
packetsend from=1 to=2 state=0 leader=2 zxid=0 electionEpoch=1 peerEpoch=0
packetsend from=0 to=2 state=0 leader=2 zxid=0 electionEpoch=1 peerEpoch=0
diskwrite nodeId=0 write=currentEpoch
nodecrash id=0
nodecrash id=1
nodestart id=0
nodestart id=1
diskwrite nodeId=2 write=currentEpoch
packetsend from=2 to=0 state=0 leader=2 zxid=0 electionEpoch=1 peerEpoch=0
packetsend from=0 to=2 state=0 leader=0 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=2 to=1 state=0 leader=2 zxid=0 electionEpoch=1 peerEpoch=0
packetsend from=0 to=1 state=0 leader=0 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=1 to=0 state=0 leader=1 zxid=0 electionEpoch=1 peerEpoch=0
packetsend from=1 to=2 state=0 leader=1 zxid=0 electionEpoch=1 peerEpoch=0
packetsend from=2 to=0 state=2 leader=2 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=1 to=0 state=0 leader=0 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=2 to=1 state=2 leader=2 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=1 to=2 state=0 leader=0 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=2 to=1 state=2 leader=2 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=1 to=0 state=0 leader=0 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=1 to=2 state=0 leader=0 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=0 to=1 state=2 leader=0 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=2 to=1 state=2 leader=2 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=1 to=0 state=0 leader=0 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=1 to=2 state=0 leader=0 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=0 to=1 state=2 leader=0 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=2 to=1 state=2 leader=2 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=1 to=0 state=0 leader=0 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=1 to=2 state=0 leader=0 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=0 to=1 state=2 leader=0 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=2 to=0 state=0 leader=2 zxid=0 electionEpoch=2 peerEpoch=1
packetsend from=2 to=1 state=0 leader=2 zxid=0 electionEpoch=2 peerEpoch=1
packetsend from=0 to=2 state=2 leader=0 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=2 to=0 state=0 leader=2 zxid=0 electionEpoch=2 peerEpoch=1
packetsend from=1 to=0 state=0 leader=2 zxid=0 electionEpoch=2 peerEpoch=1
packetsend from=1 to=2 state=0 leader=2 zxid=0 electionEpoch=2 peerEpoch=1
packetsend from=2 to=1 state=0 leader=2 zxid=0 electionEpoch=2 peerEpoch=1
packetsend from=0 to=2 state=2 leader=0 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=2 to=0 state=0 leader=2 zxid=0 electionEpoch=2 peerEpoch=1
packetsend from=0 to=1 state=2 leader=0 zxid=0 electionEpoch=1 peerEpoch=1
packetsend from=2 to=1 state=0 leader=2 zxid=0 electionEpoch=2 peerEpoch=1
packetsend from=0 to=2 state=2 leader=0 zxid=0 electionEpoch=1 peerEpoch=1
diskwrite nodeId=2 write=currentEpoch
diskwrite nodeId=1 write=currentEpoch",[],Bug,ZOOKEEPER-1912,Critical,Tanakorn Leesatapornwongsa,Not A Problem,2016-02-07T04:13:28.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Leader election lets 2 leaders happen,2016-02-07T04:13:29.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",4.0
Sean Mackrory,[],2014-04-11T15:08:25.000+0000,Sean Mackrory,"If you compile the REST contrib module, the tarball will only include the main JAR. It will not bundle the required dependencies or include the minimal working configuration files.","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1911,Major,Sean Mackrory,Fixed,2014-04-25T08:12:27.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,REST contrib module does not include all required files when packaged,2014-04-25T11:49:41.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",5.0
Rakesh Radhakrishnan,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2014-04-11T08:08:12.000+0000,Rakesh Radhakrishnan,"Consider a case where zkclient has added 2 data watchers(say 'w1' and 'w2') on '/node1'.

Now user has removed w1, but this is deleting the 'CnxnWatcher' in ZK server against the ""/node1"" path. This will affect other data watchers(if any) of same client on same path. In our case 'w2' would not be notified.

Note: please see the attached test case to understand more.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1910,Major,Rakesh Radhakrishnan,Fixed,2014-04-29T02:31:19.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,RemoveWatches wrongly removes the watcher if multiple watches exists on a path,2018-12-16T14:46:07.000+0000,[],8.0
Raúl Gutiérrez Segalés,"[<JIRA Component: name='server', id='12312382'>]",2014-04-09T23:51:28.000+0000,Raúl Gutiérrez Segalés,"ZOOKEEPER-442 introduced support for a new opcode: removeWatches. The way it was implemented though, implies that you need to check on the client side if a watch/watcher is set *before* you send your request to the server. If you don't, ZK will just swallow your request and won't return an error code if there isn't a watch set for that path.

I noticed this whilst implementing removeWatches for Kazoo [1]. As mentioned, I guess it could be expected that clients should do the check on their side but I think that the correct thing would to have the server do the validation and return the error code accordingly as well.

[~rakeshr], [~phunt]: thoughts?

[1] https://github.com/rgs1/kazoo/commit/44ca48e975aeea3fd0664fe13136a72caf89e54f","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1909,Major,Raúl Gutiérrez Segalés,Fixed,2014-04-17T06:52:39.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,removeWatches doesn't return NOWATCHER when there is no watch set,2018-12-16T14:47:45.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
Nikita Vetoshkin,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2014-03-31T07:52:07.000+0000,Nikita Vetoshkin,"In python if we ask {{zookeeper.get}} (which translates into {{pyzoo_get}}) for empty node we can get trash in result on Python level. Issue is pretty tricky. It goes like this:
  * python C extension allocates buffer with malloc {{buffer = malloc(sizeof(char)*buffer_len);}} and calls {{zoo_wget}} providing both {{buffer}} and {{buffer_len}}.
  * deserialize_GetDataResponse deserializes empty buffer and sets {{buffer_len}} to -1 and {{zoo_wget}} returns.
  * python C extension calls {{Py_BuildValue( ""(s#,N)"", buffer,buffer_len ...}} with {{buffer_len}} set to -1.
  * {{Py_BuildValue}} calls {{do_mkvalue}} to build python string which falls back to {{strlen(str)}} in case string length ({{buffer_len < 0}}) - that's our case.
  * *usually* strlen returns 0, because e.g. linux uses magic zero filled page as result of mmap (which is being copied upon page fault, i.e. when you want to write to it)
  * everything works!

But on FreeBSD (not always) we can get random data in {{malloc}} result and this trash will be exposed to the user.

Not sure about the right way to fix this, but something like
{noformat}
Index: src/contrib/zkpython/src/c/zookeeper.c
===================================================================
--- src/contrib/zkpython/src/c/zookeeper.c	(revision 1583238)
+++ src/contrib/zkpython/src/c/zookeeper.c	(working copy)
@@ -1223,7 +1223,7 @@
   }
 
   PyObject *stat_dict = build_stat( &stat );
-  PyObject *ret = Py_BuildValue( ""(s#,N)"", buffer,buffer_len, stat_dict );
+  PyObject *ret = Py_BuildValue( ""(s#,N)"", buffer,buffer_len < 0 ? 0 : buffer_len, stat_dict );
   free(buffer);
 
   return ret;
{noformat}
should do the trick","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1906,Major,Nikita Vetoshkin,Fixed,2014-04-01T21:02:53.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zkpython: invalid data in GetData for empty node,2014-04-02T11:10:38.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",4.0
,[],2014-03-24T14:52:18.000+0000,ROY Assink,,[],Bug,ZOOKEEPER-1903,Major,ROY Assink,Invalid,2014-04-15T17:59:15.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,30/6,2014-04-15T17:59:15.000+0000,[],2.0
,[],2014-03-24T14:51:44.000+0000,ROY Assink,,[],Bug,ZOOKEEPER-1902,Major,ROY Assink,Invalid,2014-03-24T18:11:07.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,summertime,2014-03-24T18:11:07.000+0000,[],3.0
Andrew Kyle Purtell,"[<JIRA Component: name='tests', id='12312427'>]",2014-03-23T20:08:23.000+0000,Andrew Kyle Purtell,"AsyncOpsTest, ChrootAsyncTest, and NioNettySuiteTest can fail running on Java 8 if child znodes are not added to a list in the same order as expected. 

For example

{noformat}
Testcase: testAsyncGetChildrenTwo took 0.166 sec
        FAILED
expected:<OK:/foo:[child[1, child2]]> but was:<OK:/foo:[child[2, child1]]>
junit.framework.AssertionFailedError: expected:<OK:/foo:[child[1, child2]]> but was:<OK:/foo:[child[2, child1]]>
        at org.apache.zookeeper.test.AsyncOps$AsyncCB.verify(AsyncOps.java:113)
        at org.apache.zookeeper.test.AsyncOps$ChildrenCB.verify(AsyncOps.java:298)
        at org.apache.zookeeper.test.AsyncOps$ChildrenCB.verifyGetChildrenTwo(AsyncOps.java:287)
        at org.apache.zookeeper.test.AsyncOpsTest.testAsyncGetChildrenTwo(AsyncOpsTest.java:155)
        at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
{noformat}

{noformat}
Testcase: testAsyncGetChildren2Two took 0.154 sec
        FAILED
expected:<OK:/foo:[child[1, child2]]> but was:<OK:/foo:[child[2, child1]]>
junit.framework.AssertionFailedError: expected:<OK:/foo:[child[1, child2]]> but was:<OK:/foo:[child[2, child1]]>
        at org.apache.zookeeper.test.AsyncOps$AsyncCB.verify(AsyncOps.java:113)
        at org.apache.zookeeper.test.AsyncOps$Children2CB.verify(AsyncOps.java:383)
        at org.apache.zookeeper.test.AsyncOps$Children2CB.verifyGetChildrenTwo(AsyncOps.java:372)
        at org.apache.zookeeper.test.AsyncOpsTest.testAsyncGetChildren2Two(AsyncOpsTest.java:175)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
{noformat}

This seems like a test only issue because getChildren javadoc says ""The list of children returned is not sorted and no guarantee is provided as to its natural or lexical order."" So, fix the tests by sorting the incoming lists. ","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1901,Minor,Andrew Kyle Purtell,Fixed,2014-03-23T21:04:04.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,[JDK8] Sort children for comparison in AsyncOps tests,2014-03-25T16:30:13.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",4.0
Camille Fournier,[],2014-03-21T18:11:12.000+0000,Steven Bower,"The other day we started up a ZK instance that had been down for a bit (1day) and started getting NPEs all over the place...

{noformat}
2014-20-03 11:15:42.320 INFO  QuorumPeerConfig [main] - Reading configuration from: /xxx/bin/zk/etc/zk.cfg
2014-20-03 11:15:42.350 INFO  QuorumPeerConfig [main] - Defaulting to majority quorums
2014-20-03 11:15:42.353 INFO  DatadirCleanupManager [main] - autopurge.snapRetainCount set to 3
2014-20-03 11:15:42.353 INFO  DatadirCleanupManager [main] - autopurge.purgeInterval set to 0
2014-20-03 11:15:42.353 INFO  DatadirCleanupManager [main] - Purge task is not scheduled.
2014-20-03 11:15:42.385 INFO  QuorumPeerMain [main] - Starting quorum peer
2014-20-03 11:15:42.399 INFO  NIOServerCnxnFactory [main] - binding to port 0.0.0.0/0.0.0.0:5555
2014-20-03 11:15:42.413 INFO  QuorumPeer [main] - tickTime set to 2000
2014-20-03 11:15:42.413 INFO  QuorumPeer [main] - minSessionTimeout set to -1
2014-20-03 11:15:42.413 INFO  QuorumPeer [main] - maxSessionTimeout set to -1
2014-20-03 11:15:42.413 INFO  QuorumPeer [main] - initLimit set to 10
2014-20-03 11:15:42.456 INFO  FileSnap [main] - Reading snapshot /xxx/zk_data/version-2/snapshot.2c00000000
2014-20-03 11:15:42.463 INFO  QuorumCnxManager [Thread-3] - My election bind port: 0.0.0.0/0.0.0.0:7555
2014-20-03 11:15:42.470 INFO  QuorumPeer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - LOOKING
2014-20-03 11:15:42.471 INFO  FastLeaderElection [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - New election. My id =  3, proposed zxid=0x8000000000000000
2014-20-03 11:15:42.479 INFO  FastLeaderElection [WorkerReceiver[myid=3]] - Notification: 2 (n.leader), 0x2b00000002 (n.zxid), 0x2c (n.round), FOLLOWING (n.state), 1 (n.sid), 0x2b (n.peerEPoch), LOOKING (my state)
2014-20-03 11:15:42.479 INFO  FastLeaderElection [WorkerReceiver[myid=3]] - Notification: 2 (n.leader), 0x2b00000002 (n.zxid), 0x2c (n.round), FOLLOWING (n.state), 1 (n.sid), 0x2b (n.peerEPoch), LOOKING (my state)
2014-20-03 11:15:42.482 INFO  QuorumCnxManager [WorkerSender[myid=3]] - Have smaller server identifier, so dropping the connection: (5, 3)
2014-20-03 11:15:42.482 INFO  FastLeaderElection [WorkerReceiver[myid=3]] - Notification: 2 (n.leader), 0x2b00000002 (n.zxid), 0x2c (n.round), LEADING (n.state), 2 (n.sid), 0x2b (n.peerEPoch), LOOKING (my state)
2014-20-03 11:15:42.482 INFO  FastLeaderElection [WorkerReceiver[myid=3]] - Notification: 2 (n.leader), 0x2b00000002 (n.zxid), 0x2c (n.round), LEADING (n.state), 2 (n.sid), 0x2b (n.peerEPoch), LOOKING (my state)
2014-20-03 11:15:42.482 INFO  QuorumPeer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - OBSERVING
2014-20-03 11:15:42.486 INFO  Learner [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - TCP NoDelay set to: true
2014-20-03 11:15:42.488 INFO  QuorumCnxManager [host1/###.###.###.###:7555] - Received connection request /###.###.###.###:64528
2014-20-03 11:15:42.490 INFO  ZooKeeperServer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Server environment:zookeeper.version=3.4.5-1392090, built on 09/30/2012 17:52 GMT
2014-20-03 11:15:42.490 INFO  ZooKeeperServer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Server environment:host.name=host1
2014-20-03 11:15:42.490 INFO  ZooKeeperServer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Server environment:java.version=1.6.0_20
2014-20-03 11:15:42.490 INFO  ZooKeeperServer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Server environment:java.vendor=Sun Microsystems Inc.
2014-20-03 11:15:42.490 INFO  ZooKeeperServer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Server environment:java.home=/xxx/util/common/jdk1.6.0_20_64bit/jre
2014-20-03 11:15:42.490 INFO  ZooKeeperServer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Server environment:java.class.path=/xxx/bin/zk/etc:/xxx/bin/zk/lib/slf4j-log4j12-1.7.2.jar:/xxx/bin/zk/lib/jline-0.9.94.jar:/xxx/bin/zk/lib/jul-to-slf4j-1.7.2.jar:/xxx/bin/zk/lib/ZooInspector-3.4.5.jar:/xxx/bin/zk/lib/jcl-over-slf4j-1.7.2.jar:/xxx/bin/zk/lib/log4j-1.2.17.jar:/xxx/bin/zk/lib/zookeeper-3.4.5.jar:/xxx/bin/zk/lib/slf4j-api-1.7.2.jar:/xxx/bin/zk/lib/netty-3.2.2.Final.jar
2014-20-03 11:15:42.490 INFO  ZooKeeperServer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Server environment:java.library.path=/xxx/util/common/jdk1.6.0_20_64bit/jre/lib/amd64/server:/xxx/util/common/jdk1.6.0_20_64bit/jre/lib/amd64:/xxx/util/common/jdk1.6.0_20_64bit/jre/../lib/amd64:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2014-20-03 11:15:42.490 INFO  ZooKeeperServer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Server environment:java.io.tmpdir=/tmp
2014-20-03 11:15:42.490 INFO  ZooKeeperServer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Server environment:java.compiler=<NA>
2014-20-03 11:15:42.490 INFO  ZooKeeperServer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Server environment:os.name=Linux
2014-20-03 11:15:42.490 INFO  ZooKeeperServer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Server environment:os.arch=amd64
2014-20-03 11:15:42.490 INFO  ZooKeeperServer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Server environment:os.version=2.6.32-220.2.1.el6.x86_64
2014-20-03 11:15:42.490 INFO  ZooKeeperServer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Server environment:user.name=op
2014-20-03 11:15:42.490 INFO  ZooKeeperServer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Server environment:user.home=/xxx/bin
2014-20-03 11:15:42.490 INFO  ZooKeeperServer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Server environment:user.dir=/xxx/bin
2014-20-03 11:15:42.491 INFO  ZooKeeperServer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 datadir /xxx/zk_log/version-2 snapdir /xxx/zk_data/version-2
2014-20-03 11:15:42.493 INFO  Learner [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Observing host4/###.###.###.###:6555
2014-20-03 11:15:42.495 INFO  FastLeaderElection [WorkerReceiver[myid=3]] - Notification: 2 (n.leader), 0x2b00000002 (n.zxid), 0x2c (n.round), FOLLOWING (n.state), 5 (n.sid), 0x2b (n.peerEPoch), OBSERVING (my state)
2014-20-03 11:15:42.498 WARN  Learner [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Truncating log to get in sync with the leader 0x2b00000002
2014-20-03 11:15:42.499 WARN  QuorumPeer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Unexpected exception
java.lang.NullPointerException
        at org.apache.zookeeper.server.persistence.FileTxnLog.truncate(FileTxnLog.java:352)
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.truncateLog(FileTxnSnapLog.java:259)
        at org.apache.zookeeper.server.ZKDatabase.truncateLog(ZKDatabase.java:438)
        at org.apache.zookeeper.server.quorum.Learner.syncWithLeader(Learner.java:339)
        at org.apache.zookeeper.server.quorum.Observer.observeLeader(Observer.java:72)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:727)
2014-20-03 11:15:42.500 INFO  Learner [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - shutdown called
java.lang.Exception: shutdown Observer
        at org.apache.zookeeper.server.quorum.Observer.shutdown(Observer.java:137)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:731)
2014-20-03 11:15:42.500 INFO  ZooKeeperServer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - shutting down
2014-20-03 11:15:42.500 INFO  QuorumPeer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - LOOKING
2014-20-03 11:15:42.501 INFO  FastLeaderElection [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - New election. My id =  3, proposed zxid=0x8000000000000000
2014-20-03 11:15:42.503 INFO  FastLeaderElection [WorkerReceiver[myid=3]] - Notification: 2 (n.leader), 0x2b00000002 (n.zxid), 0x2c (n.round), FOLLOWING (n.state), 1 (n.sid), 0x2b (n.peerEPoch), LOOKING (my state)
2014-20-03 11:15:42.503 INFO  FastLeaderElection [WorkerReceiver[myid=3]] - Notification: 2 (n.leader), 0x2b00000002 (n.zxid), 0x2c (n.round), LEADING (n.state), 2 (n.sid), 0x2b (n.peerEPoch), LOOKING (my state)
2014-20-03 11:15:42.503 INFO  QuorumPeer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - OBSERVING
2014-20-03 11:15:42.503 INFO  ZooKeeperServer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 datadir /xxx/zk_log/version-2 snapdir /xxx/zk_data/version-2
2014-20-03 11:15:42.504 INFO  Learner [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Observing host4/###.###.###.###:6555
2014-20-03 11:15:42.504 INFO  FastLeaderElection [WorkerReceiver[myid=3]] - Notification: 2 (n.leader), 0x2b00000002 (n.zxid), 0x2c (n.round), FOLLOWING (n.state), 5 (n.sid), 0x2b (n.peerEPoch), OBSERVING (my state)
2014-20-03 11:15:42.514 INFO  FileSnap [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Reading snapshot /xxx/zk_data/version-2/snapshot.2c00000000
2014-20-03 11:15:42.517 WARN  Learner [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Truncating log to get in sync with the leader 0x2b00000002
2014-20-03 11:15:42.518 WARN  QuorumPeer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Unexpected exception
java.lang.NullPointerException
        at org.apache.zookeeper.server.persistence.FileTxnLog.truncate(FileTxnLog.java:352)
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.truncateLog(FileTxnSnapLog.java:259)
        at org.apache.zookeeper.server.ZKDatabase.truncateLog(ZKDatabase.java:438)
        at org.apache.zookeeper.server.quorum.Learner.syncWithLeader(Learner.java:339)
        at org.apache.zookeeper.server.quorum.Observer.observeLeader(Observer.java:72)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:727)
{noformat}

This exception went on and on over and over again (more than 1M times in a day) until it then began spewing this exception:

{noformat}
2014-20-03 13:45:32.843 INFO  QuorumPeer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - LOOKING
2014-20-03 13:45:32.843 INFO  FastLeaderElection [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - New election. My id =  3, proposed zxid=0x8000000000000000
2014-20-03 13:45:32.844 INFO  FastLeaderElection [WorkerReceiver[myid=3]] - Notification: 2 (n.leader), 0x2b00000002 (n.zxid), 0x2c (n.round), FOLLOWING (n.state), 1 (n.sid), 0x2b (n.peerEPoch), LOOKING (my state)
2014-20-03 13:45:32.845 INFO  FastLeaderElection [WorkerReceiver[myid=3]] - Notification: 2 (n.leader), 0x2b00000002 (n.zxid), 0x2c (n.round), LEADING (n.state), 2 (n.sid), 0x2b (n.peerEPoch), LOOKING (my state)
2014-20-03 13:45:32.845 INFO  QuorumPeer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - OBSERVING
2014-20-03 13:45:32.845 INFO  FastLeaderElection [WorkerReceiver[myid=3]] - Notification: 2 (n.leader), 0x2b00000002 (n.zxid), 0x2c (n.round), FOLLOWING (n.state), 5 (n.sid), 0x2b (n.peerEPoch), OBSERVING (my state)
2014-20-03 13:45:32.845 INFO  ZooKeeperServer [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 datadir /xxx/zk_log/version-2 snapdir /xxx/zk_data/version-2
2014-20-03 13:45:32.845 INFO  Learner [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Observing host4/###.###.###.###:6555
2014-20-03 13:45:32.853 WARN  Learner [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Unexpected exception, tries=0, connecting to host4/###.###.###.###:6555
java.net.ConnectException: Cannot assign requested address
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:529)
        at org.apache.zookeeper.server.quorum.Learner.connectToLeader(Learner.java:224)
        at org.apache.zookeeper.server.quorum.Observer.observeLeader(Observer.java:69)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:727)
2014-20-03 13:45:33.863 INFO  FileSnap [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:5555] - Reading snapshot /xxx/zk_data/version-2/snapshot.2c00000000
{noformat}

This exception for a while was interspersed with the NPEs but eventually it just was spewing the ConnectionException.

Looking through the code a bit it seems if the FileTxnIterator when initialized cannot find any log files the {{inputStream}} is set to null which causes truncate() to NPE.. I see in 3.4.6 this has been wrapped in a try/finally which closes the iterator.. but i presume that this issue would still remain.

Looking at the system in this state there were 29k+ sockets in CLOSE_WAIT state on the system and looking at a heap dump there were tons of Socket objects waiting for GC (ie not getting properly closed).. this eventually ran the system out of ephemeral ports and hence the ConnectionExceptions..

It would seem that a quick check of {{itr.next()}} prior to attempting truncation would resolve the NPE, but it seems somewhere a connection is not being closed properly when an exception occurs.","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1900,Blocker,Steven Bower,Fixed,2014-06-30T17:20:54.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0, NullPointerException in truncate,2014-07-01T11:11:20.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.4.6', id='12323310'>]",8.0
,[],2014-03-20T15:01:55.000+0000,Srinath Mantripragada,"When running a zookeeper-cli, any error or logging information should go to STDERR and the result(s) of the command to STDOUT. For example, let's take the unix 'ls' command:

Unix, STDERR is redirected to '/dev/null' and no results are shown:

{code}
$ ls /aa  2> /dev/null
{code}

zookeeper-cli, everything goes to STDOUT where only the last line should:

{code}
zookeeper-client ls / 2> /dev/null
Connecting to localhost:2181
2014-03-20 14:53:12,220 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.5-cdh5.0.0-beta-2--1, built on 02/07/2014 18:28 GMT
2014-03-20 14:53:12,227 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=node1
2014-03-20 14:53:12,228 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.7.0_51
2014-03-20 14:53:12,229 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation
2014-03-20 14:53:12,230 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.51.x86_64/jre
2014-03-20 14:53:12,231 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/usr/lib/zookeeper/bin/../build/classes:/usr/lib/zookeeper/bin/../build/lib/*.jar:/usr/lib/zookeeper/bin/../lib/slf4j-log4j12.jar:/usr/lib/zookeeper/bin/../lib/slf4j-log4j12-1.7.5.jar:/usr/lib/zookeeper/bin/../lib/slf4j-api-1.7.5.jar:/usr/lib/zookeeper/bin/../lib/netty-3.2.2.Final.jar:/usr/lib/zookeeper/bin/../lib/log4j-1.2.15.jar:/usr/lib/zookeeper/bin/../lib/jline-0.9.94.jar:/usr/lib/zookeeper/bin/../zookeeper-3.4.5-cdh5.0.0-beta-2.jar:/usr/lib/zookeeper/bin/../src/java/lib/*.jar:/etc/zookeeper/conf::/etc/zookeeper/conf:/usr/lib/zookeeper/zookeeper.jar:/usr/lib/zookeeper/zookeeper-3.4.5-cdh5.0.0-beta-2.jar:/usr/lib/zookeeper/lib/slf4j-log4j12.jar:/usr/lib/zookeeper/lib/slf4j-api-1.7.5.jar:/usr/lib/zookeeper/lib/log4j-1.2.15.jar:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar:/usr/lib/zookeeper/lib/jline-0.9.94.jar:/usr/lib/zookeeper/lib/netty-3.2.2.Final.jar
2014-03-20 14:53:12,232 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2014-03-20 14:53:12,233 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/tmp
2014-03-20 14:53:12,234 [myid:] - INFO  [main:Environment@100] - Client environment:java.compiler=<NA>
2014-03-20 14:53:12,235 [myid:] - INFO  [main:Environment@100] - Client environment:os.name=Linux
2014-03-20 14:53:12,235 [myid:] - INFO  [main:Environment@100] - Client environment:os.arch=amd64
2014-03-20 14:53:12,236 [myid:] - INFO  [main:Environment@100] - Client environment:os.version=2.6.32-431.3.1.el6.x86_64
2014-03-20 14:53:12,237 [myid:] - INFO  [main:Environment@100] - Client environment:user.name=hdfs
2014-03-20 14:53:12,238 [myid:] - INFO  [main:Environment@100] - Client environment:user.home=/var/lib/hadoop-hdfs
2014-03-20 14:53:12,239 [myid:] - INFO  [main:Environment@100] - Client environment:user.dir=/var/lib/hadoop-hdfs
2014-03-20 14:53:12,242 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@5220c1b
2014-03-20 14:53:12,294 [myid:] - INFO  [main-SendThread(localhost.localdomain:2181):ClientCnxn$SendThread@966] - Opening socket connection to server localhost.localdomain/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2014-03-20 14:53:12,305 [myid:] - INFO  [main-SendThread(localhost.localdomain:2181):ClientCnxn$SendThread@849] - Socket connection established to localhost.localdomain/127.0.0.1:2181, initiating session
2014-03-20 14:53:12,319 [myid:] - INFO  [main-SendThread(localhost.localdomain:2181):ClientCnxn$SendThread@1207] - Session establishment complete on server localhost.localdomain/127.0.0.1:2181, sessionid = 0x144dbe27e1b001d, negotiated timeout = 30000

WATCHER::

WatchedEvent state:SyncConnected type:None path:null
[hadoop-ha, zookeeper]
{code}


For the get command STDOUT and STDERR are inverted:

Results going to STDERR:

{code}
$ zookeeper-client get /hadoop-ha/Redlabnet 1>/dev/null
cZxid = 0x300000027
ctime = Thu Mar 20 14:25:17 UTC 2014
mZxid = 0x300000027
mtime = Thu Mar 20 14:25:17 UTC 2014
pZxid = 0x300000027
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 0
numChildren = 0
{code}


Logs/Errors going to STDOUT:
{code}
$ zookeeper-client get /hadoop-ha/Redlabnet 2>/dev/null
Connecting to localhost:2181
2014-03-20 15:01:22,170 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.5-cdh5.0.0-beta-2--1, built on 02/07/2014 18:28 GMT
2014-03-20 15:01:22,177 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=ip-172-17-0-105.redlabnet.internal
2014-03-20 15:01:22,178 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.7.0_51
2014-03-20 15:01:22,179 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation
2014-03-20 15:01:22,180 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.51.x86_64/jre
2014-03-20 15:01:22,181 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/usr/lib/zookeeper/bin/../build/classes:/usr/lib/zookeeper/bin/../build/lib/*.jar:/usr/lib/zookeeper/bin/../lib/slf4j-log4j12.jar:/usr/lib/zookeeper/bin/../lib/slf4j-log4j12-1.7.5.jar:/usr/lib/zookeeper/bin/../lib/slf4j-api-1.7.5.jar:/usr/lib/zookeeper/bin/../lib/netty-3.2.2.Final.jar:/usr/lib/zookeeper/bin/../lib/log4j-1.2.15.jar:/usr/lib/zookeeper/bin/../lib/jline-0.9.94.jar:/usr/lib/zookeeper/bin/../zookeeper-3.4.5-cdh5.0.0-beta-2.jar:/usr/lib/zookeeper/bin/../src/java/lib/*.jar:/etc/zookeeper/conf::/etc/zookeeper/conf:/usr/lib/zookeeper/zookeeper.jar:/usr/lib/zookeeper/zookeeper-3.4.5-cdh5.0.0-beta-2.jar:/usr/lib/zookeeper/lib/slf4j-log4j12.jar:/usr/lib/zookeeper/lib/slf4j-api-1.7.5.jar:/usr/lib/zookeeper/lib/log4j-1.2.15.jar:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar:/usr/lib/zookeeper/lib/jline-0.9.94.jar:/usr/lib/zookeeper/lib/netty-3.2.2.Final.jar
2014-03-20 15:01:22,182 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2014-03-20 15:01:22,182 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/tmp
2014-03-20 15:01:22,183 [myid:] - INFO  [main:Environment@100] - Client environment:java.compiler=<NA>
2014-03-20 15:01:22,184 [myid:] - INFO  [main:Environment@100] - Client environment:os.name=Linux
2014-03-20 15:01:22,185 [myid:] - INFO  [main:Environment@100] - Client environment:os.arch=amd64
2014-03-20 15:01:22,186 [myid:] - INFO  [main:Environment@100] - Client environment:os.version=2.6.32-431.3.1.el6.x86_64
2014-03-20 15:01:22,186 [myid:] - INFO  [main:Environment@100] - Client environment:user.name=hdfs
2014-03-20 15:01:22,187 [myid:] - INFO  [main:Environment@100] - Client environment:user.home=/var/lib/hadoop-hdfs
2014-03-20 15:01:22,188 [myid:] - INFO  [main:Environment@100] - Client environment:user.dir=/var/lib/hadoop-hdfs
2014-03-20 15:01:22,191 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@4bb1c978
2014-03-20 15:01:22,242 [myid:] - INFO  [main-SendThread(localhost.localdomain:2181):ClientCnxn$SendThread@966] - Opening socket connection to server localhost.localdomain/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2014-03-20 15:01:22,266 [myid:] - INFO  [main-SendThread(localhost.localdomain:2181):ClientCnxn$SendThread@849] - Socket connection established to localhost.localdomain/127.0.0.1:2181, initiating session
2014-03-20 15:01:22,284 [myid:] - INFO  [main-SendThread(localhost.localdomain:2181):ClientCnxn$SendThread@1207] - Session establishment complete on server localhost.localdomain/127.0.0.1:2181, sessionid = 0x144dbe27e1b001f, negotiated timeout = 30000

WATCHER::

WatchedEvent state:SyncConnected type:None path:null

{code}


 ",[],Bug,ZOOKEEPER-1899,Major,Srinath Mantripragada,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zookeeper-cli does not use STDERR and STDOUT correctly to output information,2014-03-20T16:43:13.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",2.0
Abraham Fine,"[<JIRA Component: name='java client', id='12312381'>]",2014-03-20T14:45:36.000+0000,Srinath Mantripragada,"zookeeper-cli always return ""0"" as exit code whether the command has been successful or not.

Ex:

Unsuccessful:
{code}
-bash-4.1$ zookeeper-client aa
Connecting to localhost:2181
2014-03-20 14:43:01,361 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.5-cdh5.0.0-beta-2--1, built on 02/07/2014 18:28 GMT
2014-03-20 14:43:01,368 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=ip-172-17-0-105.redlabnet.internal
2014-03-20 14:43:01,369 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.7.0_51
2014-03-20 14:43:01,370 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation
2014-03-20 14:43:01,371 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.51.x86_64/jre
2014-03-20 14:43:01,371 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/usr/lib/zookeeper/bin/../build/classes:/usr/lib/zookeeper/bin/../build/lib/*.jar:/usr/lib/zookeeper/bin/../lib/slf4j-log4j12.jar:/usr/lib/zookeeper/bin/../lib/slf4j-log4j12-1.7.5.jar:/usr/lib/zookeeper/bin/../lib/slf4j-api-1.7.5.jar:/usr/lib/zookeeper/bin/../lib/netty-3.2.2.Final.jar:/usr/lib/zookeeper/bin/../lib/log4j-1.2.15.jar:/usr/lib/zookeeper/bin/../lib/jline-0.9.94.jar:/usr/lib/zookeeper/bin/../zookeeper-3.4.5-cdh5.0.0-beta-2.jar:/usr/lib/zookeeper/bin/../src/java/lib/*.jar:/etc/zookeeper/conf::/etc/zookeeper/conf:/usr/lib/zookeeper/zookeeper.jar:/usr/lib/zookeeper/zookeeper-3.4.5-cdh5.0.0-beta-2.jar:/usr/lib/zookeeper/lib/slf4j-log4j12.jar:/usr/lib/zookeeper/lib/slf4j-api-1.7.5.jar:/usr/lib/zookeeper/lib/log4j-1.2.15.jar:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar:/usr/lib/zookeeper/lib/jline-0.9.94.jar:/usr/lib/zookeeper/lib/netty-3.2.2.Final.jar
2014-03-20 14:43:01,372 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2014-03-20 14:43:01,373 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/tmp
2014-03-20 14:43:01,374 [myid:] - INFO  [main:Environment@100] - Client environment:java.compiler=<NA>
2014-03-20 14:43:01,375 [myid:] - INFO  [main:Environment@100] - Client environment:os.name=Linux
2014-03-20 14:43:01,375 [myid:] - INFO  [main:Environment@100] - Client environment:os.arch=amd64
2014-03-20 14:43:01,376 [myid:] - INFO  [main:Environment@100] - Client environment:os.version=2.6.32-431.3.1.el6.x86_64
2014-03-20 14:43:01,377 [myid:] - INFO  [main:Environment@100] - Client environment:user.name=hdfs
2014-03-20 14:43:01,377 [myid:] - INFO  [main:Environment@100] - Client environment:user.home=/var/lib/hadoop-hdfs
2014-03-20 14:43:01,378 [myid:] - INFO  [main:Environment@100] - Client environment:user.dir=/var/lib/hadoop-hdfs
2014-03-20 14:43:01,382 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@5220c1b
ZooKeeper -server host:port cmd args
	connect host:port
	get path [watch]
	ls path [watch]
	set path data [version]
	rmr path
	delquota [-n|-b] path
	quit 
	printwatches on|off
	create [-s] [-e] path data acl
	stat path [watch]
	close 
	ls2 path [watch]
	history 
	listquota path
	setAcl path acl
	getAcl path
	sync path
	redo cmdno
	addauth scheme auth
	delete path [version]
	setquota -n|-b val path

-bash-4.1$ echo $?
0
{code}

Successful:
{code}
-bash-4.1$ zookeeper-client ls /
Connecting to localhost:2181
2014-03-20 14:43:53,881 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.5-cdh5.0.0-beta-2--1, built on 02/07/2014 18:28 GMT
2014-03-20 14:43:53,889 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=ip-172-17-0-105.redlabnet.internal
2014-03-20 14:43:53,889 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.7.0_51
2014-03-20 14:43:53,890 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation
2014-03-20 14:43:53,891 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.51.x86_64/jre
2014-03-20 14:43:53,892 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/usr/lib/zookeeper/bin/../build/classes:/usr/lib/zookeeper/bin/../build/lib/*.jar:/usr/lib/zookeeper/bin/../lib/slf4j-log4j12.jar:/usr/lib/zookeeper/bin/../lib/slf4j-log4j12-1.7.5.jar:/usr/lib/zookeeper/bin/../lib/slf4j-api-1.7.5.jar:/usr/lib/zookeeper/bin/../lib/netty-3.2.2.Final.jar:/usr/lib/zookeeper/bin/../lib/log4j-1.2.15.jar:/usr/lib/zookeeper/bin/../lib/jline-0.9.94.jar:/usr/lib/zookeeper/bin/../zookeeper-3.4.5-cdh5.0.0-beta-2.jar:/usr/lib/zookeeper/bin/../src/java/lib/*.jar:/etc/zookeeper/conf::/etc/zookeeper/conf:/usr/lib/zookeeper/zookeeper.jar:/usr/lib/zookeeper/zookeeper-3.4.5-cdh5.0.0-beta-2.jar:/usr/lib/zookeeper/lib/slf4j-log4j12.jar:/usr/lib/zookeeper/lib/slf4j-api-1.7.5.jar:/usr/lib/zookeeper/lib/log4j-1.2.15.jar:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar:/usr/lib/zookeeper/lib/jline-0.9.94.jar:/usr/lib/zookeeper/lib/netty-3.2.2.Final.jar
2014-03-20 14:43:53,893 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2014-03-20 14:43:53,894 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/tmp
2014-03-20 14:43:53,894 [myid:] - INFO  [main:Environment@100] - Client environment:java.compiler=<NA>
2014-03-20 14:43:53,895 [myid:] - INFO  [main:Environment@100] - Client environment:os.name=Linux
2014-03-20 14:43:53,896 [myid:] - INFO  [main:Environment@100] - Client environment:os.arch=amd64
2014-03-20 14:43:53,897 [myid:] - INFO  [main:Environment@100] - Client environment:os.version=2.6.32-431.3.1.el6.x86_64
2014-03-20 14:43:53,897 [myid:] - INFO  [main:Environment@100] - Client environment:user.name=hdfs
2014-03-20 14:43:53,898 [myid:] - INFO  [main:Environment@100] - Client environment:user.home=/var/lib/hadoop-hdfs
2014-03-20 14:43:53,899 [myid:] - INFO  [main:Environment@100] - Client environment:user.dir=/var/lib/hadoop-hdfs
2014-03-20 14:43:53,902 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@5a9e40d2
2014-03-20 14:43:53,953 [myid:] - INFO  [main-SendThread(localhost.localdomain:2181):ClientCnxn$SendThread@966] - Opening socket connection to server localhost.localdomain/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2014-03-20 14:43:53,963 [myid:] - INFO  [main-SendThread(localhost.localdomain:2181):ClientCnxn$SendThread@849] - Socket connection established to localhost.localdomain/127.0.0.1:2181, initiating session
2014-03-20 14:43:53,977 [myid:] - INFO  [main-SendThread(localhost.localdomain:2181):ClientCnxn$SendThread@1207] - Session establishment complete on server localhost.localdomain/127.0.0.1:2181, sessionid = 0x144dbe27e1b0013, negotiated timeout = 30000

WATCHER::

WatchedEvent state:SyncConnected type:None path:null
[hadoop-ha, zookeeper]

-bash-4.1$ echo $?
0

{code}","[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-1898,Critical,Srinath Mantripragada,Fixed,2016-07-26T23:36:21.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"ZooKeeper Java cli shell always returns ""0"" as exit code",2017-05-18T03:43:59.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",7.0
Michael Stack,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='scripts', id='12312384'>]",2014-03-18T23:10:22.000+0000,Cameron Gandevia,"When running zookeeper 3.4.5 I was able to run commands using zkCli such as 

zkCli.sh -server 127.0.0.1:2182 ls /
zkCli.sh -server 127.0.0.1:2182 get /blah

After upgrading to 3.4.6 these commands no longer work.

I think issue https://issues.apache.org/jira/browse/ZOOKEEPER-1535 was the reason the commands were running in previous versions.

It looks like the client exits when a command is present.

{code:title=ZooKeeperMain.java}
    void run() throws KeeperException, IOException, InterruptedException {
        if (cl.getCommand() == null) {
            System.out.println(""Welcome to ZooKeeper!"");

            boolean jlinemissing = false;
            // only use jline if it's in the classpath
            try {
                Class consoleC = Class.forName(""jline.ConsoleReader"");
                Class completorC =
                    Class.forName(""org.apache.zookeeper.JLineZNodeCompletor"");

                System.out.println(""JLine support is enabled"");

                Object console =
                    consoleC.getConstructor().newInstance();

                Object completor =
                    completorC.getConstructor(ZooKeeper.class).newInstance(zk);
                Method addCompletor = consoleC.getMethod(""addCompletor"",
                        Class.forName(""jline.Completor""));
                addCompletor.invoke(console, completor);

                String line;
                Method readLine = consoleC.getMethod(""readLine"", String.class);
                while ((line = (String)readLine.invoke(console, getPrompt())) != null) {
                    executeLine(line);
                }
            } catch (ClassNotFoundException e) {
                LOG.debug(""Unable to start jline"", e);
                jlinemissing = true;
            } catch (NoSuchMethodException e) {
                LOG.debug(""Unable to start jline"", e);
                jlinemissing = true;
            } catch (InvocationTargetException e) {
                LOG.debug(""Unable to start jline"", e);
                jlinemissing = true;
            } catch (IllegalAccessException e) {
                LOG.debug(""Unable to start jline"", e);
                jlinemissing = true;
            } catch (InstantiationException e) {
                LOG.debug(""Unable to start jline"", e);
                jlinemissing = true;
            }

            if (jlinemissing) {
                System.out.println(""JLine support is disabled"");
                BufferedReader br =
                    new BufferedReader(new InputStreamReader(System.in));

                String line;
                while ((line = br.readLine()) != null) {
                    executeLine(line);
                }
            }
        }
    }
{code}","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1897,Major,Cameron Gandevia,Fixed,2014-04-03T22:41:26.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZK Shell/Cli not processing commands,2017-05-20T23:07:11.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",7.0
,"[<JIRA Component: name='server', id='12312382'>]",2014-03-17T16:41:32.000+0000,Raúl Gutiérrez Segalés,"When upgrading from 3.4.6 (rc0 actually) to 3.5.0 (trunk as of two weeks ago actually) I got this error message:

{noformat}
2014-02-26 22:12:15,446 - ERROR [WorkerReceiver[myid=4]] - Something went wrong while processing config received from 3
{noformat}

According to [~fpj]:

bq. I think you’re right that the reconfig error is harmless, but we shouldn’t be getting it. The problem is that it is not detecting that we are in backward compatibility mode. We need to fix it for 3.5.0 and perhaps ZOOKEEPER-1805 is the right place for doing it.

cc: [~shralex]",[],Bug,ZOOKEEPER-1896,Major,Raúl Gutiérrez Segalés,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Reconfig error messages when upgrading from 3.4.6 to 3.5.0,2022-02-03T08:50:25.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
Michi Mutsuzaki,[],2014-03-16T16:35:55.000+0000,Patrick D. Hunt,"From a note on the list:

Hi folks!

This is a reminder to update the year in the NOTICE files from 2013 (or older) to 2014.

From a legal POV this is not that important as some say.
But nonetheless it's good to update the year.

LieGrue,
strub","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1895,Blocker,Patrick D. Hunt,Fixed,2014-05-16T17:58:48.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"update all notice files, copyright, etc... with the new year - 2014",2014-05-20T11:09:12.000+0000,"[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.0', id='12316644'>]",3.0
Michi Mutsuzaki,"[<JIRA Component: name='quorum', id='12312379'>]",2014-03-14T02:43:02.000+0000,Michi Mutsuzaki,"ObserverTest.testObserver fails consistently on my box. It looks like the observer (myid:3) calls QuorumPeer.getQuorumVerifier() in a tight loop, and the leader (myid:2) is not getting enough CPU time to synchronize with the follower and the observer. The test passes if I increase ClientBase.CONNECTION_TIMEOUT from 30 seconds to 120 seconds. I'll attach a log file.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1894,Major,Michi Mutsuzaki,Fixed,2014-03-28T00:59:09.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ObserverTest.testObserver fails consistently,2014-03-28T01:20:01.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",7.0
Michi Mutsuzaki,"[<JIRA Component: name='c client', id='12312380'>]",2014-03-13T03:46:13.000+0000,Michi Mutsuzaki,"automake switched to run tests in parallel by default in 1.13, but zktest-st and zktest-mt can't run in parallel. We can use the serial-tests option to run tests serially but this option was introduced in automake 1.12. I don't know which version of automake buidbot has. I'll upload the patch and see.","[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-1893,Minor,Michi Mutsuzaki,Fixed,2015-03-15T01:10:41.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,automake: use serial-tests option,2015-04-17T18:11:47.000+0000,[],8.0
,"[<JIRA Component: name='c client', id='12312380'>]",2014-03-13T02:06:36.000+0000,Michi Mutsuzaki,"zookeeper_interest() already calls zoo_cycle_next_server() when the socket is set to -1, so we shouldn't call addrvec_next in handle_error. This causes the next server to get skipped. Zookeeper_simpleSystem::testFirstServerDown fails unless the client gets connected to the server during the first round because the client keeps skipping the second server after the first round.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1892,Major,Michi Mutsuzaki,Duplicate,2014-03-13T03:41:05.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,addrvec_next gets called twice when failing over to the next server,2014-03-13T03:41:05.000+0000,[],3.0
Michi Mutsuzaki,"[<JIRA Component: name='java client', id='12312381'>]",2014-03-10T23:33:59.000+0000,Michi Mutsuzaki,StaticHostProviderTest.testUpdateLoadBalancing is consistently timing out on my box. I'll attach a log file.,"[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1891,Major,Michi Mutsuzaki,Fixed,2014-05-10T10:46:33.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,StaticHostProviderTest.testUpdateLoadBalancing times out,2014-05-20T11:09:11.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
,[],2014-02-28T01:21:05.000+0000,Andrew Gaul,"When calling rollLog, FileTxnLog flushes but does not close its FileOutputStream, leaking a file descriptor.",[],Bug,ZOOKEEPER-1890,Major,Andrew Gaul,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,unclosed FileOutputStream in FileTxnLog.rollLog,2015-02-14T05:13:45.000+0000,[],4.0
,[],2014-02-26T00:38:40.000+0000,Steven Phillips,"When, for example, doing an order by with a limit, if limit << total, it would be much more efficient to maintain a priority queue instead of sorting the entire data set.

In most cases, this will greatly reduce the number of comparisons, since most incoming records will not fall in the Top N, and thus will only require a single comparison operation. Incoming records that are in the Top-N will require at most log N comparisons.

This will also allow periodic purging of record batches, reducing memory requirements.",[],Bug,ZOOKEEPER-1889,Major,Steven Phillips,Not A Problem,2014-02-26T00:52:16.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Implement Top-N sort operator,2014-02-26T00:52:16.000+0000,[],2.0
Ivan Mitic,[],2014-02-25T19:07:11.000+0000,Ivan Mitic,"This appears to be a bug in ZkCli.cmd as it does not try to locate java using the JAVA_HOME environment variable. 

Will post the patch soon.","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1888,Major,Ivan Mitic,Fixed,2014-03-14T02:55:37.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"ZkCli.cmd commands fail with ""'java' is not recognized as an internal or external command""",2014-03-14T02:55:37.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",5.0
,"[<JIRA Component: name='server', id='12312382'>]",2014-02-21T05:35:58.000+0000,Vinayakumar B,"SocketTimeoutException in {{Follower#followLeader()}} where the leader is successfully running can make this follower not able to rejoin the quorum.

Analysis:
1. SocketTimeoutException in below code, will make follower to stop following (Not process shutdown) and try to participate in leader election again. 
{code}                while (self.isRunning()) {
                    readPacket(qp);
                    processPacket(qp);
                }{code}

2. At the time of leader election {{FastLeaderElection#logicalclock}} will be incremented at only follower side, and this is more than electionEpoch of the leader.

3. Notification from the Leader will get ignored and from this follower notifications will be continously sent and again ignored.

",[],Bug,ZOOKEEPER-1886,Major,Vinayakumar B,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Exception in Follower.followLeader() where Leader is still running, can make that follower hang in LeaderElection",2014-02-21T05:35:58.000+0000,[],2.0
,[],2014-02-20T15:55:47.000+0000,Behar Veliqi,"Hi,

I'm not really sure if this is a bug or a misunderstanding on my part, but I have the problem that, when I create a znode with an ACL as follows:

{noformat}
[zk: localhost:2181(CONNECTED) 60] create /anode ""somecontent"" digest:'user:IAEttLCxci/qWhKN2QJ6u1nrQgw=':cdrwa
Created /anode
[zk: localhost:2181(CONNECTED) 61] getAcl /anode                                                               
'digest,''user:IAEttLCxci/qWhKN2QJ6u1nrQgw='
: cdrwa
{noformat}

I am not able to read or update the content of the node, as it should be:

{noformat}
[zk: localhost:2181(CONNECTED) 62] get /anode
Authentication is not valid : /anode
[zk: localhost:2181(CONNECTED) 63] set /anode ""update""                                                         
Authentication is not valid : /anode
{noformat}

But everyone without being authenticated can delete the node:

{noformat}
[zk: localhost:2181(CONNECTED) 64] delete /anode                                                               
[zk: localhost:2181(CONNECTED) 65] get /anode         
Node does not exist: /anode
{noformat}

Is this a bug or is there a way to set the ACL so that only the user having the credentials can delete the znode?

",[],Bug,ZOOKEEPER-1885,Major,Behar Veliqi,Not A Problem,2014-02-21T09:52:38.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Znodes deletable by anyone without having the rights to do so,2014-02-21T09:52:38.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",2.0
Raúl Gutiérrez Segalés,[],2014-02-20T11:58:58.000+0000,Flavio Paiva Junqueira,"Apparently, we have fixed this in trunk, but not in the 3.4 branch. When we pass only the path to create, the command is not executed because it expects an additional parameter and there is no error message because the create command exists.",[],Bug,ZOOKEEPER-1884,Minor,Flavio Paiva Junqueira,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zkCli silently ignores commands with missing parameters,2018-06-22T04:49:04.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.4.11', id='12339207'>]",4.0
Raúl Gutiérrez Segalés,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='tests', id='12312427'>]",2014-02-18T03:30:27.000+0000,Abhiraj Butala,"I am seeing unit test failure for c client after I do 'make check' as shown below. The failure is pretty consistent, but does not happen always. This is on latest check-out of zookeeper trunk.

------------------
Zookeeper_simpleSystem::testAsyncWatcherAutoReset ZooKeeper server started : elapsed 9640 : OK
Zookeeper_simpleSystem::testDeserializeString : elapsed 0 : OK
Zookeeper_simpleSystem::testFirstServerDown : elapsed 1007 : OK
Zookeeper_simpleSystem::testNullData : elapsed 1028 : OK
Zookeeper_simpleSystem::testIPV6 : elapsed 1008 : OK
Zookeeper_simpleSystem::testCreate : elapsed 1016 : OK
Zookeeper_simpleSystem::testPath : elapsed 1083 : OK
Zookeeper_simpleSystem::testPathValidation : elapsed 1046 : OK
Zookeeper_simpleSystem::testPing : elapsed 17301 : OK
Zookeeper_simpleSystem::testAcl : elapsed 1018 : OK
Zookeeper_simpleSystem::testChroot : elapsed 3057 : OK
Zookeeper_simpleSystem::testAuth ZooKeeper server started ZooKeeper server started : elapsed 29357 : OK
Zookeeper_simpleSystem::testHangingClient : elapsed 1037 : OK
Zookeeper_simpleSystem::testWatcherAutoResetWithGlobal ZooKeeper server started ZooKeeper server started ZooKeeper server started : elapsed 12983 : OK
Zookeeper_simpleSystem::testWatcherAutoResetWithLocal ZooKeeper server started ZooKeeper server started ZooKeeper server started : elapsed 13028 : OK
Zookeeper_simpleSystem::testGetChildren2 : elapsed 1031 : OK
Zookeeper_simpleSystem::testLastZxid : assertion : elapsed 2514
Zookeeper_watchers::testDefaultSessionWatcher1 : elapsed 52 : OK
Zookeeper_watchers::testDefaultSessionWatcher2 : elapsed 3 : OK
Zookeeper_watchers::testObjectSessionWatcher1 : elapsed 52 : OK
Zookeeper_watchers::testObjectSessionWatcher2 : elapsed 54 : OK
Zookeeper_watchers::testNodeWatcher1 : elapsed 55 : OK
Zookeeper_watchers::testChildWatcher1 : elapsed 3 : OK
Zookeeper_watchers::testChildWatcher2 : elapsed 3 : OK
tests/TestClient.cc:1281: Assertion: equality assertion failed [Expected: 1239, Actual  : 1238]
Failures !!!
Run: 70   Failure total: 1   Failures: 1   Errors: 0
FAIL: zktest-mt
==========================================
1 of 2 tests failed
Please report to user@zookeeper.apache.org
==========================================
make[1]: *** [check-TESTS] Error 1
make[1]: Leaving directory `/home/abutala/work/zk/zookeeper-trunk/src/c'
make: *** [check-am] Error 2
------------------

$ uname -a
Linux abutala-vBox 3.8.0-35-generic #50~precise1-Ubuntu SMP Wed Dec 4 17:25:51 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux

","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1883,Minor,Abhiraj Butala,Fixed,2014-03-13T20:58:49.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,C client unit test failures,2014-03-13T22:08:19.000+0000,[],5.0
Rakesh Radhakrishnan,"[<JIRA Component: name='quorum', id='12312379'>]",2014-02-09T04:18:47.000+0000,Rakesh Radhakrishnan,"During the startup if dataDir is not exists server will auto create this. But when user specifies different dataLogDir location which doesn't exists the server will validate and startup will fail. 

{code}
org.apache.zookeeper.server.quorum.QuorumPeerConfig$ConfigException: Error processing build\test3085582797504170966.junit.dir\zoo.cfg
	at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parse(QuorumPeerConfig.java:123)
	at org.apache.zookeeper.server.ServerConfig.parse(ServerConfig.java:79)
	at org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:81)
	at org.apache.zookeeper.server.ZooKeeperServerMainTest$MainThread.run(ZooKeeperServerMainTest.java:92)
Caused by: java.lang.IllegalArgumentException: dataLogDir build/test3085582797504170966.junit.dir/data_txnlog is missing.
	at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parseProperties(QuorumPeerConfig.java:253)
	at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parse(QuorumPeerConfig.java:119)
	... 3 more
{code}","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1878,Major,Rakesh Radhakrishnan,Fixed,2014-03-27T02:02:10.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Inconsistent behavior in autocreation of dataDir and dataLogDir,2014-03-27T11:10:42.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",5.0
Chris Chen,"[<JIRA Component: name='server', id='12312382'>]",2014-02-06T23:13:21.000+0000,Chris Chen,"Because of the way fixupACL is written in PrepRequestProcessor, a request that feeds in an ACL with null members in the Id will cause a server with skipACL=yes to crash.

A patch will be provided that re-introduces checks for well-formed ACLs even if skipACL is enabled.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1877,Critical,Chris Chen,Fixed,2014-07-24T23:32:36.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Malformed ACL Id can crash server with skipACL=yes,2014-07-25T11:25:05.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Jerry He,"[<JIRA Component: name='java client', id='12312381'>]",2014-02-03T04:06:08.000+0000,Jerry He,"We've been seeing NullPointerException while working on HBase:

{code}
14/01/30 22:15:25 INFO zookeeper.ZooKeeper: Client environment:user.dir=/home/biadmin/hbase-trunk
14/01/30 22:15:25 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=hdtest009:2181 sessionTimeout=90000 watcher=null
14/01/30 22:15:25 INFO zookeeper.ClientCnxn: Opening socket connection to server hdtest009/9.30.194.18:2181. Will not attempt to authenticate using SASL (Unable to locate a login configuration)
14/01/30 22:15:25 INFO zookeeper.ClientCnxn: Socket connection established to hdtest009/9.30.194.18:2181, initiating session
14/01/30 22:15:25 INFO zookeeper.ClientCnxn: Session establishment complete on server hdtest009/9.30.194.18:2181, sessionid = 0x143986213e67e48, negotiated timeout = 60000
14/01/30 22:15:25 ERROR zookeeper.ClientCnxn: Error while calling watcher
java.lang.NullPointerException
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:519)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:495)
{code}

The reason is the watcher is null in this part of the code:
{code}
       private void processEvent(Object event) {
          try {
              if (event instanceof WatcherSetEventPair) {
                  // each watcher will process the event
                  WatcherSetEventPair pair = (WatcherSetEventPair) event;
                  for (Watcher watcher : pair.watchers) {
                      try {
                          watcher.process(pair.event);
                      } catch (Throwable t) {
                          LOG.error(""Error while calling watcher "", t);
                      }
                  }
{code}",[],Bug,ZOOKEEPER-1875,Minor,Jerry He,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,NullPointerException in ClientCnxn$EventThread.processEvent,2022-02-03T08:50:24.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.4.10', id='12338036'>]",11.0
Helen Hastings,"[<JIRA Component: name='tests', id='12312427'>]",2014-01-28T18:45:43.000+0000,Patrick D. Hunt,"I'm seeing lots of the following failure. Seems like a flakey test (passes every so often).

{noformat}
junit.framework.AssertionFailedError: client could not connect to reestablished quorum: giving up after 30+ seconds.
	at org.apache.zookeeper.test.ReconfigTest.testNormalOperation(ReconfigTest.java:143)
	at org.apache.zookeeper.server.quorum.StandaloneDisabledTest.startSingleServerTest(StandaloneDisabledTest.java:75)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
{noformat}

I've found 3 problems:

1. QuorumCnxManager.Listener.run() leaks the socket depending on when the shutdown flag gets set.
2. QuorumCnxManager.halt() doesn't wait for the listener to terminate.
3. QuorumPeer.shuttingDownLE flag doesn't get reset when restarting the leader election.
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1870,Blocker,Patrick D. Hunt,Fixed,2014-08-28T16:49:39.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,flakey test in StandaloneDisabledTest.startSingleServerTest,2014-08-28T16:50:13.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",9.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2014-01-27T19:43:38.000+0000,Deepak Jagtap,"We have deployed zookeeper version 3.5.0.1515976, with 3 zk servers in the quorum.
The problem we are facing is that one zookeeper server in the quorum falls apart, and never becomes part of the cluster until we restart zookeeper server on that node.

Our interpretation from zookeeper logs on all nodes is as follows: 
(For simplicity assume S1=> zk server1, S2 => zk server2, S3 => zk server 3)
Initially S3 is the leader while S1 and S2 are followers.

S2 hits 46 sec latency while fsyncing write ahead log and results in loss of connection with S3.
 S3 in turn prints following error message:

Unexpected exception causing shutdown while sock still open
java.net.SocketTimeoutException: Read timed out
Stack trace
******* GOODBYE /169.254.1.2:47647(S2) ********

S2 in this case closes connection with S3(leader) and shuts down follower with following log messages:
Closing connection to leader, exception during packet send
java.net.SocketException: Socket close
Follower@194] - shutdown called
java.lang.Exception: shutdown Follower

After this point S3 could never reestablish connection with S2 and leader election mechanism keeps failing. S3 now keeps printing following message repeatedly:
Cannot open channel to 2 at election address /169.254.1.2:3888
java.net.ConnectException: Connection refused.

While S3 is in this state, S2 repeatedly keeps printing following message:
INFO [NIOServerCxnFactory.AcceptThread:/0.0.0.0:2181:NIOServerCnxnFactory$AcceptThread@296] - Accepted socket connection from /127.0.0.1:60667
Exception causing close of session 0x0: ZooKeeperServer not running
Closed socket connection for client /127.0.0.1:60667 (no session established for client)

Leader election never completes successfully and causing S2 to fall apart from the quorum.
S2 was out of quorum for almost 1 week.

While debugging this issue, we found out that both election and peer connection ports on S2  can't be telneted from any of the node (S1, S2, S3). Network connectivity is not the issue. Later, we restarted the ZK server S2 (service zookeeper-server restart) -- now we could telnet to both the ports and S2 joined the ensemble after a leader election attempt.
Any idea what might be forcing S2 to get into a situation where it won't accept any connections on the leader election and peer connection ports?",[],Bug,ZOOKEEPER-1869,Critical,Deepak Jagtap,Abandoned,2016-03-03T05:55:16.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zk server falling apart from quorum due to connection loss and couldn't connect back,2016-03-03T05:55:16.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",16.0
Edward Carter,"[<JIRA Component: name='server', id='12312382'>]",2014-01-22T00:51:50.000+0000,Thawan Kooburat,"We discovered a long leader election time today in one of our prod ensemble.

Here is the description of the event. 

Before the old leader goes down, it is able to announce notification message. So 3 out 5 (including the old leader) elected the old leader to be a new leader for the next epoch. While, the old leader is being rebooted, 2 other machines are trying to connect to the old leader.  So the quorum couldn't form until those 2 machines give up and move to the next round of leader election.

This is because Learner.connectToLeader() use a simple retry logic. The contract for this method is that it should never spend longer that initLimit trying to connect to the leader.  In our outage, each sock.connect() is probably blocked for initLimit and it is called 5 times.
",[],Bug,ZOOKEEPER-1865,Major,Thawan Kooburat,,,"This issue was once resolved, but the resolution was deemed incorrect. From here issues are either marked assigned or resolved.",Reopened,0.0,Fix retry logic in Learner.connectToLeader() ,2022-02-03T08:50:22.000+0000,[],10.0
Michi Mutsuzaki,"[<JIRA Component: name='server', id='12312382'>]",2014-01-20T06:09:53.000+0000,some one,"This bug was found when using ZK 3.5.0 with curator-test 2.3.0.
curator-test is building a QuorumPeerConfig from a Properties object and then when we try to run the quorum peer using that configuration, we get an NPE:
{noformat}
2014-01-19 21:58:39,768 [myid:] - ERROR [Thread-3:TestingZooKeeperServer$1@138] - From testing server (random state: false)
java.lang.NullPointerException
	at org.apache.zookeeper.server.quorum.QuorumPeer.setQuorumVerifier(QuorumPeer.java:1320)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:156)
	at org.apache.curator.test.TestingZooKeeperServer$1.run(TestingZooKeeperServer.java:134)
	at java.lang.Thread.run(Thread.java:722)
{noformat}
The reason that this happens is because QuorumPeerConfig:parseProperties only peforms a subset of what 'QuorumPeerConfig:parse(String path)' does. The exact additional task performed that we need in parseProperties is the dynamic config backwards compatibility check:
{noformat}
            // backward compatibility - dynamic configuration in the same file as static configuration params
            // see writeDynamicConfig() - we change the config file to new format if reconfig happens
            if (dynamicConfigFileStr == null) {
                configBackwardCompatibilityMode = true;
                configFileStr = path;................
                parseDynamicConfig(cfg, electionAlg, true);
                checkValidity();................
            }
{noformat}
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1864,Major,some one,Fixed,2014-05-17T11:30:47.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,quorumVerifier is null when creating a QuorumPeerConfig from parsing a Properties object,2014-05-20T11:09:11.000+0000,[],6.0
Dutch T. Meyer,"[<JIRA Component: name='server', id='12312382'>]",2014-01-15T18:23:17.000+0000,Dutch T. Meyer,"In CommitProcessor.java processor, if we are at the primary request handler on line 167:
{noformat}
                while (!stopped && !isWaitingForCommit() &&
                       !isProcessingCommit() &&
                       (request = queuedRequests.poll()) != null) {
                    if (needCommit(request)) {
                        nextPending.set(request);
                    } else {
                        sendToNextProcessor(request);
                    }
                }
{noformat}

A request can be handled in this block and be quickly processed and completed on another thread. If queuedRequests is empty, we then exit the block. Next, before this thread makes any more progress, we can get 2 more requests, one get_children(say), and a sync placed on queuedRequests for the processor. Then, if we are very unlucky, the sync request can complete and this object's commit() routine is called (from FollowerZookeeperServer), which places the sync request on the previously empty committedRequests queue. At that point, this thread continues.

We reach line 182, which is a check on sync requests.
{noformat}
                if (!stopped && !isProcessingRequest() &&
                    (request = committedRequests.poll()) != null) {
{noformat}

Here we are not processing any requests, because the original request has completed. We haven't dequeued either the read or the sync request in this processor. Next, the poll above will pull the sync request off the queue, and in the following block, the sync will get forwarded to the next processor.

This is a problem because the read request hasn't been forwarded yet, so requests are now out of order.

I've been able to reproduce this bug reliably by injecting a Thread.sleep(5000) between the two blocks above to make the race condition far more likely, then in a client program.

{noformat}
        zoo_aget_children(zh, ""/"", 0, getchildren_cb, NULL);
        //Wait long enough for queuedRequests to drain
        sleep(1);
        zoo_aget_children(zh, ""/"", 0, getchildren_cb, &th_ctx[0]);
        zoo_async(zh, ""/"", sync_cb, &th_ctx[0]);
{noformat}

When this bug is triggered, 3 things can happen:
1) Clients will see requests complete out of order and fail on xid mismatches.
2) Kazoo in particular doesn't handle this runtime exception well, and can orphan outstanding requests.
3) I've seen zookeeper servers deadlock, likely because the commit cannot be completed, which can wedge the commit processor.
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1863,Blocker,Dutch T. Meyer,Fixed,2014-07-15T22:40:54.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"Race condition in commit processor leading to out of order request completion, xid mismatch on client.",2016-08-01T22:27:23.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",13.0
Rakesh Radhakrishnan,"[<JIRA Component: name='tests', id='12312427'>]",2014-01-13T03:50:57.000+0000,Rakesh Radhakrishnan,"ServerCnxnTest#testServerCnxnExpiry test case is failing in the trunk build with the following exception
{code}
    [junit] 2014-01-11 10:13:07,696 [myid:] - INFO  [NIOServerCxnFactory.AcceptThread:0.0.0.0/0.0.0.0:11221:NIOServerCnxnFactory$AcceptThread@296] - Accepted socket connection from /127.0.0.1:63930
    [junit] 2014-01-11 10:13:09,000 [myid:] - INFO  [ConnnectionExpirer:NIOServerCnxn@1006] - Closed socket connection for client /127.0.0.1:63930 (no session established for client)
    [junit] 2014-01-11 10:13:10,697 [myid:] - INFO  [main:JUnit4ZKTestRunner$LoggedInvokeMethod@62] - TEST METHOD FAILED testServerCnxnExpiry
    [junit] java.net.SocketException: Software caused connection abort: recv failed
    [junit] 	at java.net.SocketInputStream.socketRead0(Native Method)
    [junit] 	at java.net.SocketInputStream.read(SocketInputStream.java:150)
    [junit] 	at java.net.SocketInputStream.read(SocketInputStream.java:121)
    [junit] 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:283)
    [junit] 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:325)
    [junit] 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:177)
    [junit] 	at java.io.InputStreamReader.read(InputStreamReader.java:184)
    [junit] 	at java.io.BufferedReader.fill(BufferedReader.java:154)
    [junit] 	at java.io.BufferedReader.readLine(BufferedReader.java:317)
    [junit] 	at java.io.BufferedReader.readLine(BufferedReader.java:382)
    [junit] 	at org.apache.zookeeper.test.ServerCnxnTest.send4LetterWord(ServerCnxnTest.java:105)
    [junit] 	at org.apache.zookeeper.test.ServerCnxnTest.sendRequest(ServerCnxnTest.java:77)
    [junit] 	at org.apache.zookeeper.test.ServerCnxnTest.testServerCnxnExpiry(ServerCnxnTest.java:64)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:601)
    [junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit] 	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:52)
    [junit] 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:69)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:48)
    [junit] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
    [junit] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
    [junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
    [junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
    [junit] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
    [junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:292)
    [junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:518)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1052)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:906)
    {code}

When analyzing the possible cause of the failure is:
 During connection expiry server will close the socket channel connection. After the socket closure, when the client tries to read a line of text will throw java.net.SocketException.

 In the failure scenario the testcase has established a socket connection and entering into the sleep. In the meantime the server side expiration would happen and closing the socket channel. Assume after the socket closure the testcase is trying to read the text using the previously established socket and is resulting in SocketException. There is a race between the reading the socket in the client side and socket closure in server side.
{code}
NIOServerCnxn#closeSock is closing the socket channel.
 sock.socket().shutdownOutput();
 sock.socket().shutdownInput();
 sock.socket().close();
 sock.close();
{code}","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1862,Major,Rakesh Radhakrishnan,Fixed,2014-03-14T23:11:03.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ServerCnxnTest.testServerCnxnExpiry is intermittently failing,2014-03-16T17:53:59.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
Ted Yu,[],2014-01-11T23:50:07.000+0000,Ted Yu,"queueSendMap is a ConcurrentHashMap.
At line 210:
{code}
            if (!queueSendMap.containsKey(sid)) {
                queueSendMap.put(sid, new ArrayBlockingQueue<ByteBuffer>(
                        SEND_CAPACITY));
{code}
By the time control enters if block, there may be another concurrent put with same sid to the ConcurrentHashMap.
putIfAbsent() should be used.

Similar issue occurs at line 307 as well.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1861,Minor,Ted Yu,Fixed,2014-02-14T03:03:33.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ConcurrentHashMap isn't used properly in QuorumCnxManager,2014-02-14T03:03:33.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",6.0
Raúl Gutiérrez Segalés,"[<JIRA Component: name='java client', id='12312381'>]",2014-01-11T20:54:28.000+0000,Raúl Gutiérrez Segalés,"This was caught by [~fournc], the async versions of reconfig in the Java client don't actually throw KeeperException nor InterruptedException. Since this is unreleased code (i.e.: for 3.5.0) I don't think there are issues with changing the API (that is, considering what exceptions are thrown part of the API). ","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1860,Major,Raúl Gutiérrez Segalés,Fixed,2014-01-13T22:34:47.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Async versions of reconfig don't actually throw KeeperException nor InterruptedException,2014-01-14T11:05:48.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
,[],2014-01-11T18:20:18.000+0000,Ted Yu,"{code}
        final PrintWriter pwriter = new PrintWriter(
                new BufferedWriter(new SendBufferWriter()));
...
        } else if (len == telnetCloseCmd) {
            cleanupWriterSocket(null);
            return true;
        }
{code}
pwriter should be closed in case of telnetCloseCmd",[],Bug,ZOOKEEPER-1859,Minor,Ted Yu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,pwriter should be closed in NIOServerCnxn#checkFourLetterWord(),2018-07-12T10:32:12.000+0000,[],4.0
Michi Mutsuzaki,"[<JIRA Component: name='c client', id='12312380'>]",2014-01-02T23:43:22.000+0000,Dutch T. Meyer,"If a client has a 2 server list, and is currently connected to the last server in that list, and that server then goes offline, the addrvec_next() call handle_error() will push the client to the start of the list and terminate the connection.

Then, the zoo_cycle_next_server() call in zookeeper_interest will be called in response to the connection failure, and the client will cycle back to the failed server.

In this way, a client who has a list of only 2 servers can get stuck on the one failed server.  This would only be an issue in an ensemble larger than 2 of course, because failing 1 out of 2 would lead to quorum loss anyway.

There are other harmonics possible if every other server in the list is failed, but this is simplest to reproduce in a 3 server ensemble where the client only knows about 2 servers, one of which then fails.  There are probably some elegant fixes here, but I think the simplest is to add a flag to track whether a server has been accessed before, and if it hasn't, don't call zoo_cycle_next_server() at the top of the zookeeper_interest() function.","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-1856,Major,Dutch T. Meyer,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,zookeeper C-client can fail to switch from a dead server in a 3+ server ensemble if the client only has a 2 server list.,2022-02-03T08:36:25.000+0000,[],5.0
,"[<JIRA Component: name='c client', id='12312380'>]",2014-01-02T23:07:11.000+0000,Dutch T. Meyer,"If one calls zoo_set_servers to update with a new server list that does not contain the currently connected server, the client will disconnect.  Fair enough, but any outstanding requests on the set_requests queue aren't completed, so the next completed request from the new server can fail with an out-of-order XID error.

The disconnect occurs in update_addrs(), when a reconfig is necessary, though it's not quite as easy as just calling cleanup_bufs there, because you could then race the call to dequeue_completion in zookeeper_process and pull NULL entries for a recently completed request

I don't have a patch for this right now, but I do have a simple repro I can post when time permits.",[],Bug,ZOOKEEPER-1855,Minor,Dutch T. Meyer,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,calls to zoo_set_server() fail to flush outstanding request queue.,2022-02-03T08:50:28.000+0000,[],3.0
Ryan Lamore,"[<JIRA Component: name='java client', id='12312381'>]",2013-12-28T22:38:35.000+0000,sekine coulibaly,"Execute the following command in zkCli.sh :

create /contacts/1  {""country"":""CA"",""name"":""De La Salle""}

The results is that only {""id"":1,""fullname"":""De is stored.
The expected result is to have the full JSON payload stored.

The CREATE command seems to be croped after the first space of the data payload. When issuing a create command, all arguments not being -s nor -e shall be treated as the actual data.

","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-1853,Minor,sekine coulibaly,Fixed,2015-11-08T22:27:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkCli.sh can't issue a CREATE command containing spaces in the data,2016-07-21T20:18:20.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",13.0
Chris Chen,"[<JIRA Component: name='quorum', id='12312379'>]",2013-12-23T23:23:17.000+0000,Chris Chen,"Recent changes to the Observer and Follower Request Processors switch on the request opcode, but create2 is left out. This leads to a condition where the create2 request is passed to the CommitProcessor, but the leader never gets the request, the CommitProcessor can't find a matching request, so the client gets disconnected.

Added tests as well.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1851,Blocker,Chris Chen,Fixed,2014-07-18T17:49:28.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Follower and Observer Request Processors Do Not Forward create2 Requests,2014-07-19T11:24:16.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",7.0
Germán Blanco,"[<JIRA Component: name='tests', id='12312427'>]",2013-12-21T17:52:15.000+0000,Germán Blanco,"This is the error:
TestZookeeperInit.cc:241: Assertion: assertion failed [Expression: zh==0]","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1850,Trivial,Germán Blanco,Won't Fix,2013-12-22T17:18:11.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,cppunit test testNonexistingHost in TestZookeeperInit is failing on Unbuntu,2014-03-13T18:17:11.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",3.0
Enis Soztutar,[],2013-12-18T01:17:17.000+0000,Enis Soztutar,"ZK uses Java NIO to create ServerSorcket's from ServerSocketChannels. Under windows, the ipv4 and ipv6 is implemented independently, and Java seems that it cannot reuse the same socket channel for both ipv4 and ipv6 sockets. We are getting ""java.net.SocketException: Address family not supported by protocol
family"" exceptions. When, ZK client resolves ""localhost"", it gets both v4 127.0.0.1 and v6 ::1 address, but the socket channel cannot bind to both v4 and v6.
The problem is reported as:
http://bugs.sun.com/view_bug.do?bug_id=6230761
http://stackoverflow.com/questions/1357091/binding-an-ipv6-server-socket-on-windows
Although the JDK bug is reported as resolved, I have tested with jdk1.6.0_33 without any success. Although JDK7 seems to have fixed this problem. 

See HBASE-6825 for reference. ",[],Bug,ZOOKEEPER-1848,Major,Enis Soztutar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,[WINDOWS] Java NIO socket channels does not work with Windows ipv6 on JDK6,2022-02-03T08:50:22.000+0000,[],3.0
Enis Soztutar,[],2013-12-17T23:05:22.000+0000,Enis Soztutar,"It is good practice to have all the code in the repository use the same line endings (LF) so that patches can be applied normally. We can add a gitattributes file so that checked out code can still have platform dependent line endings. 

More readings: 
https://help.github.com/articles/dealing-with-line-endings
http://stackoverflow.com/questions/170961/whats-the-best-crlf-handling-strategy-with-git
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1847,Major,Enis Soztutar,Duplicate,2014-02-05T21:31:38.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Normalize line endings in repository,2014-02-05T21:31:38.000+0000,[],3.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2013-12-17T01:18:00.000+0000,Benjamin Jaton,"The class QuorumPeer maintains a Map<Long, QuorumServer> quorumPeers.
Each QuorumServer is created with an instance of InetSocketAddress electionAddr, and holds it forever.

I believe this is why the ZooKeeper servers can't resolve each other dynamically: If a ZooKeeper in the ensemble cannot be resolved at startup, it will never be resolved (until restart of the JVM), constantly failing with an UnknownHostException, even when the node is back up and reachable.

I would suggest to recreate an InetSocketAddress every time we retry the connection.
",[],Bug,ZOOKEEPER-1846,Minor,Benjamin Jaton,Duplicate,2017-01-16T06:40:45.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Cached InetSocketAddresses prevent proper dynamic DNS resolution,2019-01-30T12:50:04.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",12.0
Michi Mutsuzaki,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='tests', id='12312427'>]",2013-12-17T00:13:50.000+0000,Michi Mutsuzaki,"This test waits for the leader election to settle, but it is possible that 3 follower threads join before the leader thread joins. We should wait for the leader thread to join in a loop for some time.

{noformat}
Leader hasn't joined: 5
junit.framework.AssertionFailedError: Leader hasn't joined: 5
        at org.apache.zookeeper.test.FLETest.testLE(FLETest.java:313)
        at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
{noformat}","[<JIRA Version: name='3.4.6', id='12323310'>]",Bug,ZOOKEEPER-1845,Major,Michi Mutsuzaki,Duplicate,2013-12-17T22:35:01.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,FLETest.testLE fails on windows,2014-03-13T18:17:15.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",3.0
Rakesh Radhakrishnan,"[<JIRA Component: name='server', id='12312382'>]",2013-12-16T23:58:33.000+0000,Michi Mutsuzaki,"TruncateTest has been failing consistently on windows:

https://builds.apache.org/job/ZooKeeper-trunk-WinVS2008_java/627/testReport/junit/org.apache.zookeeper.test/TruncateTest/testTruncate/","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1844,Critical,Michi Mutsuzaki,Fixed,2014-02-12T16:01:30.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,TruncateTest fails on windows,2014-03-13T18:17:13.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.5.0', id='12316644'>]",5.0
Bill Havanki,[],2013-12-16T23:09:57.000+0000,Justin SB,"I was reading ByteBufferInputStream.java; here is the skip method:

    public long skip(long n) throws IOException {
        long newPos = bb.position() + n;
        if (newPos > bb.remaining()) {
            n = bb.remaining();
        }
        bb.position(bb.position() + (int) n);
        return n;
    }

The first two lines look wrong; we compare a ""point"" (position) to a ""distance"" (remaining).  I think the test should be if (newPos >= bb.limit()).

Or more simply:
    public long skip(long n) throws IOException {
        int remaining = buffer.remaining();
        if (n > remaining) {
            n = remaining;
        }
        buffer.position(buffer.position() + (int) n);
        return n;
    }
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1843,Minor,Justin SB,Fixed,2014-04-26T21:17:54.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Oddity in ByteBufferInputStream skip,2014-04-28T11:05:13.000+0000,[],5.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2013-12-16T20:35:02.000+0000,Jeremy Stribling,"When I was testing the patch for https://issues.apache.org/jira/browse/ZOOKEEPER-1691, the test included with that patch was failing for me.  The problem happened when the tests shuts down some followers and then attempts to bring them back up:

{quote}
2013-12-13 17:31:03,976 [myid:1] - INFO [QuorumPeer[myid=1]/127.0.0.1:11227:Follower@194] - shutdown called
java.lang.Exception: shutdown Follower
        at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:194)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:971)
...
2013-12-13 17:31:03,992 [myid:1] - INFO [QuorumPeerListener:QuorumCnxManager$Listener@544] - My election bind port: localhost/127.0.0.1:11229
2013-12-13 17:31:03,992 [myid:1] - ERROR [localhost/127.0.0.1:11229:QuorumCnxManager$Listener@557] - Exception while listening
java.net.BindException: Address already in use
        at java.net.PlainSocketImpl.socketBind(Native Method)
        at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:376)
        at java.net.ServerSocket.bind(ServerSocket.java:376)
        at java.net.ServerSocket.bind(ServerSocket.java:330)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$Listener.run(QuorumCnxManager.java:546) 
{quote}

The problem appears to be that the when follower.shutdown() is called in QuorumPeer.run(), the election algorithm is never shut down, so when the node restarts it can't bind back to the same port.

I will upload a patch that calls shutdown() for both the leader and the follower in this case, but I'm not positive its the right place or fix for this issue, so feedback would be appreciated.",[],Bug,ZOOKEEPER-1842,Major,Jeremy Stribling,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Election listening thread not shutdown for leaders or followers,2013-12-16T21:10:49.000+0000,[],2.0
Alexander Shraer,"[<JIRA Component: name='quorum', id='12312379'>]",2013-12-14T11:53:47.000+0000,Bruno Freudensprung,"Submitted this bug on a suggestion of Alexander Shraer (see https://issues.apache.org/jira/browse/ZOOKEEPER-1691)

How to reproduce:

== Server 1 zoo.cfg:
standaloneEnabled=false
dynamicConfigFile=<path to>/confdyn1/zoo.cfg.dynamic

== Server 1 zoo.cfg.dynamic:
server.1=localhost:2888:3888:participant;localhost:2181

== Server 2 zoo.cfg:
standaloneEnabled=false
dynamicConfigFile=<path to>/confdyn2/zoo.cfg.dynamic

== Server 2 zoo.cfg.dynamic (it is ""aware"" of the server 1, as mentioned in the Dynamic Reconfiguration - User Manual
that I should have read more carefully yesterday):
server.1=localhost:2888:3888:participant;localhost:2181
server.2=localhost:2889:3889:participant;localhost:2182

Start server 1 
Start server 2 

== use client 1 to issue a reconfig command on server 1:
[zk: localhost:2181(CONNECTED) 1] reconfig -add server.2=localhost:2889:3889:participant;localhost:2182
Committed new configuration:
server.1=localhost:2888:3888:participant;localhost:2181
server.2=localhost:2889:3889:participant;localhost:2182
version=100000003

There are strange stack traces in both server consoles.

Server 1:
2013-12-12 22:31:40,888 [myid:1] - WARN [ProcessThread(sid:1 cport:-1)::QuorumCnxManager@390] - Cannot open channel to 2 at election address localhost/127.0.0.1:3889
java.net.ConnectException: Connection refused: connect
at java.net.PlainSocketImpl.socketConnect(Native Method)
at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:351)
at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:213)
at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:200)
at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
at java.net.Socket.connect(Socket.java:529)
at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:375)
at org.apache.zookeeper.server.quorum.QuorumPeer.connectNewPeers(QuorumPeer.java:1252)
at org.apache.zookeeper.server.quorum.QuorumPeer.setLastSeenQuorumVerifier(QuorumPeer.java:1272)
at org.apache.zookeeper.server.quorum.Leader.propose(Leader.java:1071)
at org.apache.zookeeper.server.quorum.ProposalRequestProcessor.processRequest(ProposalRequestProcessor.java:78)
at org.apache.zookeeper.server.PrepRequestProcessor.pRequest(PrepRequestProcessor.java:864)
at org.apache.zookeeper.server.PrepRequestProcessor.run(PrepRequestProcessor.java:144)
2013-12-12 22:31:41,919 [myid:1] - WARN [LearnerHandler-/127.0.0.1:52301:QuorumPeer@1259] - Restarting Leader Election
2013-12-12 22:31:41,920 [myid:1] - INFO [localhost/127.0.0.1:3888:QuorumCnxManager$Listener@571] - Leaving listener
2013-12-12 22:31:41,920 [myid:1] - INFO [QuorumPeerListener:QuorumCnxManager$Listener@544] - My election bind port: localhost/127.0.0.1:3888
2013-12-12 22:31:44,438 [myid:1] - INFO [WorkerReceiver[myid=1]:FastLeaderElection$Messenger$WorkerReceiver@410] - WorkerReceiver is down
2013-12-12 22:31:44,439 [myid:1] - INFO [WorkerSender[myid=1]:FastLeaderElection$Messenger$WorkerSender@442] - WorkerSender is down

Server 2:
2013-12-12 22:31:41,894 [myid:2] - WARN [QuorumPeer[myid=2]/127.0.0.1:2182:QuorumCnxManager@390] - Cannot open channel to 2 at election address localhost/127.0.0.1:3889
java.net.ConnectException: Connection refused: connect
at java.net.PlainSocketImpl.socketConnect(Native Method)
at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:351)
at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:213)
at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:200)
at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
at java.net.Socket.connect(Socket.java:529)
at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:375)
at org.apache.zookeeper.server.quorum.QuorumPeer.connectNewPeers(QuorumPeer.java:1252)
at org.apache.zookeeper.server.quorum.QuorumPeer.setLastSeenQuorumVerifier(QuorumPeer.java:1272)
at org.apache.zookeeper.server.quorum.Follower.processPacket(Follower.java:131)
at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:89)
at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:967)
2013-12-12 22:31:41,923 [myid:2] - WARN [QuorumPeer[myid=2]/127.0.0.1:2182:QuorumPeer@1259] - Restarting Leader Election
2013-12-12 22:31:41,924 [myid:2] - INFO [QuorumPeerListener:QuorumCnxManager$Listener@544] - My election bind port: localhost/127.0.0.1:3889",[],Bug,ZOOKEEPER-1840,Minor,Bruno Freudensprung,Fixed,2014-04-16T07:07:22.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Server tries to connect to itself during dynamic reconfig,2014-04-16T11:07:14.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
Rakesh Radhakrishnan,"[<JIRA Component: name='server', id='12312382'>]",2013-12-13T14:25:26.000+0000,Rakesh Radhakrishnan,Deadlock found during NettyServerCnxn closure. Please see the attached threaddump.,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1839,Critical,Rakesh Radhakrishnan,Fixed,2013-12-16T04:21:55.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Deadlock in NettyServerCnxn,2014-03-13T18:16:56.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",7.0
Dutch T. Meyer,"[<JIRA Component: name='c client', id='12312380'>]",2013-12-13T00:50:28.000+0000,Dutch T. Meyer,"There is a relatively innocuous but useless pointer assignment in
addrvec_next():

195	void addrvec_next(addrvec_t *avec, struct sockaddr_storage *next)
....
203	    if (!addrvec_hasnext(avec))
204	    {
205	        next = NULL;
206	        return;

That assignment on (205) has no point, as next is a local variable lost upon function return.  Likely this should be a memset to zero out the actual parameter.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1836,Trivial,Dutch T. Meyer,Fixed,2014-05-15T20:40:10.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,addrvec_next() fails to set next parameter if addrvec_hasnext() returns false,2014-05-20T11:09:12.000+0000,[],5.0
Bruno Freudensprung,"[<JIRA Component: name='quorum', id='12312379'>]",2013-12-12T21:05:27.000+0000,Bruno Freudensprung,"On Windows, reconfig fails to rename the tmp dynamic config file to the real dynamic config filename.
Javadoc of java.io.File.renameTo says the behavior is highly plateform dependent, so I guess this should not be a big surprise.
The problem occurs in src/java/main/org/apache/zookeeper/server/quorum/QuorumPeerConfig.java that could be modified like this:
+ curFile.delete();
if (!tmpFile.renameTo(curFile)) {
+ configFile.delete();
if (!tmpFile.renameTo(configFile)) {
As suggested by Alex in https://issues.apache.org/jira/browse/ZOOKEEPER-1691 (btw there is more information about my test scenario over there) it is a bit ""scary"" to delete the current configuration file.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1835,Major,Bruno Freudensprung,Fixed,2014-07-04T02:25:43.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,dynamic configuration file renaming fails on Windows,2014-07-04T11:11:39.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",7.0
Michi Mutsuzaki,[],2013-12-07T01:49:15.000+0000,Michi Mutsuzaki,"A bunch of 3.4 tests are failing on windows.

{noformat}
    [junit] 2013-12-06 08:40:59,692 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testEarlyLeaderAbandonment
    [junit] 2013-12-06 08:41:10,472 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testHighestZxidJoinLate
    [junit] 2013-12-06 08:45:31,085 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testUpdatingEpoch
    [junit] 2013-12-06 08:55:34,630 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testObserversHammer
    [junit] 2013-12-06 08:55:59,889 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testAsyncExistsFailure_NoNode
    [junit] 2013-12-06 08:56:00,571 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testAsyncGetACL
    [junit] 2013-12-06 08:56:02,626 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testAsyncGetChildrenEmpty
    [junit] 2013-12-06 08:56:03,491 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testAsyncGetChildrenSingle
    [junit] 2013-12-06 08:56:11,276 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testAsyncGetChildrenTwo
    [junit] 2013-12-06 08:56:13,878 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testAsyncGetChildrenFailure_NoNode
    [junit] 2013-12-06 08:56:16,294 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testAsyncGetChildren2Empty
    [junit] 2013-12-06 08:56:18,622 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testAsyncGetChildren2Single
    [junit] 2013-12-06 08:56:21,224 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testAsyncGetChildren2Two
    [junit] 2013-12-06 08:56:23,738 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testAsyncGetChildren2Failure_NoNode
    [junit] 2013-12-06 08:56:26,058 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testAsyncGetData
    [junit] 2013-12-06 08:56:28,482 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testAsyncGetDataFailure_NoNode
    [junit] 2013-12-06 08:57:35,527 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testStartupFailureCreate
    [junit] 2013-12-06 08:57:38,645 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testStartupFailureSet
    [junit] 2013-12-06 08:57:41,261 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testStartupFailureSnapshot
    [junit] 2013-12-06 08:59:22,222 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testClientWithWatcherObj
    [junit] 2013-12-06 09:00:05,592 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testClientCleanup
    [junit] 2013-12-06 09:01:24,113 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testBindByAddress
    [junit] 2013-12-06 09:02:14,123 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testClientwithoutWatcherObj
    [junit] 2013-12-06 09:05:56,461 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testZeroWeightQuorum
    [junit] 2013-12-06 09:08:18,747 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testResyncByDiffAfterFollowerCrashes
    [junit] 2013-12-06 09:09:42,271 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testFourLetterWords
    [junit] 2013-12-06 09:14:03,770 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testLE
    [junit] 2013-12-06 09:46:30,002 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testHierarchicalQuorum
    [junit] 2013-12-06 09:50:26,912 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testHammerBasic
    [junit] 2013-12-06 09:51:07,604 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testQuotaWithQuorum
    [junit] 2013-12-06 09:52:41,515 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testNull
    [junit] 2013-12-06 09:53:22,648 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testDeleteWithChildren
    [junit] 2013-12-06 09:56:49,061 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testClientwithoutWatcherObj
    [junit] 2013-12-06 09:58:27,705 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testGetView
    [junit] 2013-12-06 09:59:07,856 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testViewContains
    [junit] 2013-12-06 10:01:31,418 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testSessionMoved
    [junit] 2013-12-06 10:04:50,542 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testMultiToFollower
    [junit] 2013-12-06 10:07:55,361 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testBehindLeader
    [junit] 2013-12-06 10:10:57,439 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testLateLogs
    [junit] 2013-12-06 10:12:05,336 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testConnectionEvents
    [junit] 2013-12-06 10:14:02,781 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testRecovery
    [junit] 2013-12-06 10:14:37,220 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testFail
    [junit] 2013-12-06 10:14:46,925 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testRestoreCommittedLog
    [junit] 2013-12-06 10:15:30,109 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testAuth
    [junit] 2013-12-06 10:16:09,256 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testAuth
    [junit] 2013-12-06 10:16:44,586 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testAuth
    [junit] 2013-12-06 10:17:19,222 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testBadSaslAuthNotifiesWatch
    [junit] 2013-12-06 10:17:54,239 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testAuthFail
    [junit] 2013-12-06 10:18:35,623 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testAuth
    [junit] 2013-12-06 10:18:47,094 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testAuth
    [junit] 2013-12-06 10:19:06,770 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testCreateAfterCloseShouldFail
    [junit] 2013-12-06 10:20:23,884 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testBasic
{noformat}","[<JIRA Version: name='3.4.7', id='12325149'>]",Bug,ZOOKEEPER-1833,Blocker,Michi Mutsuzaki,Fixed,2015-11-03T14:46:43.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,fix windows build,2015-11-03T14:46:43.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",7.0
,[],2013-12-05T18:21:27.000+0000,Ben Hartshorne,"The ganglia zookeeper plugin does not report the number of connected clients, though this information is available from the 'stat' command. ",[],Bug,ZOOKEEPER-1832,Minor,Ben Hartshorne,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Add count of connected clients to submitted ganglia metrics,2013-12-06T21:55:50.000+0000,[],1.0
,"[<JIRA Component: name='c client', id='12312380'>]",2013-12-04T00:29:59.000+0000,Raúl Gutiérrez Segalés,"After discussing the patch for ZOOKEEPER-1632 we came to realize that many methods in the c client don't honor the internal error checking that they do (i.e.: they don't return early on error). This is bad because you might return with an error but your call (in case of the async ones) might still have been called.

So at that point it's unclear how the caller should deal with resource deallocation.  ",[],Bug,ZOOKEEPER-1828,Major,Raúl Gutiérrez Segalés,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Audit src/c/src/zookeeper.c for missing error checking & early returns,2013-12-06T21:34:45.000+0000,[],1.0
,[],2013-12-03T19:41:16.000+0000,Ben Hartshorne,"The ganglia zookeeper module uses a single 2048 byte socket.recv to get the response from the 'stat' command.  When there are more than a few clients connected, the list of connected clients fills the buffer before the script gets to the actual metrics its trying to report.

This bug is fixed in https://github.com/maplebed/zookeeper-monitoring/tree/ben.fetch_more_data",[],Bug,ZOOKEEPER-1826,Minor,Ben Hartshorne,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zookeeper ganglia module fails to read the output of stat when too many clients are connected,2013-12-06T21:34:25.000+0000,[],1.0
Ling Mao,"[<JIRA Component: name='server', id='12312382'>]",2013-11-28T01:44:04.000+0000,Raúl Gutiérrez Segalés,Some times it's handy to have LogFormatter show you the content of the transactions (i.e.: if you are storing text).,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-1823,Trivial,Raúl Gutiérrez Segalés,Fixed,2018-09-12T11:33:16.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkTxnLogToolkit -dump should support printing transaction data as a string,2019-05-20T17:50:44.000+0000,[],9.0
Germán Blanco,"[<JIRA Component: name='c client', id='12312380'>]",2013-11-23T06:43:58.000+0000,Germán Blanco,"this is the compiler output:     
3.4/src/c/src/load_gen.c:110:1: warning: control may reach end of non-void function [-Wreturn-type]
     [exec] }
     [exec] ^
3.4/src/c/src/load_gen.c:135:1: warning: control may reach end of non-void function [-Wreturn-type]
     [exec] }
     [exec] ^
3.4/src/c/src/load_gen.c:163:1: warning: control may reach end of non-void function [-Wreturn-type]
     [exec] }
     [exec] ^
3.4/src/c/src/load_gen.c:180:1: warning: control may reach end of non-void function [-Wreturn-type]
     [exec] }
     [exec] ^
i think that the code is missing a ""return ZOK"" in the end of these functions.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1821,Major,Germán Blanco,Fixed,2013-11-27T23:23:56.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,very ugly warning when compiling load_gen.c,2014-03-13T18:16:52.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",5.0
Daniel Knightly,"[<JIRA Component: name='tests', id='12312427'>]",2013-11-21T23:56:55.000+0000,Daniel Knightly,"The DeserializationPerfTest calls
SerializationPerfTest.createNodes to create serialized nodes to deserialize.

However 2 of the arguments, childcount and parentCVersion are switched in the call to the above method.  This results in all tests unintentionally testing the same scenario.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1819,Minor,Daniel Knightly,Fixed,2014-04-25T19:37:09.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,DeserializationPerfTest calls method with wrong arguments,2014-04-26T11:04:47.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Fangmin Lv,[],2013-11-16T11:53:48.000+0000,Flavio Paiva Junqueira,See umbrella jira.,"[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-1818,Blocker,Flavio Paiva Junqueira,Fixed,2018-12-17T14:20:34.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Correctly handle potential inconsistent zxid/electionEpoch and peerEpoch during leader election,2020-09-21T10:59:07.000+0000,"[<JIRA Version: name='3.5.1', id='12326786'>]",7.0
wu wen,"[<JIRA Component: name='java client', id='12312381'>]",2013-11-15T21:33:15.000+0000,Jared Winick,"In the testing of ACCUMULO-1379 and ACCUMULO-1858 it was seen that the non-blocking behavior of ClientCnxn.close(), and therefore ZooKeeper.close(), can cause a race condition when undeploying an application running in a Java container such as JBoss or Tomcat. As the close() method returns without joining on the sendThread and eventThread, those threads continue to execute/cleanup while the container is cleaning up the application's resources. If the container has unloaded classes by the time this code runs

{code}
ZooTrace.logTraceMessage(LOG, ZooTrace.getTextTraceLevel(), ""SendThread exitedloop."");
{code}

A ""java.lang.NoClassDefFoundError: org/apache/zookeeper/server/ZooTrace"" can be seen. ",[],Bug,ZOOKEEPER-1816,Minor,Jared Winick,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,ClientCnxn.close() should block until threads have died,2018-06-22T04:49:04.000+0000,"[<JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.4.11', id='12339207'>]",4.0
Daniel Peon,"[<JIRA Component: name='leaderElection', id='12312378'>]",2013-11-15T10:37:23.000+0000,Daniel Peon,"FastLeader election takes long time because of the exponential backoff. Currently the time is 60 seconds.

It would be interesting to give the possibility to configure this parameter, like for example for a Server shutdown.

Otherwise, it sometimes takes so long and it has been detected a test failure when executing: org.apache.zookeeper.server.quorum.QuorumPeerMainTest.

This test case waits until 30 seconds and this is smaller than the 60 seconds where the leader election can be waiting for at the moment of shutting down.

Considering the failure during the test case, this issue was considered a possible bug.","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-1814,Major,Daniel Peon,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Reduction of waiting time during Fast Leader Election,2022-02-03T08:36:25.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.5.0', id='12316644'>]",7.0
,[],2013-11-15T06:40:25.000+0000,Vinayakumar B,"Due to following exception Zookeeper restart is failing

{noformat}java.io.IOException: Failed to process transaction type: 1 error: KeeperErrorCode = NoNode for /test/subdir2/subdir2/subdir
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:183)
	at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:222)
	at org.apache.zookeeper.server.ZooKeeperServer.loadData(ZooKeeperServer.java:255)
	at org.apache.zookeeper.server.ZooKeeperServer.startdata(ZooKeeperServer.java:380)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.startup(NIOServerCnxnFactory.java:748)
	at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:111)
	at org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:90)
	at org.apache.zookeeper.server.ZooKeeperServerMainTest$2.run(ZooKeeperServerMainTest.java:218)
Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /test/subdir2/subdir2/subdir
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.processTransaction(FileTxnSnapLog.java:268)
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:181)
	... 7 more{noformat}","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1813,Major,Vinayakumar B,Duplicate,2013-11-15T09:15:47.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper restart fails due to missing node from snapshot,2013-12-06T21:33:03.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.5.0', id='12316644'>]",3.0
Benjamin Jaton,"[<JIRA Component: name='contrib', id='12312700'>]",2013-11-10T20:28:57.000+0000,Benjamin Jaton,"Steps to reproduce:

- Connect to localhost:2181 when ZooKeeper server is down. After a few seconds, ZooInspector warns that the connection has failed
- start the ZooKeeper server
- Reconnect to localhost:2181, ZooInspector will still not be able to connect to the server.

The workaround is to relaunch ZooInspector.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1812,Minor,Benjamin Jaton,Fixed,2013-11-12T06:30:52.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooInspector reconnection always fails if first connection fails,2014-03-13T18:17:08.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.5.0', id='12316644'>]",4.0
Harsh J,"[<JIRA Component: name='java client', id='12312381'>]",2013-11-07T05:37:52.000+0000,Harsh J,"The ClientCnxn class in ZK instantiates the ZooKeeperSaslClient with a hardcoded service name of ""zookeeper"". This causes all apps to fail in accessing ZK in a secure environment where the administrator has changed the principal name ZooKeeper runs as.

The service name should be configurable.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1811,Major,Harsh J,Fixed,2014-02-10T21:33:02.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"The ZooKeeperSaslClient service name principal is hardcoded to ""zookeeper""",2015-07-23T18:54:12.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",6.0
Germán Blanco,"[<JIRA Component: name='leaderElection', id='12312378'>]",2013-11-06T16:03:14.000+0000,Flavio Paiva Junqueira,The same as ZOOKEEPER-1808 but for trunk.,"[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1810,Major,Flavio Paiva Junqueira,Fixed,2014-07-08T03:26:40.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Add version to FLE notifications for trunk,2018-11-15T17:53:25.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",8.0
,[],2013-11-06T05:06:42.000+0000,Shaun Senecal,"We have been running into a situation where we attempt to recreate our ephemeral nodes after a session expiry, only to find that the node already exists.   Admittedly, this is only happening when we are aggresively killing and recreating sessions in a tight loop, but I thought it might point to a larger issue which may need to be addressed.

Attached is a small app which demonstrates the issue, and a log file (client and server in the same log) which shows the issue as it occured.  Reproducing the bug is a tedious process of rerunning the test over and over again, but I have typically been able to reproduce it within 15mins of trying.  The test app is using Curator, however, I think the issue is occuring at the ZK level since the logs clearly indicate the ephermal node is deleted after the session expiry.

Interesting bits from the log
{noformat}
...
2013/11/06 13:46:03,065 INFO  [ConnectionStateManager-0] Recreating node: /test
...
2013/11/06 13:46:03,070 DEBUG [SyncThread:0] Processing request:: sessionid:0x1422bbb36d10002 type:create cxid:0x2 zxid:0x8 txntype:1 reqpath:n/a
...
2013/11/06 13:46:03,071 DEBUG [main] Closing client for session: 0x1422bbb36d10002
2013/11/06 13:46:03,075 INFO  [ProcessThread(sid:0 cport:-1):] Processed session termination for sessionid: 0x1422bbb36d10002
2013/11/06 13:46:03,079 DEBUG [SyncThread:0] Processing request:: sessionid:0x1422bbb36d10002 type:closeSession cxid:0x1 zxid:0x9 txntype:-11 reqpath:n/a
2013/11/06 13:46:03,080 DEBUG [SyncThread:0] Deleting ephemeral node /test for session 0x1422bbb36d10002
2013/11/06 13:46:03,080 DEBUG [SyncThread:0] sessionid:0x1422bbb36d10002 type:closeSession cxid:0x1 zxid:0x9 txntype:-11 reqpath:n/a
...
2013/11/06 13:46:04,459 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:43462] Client attempting to renew session 0x1422bbb36d10002 at /127.0.0.1:59559
2013/11/06 13:46:04,459 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:43462] Invalid session 0x1422bbb36d10002 for client /127.0.0.1:59559, probably expired
2013/11/06 13:46:04,460 INFO  [main-SendThread(localhost:43462)] Unable to reconnect to ZooKeeper service, session 0x1422bbb36d10002 has expired, closing socket connection
2013/11/06 13:46:04,460 DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:43462] Dropping request: No session with sessionid 0x1422bbb36d10002 exists, probably expired and removed
...
2013/11/06 13:46:04,463 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:43462] Client attempting to establish new session at /127.0.0.1:59560
2013/11/06 13:46:04,466 DEBUG [SyncThread:0] Processing request:: sessionid:0x1422bbb36d10003 type:createSession cxid:0x0 zxid:0xa txntype:-10 reqpath:n/a
2013/11/06 13:46:04,466 DEBUG [SyncThread:0] sessionid:0x1422bbb36d10003 type:createSession cxid:0x0 zxid:0xa txntype:-10 reqpath:n/a
2013/11/06 13:46:04,467 INFO  [SyncThread:0] Established session 0x1422bbb36d10003 with negotiated timeout 30000 for client /127.0.0.1:59560
...
2013/11/06 13:46:04,473 INFO  [ConnectionStateManager-0] Recreating node: /test
2013/11/06 13:46:04,474 DEBUG [SyncThread:0] Processing request:: sessionid:0x1422bbb36d10003 type:exists cxid:0x2 zxid:0xfffffffffffffffe txntype:unknown reqpath:/___CURATOR_KILL_SESSION___15970538640754
2013/11/06 13:46:04,474 DEBUG [SyncThread:0] sessionid:0x1422bbb36d10003 type:exists cxid:0x2 zxid:0xfffffffffffffffe txntype:unknown reqpath:/___CURATOR_KILL_SESSION___15970538640754
2013/11/06 13:46:04,475 INFO  [ProcessThread(sid:0 cport:-1):] Got user-level KeeperException when processing sessionid:0x1422bbb36d10003 type:create cxid:0x3 zxid:0xc txntype:-1 reqpath:n/a Error Path:/test Error:KeeperErrorCode = NodeExists for /test
2013/11/06 13:46:04,475 DEBUG [main-SendThread(localhost:43462)] Reading reply sessionid:0x1422bbb36d10003, packet:: clientPath:null serverPath:null finished:false header:: 2,3  replyHeader:: 2,11,-101  request:: '/___CURATOR_KILL_SESSION___15970538640754,T  response::  
2013/11/06 13:46:04,476 INFO  [main] Initiating client connection, connectString=127.0.0.1:43462 sessionTimeout=10000 watcher=com.netflix.curator.test.KillSession$2@4067d00a sessionId=1422bbb36d10003 sessionPasswd=<hidden>
...
2013/11/06 13:46:04,479 ERROR [ConnectionStateManager-0] Failed to recreate ephemeral node
org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /test
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:119)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)
	at com.netflix.curator.framework.imps.CreateBuilderImpl$10.call(CreateBuilderImpl.java:625)
	at com.netflix.curator.framework.imps.CreateBuilderImpl$10.call(CreateBuilderImpl.java:609)
	at com.netflix.curator.RetryLoop.callWithRetry(RetryLoop.java:106)
	at com.netflix.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:605)
	at com.netflix.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:428)
	at com.netflix.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:408)
	at com.netflix.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:41)
	at com.rakuten.sandbox.sessionexpiry.nodeexists.SessionExpiryTest$2.stateChanged(SessionExpiryTest.java:72)
	at com.netflix.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:184)
	at com.netflix.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:180)
	at com.netflix.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:262)
	at com.netflix.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at com.netflix.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:177)
	at com.netflix.curator.framework.state.ConnectionStateManager.access$000(ConnectionStateManager.java:40)
	at com.netflix.curator.framework.state.ConnectionStateManager$1.call(ConnectionStateManager.java:104)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
...
{noformat}",[],Bug,ZOOKEEPER-1809,Minor,Shaun Senecal,Not A Problem,2013-11-07T03:54:43.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ephemeral node not deleted (or recreated?) after session expiry,2013-12-06T21:32:15.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.5.0', id='12316644'>]",4.0
Alexander Shraer,[],2013-11-01T23:32:47.000+0000,Raúl Gutiérrez Segalés,"Hey [~shralex],

I noticed today that my Observers are spamming each other trying to open connections to the election port. I've got tons of these:

{noformat}
2013-11-01 22:19:45,819 - DEBUG [WorkerSender[myid=13]] - There is a connection already for server 9
2013-11-01 22:19:45,819 - DEBUG [WorkerSender[myid=13]] - There is a connection already for server 10
2013-11-01 22:19:45,819 - DEBUG [WorkerSender[myid=13]] - There is a connection already for server 6
2013-11-01 22:19:45,819 - DEBUG [WorkerSender[myid=13]] - There is a connection already for server 12
2013-11-01 22:19:45,819 - DEBUG [WorkerSender[myid=13]] - There is a connection already for server 14
{noformat}

and so and so on ad nauseam. 

Now, looking around I found this inside FastLeaderElection.java from when you committed ZOOKEEPER-107:

{noformat}
     private void sendNotifications() {
-        for (QuorumServer server : self.getVotingView().values()) {
-            long sid = server.id;
-
+        for (long sid : self.getAllKnownServerIds()) {
+            QuorumVerifier qv = self.getQuorumVerifier();
{noformat}

Is that really desired? I suspect that is what's causing Observers to try to connect to each other (as opposed as just connecting to participants). I'll give it a try now and let you know. (Also, we use observer ids that are > 0, and I saw some parts of the code that might not deal with that assumption - so it could be that too..). ","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-1807,Blocker,Raúl Gutiérrez Segalés,Fixed,2018-05-10T20:01:29.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Observers spam each other creating connections to the election addr,2018-05-10T20:01:29.000+0000,[],11.0
Alexander Shraer,"[<JIRA Component: name='server', id='12312382'>]",2013-10-31T21:47:12.000+0000,Patrick D. Hunt,"testCurrentServersAreObserversInNextConfig failing frequently on trunk with jdk7

I see a number of failures recently on this test. Is it a real issue or flakey? Perhaps due to re-ordering/cleanup with jdk7 as we've seen with some other tests? (I don't see this test failing with jdk6)","[<JIRA Version: name='3.5.3', id='12335444'>]",Bug,ZOOKEEPER-1806,Major,Patrick D. Hunt,Duplicate,2016-08-10T17:04:33.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,testCurrentServersAreObserversInNextConfig failing frequently on trunk with non-jdk6,2019-12-19T23:01:52.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",3.0
Flavio Paiva Junqueira,[],2013-10-30T03:18:13.000+0000,Flavio Paiva Junqueira,This is an issue that has been originally reported in ZOOKEEPER-1732.,"[<JIRA Version: name='3.4.6', id='12323310'>]",Bug,ZOOKEEPER-1805,Blocker,Flavio Paiva Junqueira,Fixed,2014-03-13T18:25:33.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"""Don't care"" value in ZooKeeper election breaks rolling upgrades",2014-03-13T18:31:49.000+0000,[],9.0
Mohammad Arshad,"[<JIRA Component: name='documentation', id='12312422'>]",2013-10-26T11:50:47.000+0000,Leader Ni,"The Stat（org.apache.zookeeper.data.Stat） Structures has the filed pzxid, but no document about it in  programmer's guide（http://zookeeper.apache.org/doc/r3.4.3/zookeeperProgrammers.html#sc_zkStatStructure）","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-1803,Major,Leader Ni,Fixed,2015-09-25T06:52:20.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Add description for pzxid in programmer's guide.,2016-07-21T20:18:22.000+0000,[],6.0
Marshall McMullen,"[<JIRA Component: name='quorum', id='12312379'>]",2013-10-25T12:16:29.000+0000,Flavio Paiva Junqueira,"This is the message:

{noformat}
/home/jenkins/jenkins-slave/workspace/ZooKeeper-trunk/trunk/src/c/tests/TestReconfig.cc:183: Assertion: equality assertion failed [Expected: 1, Actual  : 0]
{noformat}

 https://builds.apache.org/job/ZooKeeper-trunk/2100/",[],Bug,ZOOKEEPER-1801,Major,Flavio Paiva Junqueira,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,TestReconfig failure,2022-02-03T08:50:14.000+0000,[],3.0
Thawan Kooburat,"[<JIRA Component: name='tests', id='12312427'>]",2013-10-24T17:04:26.000+0000,Patrick D. Hunt,"https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper-trunk-jdk7/691/testReport/junit/org.apache.zookeeper.test/GetProposalFromTxnTest/testGetProposalFromTxn/

test was introduced in ZOOKEEPER-1413, seems to have failed twice so far this month.
",[],Bug,ZOOKEEPER-1800,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,jenkins failure in testGetProposalFromTxn,2022-02-03T08:50:15.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",1.0
Jeffrey Zhong,"[<JIRA Component: name='tests', id='12312427'>]",2013-10-22T18:20:50.000+0000,Jeffrey Zhong,"org.apache.zookeeper.test.SaslAuthFailDesignatedClientTest.testAuth often fails on SUSE with the following error stack trace:
{code}
junit.framework.AssertionFailedError: expected [0x141ccb60d870000] expected:<1> but was:<0>
	at org.apache.zookeeper.test.JMXEnv.ensureAll(JMXEnv.java:115)
	at org.apache.zookeeper.test.ClientBase.createClient(ClientBase.java:200)
	at org.apache.zookeeper.test.ClientBase.createClient(ClientBase.java:174)
	at org.apache.zookeeper.test.ClientBase.createClient(ClientBase.java:159)
	at org.apache.zookeeper.test.ClientBase.createClient(ClientBase.java:152)
	at org.apache.zookeeper.test.SaslAuthFailDesignatedClientTest.testAuth(SaslAuthFailDesignatedClientTest.java:87)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
{code}

The reason is that this is a negative test. After authentication fails, the client connection is closed at the server side so does the session right before test case calls JMXEnv.ensureAll  to verify the session. Below are the log events show the sequence and you can see the session was already closed before client JMXEnv.ensureAll.

{code}
2013-10-18 10:56:25,320 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@595] - Established session 0x141ccb60d870000 with negotiated timeout 30000 for client /127.0.0.1:58272
2013-10-18 10:56:25,327 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11221:ZooKeeperServer@940] - Client failed to SASL authenticate: javax.security.sasl.SaslException: DIGEST-MD5: digest response format violation. Mismatched response.
2013-10-18 10:56:25,327 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11221:ZooKeeperServer@946] - Closing client connection due to SASL authentication failure.
2013-10-18 10:56:25,329 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11221:NIOServerCnxn@1001] - Closed socket connection for client /127.0.0.1:58272 which had sessionid 0x141ccb60d870000
....
2013-10-18 10:56:25,330 [myid:] - INFO  [main-SendThread(localhost:11221):ClientCnxn$SendThread@1089] - Unable to read additional data from server sessionid 0x141ccb60d870000, likely server has closed socket, closing socket connection and attempting reconnect
2013-10-18 10:56:25,332 [myid:] - INFO  [main:JMXEnv@105] - expect:0x141ccb60d870000
{code}","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1799,Minor,Jeffrey Zhong,Fixed,2013-10-22T23:17:10.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,SaslAuthFailDesignatedClientTest.testAuth fails frequently on SUSE,2014-03-13T18:17:09.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",4.0
Thawan Kooburat,[],2013-10-20T08:52:04.000+0000,Flavio Paiva Junqueira,"This is the output messges:

<noformat>
Testcase: testNormalObserverRun took 4.221 sec
        FAILED
expected:<data[2]> but was:<data[1]>
junit.framework.AssertionFailedError: expected:<data[2]> but was:<data[1]>
        at org.apache.zookeeper.server.quorum.Zab1_0Test$8.converseWithObserver(Zab1_0Test.java:1118)
        at org.apache.zookeeper.server.quorum.Zab1_0Test.testObserverConversation(Zab1_0Test.java:546)
        at org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalObserverRun(Zab1_0Test.java:994)
<noformat>","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1798,Blocker,Flavio Paiva Junqueira,Fixed,2013-11-14T04:47:23.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Fix race condition in testNormalObserverRun,2016-08-26T21:16:28.000+0000,[],4.0
Rakesh Radhakrishnan,"[<JIRA Component: name='server', id='12312382'>]",2013-10-18T18:57:47.000+0000,Derek Dagit,"org.apache.zookeeper.server.PurgeTxnLog deletes old data logs and snapshots, keeping the newest N snapshots and any data logs that have been written since the snapshot.

It does this by listing the available snapshots & logs and creates a blacklist of snapshots and logs that should not be deleted.  Then, it searches for and deletes all logs and snapshots that are not in this list.

It appears that if logs are rolling or a new snapshot is created during this process, then these newer files will be unintentionally deleted.","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1797,Blocker,Derek Dagit,Fixed,2014-05-18T01:05:03.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,PurgeTxnLog may delete data logs during roll,2016-09-11T00:06:58.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",8.0
Raúl Gutiérrez Segalés,"[<JIRA Component: name='c client', id='12312380'>]",2013-10-14T19:38:51.000+0000,Patrick D. Hunt,"Seems there is an issue for Ubuntu (I'm on 13.04), however I'm only seeing it on trunk and not branch 34

{noformat}
make check
make  zktest-st zktest-mt
make[1]: Entering directory `/home/phunt/dev/svn/svn-zookeeper/src/c'
g++ -DHAVE_CONFIG_H -I.  -I./include -I./tests -I./generated  -DUSE_STATIC_LIB -DZKSERVER_CMD=""\""./tests/zkServer.sh\"""" -DZOO_IPV6_ENABLED -g -O2 -MT zktest_st-TestReconfigServer.o -MD -MP -MF .deps/zktest_st-TestReconfigServer.Tpo -c -o zktest_st-TestReconfigServer.o `test -f 'tests/TestReconfigServer.cc' || echo './'`tests/TestReconfigServer.cc
tests/TestReconfigServer.cc: In member function 'bool TestReconfigServer::waitForConnected(zhandle_t*, uint32_t)':
tests/TestReconfigServer.cc:128:16: error: 'sleep' was not declared in this scope
make[1]: *** [zktest_st-TestReconfigServer.o] Error 1
make[1]: Leaving directory `/home/phunt/dev/svn/svn-zookeeper/src/c'
make: *** [check-am] Error 2
{noformat}

I have 

{noformat}
g++ --version
g++ (Ubuntu/Linaro 4.7.3-1ubuntu1) 4.7.3
Copyright (C) 2012 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
{noformat}","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1795,Blocker,Patrick D. Hunt,Fixed,2013-10-14T23:57:22.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,unable to build c client on ubuntu,2013-12-06T21:30:52.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>, <JIRA Component: name='tests', id='12312427'>]",2013-10-11T04:43:59.000+0000,Alexander Shraer,"not sure if this is due to a known issue or not.

// check and make sure the change is persisted
zkDb2 = new ZKDatabase(new FileTxnSnapLog(logDir, snapDir));
lastZxid = zkDb2.loadDataBase();
Assert.assertEquals(""data2"", new String(zkDb2.getData(""/foo"", stat, null)));

this assert periodically (once every 3 runs of the test or so) fails saying that  getData returns data1 and not data2.",[],Bug,ZOOKEEPER-1793,Major,Alexander Shraer,Duplicate,2013-10-24T18:54:28.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zab1_0Test.testNormalObserverRun() is flaky,2013-12-06T21:30:17.000+0000,[],4.0
Mahadev Konar,"[<JIRA Component: name='build', id='12312383'>]",2013-10-11T00:07:20.000+0000,Mahadev Konar,"ZooKeeper package includes unnecessary jars that are part of the package.

Packages like fatjar and 

{code}
maven-ant-tasks-2.1.3.jar
maven-artifact-2.2.1.jar
maven-artifact-manager-2.2.1.jar
maven-error-diagnostics-2.2.1.jar
maven-model-2.2.1.jar
maven-plugin-registry-2.2.1.jar
maven-profile-2.2.1.jar
maven-project-2.2.1.jar
maven-repository-metadata-2.2.1.jar
{code}

are part of the zookeeper package and rpm (via bigtop). ","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1791,Major,Mahadev Konar,Fixed,2014-05-18T02:42:39.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZooKeeper package includes unnecessary jars that are part of the package.,2014-05-20T11:09:12.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Alexander Shraer,"[<JIRA Component: name='server', id='12312382'>]",2013-10-10T23:59:13.000+0000,Alexander Shraer,"QuorumCnxManager.receiveConnection assumes that a negative sid means that this is a 3.5.0 server, which has a different communication protocol. This doesn't account for the fact that ObserverId = -1 is a special id that may be used by observers and is also negative. 

This requires a fix to trunk and a separate fix to 3.4 branch, where this function is different (see ZOOKEEPER-1633)","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1790,Major,Alexander Shraer,Not A Problem,2013-10-13T05:50:39.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Deal with special ObserverId in QuorumCnxManager.receiveConnection,2014-03-13T18:16:54.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",4.0
Alexander Shraer,[],2013-10-10T23:10:54.000+0000,Raúl Gutiérrez Segalés,"(assigning to Alex because this was introduced by ZOOKEEPER-107, but will upload a patch as well.)

I have a 5 participants cluster running what will be 3.5.0 (i.e.: trunk as of today) and an observer running 3.4 (trunk from 3.4 branch). When the observer tries to establish a connection to the participants I get:

{noformat}
Thread Thread[10.40.78.121:3888,5,main] died java.lang.NullPointerException at org.apache.zookeeper.server.quorum.QuorumCnxManager.receiveConnection(QuorumCnxManager.java:240)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$Listener.run(QuorumCnxManager.java:552)
{noformat}

Looking at QuorumCnxManager.java:240:

{noformat}
            if (protocolVersion >= 0) { // this is a server id and not a protocol version                                                             
               sid = protocolVersion;
                electionAddr = self.getVotingView().get(sid).electionAddr;
            } else {
{noformat}

and self.getVotingView().get(sid) will be null for Observers.  So this block should cover that case.  
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1789,Critical,Raúl Gutiérrez Segalés,Fixed,2014-07-23T17:36:53.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,3.4.x observer causes NPE on 3.5.0 (trunk) participants,2014-07-24T11:22:01.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",6.0
Raúl Gutiérrez Segalés,"[<JIRA Component: name='server', id='12312382'>]",2013-10-09T18:31:20.000+0000,Thawan Kooburat,"Currently, local session need to be enable by stopping the entire ensemble. If a rolling upgrade is used, all write request from a local session will fail with session move until the local session is enabled on the leader.",[],Bug,ZOOKEEPER-1787,Minor,Thawan Kooburat,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Add support for enabling local session in rolling upgrade,2014-08-23T15:43:36.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
Niraj Tolia,"[<JIRA Component: name='documentation', id='12312422'>]",2013-10-09T08:10:36.000+0000,Niraj Tolia,"When I look at https://zookeeper.apache.org/doc/trunk/zookeeperProgrammers.html#ch_zkDataModel, I see two things that seem wrong in terms of restricted characters:

* \uXFFFE - \uXFFFF (where X is a digit 1 - E)
* \uF0000 - \uFFFFF

These definitions are invalid characters in Java and aren't reflected in PathUtils either (or PathUtilsTest). In fact the code in PathUtils states:
{code:borderStyle=solid}
            } else if (c > '\u0000' && c <= '\u001f'
                    || c >= '\u007f' && c <= '\u009F'
                    || c >= '\ud800' && c <= '\uf8ff'
                    || c >= '\ufff0' && c <= '\uffff') {
                reason = ""invalid charater @"" + i;
                break;
            }
{code}

Unless I am missing something, this simple patch should fix the documentation problem:
{code}
Index: src/docs/src/documentation/content/xdocs/zookeeperProgrammers.xml
===================================================================
--- src/docs/src/documentation/content/xdocs/zookeeperProgrammers.xml	(revision 1530514)
+++ src/docs/src/documentation/content/xdocs/zookeeperProgrammers.xml	(working copy)
@@ -139,8 +139,7 @@

       <listitem>
         <para>The following characters are not allowed: \ud800 - uF8FF,
-        \uFFF0 - uFFFF, \uXFFFE - \uXFFFF (where X is a digit 1 - E), \uF0000 -
-        \uFFFFF.</para>
+        \uFFF0 - uFFFF.</para>
       </listitem>

       <listitem>
{code}","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1786,Minor,Niraj Tolia,Fixed,2013-11-15T18:07:18.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeper data model documentation is incorrect,2014-03-13T18:16:59.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",4.0
Alexander Shraer,"[<JIRA Component: name='scripts', id='12312384'>]",2013-10-09T00:22:29.000+0000,Alexander Shraer,"The problem can be reproduced by running a server with the following type of config file:

dataDir=/Users/shralex/zookeeper-test/zookeeper1
syncLimit=2
initLimit=5
tickTime=2000
server.1=localhost:2721:2731:participant;2791
server.2=localhost:2722:2732:participant;2792

and then trying to do ""zkServer.sh status""

Here I specified the servers using the new config format but still used the static config file and didn't include the ""clientPort"" key.

zkServer.sh already supports the new configuration format, but expects server spec to appear in the dynamic config file if it uses the new format.
So in the example above it will not find the client port. 

The current logic for executing something like 'zkServer.sh status'  is:

1. Look for clientPort keyword in the static config file
2. Look for the client port in the server spec in the dynamic config file

The attached patch adds an intermediate step:
1'. Look for the client port in the server spec in the static config file

","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1785,Minor,Alexander Shraer,Not A Problem,2014-03-29T22:48:43.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Small fix in zkServer.sh to support new configuration format,2014-03-29T22:48:43.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",2.0
Raúl Gutiérrez Segalés,[],2013-10-08T19:18:12.000+0000,Raúl Gutiérrez Segalés,"If you look at Learner#syncWithLeader:

{noformat}
            while (self.isRunning()) {
                readPacket(qp);
                switch(qp.getType()) {
.......

                case Leader.INFORM:
                case Leader.INFORMANDACTIVATE:
                    PacketInFlight packet = new PacketInFlight();
                    packet.hdr = new TxnHeader();

                    if (qp.getType() == Leader.COMMITANDACTIVATE) {
{noformat}

I guess ""qp.getType() == Leader.COMMITANDACTIVATE"" is a typo that should read ""qp.getType() == Leader.INFORMANDACTIVATE"".

Assigning to Alexander for now since this is part of ZOOKEEPER-107.
","[<JIRA Version: name='3.5.1', id='12326786'>]",Bug,ZOOKEEPER-1784,Major,Raúl Gutiérrez Segalés,Fixed,2015-01-25T00:23:16.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Logic to process INFORMANDACTIVATE packets in syncWithLeader seems bogus,2015-04-07T21:15:25.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",6.0
Alexander Shraer,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2013-10-07T18:01:56.000+0000,Alexander Shraer,"We need a way to distinguish an initial config of a server and an initial config of a running ensemble (before any reconfigs happen). Currently both have version 0. 

The version of a config increases with each reconfiguration, so the problem is just with the initial config.
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1783,Major,Alexander Shraer,Fixed,2013-11-07T17:07:53.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Distinguish initial configuration from first established configuration,2014-03-29T22:19:33.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Robert Joseph Evans,[],2013-10-04T18:24:07.000+0000,Robert Joseph Evans,"The zookeeper.superUser system property does not fully grant super user privileges, like zookeeper.DigestAuthenticationProvider.superDigest does.

zookeeper.superUser only has as many privileges as the sasl ACLs on the znode being accessed.  This means that if a znode only has digest ACLs zookeeper.superUser is ignored.  Or if a znode has a single sasl ACL that only has read privileges zookeeper.superUser only has read privileges.

The reason for this is that SASLAuthenticationProvider implements the superUser check in the matches method, instead of having the super user include a new Id(""super"","""") as Digest does.","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-1782,Major,Robert Joseph Evans,Fixed,2017-06-23T16:40:16.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zookeeper.superUser is not as super as superDigest,2017-06-23T17:45:43.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",7.0
Takashi Ohnishi,"[<JIRA Component: name='quorum', id='12312379'>]",2013-10-04T01:19:27.000+0000,Takashi Ohnishi,"If snapCount is set to 1, ZooKeeper Server can start but it fails with the below error:

2013-10-02 18:09:07,600 [myid:1] - ERROR [SyncThread:1:SyncRequestProcessor@151] - Severe unrecoverable error, exiting
java.lang.IllegalArgumentException: n must be positive
        at java.util.Random.nextInt(Random.java:300)
        at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:93)

In source code,  it maybe be supposed that snapCount must be 2 or more:
{code:title=org.apache.zookeeper.server.SyncRequestProcessor.java|borderStyle=solid}
     91             // we do this in an attempt to ensure that not all ofthe servers
     92             // in the ensemble take a snapshot at the same time
     93             int randRoll = r.nextInt(snapCount/2);
{code}

I think this supposition is not bad because snapCount = 1 is not realistic setting...
But, it may be better to mention this restriction in documentation or add a validation in the source code.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1781,Minor,Takashi Ohnishi,Fixed,2013-10-07T23:38:49.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeper Server fails if snapCount is set to 1 ,2014-03-13T18:17:07.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",5.0
Abhiraj Butala,"[<JIRA Component: name='tests', id='12312427'>]",2013-10-02T18:42:57.000+0000,Patrick D. Hunt,"After running the ReconfigTest I saw a number of the following files in the source root (not in a subdir of the build directory as would be expected)

zoo_replicated1.dynamic
(saw files zoo_replicated{1-9})","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1779,Critical,Patrick D. Hunt,Fixed,2014-03-05T21:55:06.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ReconfigTest littering the source root with test files,2014-03-06T11:09:22.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Germán Blanco,"[<JIRA Component: name='quorum', id='12312379'>]",2013-10-02T10:09:42.000+0000,Germán Blanco,"In a 3-servers ensemble, one of the followers doesn't see part of the ephemeral nodes that are present in the leader and the other follower. 
The 8 missing nodes in ""the follower that is not ok"" were created in the end of epoch 1, the ensemble is running in epoch 2.",[],Bug,ZOOKEEPER-1777,Major,Germán Blanco,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Missing ephemeral nodes in one of the members of the ensemble,2022-02-03T08:50:21.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",7.0
Germán Blanco,"[<JIRA Component: name='quorum', id='12312379'>]",2013-10-02T10:07:01.000+0000,Germán Blanco,"In a 3-servers ensemble, one of the followers doesn't see part of the ephemeral nodes that are present in the leader and the other follower. 
The 8 missing nodes in ""the follower that is not ok"" were created in the end of epoch 1, the ensemble is running in epoch 2.
 ","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1776,Major,Germán Blanco,Invalid,2013-10-02T10:51:30.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Ephemeral nodes not present in one of the members of the ensemble,2014-03-13T18:17:15.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",2.0
Germán Blanco,"[<JIRA Component: name='quorum', id='12312379'>]",2013-10-02T10:05:17.000+0000,Germán Blanco,"In a 3-servers ensemble, one of the followers doesn't see part of the ephemeral nodes that are present in the leader and the other follower. 
The 8 missing nodes in ""the follower that is not ok"" were created in the end of epoch 1, the ensemble is running in epoch 2.
 ","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1775,Major,Germán Blanco,Invalid,2013-10-02T10:52:00.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Ephemeral nodes not present in one of the members of the ensemble,2014-03-13T18:17:12.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='tests', id='12312427'>]",2013-10-01T23:14:39.000+0000,Patrick D. Hunt,"QuorumPeerMainTest fails consistently with ""complains about host"" assertion failure.


{noformat}
2013-10-01 16:09:17,962 [myid:] - INFO  [main:JUnit4ZKTestRunner$LoggedInvokeMethod@54] - TEST METHOD FAILED testBadPeerAddressInQuorum
java.lang.AssertionError: complains about host
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testBadPeerAddressInQuorum(QuorumPeerMainTest.java:434)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:518)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1052)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:906)
2013-10-01 16:09:17,963 [myid:] - INFO  [main:ZKTestCase$1@65] - FAILED testBadPeerAddressInQuorum
java.lang.AssertionError: complains about host
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testBadPeerAddressInQuorum(QuorumPeerMainTest.java:434)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:518)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1052)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:906)
{noformat}","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1774,Blocker,Patrick D. Hunt,Fixed,2013-10-08T05:28:27.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"QuorumPeerMainTest fails consistently with ""complains about host"" assertion failure",2014-03-13T18:17:00.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",6.0
Manikumar,"[<JIRA Component: name='documentation', id='12312422'>]",2013-10-01T21:20:56.000+0000,Patrick D. Hunt,"The docs refer to an old version of jline

{noformat}
src/docs/src/documentation/content/xdocs/zookeeperAdmin.xml
227:              <para><computeroutput>$ java -cp zookeeper.jar:lib/slf4j-api-1.6.1.jar:lib/slf4j-log4j12-1.6.1.jar:lib/log4j-1.2.15.jar:conf:src/java/lib/jline-0.9.94.jar \

src/docs/src/documentation/content/xdocs/zookeeperQuotas.xml
46:	<para><computeroutput>$java -cp zookeeper.jar:src/java/lib/log4j-1.2.15.jar/conf:src/java/lib/jline-0.9.94.jar \
{noformat}
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1773,Major,Patrick D. Hunt,Fixed,2013-10-03T16:57:32.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,incorrect reference to jline version/lib in docs,2013-10-04T11:07:31.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",3.0
,"[<JIRA Component: name='c client', id='12312380'>]",2013-10-01T13:43:13.000+0000,Chris Hall,"Was this intentional?  If so, why?",[],Bug,ZOOKEEPER-1772,Major,Chris Hall,Implemented,2013-10-01T21:57:11.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,config.guess downgraded from timestamp='2012-02-10' in 3.3.x to timestamp='2005-07-08' in 3.4.x,2013-10-01T21:57:11.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",1.0
Germán Blanco,[],2013-09-30T08:00:45.000+0000,Germán Blanco,"SnapshotFormatter fails with a NullPointerException when parsing one snapshot (with ""null"" data in one Znode):
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.zookeeper.server.SnapshotFormatter.printZnode(SnapshotFormatter.java:90)
        at org.apache.zookeeper.server.SnapshotFormatter.printZnode(SnapshotFormatter.java:95)
        at org.apache.zookeeper.server.SnapshotFormatter.printZnode(SnapshotFormatter.java:95)
        at org.apache.zookeeper.server.SnapshotFormatter.printZnode(SnapshotFormatter.java:95)
        at org.apache.zookeeper.server.SnapshotFormatter.printZnodeDetails(SnapshotFormatter.java:79)
        at org.apache.zookeeper.server.SnapshotFormatter.printDetails(SnapshotFormatter.java:71)
        at org.apache.zookeeper.server.SnapshotFormatter.run(SnapshotFormatter.java:67)
        at org.apache.zookeeper.server.SnapshotFormatter.main(SnapshotFormatter.java:51)
","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1770,Minor,Germán Blanco,Fixed,2013-10-01T23:46:31.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,NullPointerException in SnapshotFormatter,2014-03-13T18:16:52.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",4.0
Benjamin Jaton,"[<JIRA Component: name='contrib', id='12312700'>]",2013-09-29T17:12:17.000+0000,Benjamin Jaton,"There seem to be a few bugs in the trunk that prevent ZooInspector to load the node viewers ( the 3 tabs in the main windows when you select a ZK node in the tree don't show up any more ).
Apparently it has been introduced 2 years ago after a refactoring about icons and another about partially fixing a typo (""veiwer"" -> ""viewer"").

Note: the bug is only in trunk, 3.4 is fine.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1769,Minor,Benjamin Jaton,Fixed,2013-10-01T05:56:53.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZooInspector can't display node data/metadata/ACLs,2013-10-01T11:11:23.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Flavio Paiva Junqueira,"[<JIRA Component: name='leaderElection', id='12312378'>]",2013-09-27T09:19:10.000+0000,yuxin.yan,"Hi, 
I have a five nodes cluster versioned 3.4.5 and now i find one node is offline.
Firstly i restart the node but i find that ""Error contacting service. It is probably not running."" and i find that the node always elect the leader and always sync the snapshot logs and the device will be full every ten mins. 
so could someone help me？ i will put the log and zoo.cfg in the attachment.
Thanks all.
yyx,",[],Bug,ZOOKEEPER-1768,Major,yuxin.yan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Cluster fails election loop until the device is full,2022-02-03T08:50:25.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2013-09-25T22:21:01.000+0000,Flavio Paiva Junqueira,,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1765,Trivial,Flavio Paiva Junqueira,Fixed,2013-10-02T00:03:29.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Update code conventions link on ""How to contribute"" page",2014-03-13T18:17:12.000+0000,[],2.0
,"[<JIRA Component: name='java client', id='12312381'>]",2013-09-25T09:51:41.000+0000,Kuba Skopal,"We are using a proprietary SASL solution, but we don't want to use it with ZooKeeper. Unfortunately it seems, that there is no way to disable SASL for ZooKeeper as the code only checks for presence of ""java.security.auth.login.config"" system property to determine whether SASL should be used or not.
For us it means, that ZooKeeper client just shuts down after SASL is initialized. What happens:

1) System.getProperty(""java.security.auth.login.config"") is initially null
2) ZooKeeper is initialized and used
3) Our JAAS/SASL component is initialized
4) System.getProperty(""java.security.auth.login.config"") is not null anymore
5) ZooKeeperSaslClient.clientTunneledAuthenticationInProgress() suddenly picks up the new property and starts returning true
6) ClientCnxnSocketNIO.findSendablePacket() suddenly stops returning any packets since clientTunneledAuthenticationInProgress is always true

The communication is halted and eventually times out.
","[<JIRA Version: name='3.4.6', id='12323310'>]",Bug,ZOOKEEPER-1764,Major,Kuba Skopal,Implemented,2013-09-25T17:05:30.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeper attempts at SASL eventhough it shouldn't,2014-03-13T18:17:09.000+0000,"[<JIRA Version: name='3.4.4', id='12319841'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2013-09-12T23:57:15.000+0000,Thawan Kooburat,"I was investigating data inconsistency bug in our internal branch. One possible area is snapshot/txnlog corruption. So I wrote a more robust corruption test and found that it is easy to break our checksum algorithm which is Adler32.

When this happen, it is more likely that corrupted data will fail other sanity check during deserialization phase, but it is still scary that it can pass the checksum.",[],Bug,ZOOKEEPER-1757,Minor,Thawan Kooburat,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Adler32 may not be sufficient to protect against data corruption,2013-09-14T02:04:25.000+0000,[],5.0
Eric Lindvall,"[<JIRA Component: name='c client', id='12312380'>]",2013-09-08T04:45:23.000+0000,Eric Lindvall,"If the client is connected to a zookeeper server that has hung while there is an outstanding request, zookeeper_interest() can return a timeval of 0 because send_to will be negative.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1756,Major,Eric Lindvall,Fixed,2013-12-16T21:50:34.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper_interest() in C client can return a timeval of 0,2014-03-13T18:17:06.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.5', id='12321883'>]",5.0
Rakesh Radhakrishnan,"[<JIRA Component: name='server', id='12312382'>]",2013-09-07T18:12:11.000+0000,Rakesh Radhakrishnan,"Potential problem occurs, when executing four letter 'dump' command and at the meantime zkserver has triggered session closure and removing the related information from the DataTree.

Please see the exception:
{code}
java.lang.NullPointerException
	at org.apache.zookeeper.server.DataTree.dumpEphemerals(DataTree.java:1278)
	at org.apache.zookeeper.server.DataTreeTest$1.run(DataTreeTest.java:82)
{code}","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1755,Major,Rakesh Radhakrishnan,Fixed,2014-02-19T01:38:55.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Concurrent operations of four letter 'dump' ephemeral command and killSession causing NPE,2014-03-13T18:17:02.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",9.0
Rakesh Radhakrishnan,"[<JIRA Component: name='server', id='12312382'>]",2013-09-06T15:42:27.000+0000,Rakesh Radhakrishnan,,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1754,Critical,Rakesh Radhakrishnan,Fixed,2013-09-17T23:14:25.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Read-only server allows to create znode,2014-03-13T18:17:10.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",7.0
Rakesh Radhakrishnan,"[<JIRA Component: name='java client', id='12312381'>]",2013-09-05T14:37:15.000+0000,Rakesh Radhakrishnan,"While pinging to the RwServer, ClientCnxn is opening a socket and using BufferedReader. These are not properly closed in finally block and could cause leaks on exceptional cases.

ClientCnxn#pingRwServer()
{code}
            try {
                Socket sock = new Socket(addr.getHostName(), addr.getPort());
                BufferedReader br = new BufferedReader(
                        new InputStreamReader(sock.getInputStream()));
                ......
                sock.close();
                br.close();
            } catch (ConnectException e) {
                // ignore, this just means server is not up
            } catch (IOException e) {
                // some unexpected error, warn about it
                LOG.warn(""Exception while seeking for r/w server "" +
                        e.getMessage(), e);
            }
{code}","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1753,Major,Rakesh Radhakrishnan,Fixed,2013-09-18T13:15:20.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"ClientCnxn is not properly releasing the resources, which are used to ping RwServer",2014-03-13T18:17:00.000+0000,[],7.0
Patrick D. Hunt,[],2013-09-01T23:55:01.000+0000,Sebb,"The download area under http://www.apache.org/dist/zookeeper/ contains several superseded releases.

It is important that only the latest release of each currently maintained product line is stored on the main ASF mirrors. Links to older releases can be provided on download pages, but the links should be to the ASF archive server http://archive.apache.org/dist/zookeeper/

See http://www.apache.org/dev/release.html#when-to-archive",[],Bug,ZOOKEEPER-1752,Major,Sebb,Fixed,2019-03-03T17:50:10.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Download area contains obsolete releases,2019-03-03T23:33:58.000+0000,[],2.0
Jeffrey Zhong,[],2013-08-31T00:56:14.000+0000,Jeffrey Zhong,"We could throw SessionTimeoutException exception even when timeToNextPing may also be negative depending on the time when the following line is executed by the thread because we check time out before sending a ping.
{code}
  to = readTimeout - clientCnxnSocket.getIdleRecv();
{code}

In addition, we only ping twice no matter how long the session time out value is. For example, we set session time out = 60mins then we only try ping twice in 40mins window. Therefore, the connection could be dropped by OS after idle time out.

The issue is causing randomly ""connection loss"" or ""session expired"" issues in client side which is bad for applications like HBase.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1751,Major,Jeffrey Zhong,Fixed,2013-09-18T02:08:52.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ClientCnxn#run could miss the second ping or connection get dropped before a ping,2014-03-13T18:17:06.000+0000,"[<JIRA Version: name='3.3.5', id='12319081'>, <JIRA Version: name='3.4.5', id='12321883'>]",7.0
Rakesh Radhakrishnan,"[<JIRA Component: name='server', id='12312382'>]",2013-08-30T22:57:04.000+0000,Helen Hastings," The socket is closed and the variable ""sock"" is set to null for normal reasons, but the toString method is called before ""sock"" can be set again, producing a NullPointerException.

Stack trace: 

2013-08-29 01:49:19,991 6277 [CommitProcWorkThread-3] WARN org.apache.zookeeper.server.WorkerService  - Unexpected exception
java.lang.NullPointerException
    at org.apach.zookeeper.server.NIOServerCnxn.toString(NIOServerCnxn.java:961)
    at java.lang.String.valueOf(String.java:2854)
    at java.lang.StringBuilder.append(StringBuilder.java:128)
    at org.apache.zookeeper.server.NIOServerCnxn.process(NIOServerCnxn.java:1104)
    at org.apache.zookeeper.server.WatchManager.triggerWatch(WatchManager.java:120)
    at org.apache.zookeeper.server.WatchManager.triggerWatch(WatchManager.java:92)
    at org.apache.zookeeper.server.DataTree.createNode(DataTree.java:544)
    at org.apache.zookeeper.server.DataTree.processTxn(DataTree.java:805)
    at org.apache.zookeeper.server.ZKDatabase.processTxn(ZKDatabase.java:319)
    at org.apache.zookeeper.server.ZooKeeperServer.processTxn(ZooKeeperServer.java:967)
    at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:115)
    at org.apache.zookeeper.server.quorum.Leader$ToBeAppliedRequestProcessor.processRequest(Leader.java:859)
    at org.apache.zookeeper.server.quorum.CommitProcessor$CommitWorkRequest.doWork(CommitProcessor.java:271)
    at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:152)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1750,Minor,Helen Hastings,Fixed,2013-10-04T23:16:49.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Race condition producing NPE in NIOServerCnxn.toString,2014-03-13T18:16:54.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",9.0
,"[<JIRA Component: name='server', id='12312382'>]",2013-08-29T14:04:29.000+0000,Sergey Maslyakov,"On multiple occasions when ZK was not able to write out a transaction log or a snapshot file, the consequent attempt to restart the server fails. Usually it happens when the underlying file system filled up; thus, preventing ZK server from writing out consistent data file.

Upon start-up, the server reads in the snapshot and the transaction log. If the deserializer fails and throws an exception, server terminates. Please see the stack trace below.

Server not coming up for whatever reason is often an undesirable condition. It would be nice to have an option to force-ignore parsing errors, especially, in the transaction log. A check sum on the data could be a possible solution to ensure the integrity and ""parsability"".

Another robustness enhancement could be via proper handling of the condition when snapshot or transaction log cannot be completely written to disk. Basically, better handling of write errors.


{noformat}
2013-08-28 12:05:30,732 ERROR [ZooKeeperServerMain] Unexpected exception, exiting abnormally
java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:375)
        at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
        at org.apache.zookeeper.server.persistence.FileHeader.deserialize(FileHeader.java:64)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.inStreamCreated(FileTxnLog.java:558)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.createInputArchive(FileTxnLog.java:577)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.goToNextLog(FileTxnLog.java:543)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:625)
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:160)
        at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:223)
        at org.apache.zookeeper.server.ZooKeeperServer.loadData(ZooKeeperServer.java:250)
        at org.apache.zookeeper.server.ZooKeeperServer.startdata(ZooKeeperServer.java:383)
        at org.apache.zookeeper.server.NIOServerCnxnFactory.startup(NIOServerCnxnFactory.java:122)
        at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:112)
        at org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:86)
        at org.apache.zookeeper.server.ZooKeeperServerMain.main(ZooKeeperServerMain.java:52)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:129)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)
{noformat}",[],Bug,ZOOKEEPER-1747,Major,Sergey Maslyakov,Duplicate,2013-09-12T15:05:18.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper server fails to start if transaction log file is corrupted,2013-09-12T15:06:54.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",2.0
Jean-Baptiste Onofré,"[<JIRA Component: name='server', id='12312382'>]",2013-08-26T09:25:08.000+0000,Xilai Dai,"Import-Package: javax.management,org.apache.log4j,org.osgi.framework;v
 ersion=""[1.4,2.0)"",org.osgi.util.tracker;version=""[1.1,2.0)""

the ""org.apache.log4j"" should be replaced by ""org.slf4j"", because from the source codes, zookeeper server classes import org.slf4j.* for logging.

currently will get: 

Caused by: java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory
	at org.apache.zookeeper.server.quorum.QuorumPeerConfig.<clinit>(QuorumPeerConfig.java:46)

when try to create instance for some of its classes in OSGi container (e.g. apache karaf)","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1745,Major,Xilai Dai,Fixed,2014-04-25T01:58:02.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Wrong Import-Package in the META-INF/MANIFEST.MF of zookeeper 3.4.5 bundle,2014-04-25T01:58:02.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",6.0
Nick Ohanian,"[<JIRA Component: name='scripts', id='12312384'>]",2013-08-22T20:23:56.000+0000,Nick Ohanian,"When ""clientPortAddress"" is used in the config file (zoo.cfg), zkServer.sh's status command runs a grep command that matches both ""clientPort"" and ""clientPortAddress"".  This creates an extra argument for FourLetterWordMain, which fails, so the status command incorrectly indicates that it couldn't connect to the server.

Also, ""localhost"" is hardcoded as the target host for FourLetterWordMain.  The ""clientPortAddress"" should be used if it is provided in the config file.
","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1744,Critical,Nick Ohanian,Fixed,2013-10-24T05:12:36.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"clientPortAddress breaks ""zkServer.sh status"" ",2014-03-13T18:17:10.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",6.0
Michael Han,[],2013-08-21T15:51:23.000+0000,Flavio Paiva Junqueira,"There are two problems I have spotted when running ""make check"" with the C client. First, it complains that the sleep call is not defined in two test files: tests/ZooKeeperQuorumServer.cc and tests/TestReconfigServer.cc. Including unistd.h works. The second problem is with linker options. It complains that ""--wrap"" is not a valid. I'm not sure how to deal with this one yet, since I'm not sure why we are using it.  ",[],Bug,ZOOKEEPER-1742,Major,Flavio Paiva Junqueira,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"""make check"" doesn't work on macos",2022-02-03T08:50:19.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.5.0', id='12316644'>]",6.0
Max Lapan,"[<JIRA Component: name='scripts', id='12312384'>]",2013-08-16T12:46:01.000+0000,Max Lapan,"Symlinks on bin scripts are not dereferenced correctly (""set -x"" added):
{noformat}
[root@tsthdp1 noarch]# which zookeeper-client
/usr/local/bin/zookeeper-client
[root@tsthdp1 noarch]# ls -la /usr/local/bin/zookeeper-client
lrwxrwxrwx 1 root root 40 Авг 16 15:56 /usr/local/bin/zookeeper-client -> /usr/local/hadoop/zookeeper/bin/zkCli.sh
[root@tsthdp1 noarch]# ls -la /usr/local/hadoop/zookeeper/bin
итого 36
drwxr-xr-x 2 root root 4096 Авг 16 16:24 .
drwxr-xr-x 5 root root 4096 Авг 16 15:56 ..
-rwxr-xr-x 1 root root 1909 Авг 16 15:56 zkCleanup.sh
-rwxr-xr-x 1 root root 1536 Авг 16 16:22 zkCli.sh
-rwxr-xr-x 1 root root 2599 Авг 16 15:56 zkEnv.sh
-rwxr-xr-x 1 root root 4559 Авг 16 15:56 zkServer-initialize.sh
-rwxr-xr-x 1 root root 6246 Авг 16 15:56 zkServer.sh
[root@tsthdp1 noarch]# zookeeper-client
+ ZOOBIN=/usr/local/bin/zookeeper-client
++ dirname /usr/local/bin/zookeeper-client
+ ZOOBIN=/usr/local/bin
++ cd /usr/local/bin
++ pwd
+ ZOOBINDIR=/usr/local/bin
+ '[' -e /usr/local/bin/../libexec/zkEnv.sh ']'
+ . /usr/local/bin/zkEnv.sh
/usr/local/bin/zookeeper-client: line 37: /usr/local/bin/zkEnv.sh: no such file or directory
{noformat}",[],Bug,ZOOKEEPER-1741,Trivial,Max Lapan,Duplicate,2013-10-02T17:02:18.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,bin scripts don't dereference symlinks,2013-10-02T17:02:37.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='server', id='12312382'>]",2013-08-15T18:18:53.000+0000,Neha Narkhede,"The current behavior of zookeeper for ephemeral nodes is that session expiration and ephemeral node deletion is not an atomic operation. 

The side-effect of the above zookeeper behavior in Kafka, for certain corner cases, is that ephemeral nodes can be lost even if the session is not expired. The sequence of events that can lead to lossy ephemeral nodes is as follows - 

1. The session expires on the client, it assumes the ephemeral nodes are deleted, so it establishes a new session with zookeeper and tries to re-create the ephemeral nodes. 
2. However, when it tries to re-create the ephemeral node,zookeeper throws back a NodeExists error code. Now this is legitimate during a session disconnect event (since zkclient automatically retries the 
operation and raises a NodeExists error). Also by design, Kafka server doesn't have multiple zookeeper clients create the same ephemeral node, so Kafka server assumes the NodeExists is normal. 
3. However, after a few seconds zookeeper deletes that ephemeral node. So from the client's perspective, even though the client has a new valid session, its ephemeral node is gone. 

This behavior is triggered due to very long fsync operations on the zookeeper leader. When the leader wakes up from such a long fsync operation, it has several sessions to expire. And the time between the session expiration and the ephemeral node deletion is magnified. Between these 2 operations, a zookeeper client can issue a ephemeral node creation operation, that could've appeared to have succeeded, but the leader later deletes the ephemeral node leading to permanent ephemeral node loss from the client's perspective. 

Thread from zookeeper mailing list: http://zookeeper.markmail.org/search/?q=Zookeeper+3.3.4#query:Zookeeper%203.3.4%20date%3A201307%20+page:1+mid:zma242a2qgp6gxvx+state:results

The way to reproduce this behavior is as follows -

1. Bring up a zookeeper 3.3.4 cluster and create several sessions with ephemeral ndoes on it using zkclient. Make sure the session expiration callback is implemented and it re-registers the ephemeral node.
2. Run the following script on the zookeeper leader -
while true
 do
   kill -STOP $1
   sleep 8
   kill -CONT $1
   sleep 60
 done
3. Run another script to check for existence of ephemeral nodes.

This script shows that zookeeper loses the ephemeral nodes and the clients still have a valid session.

",[],Bug,ZOOKEEPER-1740,Critical,Neha Narkhede,Fixed,2016-02-07T04:15:34.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper 3.3.4 loses ephemeral nodes under stress,2016-02-07T04:15:34.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>]",10.0
qingjie qiao,"[<JIRA Component: name='leaderElection', id='12312378'>]",2013-08-09T04:29:19.000+0000,qingjie qiao,"I am reading the trunk source code recently and find a thread-safe problem, but i'm not quite sure.

in FastLeaderElection:

{code}
class WorkerSender implements Runnable { 
    volatile boolean stop; 
    QuorumCnxManager manager; 

    WorkerSender(QuorumCnxManager manager){ 
        this.stop = false; 
        this.manager = manager; 
    } 

    public void run() {
 ...
    }
}

...

Messenger(QuorumCnxManager manager) {

    this.ws = new WorkerSender(manager);

    Thread t = new Thread(this.ws,
            ""WorkerSender[myid="" + self.getId() + ""]"");
    t.setDaemon(true);
    t.start();

    this.wr = new WorkerReceiver(manager);

    t = new Thread(this.wr,
            ""WorkerReceiver[myid="" + self.getId() + ""]"");
    t.setDaemon(true);
    t.start();
}
...
{code}

The instance of WorkerSender is constructed in main thread, and its field manager is assigned , and it is used in another thread. The later thread may see that WorkerSender.manager is the default value null. The solution may be:
(1) change
{code} 
WorkerSender(QuorumCnxManager manager){ 
        this.stop = false; 
        this.manager = manager; 
} 
{code}

to 

{code}
WorkerSender(QuorumCnxManager manager){ 
	this.manager = manager; 
	this.stop = false; 
} 
{code}

or(2)
change 

{code}
QuorumCnxManager manager; 
{code}

to 

{code}
final QuorumCnxManager manager;
{code}",[],Bug,ZOOKEEPER-1739,Minor,qingjie qiao,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"thread safe bug in FastLeaderElection: instance of WorkerSender is not safe published, WorkerSender thread may see that WorkerSender.manager is the default value null",2013-08-11T03:01:38.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",4.0
,[],2013-08-07T14:43:38.000+0000,Vincent Bernat,"This happens in the context of HBase master nodes getting connections from HBase region server. Once an HBase region server joins the cluster, I get the following error:

{code}
2013-08-07 13:35:18,676 WARN org.apache.zookeeper.ClientCnxn: Session 0xd4058c4d7940003 for server zk-01.dev.dailymotion.com/10.194.60.13:2181, unexpected error, closing socket connection and attempting reconnect
java.io.IOException: Xid out of order. Got Xid 56 with err -101 expected Xid 55 for a packet with details: clientPath:null serverPath:null finished:false header:: 55,14  replyHeader:: 0,0,-4  request:: org.apache.zookeeper.MultiTransactionRecord@360193e5 response:: org.apache.zookeeper.MultiResponse@0
        at org.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:795)
        at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:94)
        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:355)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068)
2013-08-07 13:35:18,676 WARN org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper exception: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
2013-08-07 13:35:18,676 ERROR org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: ZooKeeper multi failed after 3 retries
2013-08-07 13:35:18,677 ERROR org.apache.hadoop.hbase.master.AssignmentManager: Unable to ensure that the table -ROOT- will be enabled because of a ZooKeeper issue
2013-08-07 13:35:18,677 FATAL org.apache.hadoop.hbase.master.HMaster: Master server abort: loaded coprocessors are: []
2013-08-07 13:35:18,677 FATAL org.apache.hadoop.hbase.master.HMaster: Unable to ensure that the table -ROOT- will be enabled because of a ZooKeeper issue
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
        at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:931)
        at org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:911)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.multi(RecoverableZooKeeper.java:531)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.multiOrSequential(ZKUtil.java:1440)
        at org.apache.hadoop.hbase.zookeeper.ZKTable.setTableState(ZKTable.java:245)
        at org.apache.hadoop.hbase.zookeeper.ZKTable.setEnabledTable(ZKTable.java:325)
        at org.apache.hadoop.hbase.master.AssignmentManager.setEnabledTable(AssignmentManager.java:3576)
        at org.apache.hadoop.hbase.master.AssignmentManager.setEnabledTable(AssignmentManager.java:2340)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1674)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1424)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1399)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1394)
        at org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.process(ClosedRegionHandler.java:105)
        at org.apache.hadoop.hbase.master.AssignmentManager.addToRITandCallClose(AssignmentManager.java:675)
        at org.apache.hadoop.hbase.master.AssignmentManager.processRegionsInTransition(AssignmentManager.java:586)
        at org.apache.hadoop.hbase.master.AssignmentManager.processRegionInTransition(AssignmentManager.java:525)
        at org.apache.hadoop.hbase.master.AssignmentManager.processRegionInTransitionAndBlockUntilAssigned(AssignmentManager.java:489)
        at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:679)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:583)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:395)
        at java.lang.Thread.run(Thread.java:722)
2013-08-07 13:35:18,678 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
2013-08-07 13:35:18,678 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Server stopped; skipping assign of -ROOT-,,0.70236052 state=OFFLINE, ts=1375881792131, server=null
2013-08-07 13:35:18,678 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Waiting on 70236052/-ROOT-
2013-08-07 13:35:18,678 INFO org.apache.hadoop.hbase.master.AssignmentManager$TimeoutMonitor: masternode-01.dev.dailymotion.com,60000,1375880747185.timeoutMonitor exiting
2013-08-07 13:35:18,679 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=M_ZK_REGION_OFFLINE, server=masternode-01.dev.dailymotion.com,60000,1375880747185, region=70236052/-ROOT-, which is more than 15 seconds late
2013-08-07 13:35:18,776 WARN org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper exception: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/root-region-server
2013-08-07 13:35:18,776 WARN org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper exception: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase
2013-08-07 13:35:18,776 ERROR org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: ZooKeeper getData failed after 3 retries
2013-08-07 13:35:18,777 INFO org.apache.hadoop.hbase.util.RetryCounter: Sleeping 2000ms before retry #1...
{code}",[],Bug,ZOOKEEPER-1738,Major,Vincent Bernat,Invalid,2013-10-24T05:41:20.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Xid out of order from a 3.4.5 client to a 3.3.5 cluster,2013-10-24T05:41:20.000+0000,"[<JIRA Version: name='3.3.5', id='12319081'>]",1.0
Chris Seawood,"[<JIRA Component: name='scripts', id='12312384'>]",2013-08-05T19:12:28.000+0000,Chris Seawood,"At some point since 3.3, the shell scripts were updated to move away from using readlink to using BASH_SOURCE.  The problem is that BASH_SOURCE doesn't resolve symlinks so when /usr/bin/zookeeper-cli is symlinked to /usr/lib/zookeeper/bin/zkCli.sh it fails every single time.

",[],Bug,ZOOKEEPER-1737,Major,Chris Seawood,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,zk scripts no longer work when symlinked,2013-10-01T00:14:52.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2013-07-26T08:40:39.000+0000,AntonioS,"Hello.
I have configured Zookeeper to provide SASL authentication, using ordinary username and password stored in the JAAS.conf as a DigestLoginModule
I have created a simple jaas.conf file:

Server {
    org.apache.zookeeper.server.auth.DigestLoginModule required
    user_admin=""admin"";
};
Client {
    org.apache.zookeeper.server.auth.DigestLoginModule required
    username=""admin""
    password=""admin"";
};

I have the zoo.cfg correctly configured for security, adding the following:
requireClientAuthScheme=sasl
authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider
jaasLoginRenew=3600000
zookeeper.allowSaslFailedClients=false

And I also have the java.env file:
export JVMFLAGS=""-Djava.security.auth.login.config=/etc/zookeeper/conf/jaas.conf -Dzookeeper.allowSaslFailedClients=false""


Everything looks good. If I put the right username and password I authenticate, otherwise not and I get an exception.
The problem is when I don’t put any username and password at all, zookeeper allows me to go through.
I tried different things but nothing stops anonymous users to log in.
I was looking at the source code,  in particular the  ZookeeperServer.java, this method:

    public void processPacket(ServerCnxn cnxn, ByteBuffer incomingBuffer) throws IOException {

The section below:

} else {
            if (h.getType() == OpCode.sasl) {
                Record rsp = processSasl(incomingBuffer,cnxn);
                ReplyHeader rh = new ReplyHeader(h.getXid(), 0, KeeperException.Code.OK.intValue());
                cnxn.sendResponse(rh,rsp, ""response""); // not sure about 3rd arg..what is it?
            }
            else {
                Request si = new Request(cnxn, cnxn.getSessionId(), h.getXid(),
                  h.getType(), incomingBuffer, cnxn.getAuthInfo());
                si.setOwner(ServerCnxn.me);
                submitRequest(si);
            }
        }

The else flow  appears to just forward any anonymous request  to the handler, without attempting any authentication.

Is this a bug? Is there any way to stop anonymous users connecting to Zookeeper?
Thanks

Antonio


",[],Bug,ZOOKEEPER-1736,Major,AntonioS,Not A Problem,2013-10-10T17:43:55.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper SASL authentication allows anonymus users to log in,2019-03-19T12:58:19.000+0000,[],7.0
,"[<JIRA Component: name='java client', id='12312381'>]",2013-07-22T20:28:52.000+0000,Andy Grove,"We use Amazon Elastic IP for zookeeper hosts so that the zookeeper hosts have the same IP address after a restart.

The issue is, if one host is down then we cannot connect to the other hosts.

Here is an example connect string:

""ec2-1-2-3-4.compute-1.amazonaws.com, ec2-4-3-2-1.compute-1.amazonaws.com, ec2-5-5-5-5.compute-1.amazonaws.com""

If all three hosts are up, we can connect. If one host is down, then we cannot create a Zookeeper instance due to an UnknownHost exception, even though the other servers in the connect string are valid.

java.net.UnknownHostException: ec2-5-5-5-5.compute-1.amazonaws.com 
at java.net.InetAddress.getAllByName0(InetAddress.java:1243) 
at java.net.InetAddress.getAllByName(InetAddress.java:1155) 
at java.net.InetAddress.getAllByName(InetAddress.java:1091) 
at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:60) 
at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445) ",[],Bug,ZOOKEEPER-1734,Major,Andy Grove,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper fails to connect if one zookeeper host is down on EC2 when using elastic IP (UnknownHostException),2014-07-15T17:22:51.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",7.0
Jeffrey Zhong,[],2013-07-20T00:31:42.000+0000,Jeffrey Zhong,"FLETest#testLE fail intermittently on windows boxes. The reason is that in LEThread#run() we have:
{code}
                                if(leader == i){
                                    synchronized(finalObj){
                                        successCount++;
                                        if(successCount > (count/2)) finalObj.notify();
                                    }

                                    break;
                                }
{code}

Basically once we have a confirmed leader, the leader thread dies due to the ""break"" of while loop. 

While in the verification step, we check if the leader thread alive or not as following:
{code}
       if(threads.get((int) leader).isAlive()){
           Assert.fail(""Leader hasn't joined: "" + leader);
       }
{code}
On windows boxes, the above verification step fails frequently because leader thread most likely already exits.

Do we know why we have the leader alive verification step only lead thread can bump up successCount >= count/2?
","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1733,Major,Jeffrey Zhong,Fixed,2013-12-18T15:48:59.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,FLETest#testLE is flaky on windows boxes,2014-03-13T18:17:12.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",6.0
Germán Blanco,"[<JIRA Component: name='leaderElection', id='12312378'>]",2013-07-19T16:14:57.000+0000,Germán Blanco,"I have a test in which I do a rolling restart of three ZooKeeper servers and it was failing from time to time.
I ran the tests in a loop until the failure came out and it seems that at some point one of the servers is unable to join the enssemble formed by the other two.
","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1732,Blocker,Germán Blanco,Fixed,2013-10-30T03:22:24.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeper server unable to join established ensemble,2014-03-13T18:17:14.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",12.0
Dave Latham,[],2013-07-16T17:59:56.000+0000,Dave Latham,"We had a cluster of 3 peers (running 3.4.3) fail after we took down 1 peer briefly for maintenance.  A second peer became unresponsive and the leader lost quorum.  Thread dumps on the second peer showed two threads consistently stuck in these states:

{noformat}
""QuorumPeer[myid=0]/0.0.0.0:2181"" prio=10 tid=0x00002aaab8d20800 nid=0x598a runnable [0x000000004335d000]
   java.lang.Thread.State: RUNNABLE
        at java.util.HashMap.put(HashMap.java:405)
        at org.apache.zookeeper.server.ServerCnxnFactory.registerConnection(ServerCnxnFactory.java:131)
        at org.apache.zookeeper.server.ZooKeeperServer.finishSessionInit(ZooKeeperServer.java:572)
        at org.apache.zookeeper.server.quorum.Learner.revalidate(Learner.java:444)
        at org.apache.zookeeper.server.quorum.Follower.processPacket(Follower.java:133)
        at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:86)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:740)


""NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181"" daemon prio=10 tid=0x00002aaab84b0800 nid=0x5986 runnable [0x0000000040878000]
   java.lang.Thread.State: RUNNABLE
        at java.util.HashMap.removeEntryForKey(HashMap.java:614)
        at java.util.HashMap.remove(HashMap.java:581)
        at org.apache.zookeeper.server.ServerCnxnFactory.unregisterConnection(ServerCnxnFactory.java:120)
        at org.apache.zookeeper.server.NIOServerCnxn.close(NIOServerCnxn.java:971)
        - locked <0x000000078d8a51f0> (a java.util.HashSet)
        at org.apache.zookeeper.server.NIOServerCnxnFactory.closeSessionWithoutWakeup(NIOServerCnxnFactory.java:307)
        at org.apache.zookeeper.server.NIOServerCnxnFactory.closeSession(NIOServerCnxnFactory.java:294)
        - locked <0x000000078d82c750> (a org.apache.zookeeper.server.NIOServerCnxnFactory)
        at org.apache.zookeeper.server.ZooKeeperServer.processConnectRequest(ZooKeeperServer.java:834)
        at org.apache.zookeeper.server.NIOServerCnxn.readConnectRequest(NIOServerCnxn.java:410)
        at org.apache.zookeeper.server.NIOServerCnxn.readPayload(NIOServerCnxn.java:200)
        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:236)
        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:224)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

It shows both threads concurrently modifying ServerCnxnFactory.connectionBeans which is a java.util.HashMap.

This cluster was serving thousands of clients, which seems to make this condition more likely as it appears to occur when one client connects and another disconnects at about the same time.","[<JIRA Version: name='3.4.6', id='12323310'>]",Bug,ZOOKEEPER-1731,Critical,Dave Latham,Fixed,2013-08-02T17:45:21.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Unsynchronized access to ServerCnxnFactory.connectionBeans results in deadlock,2016-03-03T01:33:04.000+0000,[],8.0
,[],2013-07-13T00:45:39.000+0000,Justin SB,"The dynamic reconfiguration feature is great.  But it doesn't seem to be possible to go from 1 server to 2 servers (1 server + 1 observer).  When there's only one server, ZK automatically starts in single server mode; when in single server mode trying to add a server causes a class cast exception because the server is a ZooKeeperServer, not a LeaderZooKeeperServer.",[],Bug,ZOOKEEPER-1726,Major,Justin SB,Duplicate,2013-07-13T00:53:51.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,No way to dynamically go from 1 ZK server -> 2 ZK servers?,2013-07-13T00:53:51.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",2.0
Michi Mutsuzaki,[],2013-07-13T00:43:31.000+0000,Justin SB,"When writing the dynamic configuration out, Zookeeper writes out hostnames, even if an IP address is supplied.  These may not correctly round-trip (e.g. 127.0.0.1 might be written as localhost which may then resolve to 127.0.0.1 and another IP address).

This isn't actually causing problems for me right now, but seems very likely to cause hard-to-track-down problems in future.
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1725,Minor,Justin SB,Fixed,2014-03-31T23:57:21.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper Dynamic Conf writes out hostnames when IPs are supplied,2014-04-01T11:10:52.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
,"[<JIRA Component: name='server', id='12312382'>]",2013-07-08T20:46:18.000+0000,Mohammad Shamma,"Zookeeper ensembles need an identifier that would prevent misconfigured zookeeper server from clobbering the configuration of a zookeeper ensemble.

Use case:

- A zookeeper based distributed system that grows its zookeeper ensemble incrementally.
- The system is reset, where the new zookeeper ensemble is a subset of the old zookeeper ensemble (the history of the new ensemble have been reset too).
- The old zookeeper servers will attempt to communicate with the new servers (assuming the network end-points remain the same).
- The new zookeeper servers will notice that the old zookeeper servers have a higher configuration version and will attempt to reconfigure based on the old ensemble configuration info.

Note that this can be solved if the reset process would stop every zookeeper server in the old deployment and delete its history. However, some of these servers might be down at the time of reset, therefore this solution is not reliable.

I am sure this is not the most generic description of the problem of not having ensemble identifiers, but it presents a use case for introducing them to prevent servers from cross-talking across different ensembles. Otherwise they will automatically join in to form a single ensemble.",[],Bug,ZOOKEEPER-1723,Major,Mohammad Shamma,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,unique ensemble identifier,2013-07-08T20:46:18.000+0000,[],1.0
,"[<JIRA Component: name='c client', id='12312380'>]",2013-07-06T03:34:55.000+0000,Kevin Jamieson,"Using ZK 3.5.4, zookeeper_close() occasionally hangs with a backtrace of the form:

{noformat}
#0  0x00002b255fab489c in __lll_lock_wait () from /lib/x86_64-linux-gnu/libpthread.so.0
#1  0x00002b255fab26b0 in pthread_cond_broadcast@@GLIBC_2.3.2 () from /lib/x86_64-linux-gnu/libpthread.so.0
#2  0x00002b2560568ced in unlock_completion_list (l=0x13f5430) at src/mt_adaptor.c:69
#3  0x00002b256055b9ec in free_completions (zh=0x13f5270, callCompletion=1, reason=-116) at src/zookeeper.c:1521
#4  0x00002b256055d3bc in zookeeper_close (zh=0x13f5270) at src/zookeeper.c:2954
{noformat}

At which point the zhandle_t struct appears to have already been freed, as it contains garbage:

{noformat}
(gdb) p zh->sent_requests.cond
$19 = {
  __data = {
    __lock = 2, 
    __futex = 0, 
    __total_seq = 18446744073709551615, 
    __wakeup_seq = 0, 
    __woken_seq = 0, 
    __mutex = 0x0, 
    __nwaiters = 0, 
    __broadcast_seq = 0
  }, 
  __size = ""\002\000\000\000\000\000\000\000\377\377\377\377\377\377\377\377"", '\000' <repeats 31 times>, 
  __align = 2
}
{noformat}

There appears to be a race condition in the following code:

{noformat}
int api_epilog(zhandle_t *zh,int rc)
{
    if(inc_ref_counter(zh,-1)==0 && zh->close_requested!=0)
        zookeeper_close(zh);
    return rc;
}

int zookeeper_close(zhandle_t *zh)
{
    int rc=ZOK;
    if (zh==0)
        return ZBADARGUMENTS;

    zh->close_requested=1;
    if (inc_ref_counter(zh,1)>1) {
{noformat}

As api_epilog() may free zh in between zookeeper_close() setting zh->close_requested=1 and incrementing the reference count.

The following patch should fix the problem:

{noformat}
diff --git a/src/c/src/zookeeper.c b/src/c/src/zookeeper.c
index 6943243..61a263a 100644
--- a/src/c/src/zookeeper.c
+++ b/src/c/src/zookeeper.c
@@ -1051,6 +1051,7 @@ zhandle_t *zookeeper_init(const char *host, watcher_fn watcher,
         goto abort;
     }
 
+    api_prolog(zh);
     return zh;
 abort:
     errnosave=errno;
@@ -2889,7 +2890,7 @@ int zookeeper_close(zhandle_t *zh)
         return ZBADARGUMENTS;
 
     zh->close_requested=1;
-    if (inc_ref_counter(zh,1)>1) {
+    if (inc_ref_counter(zh,0)>1) {
         /* We have incremented the ref counter to prevent the
          * completions from calling zookeeper_close before we have
          * completed the adaptor_finish call below. */
{noformat}",[],Bug,ZOOKEEPER-1720,Major,Kevin Jamieson,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Race in zookeeper_close() leads to hang,2017-10-17T12:14:21.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Marshall McMullen,[],2013-06-25T18:00:17.000+0000,Marshall McMullen,"This fix from ZOOKEEPER-1663 is incorrect. It assumes the shell is bash since it uses bash array construction, e.g.:

{code}
 96   LIBPATH=(""${ZOOKEEPER_PREFIX}""/share/zookeeper/*.jar)
{code}

This does NOT work if /bin/sh points to /bin/dash as it does on Ubuntu. 

It fails as so:

{quote}
zkEnv.sh: 96: zkEnv.sh: Syntax error: ""("" unexpected (expecting ""fi"")                                                                                                                                                                                              
{quote}

If I change the shebang at the top to use ""/bin/bash"" instead of ""/bin/sh"" it works as expected. I don't know the full details of why using a bash array was chosen as the solution but I don't think it is the right way to deal with spaces in these paths...
","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1719,Major,Marshall McMullen,Fixed,2013-06-28T16:28:58.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"zkCli.sh, zkServer.sh and zkEnv.sh regression caused by ZOOKEEPER-1663",2014-07-25T11:25:04.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.5.0', id='12316644'>]",5.0
,[],2013-06-14T15:22:14.000+0000,hareesh,"In my flex application , when i debug the code it's giving the correct result. but , if i run in run mode it's not giving correct result. I tried to know what's happening .but , i didn't get anything.  Could you please give me some suggestions ?
","[<JIRA Version: name='4.0.0', id='12313382'>]",Bug,ZOOKEEPER-1717,Major,hareesh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Flex code works in debug mode , not in run mode",2013-09-01T13:27:08.000+0000,"[<JIRA Version: name='4.0.0', id='12313382'>]",2.0
Charlie Helin,"[<JIRA Component: name='jute', id='12312385'>]",2013-06-11T21:48:45.000+0000,Robert Joseph Evans,"I was trying to use org.apache.zookeeper.server.LogFormatter to analyze the access pattern of a particular application.  As part of this I wanted to get the size of the data that was being written into ZK.

I ran into an issue where in some cases the hex data had an odd length.  I looked into it and found that the buffer is being written out using Integer.toHexString(barr[idx])

Looking at the javadoce for toHexString it indicates that it does not pad the bits at all, and will output the twos compliment of the number if it is negative.  I then looked at how the data was being parsed and it assumed that every byte consisted of exactly two characters, which is not true.
{code}
Utils.toCSVBuffer(new byte[] {0xff}) returns ""#ffffffff""
Utils.toCSVBuffer(new byte[] {0x01}) returns ""#1""

If I combine those 
Utils.fromCSVBuffer(Utils.toCSVBuffer(new byte[] {0xff, 0x01, 0xff})) will return {0xff, 0xff, 0xff, 0xff, 0x1f, 0xff, 0xff, 0xff}
{code}

I think what we want is something like
{code}
static final char[] NIBBLE_TO_HEX = {
  '0', '1', '2', '3', '4', '5', '6', '7',
  '8', '9', 'a', 'b', 'c', 'd', 'e', 'f'
};

static String toCSVBuffer(byte barr[]) {
    if (barr == null || barr.length == 0) {
        return """";
    }
    StringBuilder sb = new StringBuilder(barr.length + 1);
    sb.append('#');
    for(int idx = 0; idx < barr.length; idx++) {
        byte b = barr[idx];
        sb.append(NIBBLE_TO_HEX[b&0x0f]);
        sb.append(NIBBLE_TO_HEX[(b&0xf0)>>4]);
    }
    return sb.toString();
}
{code}",[],Bug,ZOOKEEPER-1716,Major,Robert Joseph Evans,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,jute/Utils.fromCSVBuffer cannot parse data returnd by toCSVBuffer ,2015-10-14T21:01:37.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",2.0
Botond Hejj,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2013-06-06T13:06:32.000+0000,Botond Hejj,if ZOO_READ_ACL_UNSAFE or ZOO_CREATOR_ALL_ACL constant is used than the client core dumps with segmentation fault.,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1714,Minor,Botond Hejj,Fixed,2013-06-21T20:01:11.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,perl client segfaults if ZOO_READ_ACL_UNSAFE constant is used,2014-03-13T18:17:08.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",7.0
Germán Blanco,[],2013-06-06T09:51:46.000+0000,Germán Blanco,"A colleague of mine has spotted this error in time calculation in the code in zkfuse.cc lines 81 to 85:
inline
uint64_t nanosecsToMillisecs(uint64_t nanosecs)
{
    return nanosecs * 1000000;
}
I am not sure how this method is used, but for sure it will make something wrong happen if it is.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1713,Trivial,Germán Blanco,Fixed,2013-09-02T20:23:07.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,wrong time calculation in zkfuse.cc,2014-03-13T18:16:57.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",5.0
Marshall McMullen,[],2013-06-04T17:00:18.000+0000,Camille Fournier,"From the latest build logs:
  [exec] Zookeeper_watchers::testChildWatcher2 : elapsed 54 : OK
     [exec] /home/jenkins/jenkins-slave/workspace/ZooKeeper-trunk/trunk/src/c/tests/TestReconfig.cc:183: Assertion: equality assertion failed [Expected: 1, Actual  : 0]
     [exec] Failures !!!
     [exec] Run: 67   Failure total: 1   Failures: 1   Errors: 0
     [exec] FAIL: zktest-mt
     [exe",[],Bug,ZOOKEEPER-1712,Major,Camille Fournier,Duplicate,2013-06-04T17:24:42.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,transient test failure in TestReconfig.cc,2016-04-16T13:04:49.000+0000,[],3.0
,"[<JIRA Component: name='server', id='12312382'>]",2013-05-31T11:16:40.000+0000,Germán Blanco,"Unlike current ZooKeeper version in trunk intended for release as 3.5.0, the current ZooKeeper server version 3.4.5 binds to all ip addresses on the specified port for election. It only makes sense to bind to the ip address indicated in the configuration file, which is where the other servers will connect. Listening to other ip addresses could have bad security implications.","[<JIRA Version: name='3.4.6', id='12323310'>]",Bug,ZOOKEEPER-1711,Minor,Germán Blanco,Duplicate,2013-08-29T13:22:59.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeper server binds to all ip addresses for leader election and broadcast,2014-03-13T18:17:12.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",3.0
Johan Hillertz,[],2013-05-17T15:01:38.000+0000,Johan Hillertz,"After building the deb package it is not installable because of missing dependencies in the control file. 

Path:
src/packages/deb/zookeeper.control/control

If I remember correctly the package 'sun-java6-jre' is no longer provided by Ubuntu.

If it is possible to run zookeeper in openjdk the correct string in the control file should be:

""Depends: openjdk-6-jre""
Or 
""Depends: openjdk-7-jre""

",[],Bug,ZOOKEEPER-1708,Minor,Johan Hillertz,Won't Fix,2016-03-03T16:23:35.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Wrong version of java in control file for deb packages,2016-03-03T16:23:36.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",3.0
Chris Nauroth,"[<JIRA Component: name='documentation', id='12312422'>]",2013-05-17T14:16:39.000+0000,Johan Hillertz,"Since I faild to build a deb package from the instructions. I found that the documentation in 'README_packaging.txt' for building Ubuntu packages can be improved. I have attached a suggested patch.

Tested on Ubuntu 12.04 LTS",[],Bug,ZOOKEEPER-1707,Minor,Johan Hillertz,Won't Fix,2016-03-03T16:23:01.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Incorrect documentation of build dependencies for deb and rpm packages.,2016-03-03T16:23:07.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",3.0
Jingguo Yao,"[<JIRA Component: name='documentation', id='12312422'>]",2013-05-13T06:05:28.000+0000,Jingguo Yao,"For the Double Barriers example in the ""ZooKeeper Recipes and Solutions"" page, the P should be L in line 4 of the Leave pseudo code.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1706,Minor,Jingguo Yao,Fixed,2013-05-13T07:34:49.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Typo in Double Barriers example,2014-03-13T18:17:11.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",4.0
,"[<JIRA Component: name='c client', id='12312380'>]",2013-05-10T18:56:16.000+0000,Stephen Tyree,"Using libzookeeper_mt on an unsupported platform (OpenVMS) with a 5 server connection string, the fourth server in the connection string gets selected approximately only 6% of the time. This appears to be due to some strange properties of the LCG used in OpenVMS's C rand() function. Linux does not exhibit this behavior, but I can't speak for Windows, BSD, etc.

It would be prudent, if libzookeeper_mt's behavior is intended to be the same on every platform it operates on (not that OpenVMS is one of those platforms), to use a PRNG of its own choosing. Integrating a defined PRNG, such as the mersenne twister, would give all platforms the same, correct behavior.",[],Bug,ZOOKEEPER-1705,Minor,Stephen Tyree,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Certain implementations of C's rand() function coupled with the shuffle in libzookeeper_mt's getaddrs() produce a biased distribution of connections.,2013-05-10T18:56:16.000+0000,[],1.0
Chris Nauroth,"[<JIRA Component: name='java client', id='12312381'>]",2013-05-09T20:24:17.000+0000,Chris Nauroth,"The problem occurs when a connection attempt is pending and there are multiple outbound packets in the queue for other operations.  In {{ClientCnxnSocketNIO#doIO}}, it is possible to receive notification that the socket is writable for the next operation packet before receiving notification that the socket is readable for the connection response from the server.  If the server decides that the session is expired, then it responds by immediately closing the socket on its side.  If the client has written packets after the server has closed its end of the socket, then the TCP stack may choose to abort the connection with an RST.  When this happens, the client doesn't receive an orderly shutdown, and ultimately it fails to deliver a session expired event to the application.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1702,Major,Chris Nauroth,Fixed,2013-07-01T23:24:41.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"ZooKeeper client may write operation packets before receiving successful response to connection request, can cause TCP RST",2014-03-13T18:17:13.000+0000,"[<JIRA Version: name='3.4.2', id='12319196'>]",10.0
Patrick D. Hunt,"[<JIRA Component: name='quorum', id='12312379'>]",2013-05-08T00:43:31.000+0000,Patrick D. Hunt,"I'm consistently seeing a failure on my laptop when running the FLETest ""testJoin"" test. What seems to be happening is that the call to setLastSeenQuorumVerifier is hanging.

See the following log from the test, notice 17:35:57 for the period in question. Note that I turned on debug logging and added a few log messages around the call to setLastSeenQuorumVerifier (you can see the code enter but never leave)

Note: I've applied ZOOKEEPER-1324 to trunk code and then run this test but that doesn't seem to help. Also note that this test is passing consistently when run against branch-3.4.

{noformat}
2013-05-07 17:35:57,859 [myid:] - INFO  [QuorumPeer[myid=0]/0:0:0:0:0:0:0:0:11221:Follower@65] - FOLLOWING - LEADER ELECTION TOOK - 16
2013-05-07 17:35:57,859 [myid:] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:Leader@436] - LEADING - LEADER ELECTION TOOK - 17
2013-05-07 17:35:57,863 [myid:] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:FileTxnSnapLog@270] - Snapshotting: 0x0 to /home/phunt/dev/zookeeper-trunk/build/test/tmp/test3690487600947307322.junit.dir/version-2/snapshot.0
2013-05-07 17:35:57,873 [myid:] - INFO  [LearnerHandler-/127.0.0.1:34262:LearnerHandler@269] - Follower sid: 0 : info : 0.0.0.0:11222:11223:participant;0.0.0.0:11221
2013-05-07 17:35:57,878 [myid:] - INFO  [LearnerHandler-/127.0.0.1:34262:LearnerHandler@328] - Synchronizing with Follower sid: 0 maxCommittedLog=0x0 minCommittedLog=0x0 peerLastZxid=0x0
2013-05-07 17:35:57,878 [myid:] - DEBUG [LearnerHandler-/127.0.0.1:34262:LearnerHandler@395] - committedLog is empty but leader and follower are in sync, zxid=0x0
2013-05-07 17:35:57,878 [myid:] - INFO  [LearnerHandler-/127.0.0.1:34262:LearnerHandler@404] - Sending DIFF
2013-05-07 17:35:57,879 [myid:] - DEBUG [LearnerHandler-/127.0.0.1:34262:LearnerHandler@411] - Sending NEWLEADER message to 0
2013-05-07 17:35:57,880 [myid:] - INFO  [QuorumPeer[myid=0]/0:0:0:0:0:0:0:0:11221:Learner@331] - Getting a diff from the leader 0x0
2013-05-07 17:35:57,885 [myid:] - INFO  [QuorumPeer[myid=0]/0:0:0:0:0:0:0:0:11221:Learner@457] - Learner received NEWLEADER message
2013-05-07 17:35:57,885 [myid:] - INFO  [QuorumPeer[myid=0]/0:0:0:0:0:0:0:0:11221:Learner@460] - NEWLEADER calling configfromstring
2013-05-07 17:35:57,885 [myid:] - INFO  [QuorumPeer[myid=0]/0:0:0:0:0:0:0:0:11221:Learner@462] - NEWLEADER setting quorum verifier
2013-05-07 17:35:57,886 [myid:] - WARN  [QuorumPeer[myid=0]/0:0:0:0:0:0:0:0:11221:QuorumPeer@1218] - setLastSeenQuorumVerifier called with stale config 0. Current version: 0
2013-05-07 17:36:01,880 [myid:] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:Leader@585] - Shutting down
2013-05-07 17:36:01,881 [myid:] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:Leader@591] - Shutdown called
java.lang.Exception: shutdown Leader! reason: Waiting for a quorum of followers, only synced with sids: [ [1] ]
	at org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:591)
	at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:487)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:949)
2013-05-07 17:36:01,881 [myid:] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:ZooKeeperServer@398] - shutting down
2013-05-07 17:36:01,881 [myid:] - INFO  [LearnerCnxAcceptor-0.0.0.0/0.0.0.0:11225:Leader$LearnerCnxAcceptor@398] - exception while shutting down acceptor: java.net.SocketException: Socket closed
2013-05-07 17:36:01,882 [myid:] - WARN  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:QuorumPeer@979] - PeerState set to LOOKING
2013-05-07 17:36:01,882 [myid:] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:QuorumPeer@863] - LOOKING
2013-05-07 17:36:01,883 [myid:] - DEBUG [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:QuorumPeer@792] - Initializing leader election protocol...
{noformat}
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1700,Critical,Patrick D. Hunt,Fixed,2013-05-11T12:51:43.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,FLETest consistently failing - setLastSeenQuorumVerifier seems to be hanging,2013-05-12T11:09:13.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
Alexander Shraer,"[<JIRA Component: name='server', id='12312382'>]",2013-05-03T22:47:05.000+0000,Alexander Shraer,"A leader gives up leadership when losing a quorum of the current configuration.
This doesn't take into account any proposed configuration. So, if
a reconfig operation is in progress and a quorum of the new configuration is not
responsive, the leader will just get stuck waiting for it to ACK the reconfig operation, and will never timeout. ","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1699,Blocker,Alexander Shraer,Fixed,2014-05-21T17:49:06.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Leader should timeout and give up leadership when losing quorum of last proposed configuration,2014-05-21T22:54:42.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",10.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2013-05-01T00:38:05.000+0000,Patrick D. Hunt,"I keep seeing this on the leader:

2013-04-30 01:18:39,754 INFO
org.apache.zookeeper.server.quorum.Leader: Shutdown called
java.lang.Exception: shutdown Leader! reason: Only 0 followers, need 2
at org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:447)
at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:422)
at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:753)

The followers are downloading the snapshot when this happens, and are
trying to do their first ACK to the leader, the ack fails with broken
pipe.

In this case the snapshots are large and the config has increased the
initLimit. syncLimit is small - 10 or so with ticktime of 2000. Note
this is 3.4.3 with ZOOKEEPER-1521 applied.

I originally speculated that
https://issues.apache.org/jira/browse/ZOOKEEPER-1521 might be related.
I thought I might have broken something for this environment. That
doesn't look to be the case.

As it looks now it seems that 1521 didn't go far enough. The leader
verifies that all followers have ACK'd to the leader within the last
""syncLimit"" time period. This runs all the time in the background on
the leader to identify the case where a follower drops. In this case
the followers take so long to load the snapshot that this check fails
the very first time, as a result the leader drops (not enough ack'd
followers w/in the sync limit) and re-election happens. This repeats
forever. (the above error)

this is the call:
org.apache.zookeeper.server.quorum.LearnerHandler.synced() that's at
odds.

look at setting of tickOfLastAck in
org.apache.zookeeper.server.quorum.LearnerHandler.run()
It's not set until the follower first acks - in this case I can see
that the followers are not getting to the ack prior to the leader
shutting down due to the error log above.

It seems that sync() should probably use the init limit until the
first ack comes in from the follower. I also see that while tickOfLastAck and leader.self.tick is shared btw two threads there is no synchronization of the shared resources.
","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1697,Critical,Patrick D. Hunt,Fixed,2013-05-11T13:35:32.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,large snapshots can cause continuous quorum failure,2014-03-13T18:17:11.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.5.0', id='12316644'>]",12.0
Jeffrey Zhong,"[<JIRA Component: name='java client', id='12312381'>]",2013-04-24T13:39:23.000+0000,Dmitry Konstantinov,"The problem in details is described here: http://comments.gmane.org/gmane.comp.java.zookeeper.user/2897
The provided link also contains a reference to fix implementation.

{noformat}
####<Apr 24, 2013 1:03:28 PM MSK> <Warning> <org.apache.zookeeper.ClientCnxn> <devapp090> <clust2> <[ACTIVE] ExecuteThread: '2' for queue: 'weblogic.kernel.Default (devapp090:2182)> <internal> <> <> <1366794208810> <BEA-000000> <WARN  org.apache.zookeeper.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.lang.IllegalArgumentException: No Configuration was registered that can handle the configuration named Client
                at com.bea.common.security.jdkutils.JAASConfiguration.getAppConfigurationEntry(JAASConfiguration.java:130)
                at org.apache.zookeeper.client.ZooKeeperSaslClient.<init>(ZooKeeperSaslClient.java:97)
                at org.apache.zookeeper.ClientCnxn$SendThread.startConnect(ClientCnxn.java:943)
                at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:993)
>

{noformat}","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1696,Critical,Dmitry Konstantinov,Fixed,2013-09-27T23:26:07.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Fail to run zookeeper client on Weblogic application server,2014-03-13T18:16:52.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",12.0
Michi Mutsuzaki,"[<JIRA Component: name='server', id='12312382'>]",2013-04-23T23:28:59.000+0000,Thawan Kooburat,"From KeeperException.Code, RECONFIGINPROGRESS and NEWCONFIGNOQUORUM are declared as system errors. However, their error code suggested that they are API errors. 

We either need to move it to the right type or use the code from the right range","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1695,Blocker,Thawan Kooburat,Fixed,2014-04-30T02:34:19.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Inconsistent error code and type for new errors introduced by dynamic reconfiguration  ,2014-04-30T10:33:20.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",6.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2013-04-22T10:08:25.000+0000,Germán Blanco,This is at least what it seems in the logs. This also seems to cause a second snapshot in the follower.,"[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1694,Minor,Germán Blanco,Duplicate,2013-04-22T12:43:11.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZooKeeper Leader sends a repeated NEWLEADER quorum packet to followers,2013-04-22T20:02:50.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.5.0', id='12316644'>]",3.0
,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>]",2013-04-19T12:14:21.000+0000,Jacky007,"The xid will be confused with AUTHXID(-4) when it is overflowed.

If the process send 4000 requests per second, it may core or hang after about ten days.",[],Bug,ZOOKEEPER-1693,Major,Jacky007,Duplicate,2013-10-09T06:38:48.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,process may core or hang when xid is overflowed,2013-10-09T06:38:48.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",1.0
,[],2013-04-15T11:53:33.000+0000,Jacky007,"In NIOServerCnxn.java
     public void close() {
            closeSock();
            ...
            sk.cancel();

Close sock first, then cancel the channel.
    
    public void sendBuffer(ByteBuffer bb) {
                if ((sk.interestOps() & SelectionKey.OP_WRITE) == 0) {
                        ...
                        sock.write(bb);

Get ops of the channel, then read sock (may be null)

I have noticed that the 3.5.0-branch has fixed the problem.",[],Bug,ZOOKEEPER-1690,Major,Jacky007,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Race condition when close sock may cause a NPE in sendBuffer ,2013-04-15T11:59:25.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",2.0
,"[<JIRA Component: name='scripts', id='12312384'>]",2013-04-12T15:30:01.000+0000,Jeff Lord,"In zkCli.sh, the CLIENT_JVMFLAGS are being passed along with regular JVMFLAGS, so the latter ends up overriding it anyhow if set. Can we please remove JVMFLAGS completely from clients, if CLIENT_JVMFLAGS are also set (i.e. use just one). 

One example of how this can be detrimental is if you attempt to start a zookeeper-client session on the same host that is already running zookeeper and use the default config directory. If the zookeeper server has jmx enabled than the client will also pick up that port and attempt to bind resulting in a failure

# /usr/bin/zookeeper-client 
Error: Exception thrown by the agent : java.rmi.server.ExportException: Port already in use: 9010; nested exception is: 
java.net.BindException: Address already in use 
",[],Bug,ZOOKEEPER-1689,Minor,Jeff Lord,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Remove JVMFLAGS completely from clients, if CLIENT_JVMFLAGS are also set",2017-02-06T21:24:03.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='tests', id='12312427'>]",2013-04-05T22:17:31.000+0000,Todd Lipcon,ZooKeeper 3.4.2 used to publish a jar with the tests classifier for use by downstream project tests. It seems this didn't get published for 3.4.4 or 3.4.5 (see https://repository.apache.org/index.html#nexus-search;quick~org.apache.zookeeper). Would someone mind please publishing these artifacts?,"[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.4.5', id='12321883'>]",Bug,ZOOKEEPER-1686,Major,Todd Lipcon,Fixed,2013-09-30T03:58:23.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Publish ZK 3.4.5 test jar,2013-10-03T23:54:02.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",6.0
,[],2013-04-05T21:03:09.000+0000,Arpit Gupta,"Noticed this while debugging a secure deploy. The server was started with the principal zk/_HOST

When a client tried to connect to this it tried to setup a secure connection to server zookeeper/_HOST and failed authentication.

In ClientCnxn.java

{code}
try {
                zooKeeperSaslClient = new ZooKeeperSaslClient(""zookeeper/""+addr.getHostName());
            } catch (LoginException e) {
{code}",[],Bug,ZOOKEEPER-1685,Major,Arpit Gupta,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper client hard codes the server principal to zookeeper,2019-05-30T15:38:15.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",3.0
,[],2013-04-05T20:43:36.000+0000,Shevek,"I quote:

void registerAndConnect(SocketChannel sock, InetSocketAddress addr)
throws IOException {
sockKey = sock.register(selector, SelectionKey.OP_CONNECT);
boolean immediateConnect = sock.connect(addr);
if (immediateConnect)
{ sendThread.primeConnection(); }

}

In the immediate case, there are several bugs:

a) updateSocketAddresses() is never called, as it is when the select-loop in doTransport(). This means that clientCnxnSocket.getRemoteSocketAddress() will return null for the lifetime of this socket?
b) CONNECT still in the interest set for the socket.
c) updateLastSendAndHeard() is never called either.",[],Bug,ZOOKEEPER-1684,Major,Shevek,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Failure to update socket addresses on immedate connection,2022-02-03T08:50:15.000+0000,[],1.0
Alexander Shraer,"[<JIRA Component: name='java client', id='12312381'>]",2013-04-04T22:20:38.000+0000,Shevek,"2013-04-04 22:16:15,872 ERROR [pool-4-thread-1] com.netflix.curator.ConnectionState.getZooKeeper (ConnectionState.java:84) - Background exception caught
java.lang.NullPointerException
        at org.apache.zookeeper.client.StaticHostProvider.updateServerList(StaticHostProvider.java:161) ~[zookeeper-3.5.0.jar:3.5.0--1]
        at org.apache.zookeeper.ZooKeeper.updateServerList(ZooKeeper.java:183) ~[zookeeper-3.5.0.jar:3.5.0--1]
        at com.netflix.curator.HandleHolder$1$1.setConnectionString(HandleHolder.java:121) ~[curator-client-1.3.5-SNAPSHOT.jar:?]


The duff code is this:

ClientCnxnSocket clientCnxnSocket = cnxn.sendThread.getClientCnxnSocket();
InetSocketAddress currentHost = (InetSocketAddress) clientCnxnSocket.getRemoteSocketAddress();
boolean reconfigMode = hostProvider.updateServerList(serverAddresses, currentHost);

Now, currentHost might be null, if we're not yet connected. But StaticHostProvider.updateServerList indirects on it unconditionally.

This would be caught by findbugs.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1683,Major,Shevek,Fixed,2014-07-17T20:58:04.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZooKeeper client NPE when updating server list on disconnected client,2014-07-18T11:35:00.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",8.0
,[],2013-03-29T23:39:26.000+0000,Shevek,"While the API permits construction of a ZooKeeper client object with a given sessionId, the sessionId can never be used:

ClientCnxn line 850: long sessId = (seenRwServerBefore) ? sessionId : 0;

The only person who sets seenRwServerBefore is onConnected().

Therefore, it appears that passing a sessionId into a ZooKeeper constructor has no effect, as the ClientCnxn has never seen an RW server before, so it discards it anyway.",[],Bug,ZOOKEEPER-1680,Major,Shevek,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Cannot connect with a given sessionId - it is discarded,2013-04-02T21:33:05.000+0000,[],1.0
,"[<JIRA Component: name='leaderElection', id='12312378'>]",2013-03-29T00:19:02.000+0000,Julio Lopez,"In a 5-node ZK cluster setup, in the following state:
* 1 host is down / not reachable.
* 4 hosts are up.
* 3 ZK servers are in quorum.
* a 4th ZK server was restarted and is trying to re-join the quorum.

The 4th server is not able to rejoin the quorum because the connection to the host that is not established, and apparently takes to long to timeout.

Stack traces and additional information coming.",[],Bug,ZOOKEEPER-1678,Major,Julio Lopez,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Server fails to join quorum when a peer is unreachable (5 ZK server setup),2013-07-09T09:43:18.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",7.0
,[],2013-03-28T21:00:50.000+0000,Shevek,"    ZOOKEEPER-1355. Add zk.updateServerList(newServerList) (Alex Shraer, Marshall McMullen via fpj)
    
    
    
    git-svn-id: https://svn.apache.org/repos/asf/zookeeper/trunk@1410731 13f79535-47bb-0310-9956-ffa450edef68


+int addrvec_contains(const addrvec_t *avec, const struct sockaddr_storage *addr)
+{
+    if (!avec || !addr)
+    { 
+        return 0;
+    }
+
+    int i = 0;
+    for (i = 0; i < avec->count; i++)
+    {
+        if(memcmp(&avec->data[i], addr, INET_ADDRSTRLEN) == 0)
+            return 1;
+    }
+
+    return 0;
+}


Pretty sure that should be sizeof(sockaddr_storage). INET_ADDRSTRLEN is the size of the character buffer which needs to be allocated for the return value of inet_ntop, which seems to be totally wrong.","[<JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.6.1', id='12346764'>, <JIRA Version: name='3.5.8', id='12346950'>]",Bug,ZOOKEEPER-1677,Major,Shevek,Duplicate,2020-04-14T10:02:13.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Misuse of INET_ADDRSTRLEN,2020-05-11T15:41:08.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",6.0
Yunong Xiao,"[<JIRA Component: name='c client', id='12312380'>]",2013-03-28T17:09:18.000+0000,Yunong Xiao,"I have a fairly simple single-threaded C client set up -- single-threaded
because we are embedding zk in the node.js/libuv runtime -- which consists of
the following algorithm:

zookeeper_interest(); select();
// perform zookeeper api calls
zookeeper_process();

I've noticed that zookeeper_interest in the C client never returns error if it
is unable to connect to the zk server.

From the spec of the zookeeper_interest API, I see that zookeeper_interest is
supposed to return ZCONNECTIONLOSS when disconnected from the client. However,
digging into the code, I see that the client is making a non-blocking connect
call
https://github.com/apache/zookeeper/blob/trunk/src/c/src/zookeeper.c#L1596-1613
,  and returning ZOK
https://github.com/apache/zookeeper/blob/trunk/src/c/src/zookeeper.c#L1684

If we assume that the server is not up, this will mean that the subsequent
select() call would return 0, since the fd is not ready, and future calls to
zookeeper_interest will always return 0 and not the expected ZCONNECTIONLOSS.
Thus an upstream client will never be aware that the connection is lost.

I don't think this is the expected behavior. I have temporarily patched the zk
C client such that zookeeper_interest will return ZCONNECTIONLOSS if it's still
unable to connect after session_timeout has been exceeded. 

I have included a patch for the client which fixes this for release 3.4.3 6b35e96 in this branch: https://github.com/yunong/zookeeper/tree/release-3.4.3-patched Here's the patch https://gist.github.com/yunong/efe869a0345867d54adf

For more information, please see this email thread. http://mail-archives.apache.org/mod_mbox/zookeeper-dev/201211.mbox/%3C11A8E7C3-4DDE-45D8-ABEC-A8A4D32CF647@gmail.com%3E",[],Bug,ZOOKEEPER-1676,Blocker,Yunong Xiao,Not A Problem,2016-05-22T22:18:16.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,C client zookeeper_interest returning ZOK on Connection Loss,2016-09-04T04:57:53.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",8.0
Michael Han,[],2013-03-26T20:49:38.000+0000,Alexander Shraer,"sync + read is supposed to return at least the latest write that completes before the sync starts. This is true if the leader doesn't change, but when it does it may not work. The problem happens when the old leader L1 still thinks that it is the leader but some other leader L2 was already elected and committed some operations. Suppose that follower F is connected to L1 and invokes a sync. Even though L1 responds to the sync, the recent operations committed by L2 will not be flushed to F so a subsequent read on F will not see these operations. 

To prevent this we should broadcast the sync like updates.

This problem is also mentioned in Section 4.4 of the ZooKeeper peper (but the proposed solution there is insufficient to solve the issue).",[],Bug,ZOOKEEPER-1675,Major,Alexander Shraer,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Make sync a quorum operation,2019-10-31T10:44:49.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",8.0
Craig Condit,[],2013-03-19T10:01:18.000+0000,Lipin Dmitriy,"Currently, when i try to set ACL with cidr in expression, i get exception:

{code}
[zk: localhost:2181(CONNECTED) 2] setAcl /AS0 ip:127.0.0.1/8:cdrwa
Exception in thread ""main"" org.apache.zookeeper.KeeperException$InvalidACLException: KeeperErrorCode = InvalidACL for /AS0
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:112)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
	at org.apache.zookeeper.ZooKeeper.setACL(ZooKeeper.java:1175)
	at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:716)
	at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:581)
	at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:353)
	at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:311)
	at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:270)
{code}

Also, there is no support for CIDR in IPAuthenticationProvider.isValid, but IPAuthenticationProvider.matches has it.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1673,Minor,Lipin Dmitriy,Fixed,2014-04-25T21:41:23.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper don't support cidr in expression in ACL with ip scheme,2014-04-26T11:04:47.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",8.0
Xiaoshuang Wang,"[<JIRA Component: name='java client', id='12312381'>]",2013-03-19T00:40:09.000+0000,Xiaoshuang Wang,"Without the modification to src/java/main/org/apache/zookeeper/cli/ReconfigCommand.java line 88, the reconfig command will not accept ""-member"" options by complaining not using the right usage.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1672,Trivial,Xiaoshuang Wang,Fixed,2013-03-20T06:21:17.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"zookeeper client does not accept ""-members"" option in reconfig command",2013-03-20T11:30:58.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
,[],2013-03-18T13:22:35.000+0000,Alex Blewitt,"The zookeeper dependency 3.4.5 (latest) depends explicitly on log4j 1.2.15, which has dependencies on com.sun.jmx which can't be resolved from Maven central.

Please change the dependency to either 1.2.16, which declares these as optional, or 1.2.14, which doesn't have them at all.

http://search.maven.org/remotecontent?filepath=org/apache/zookeeper/zookeeper/3.4.5/zookeeper-3.4.5.pom

<dependency>
<groupId>log4j</groupId>
<artifactId>log4j</artifactId>
<version>1.2.15</version>
<scope>compile</scope>
</dependency>

This should be modified to 1.2.14 or 1.2.16 as above.

It's also not clear why this is used at all; it would be better for ZooKeeper to depend only on slf4j-api, and let users determine what the right slf4j logging implementation is. With this approach, it's not possible to swap out log4j for something else.",[],Bug,ZOOKEEPER-1671,Minor,Alex Blewitt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Remove dependency on log4j 1.2.15,2013-03-18T13:22:35.000+0000,[],1.0
Flavio Paiva Junqueira,[],2013-03-15T21:36:54.000+0000,Arpit Gupta,"We noticed this with jdk 1.6 where if no heap size is set the process takes up to 1/4 of mem available on the machine.

More info http://stackoverflow.com/questions/3428251/is-there-a-default-xmx-setting-for-java-1-5

You can run the following command to see what are the defaults for your machine

{code}
java -XX:+PrintFlagsFinal -version 2>&1 | grep -i -E 'heapsize|permsize|version'
{code}

And we noticed on two different class of machines that this was 1/4th of total memory on the machine.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1670,Major,Arpit Gupta,Fixed,2013-10-01T21:32:01.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zookeeper should set a default value for SERVER_JVMFLAGS and CLIENT_JVMFLAGS so that memory usage is controlled,2019-12-18T13:34:05.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",7.0
Flavio Paiva Junqueira,"[<JIRA Component: name='server', id='12312382'>]",2013-03-14T08:56:56.000+0000,Jacky007,"When a client reestablish to a server, it will send the watches which have not been triggered. But the code in DataTree does not handle it correctly.

It is obvious, we just do not notice it :)

scenario:
1) Client a set a data watch on /d, then disconnect, client b delete /d and create it again. When client a reestablish to zk, it will receive a NodeCreated rather than a NodeDataChanged.
2) Client a set a exists watch on /e(not exist), then disconnect, client b create /e. When client a reestablish to zk, it will receive a NodeDataChanged rather than a NodeCreated.

","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1667,Blocker,Jacky007,Fixed,2013-10-22T10:56:58.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Watch event isn't handled correctly when a client reestablish to a server,2015-05-12T05:37:14.000+0000,"[<JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.4.5', id='12321883'>]",14.0
,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2013-03-13T18:58:01.000+0000,Boaz Kelmer,"Java on Linux/Solaris can be set up to use the native (via C library)
GSS implementation. This is configured by setting the system property
   sun.security.jgss.native=true
When using this feature, ZooKeeper Sasl/JGSS authentication doesn't work.
The reason is explained in
http://docs.oracle.com/javase/6/docs/technotes/guides/security/jgss/jgss-features.html

""""""
[when using native GSS...]
In addition, when performing operations as a particular Subject, e.g. 
Subject.doAs(...) or Subject.doAsPrivileged(...), the to-be-used 
GSSCredential should be added to Subject's private credential set. 
Otherwise, the GSS operations will fail since no credential is found.
""""""",[],Bug,ZOOKEEPER-1664,Major,Boaz Kelmer,Fixed,2013-09-11T22:00:36.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Kerberos auth doesn't work with native platform GSS integration,2013-09-11T22:00:36.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.5.0', id='12316644'>]",6.0
Amichai Rothman,"[<JIRA Component: name='scripts', id='12312384'>]",2013-03-12T15:25:42.000+0000,Amichai Rothman,The shell scripts (bin/zk*.sh) don't work when there are spaces in the zookeeper or java paths.,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1663,Minor,Amichai Rothman,Fixed,2013-05-20T17:12:13.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,scripts don't work when path contains spaces,2014-06-26T11:19:07.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",6.0
Alexander Shraer,"[<JIRA Component: name='tests', id='12312427'>]",2013-03-09T00:27:54.000+0000,Alexander Shraer,"Fix to two small bugs in ReconfigTest.testPortChange():
1. the test expected a port change to happen immediately, which is not necessarily
going to happen. The fix waits a bit and also tries several times.
2. when a client port changes, the test created a new ZooKeeper handle, but didn't specify a Watcher object, which generated some NullPointerException events when the watcher was triggered.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1662,Minor,Alexander Shraer,Fixed,2014-03-11T01:45:38.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Fix to two small bugs in ReconfigTest.testPortChange(),2014-03-11T11:11:05.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
,"[<JIRA Component: name='server', id='12312382'>]",2013-03-07T21:11:58.000+0000,Yan Pujante,"I have a client connecting to ZooKeeper and I am sometimes seeing a 5s delay before the opening of the socket connection:

Here is the output on the client side:
{noformat}
2013/03/07 10:53:48.729 INFO [org.apache.zookeeper.ZooKeeper] Client environment:zookeeper.version=3.4.5-1392090, built on 09/30/2012 17:52 GMT
2013/03/07 10:53:48.729 INFO [org.apache.zookeeper.ZooKeeper] Client environment:host.name=xeon
2013/03/07 10:53:48.729 INFO [org.apache.zookeeper.ZooKeeper] Client environment:java.version=1.6.0_41
2013/03/07 10:53:48.729 INFO [org.apache.zookeeper.ZooKeeper] Client environment:java.vendor=Apple Inc.
2013/03/07 10:53:48.729 INFO [org.apache.zookeeper.ZooKeeper] Client environment:java.home=/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home
2013/03/07 10:53:48.729 INFO [org.apache.zookeeper.ZooKeeper] Client environment:java.class.path=/local/java/lib/tools.jar:lib/ant-1.8.4.jar:lib/ant-antlr-1.8.4.jar:lib/ant-junit-1.8.4.jar:lib/ant-launcher-1.8.4.jar:lib/antlr-2.7.7.jar:lib/asm-4.0.jar:lib/asm-analysis-4.0.jar:lib/asm-commons-4.0.jar:lib/asm-tree-4.0.jar:lib/asm-util-4.0.jar:lib/commons-cli-1.2.jar:lib/groovy-2.0.7.jar:lib/groovy-ant-2.0.7.jar:lib/groovy-groovydoc-2.0.7.jar:lib/groovy-templates-2.0.7.jar:lib/groovy-xml-2.0.7.jar:lib/jackson-annotations-2.1.4.jar:lib/jackson-core-2.1.4.jar:lib/jackson-databind-2.1.4.jar:lib/jline-0.9.94.jar:lib/json-20090211.jar:lib/jul-to-slf4j-1.6.2.jar:lib/junit-3.8.1.jar:lib/log4j-1.2.16.jar:lib/netty-3.2.2.Final.jar:lib/org.linkedin.util-core-1.8.glu47.0.jar:lib/org.linkedin.util-groovy-1.8.glu47.0.jar:lib/org.linkedin.zookeeper-cli-impl-1.5.glu47.0-SNAPSHOT.jar:lib/org.linkedin.zookeeper-impl-1.5.glu47.0-SNAPSHOT.jar:lib/slf4j-api-1.6.2.jar:lib/slf4j-log4j12-1.6.2.jar:lib/zookeeper-3.4.5.jar
2013/03/07 10:53:48.730 INFO [org.apache.zookeeper.ZooKeeper] Client environment:java.library.path=/local/instantclient10:.:/Users/ypujante/Library/Java/Extensions:/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java
2013/03/07 10:53:48.730 INFO [org.apache.zookeeper.ZooKeeper] Client environment:java.io.tmpdir=/var/folders/dj/qmkmx5648xjf2n006s7hc1v80000gq/T/
2013/03/07 10:53:48.730 INFO [org.apache.zookeeper.ZooKeeper] Client environment:java.compiler=<NA>
2013/03/07 10:53:48.730 INFO [org.apache.zookeeper.ZooKeeper] Client environment:os.name=Mac OS X
2013/03/07 10:53:48.730 INFO [org.apache.zookeeper.ZooKeeper] Client environment:os.arch=x86_64
2013/03/07 10:53:48.730 INFO [org.apache.zookeeper.ZooKeeper] Client environment:os.version=10.8.2
2013/03/07 10:53:48.730 INFO [org.apache.zookeeper.ZooKeeper] Client environment:user.name=ypujante
2013/03/07 10:53:48.730 INFO [org.apache.zookeeper.ZooKeeper] Client environment:user.home=/Users/ypujante
2013/03/07 10:53:48.730 INFO [org.apache.zookeeper.ZooKeeper] Client environment:user.dir=/export/content/linkedin-zookeeper/org.linkedin.zookeeper-cli-1.5.glu47.0-SNAPSHOT
2013/03/07 10:53:48.731 INFO [org.apache.zookeeper.ZooKeeper] Initiating client connection, connectString=localhost:2181 sessionTimeout=100 watcher=org.linkedin.zookeeper.client.ZKClient@3823bdd1
2013/03/07 10:53:48.737 DEBUG [org.apache.zookeeper.ClientCnxn] zookeeper.disableAutoWatchReset is false
2013/03/07 10:53:48.756 DEBUG [org.linkedin.zookeeper.cli.ClientMain] Talking to zookeeper on localhost:2181
2013/03/07 10:53:53.763 INFO [org.apache.zookeeper.ClientCnxn] Opening socket connection to server fe80:0:0:0:0:0:0:1%1/fe80:0:0:0:0:0:0:1%1:2181. Will not attempt to authenticate using SASL (Unable to locate a login configuration)
{noformat}

From this output you can see the line at 10:53:48 => Initiating client connection
And then 5s later, at 10:53:53 => opening socket connection

Note that I did not see this delay/problem prior to upgrading to 3.4.5 (from 3.3.3)

Also note that sometimes there is no delay as in the following output!

{noformat}
2013/03/07 11:04:06.084 INFO [org.apache.zookeeper.ZooKeeper] Initiating client connection, connectString=localhost:2181 sessionTimeout=100 watcher=org.linkedin.zookeeper.client.ZKClient@1e670479
2013/03/07 11:04:06.089 DEBUG [org.apache.zookeeper.ClientCnxn] zookeeper.disableAutoWatchReset is false
2013/03/07 11:04:06.109 DEBUG [org.linkedin.zookeeper.cli.ClientMain] Talking to zookeeper on localhost:2181
2013/03/07 11:04:06.116 INFO [org.apache.zookeeper.ClientCnxn] Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (Unable to locate a login configuration)
{noformat}

I will be more than happy to provide more details if necessary. The client code is open source and hosted on github @ https://github.com/linkedin/linkedin-zookeeper/blob/master/org.linkedin.zookeeper-cli-impl/src/main/groovy/org/linkedin/zookeeper/cli/ClientMain.groovy#L65

and is not doing mech but (under the cover)

new ZooKeeper(""localhost:2181"", 100, watcher)

and then wait until the SyncConnected even is received...

Thanks
Yan
",[],Bug,ZOOKEEPER-1661,Major,Yan Pujante,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Random (?) 5s delay when establishing connection,2013-03-12T10:41:09.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",6.0
Rakesh Radhakrishnan,"[<JIRA Component: name='server', id='12312382'>]",2013-03-07T06:50:40.000+0000,Alexander Shraer,"We need to update JMX during reconfigurations. Currently, reconfiguration changes are not reflected in JConsole.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1659,Blocker,Alexander Shraer,Fixed,2014-06-04T20:08:32.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Add JMX support for dynamic reconfiguration,2014-06-04T20:50:45.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",10.0
Philip K. Warren,"[<JIRA Component: name='java client', id='12312381'>]",2013-03-01T16:44:47.000+0000,Gunnar Wagenknecht,"I did some profiling in one of our Java environments and found an interesting footprint in ZooKeeper. The SASL support seems to trigger a lot times on the client although it's not even in use.

Is there a switch to disable SASL completely?

The attached screenshot shows a 10-minute profiling session on one of our production Jetty servers. The Jetty server handles ~1k web requests per minute. The average response time per web request is a few milli seconds. The profiling was performed on a machine running for >24h. 

We noticed a significant CPU increase on our servers when deploying an update from ZooKeeper 3.3.2 to ZooKeeper 3.4.5. Thus, we started investigating. The screenshot shows that only 32% CPU time are spent in Jetty. In contrast, 65% are spend in ZooKeeper. 

A few notes/thoughts:
* {{ClientCnxn$SendThread.clientTunneledAuthenticationInProgress}} seems to be the culprit
* {{javax.security.auth.login.Configuration.getConfiguration}} seems to be called very often?
* There is quite a bit reflection involved in {{java.security.AccessController.doPrivileged}}
* No security manager is active in the JVM: I tend to place an if-check in the code before calling {{AccessController.doPrivileged}}. When no SM is installed, the runnable can be called directly which safes cycles.
","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1657,Major,Gunnar Wagenknecht,Fixed,2013-09-18T10:58:58.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Increased CPU usage by unnecessary SASL checks,2014-03-13T18:16:57.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",10.0
,[],2013-03-01T16:20:39.000+0000,Florian Pirchner,"""Import package"" are missing for bundle org.apache.hadoop.zookeeper.

I am getting an exception running the Zookeeper server in an OSGi environment.

ZookeeperServerMain uses
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

But there is no import in MANIFEST.mf:
Import-Package: javax.management,org.apache.log4j,org.osgi.framework;v
 ersion=""[1.4,2.0)"",org.osgi.util.tracker;version=""[1.1,2.0)""


I am sure that another missing package would be the subpackage of org.apache.log4j like org.apache.log4j.jmx.


Best, Florian




",[],Bug,ZOOKEEPER-1656,Major,Florian Pirchner,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,OSGI - Missing import package - ClassNotFoundException,2013-10-09T06:33:24.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",4.0
Thomas Weise,"[<JIRA Component: name='build', id='12312383'>]",2013-02-28T05:07:28.000+0000,Thomas Weise,"Old JLine version used in ZK CLI should not be pulled into downstream projects. 
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1655,Major,Thomas Weise,Fixed,2013-10-01T21:42:57.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Make jline dependency optional in maven pom,2016-10-11T18:15:44.000+0000,"[<JIRA Version: name='3.4.2', id='12319196'>]",7.0
Michael Han,[],2013-02-27T21:00:44.000+0000,Camille Fournier,"If you go to this page:
http://zookeeper.apache.org/doc/trunk/zookeeperProgrammers.html

Then try to click on Developer -> API Docs you'll get to 
http://zookeeper.apache.org/doc/trunk/api/index.html

Which does not exist. Should point to:

http://zookeeper.apache.org/doc/current/api/index.html",[],Bug,ZOOKEEPER-1654,Minor,Camille Fournier,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,bad documentation link on site,2017-09-06T13:44:19.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",3.0
Michi Mutsuzaki,"[<JIRA Component: name='quorum', id='12312379'>]",2013-02-27T01:59:38.000+0000,Michi Mutsuzaki,"It looks like QuorumPeer.loadDataBase() could fail if the server was restarted after zk.takeSnapshot() but before finishing self.setCurrentEpoch(newEpoch) in Learner.java.

{code:java}
case Leader.NEWLEADER: // it will be NEWLEADER in v1.0
    zk.takeSnapshot();
    self.setCurrentEpoch(newEpoch); // <<< got restarted here
    snapshotTaken = true;
    writePacket(new QuorumPacket(Leader.ACK, newLeaderZxid, null, null), true);
    break;
{code}

The server fails to start because currentEpoch is still 1 but the last processed zkid from the snapshot has been updated.

{noformat}
2013-02-20 13:45:02,733 5543 [pool-1-thread-1] ERROR org.apache.zookeeper.server.quorum.QuorumPeer  - Unable to load database on disk
java.io.IOException: The current epoch, 1, is older than the last zxid, 8589934592
        at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:439)
        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:413)
        ...
{noformat}

{noformat}
$ find datadir                                     
datadir
datadir/version-2
datadir/version-2/currentEpoch.tmp
datadir/version-2/acceptedEpoch
datadir/version-2/snapshot.0
datadir/version-2/currentEpoch
datadir/version-2/snapshot.200000000

$ cat datadir/version-2/currentEpoch.tmp
2%
$ cat datadir/version-2/acceptedEpoch
2%
$ cat datadir/version-2/currentEpoch
1%
{noformat}
","[<JIRA Version: name='3.4.6', id='12323310'>]",Bug,ZOOKEEPER-1653,Blocker,Michi Mutsuzaki,Fixed,2013-11-26T23:44:21.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper fails to start because of inconsistent epoch,2021-03-29T11:05:18.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",11.0
Sean Bridges,"[<JIRA Component: name='java client', id='12312381'>]",2013-02-26T18:45:38.000+0000,Sean Bridges,"When connecting to zookeeper, the client does a reverse dns lookup on the hostname.  In our environment, the reverse dns lookup takes 5 seconds to fail, causing zookeeper clients to connect slowly.

The reverse dns lookup occurs in ClientCnx in the calls to adr.getHostName()

{code}
            setName(getName().replaceAll(""\\(.*\\)"",
                    ""("" + addr.getHostName() + "":"" + addr.getPort() + "")""));
            try {
                zooKeeperSaslClient = new ZooKeeperSaslClient(""zookeeper/""+addr.getHostName());
            } catch (LoginException e) {
{code}",[],Bug,ZOOKEEPER-1652,Critical,Sean Bridges,Duplicate,2013-11-05T02:34:32.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zookeeper java client does a reverse dns lookup when connecting,2016-10-26T14:25:18.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",10.0
Rakesh Radhakrishnan,"[<JIRA Component: name='tests', id='12312427'>]",2013-02-20T17:30:38.000+0000,Patrick D. Hunt,"testServerCnxnExpiry is failing consistently on solaris apache jenkins:
https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper-trunk-solaris/475/testReport/org.apache.zookeeper.test/ServerCnxnTest/testServerCnxnExpiry/

Seems to have started around the time the NIO multi-threading changes were introduced - but it's hard to say (some of the history has been lost already).

Possibly just a bad test or timeouts not long enough...","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1650,Blocker,Patrick D. Hunt,Duplicate,2014-03-16T12:21:05.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,testServerCnxnExpiry failing consistently on solaris apache jenkins,2014-03-16T16:37:50.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",2.0
,"[<JIRA Component: name='build', id='12312383'>]",2013-02-19T09:47:41.000+0000,Shining,"ant rpm 
--------------------

rpm:
     [copy] Copying 1 file to /tmp/zkpython_build_nshi/SOURCES
      [rpm] Building the RPM based on the zkpython.spec file
      [rpm] Executing(%prep): /bin/sh -e /var/tmp/rpm-tmp.62078
      [rpm] Executing(%build): /bin/sh -e /var/tmp/rpm-tmp.62078
      [rpm] Executing(%install): /bin/sh -e /var/tmp/rpm-tmp.62078
      [rpm] 
      [rpm] 
      [rpm] RPM build errors:
      [rpm] + umask 022
      [rpm] + cd /tmp/zkpython_build_nshi/BUILD
      [rpm] + LANG=C
      [rpm] + export LANG
      [rpm] + unset DISPLAY
      [rpm] + tar fxz /tmp/zkpython_build_nshi/SOURCES/ZooKeeper-0.4.linux-x86_64.tar.gz -C /tmp/zkpython_build_nshi/BUILD
      [rpm] + exit 0
      [rpm] + umask 022
      [rpm] + cd /tmp/zkpython_build_nshi/BUILD
      [rpm] + LANG=C
      [rpm] + export LANG
      [rpm] + unset DISPLAY
      [rpm] + exit 0
      [rpm] + umask 022
      [rpm] + cd /tmp/zkpython_build_nshi/BUILD
      [rpm] + LANG=C
      [rpm] + export LANG
      [rpm] + unset DISPLAY
      [rpm] + /bin/mv /tmp/zkpython_build_nshi/BUILD/usr /tmp/zkpython_build_nshi/BUILD
      [rpm] /bin/mv: `/tmp/zkpython_build_nshi/BUILD/usr' and `/tmp/zkpython_build_nshi/BUILD/usr' are the same file
      [rpm] error: Bad exit status from /var/tmp/rpm-tmp.62078 (%install)
      [rpm]     Bad exit status from /var/tmp/rpm-tmp.62078 (%install)

BUILD FAILED
/home/nshi/workspace/zookeeper-3.4.5/build.xml:955: The following error occurred while executing this line:
/home/nshi/workspace/zookeeper-3.4.5/src/contrib/build.xml:75: The following error occurred while executing this line:
/home/nshi/workspace/zookeeper-3.4.5/src/contrib/zkpython/build.xml:144: '/usr/bin/rpmbuild' failed with exit code 1
-------------",[],Bug,ZOOKEEPER-1649,Major,Shining,Won't Fix,2016-03-03T16:22:27.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Build RPM Package Error on CentOS 5,2016-03-03T16:22:29.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",1.0
Thawan Kooburat,"[<JIRA Component: name='tests', id='12312427'>]",2013-02-18T23:27:48.000+0000,Thawan Kooburat,JDK7 run unit tests in random order causing intermittent WatcherTest failure. The fix is to clean up static variable that interfere with other tests. ,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1648,Minor,Thawan Kooburat,Fixed,2013-02-19T07:56:27.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Fix WatcherTest in JDK7,2014-03-13T18:17:02.000+0000,[],4.0
Arnoud Glimmerveen,[],2013-02-17T15:03:40.000+0000,Arnoud Glimmerveen,"Two recent changes related to the OSGi headers Import-Package and Export-Package (ZOOKEEPER-1334 and ZOOKEEPER-1645) were only applied to the JAR created in ant target *jar*, leaving the JAR created in target *bin-jar* (to be uploaded to Maven central) with the old (incorrect) OSGi headers.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1647,Major,Arnoud Glimmerveen,Fixed,2013-02-19T08:29:42.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,OSGi package import/export changes not applied to bin-jar,2014-03-13T18:16:57.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",4.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='tests', id='12312427'>]",2013-02-12T10:07:43.000+0000,James Page,"Misc tests fail in the c client binding under the current Ubuntu development release:

./zktest-mt 
 ZooKeeper server startedRunning 
Zookeeper_clientretry::testRetry ZooKeeper server started ZooKeeper server started : elapsed 9315 : OK
Zookeeper_operations::testAsyncWatcher1 : assertion : elapsed 1054
Zookeeper_operations::testAsyncGetOperation : assertion : elapsed 1055
Zookeeper_operations::testOperationsAndDisconnectConcurrently1 : assertion : elapsed 1066
Zookeeper_operations::testOperationsAndDisconnectConcurrently2 : elapsed 0 : OK
Zookeeper_operations::testConcurrentOperations1 : assertion : elapsed 1055
Zookeeper_init::testBasic : elapsed 1 : OK
Zookeeper_init::testAddressResolution : elapsed 0 : OK
Zookeeper_init::testMultipleAddressResolution : elapsed 0 : OK
Zookeeper_init::testNullAddressString : elapsed 0 : OK
Zookeeper_init::testEmptyAddressString : elapsed 0 : OK
Zookeeper_init::testOneSpaceAddressString : elapsed 0 : OK
Zookeeper_init::testTwoSpacesAddressString : elapsed 0 : OK
Zookeeper_init::testInvalidAddressString1 : elapsed 0 : OK
Zookeeper_init::testInvalidAddressString2 : elapsed 175 : OK
Zookeeper_init::testNonexistentHost : elapsed 92 : OK
Zookeeper_init::testOutOfMemory_init : elapsed 0 : OK
Zookeeper_init::testOutOfMemory_getaddrs1 : elapsed 0 : OK
Zookeeper_init::testOutOfMemory_getaddrs2 : elapsed 1 : OK
Zookeeper_init::testPermuteAddrsList : elapsed 0 : OK
Zookeeper_close::testIOThreadStoppedOnExpire : assertion : elapsed 1056
Zookeeper_close::testCloseUnconnected : elapsed 0 : OK
Zookeeper_close::testCloseUnconnected1 : elapsed 91 : OK
Zookeeper_close::testCloseConnected1 : assertion : elapsed 1056
Zookeeper_close::testCloseFromWatcher1 : assertion : elapsed 1076
Zookeeper_simpleSystem::testAsyncWatcherAutoReset ZooKeeper server started : elapsed 12155 : OK
Zookeeper_simpleSystem::testDeserializeString : elapsed 0 : OK
Zookeeper_simpleSystem::testNullData : elapsed 1031 : OK
Zookeeper_simpleSystem::testIPV6 : elapsed 1005 : OK
Zookeeper_simpleSystem::testPath : elapsed 1024 : OK
Zookeeper_simpleSystem::testPathValidation : elapsed 1053 : OK
Zookeeper_simpleSystem::testPing : elapsed 17287 : OK
Zookeeper_simpleSystem::testAcl : elapsed 1019 : OK
Zookeeper_simpleSystem::testChroot : elapsed 3052 : OK
Zookeeper_simpleSystem::testAuth : assertion : elapsed 7010
Zookeeper_simpleSystem::testHangingClient : elapsed 1015 : OK
Zookeeper_simpleSystem::testWatcherAutoResetWithGlobal ZooKeeper server started ZooKeeper server started ZooKeeper server started : elapsed 20556 : OK
Zookeeper_simpleSystem::testWatcherAutoResetWithLocal ZooKeeper server started ZooKeeper server started ZooKeeper server started : elapsed 20563 : OK
Zookeeper_simpleSystem::testGetChildren2 : elapsed 1041 : OK
Zookeeper_multi::testCreate : elapsed 1017 : OK
Zookeeper_multi::testCreateDelete : elapsed 1007 : OK
Zookeeper_multi::testInvalidVersion : elapsed 1011 : OK
Zookeeper_multi::testNestedCreate : elapsed 1009 : OK
Zookeeper_multi::testSetData : elapsed 6019 : OK
Zookeeper_multi::testUpdateConflict : elapsed 1014 : OK
Zookeeper_multi::testDeleteUpdateConflict : elapsed 1007 : OK
Zookeeper_multi::testAsyncMulti : elapsed 2001 : OK
Zookeeper_multi::testMultiFail : elapsed 1006 : OK
Zookeeper_multi::testCheck : elapsed 1020 : OK
Zookeeper_multi::testWatch : elapsed 2013 : OK
Zookeeper_watchers::testDefaultSessionWatcher1zktest-mt: tests/ZKMocks.cc:271: SyncedBoolCondition DeliverWatchersWrapper::isDelivered() const: Assertion `i<1000' failed.
Aborted (core dumped)

It would appear that the zookeeper connection does not transition to connected within the required time; I increased the time allowed but no change.

Ubuntu raring has glibc 2.17; the test suite works fine on previous Ubuntu releases and this is the only difference that stood out.

Interestingly the cli_mt worked just fine connecting to the same zookeeper instance that the tests left lying around so I'm assuming this is a test error rather than an actual bug.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1646,Blocker,James Page,Fixed,2013-10-17T17:08:55.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,mt c client tests fail on Ubuntu Raring,2014-03-13T18:17:03.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.5.0', id='12316644'>]",5.0
Arnoud Glimmerveen,[],2013-02-12T07:08:41.000+0000,Arnoud Glimmerveen,"The ZooKeeper bundle relies on three packages it currently does not declare in the Import-Package MANIFEST header: {{javax.security.auth.callback}} , {{javax.security.auth.login}} and {{javax.security.sasl}} . By adding these the ZooKeeper jar will be a valid OSGi bundle.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1645,Major,Arnoud Glimmerveen,Fixed,2013-02-16T00:50:59.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeper OSGi package imports not complete,2014-03-13T18:16:59.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",5.0
Erik Anderson,"[<JIRA Component: name='c client', id='12312380'>]",2013-02-09T01:44:26.000+0000,Erik Anderson,"Note: While I am using a really old version of ZK, I did do enough ""SVN Blame"" operations to realize that this code hasn't changed.

I am currently attempting to compile the C client under MSVC 2005 arch=x64.  There are three things I can see with fetch_and_add() inside of /src/c/src/mt_adapter.c

(1) MSVC 2005 64bit will not compile inline _asm sections.  I'm moderately sure this code is x86-specific so I'm unsure whether it should attempt to either.

(2) The Windows intrinsic InterlockedExchangeAdd [http://msdn.microsoft.com/en-us/library/windows/desktop/ms683597(v=vs.85).aspx] appears to do the same thing this code is attempting to do

(3) I'm really rusty on my assembly, but why are we doing two separate XADD operations here, and is the code as-written anything approaching atomicity?

If you want an official patch I likely can do an SVN checkout and submit a patch the replaces the entire #else on lines 495-505 with a ""return InterlockedExchangeAdd(operand, incr);""

Usually when I'm scratching my head this badly there's something I'm missing though.  As far as I can tell there has been no prior discussion on this code.","[<JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.4.11', id='12339207'>]",Bug,ZOOKEEPER-1643,Major,Erik Anderson,Fixed,2013-02-20T05:26:47.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"Windows: fetch_and_add not 64bit-compatible, may not be correct",2017-08-01T15:42:31.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",6.0
Flavio Paiva Junqueira,[],2013-02-08T10:30:06.000+0000,Flavio Paiva Junqueira,"The leader server currently loads the database before running leader election when trying to figure out the zxid it needs to use for the election and again when it starts leading. This is problematic for larger databases so we should remove the redundant load if possible. 

The code references are:

# getLastLoggedZxid() in QuorumPeer;
# loadData() in ZooKeeperServer.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1642,Major,Flavio Paiva Junqueira,Fixed,2013-05-16T17:33:30.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Leader loading database twice,2014-03-13T18:17:11.000+0000,[],7.0
Ben Hartshorne,"[<JIRA Component: name='contrib', id='12312700'>]",2013-02-06T18:17:35.000+0000,Ben Hartshorne,"The ganglia python module uses 'slope=positive' when submitting zk_packets_received and zk_packets_sent.  This results in a graph that is jagged (alternating valid results with zeros) at the highest resolution and under-represents the actual value at all averaged resolutions (>1hr).

The module should be changed to calculate the delta in requests and report requests per second instead.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1641,Minor,Ben Hartshorne,Fixed,2013-02-16T01:00:08.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Using slope=positive results in a jagged ganglia graph of packets rcvd/sent,2013-02-16T11:02:39.000+0000,[],3.0
,[],2013-02-03T03:05:41.000+0000,Alexander Shraer,"Before the call to zk.getZKDatabase().deserializeSnapshot in Learner.java, 
zk.getZKDatabase().getDataTree().getNode(""/zookeeper"") == zk.getZKDatabase().getDataTree().procDataNode, which means that this is the same znode, as it should be. 

However, after this call, they are not equal. The node actually being used in client operations is zk.getZKDatabase().getDataTree().getNode(""/zookeeper""), but the other old node procDataNode is still there and not replaced (in fact it is a final field).",[],Bug,ZOOKEEPER-1639,Major,Alexander Shraer,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zk.getZKDatabase().deserializeSnapshot adds new system znodes instead of replacing existing ones,2013-10-08T20:31:48.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",3.0
,[],2013-02-01T20:29:21.000+0000,Robert Schultheis,"We are getting an intermittent segfault.  This is OSX, zookeeper compiled using brew.  I've tried 3.4.3 - 3.4.5.

I used GDB to get the following backtrace:

{code}
Program received signal EXC_BAD_ACCESS, Could not access memory. 
Reason: 13 at address: 0x0000000000000000 
[Switching to process 10366 thread 0x1d03] 
0x00007fff8e0984f0 in strlen () 
(gdb) backtrace 
#0 0x00007fff8e0984f0 in strlen () 
#1 0x00000001004983cc in prepend_string () 
#2 0x0000000100498451 in Request_path_init () 
#3 0x0000000100499e94 in zoo_awexists () 
#4 0x000000010049a036 in zoo_wexists () 
#5 0x000000010048170b in pyzoo_exists () 
#6 0x000000010008c5d8 in PyEval_EvalFrameEx () 
#7 0x000000010008ecd8 in PyEval_EvalCodeEx () 
#8 0x000000010008ee6c in PyEval_EvalCode () 
#9 0x000000010008be0a in PyEval_EvalFrameEx () 
#10 0x000000010008ecd8 in PyEval_EvalCodeEx () 
#11 0x000000010008ee6c in PyEval_EvalCode () 
#12 0x000000010008be0a in PyEval_EvalFrameEx () 
#13 0x000000010008ecd8 in PyEval_EvalCodeEx () 
#14 0x000000010002cabf in PyClassMethod_New () 
#15 0x000000010000bd32 in PyObject_Call () 
#16 0x000000010008c5ec in PyEval_EvalFrameEx () 
#17 0x000000010008ecd8 in PyEval_EvalCodeEx () 
#18 0x000000010002cabf in PyClassMethod_New () 
#19 0x000000010000bd32 in PyObject_Call () 
#20 0x000000010001a6e9 in PyInstance_New () 
#21 0x000000010000bd32 in PyObject_Call () 
#22 0x0000000100055c5d in _PyObject_SlotCompare () 
#23 0x000000010000bd32 in PyObject_Call () 
#24 0x000000010008bf63 in PyEval_EvalFrameEx () 
#25 0x000000010008ecd8 in PyEval_EvalCodeEx () 
#26 0x000000010008ee6c in PyEval_EvalCode () 
#27 0x000000010008be0a in PyEval_EvalFrameEx () 
#28 0x000000010008edf7 in PyEval_EvalCode () 
#29 0x000000010008be0a in PyEval_EvalFrameEx () 
#30 0x000000010008ecd8 in PyEval_EvalCodeEx () 
#31 0x000000010002cabf in PyClassMethod_New () 
#32 0x000000010000bd32 in PyObject_Call () 
#33 0x000000010001a6e9 in PyInstance_New () 
#34 0x000000010000bd32 in PyObject_Call () 
#35 0x0000000100087c40 in PyEval_CallObjectWithKeywords () 
#36 0x00000001000b940d in initthread () 
#37 0x00007fff8e0448bf in _pthread_start () 
#38 0x00007fff8e047b75 in thread_start () 
{code}

",[],Bug,ZOOKEEPER-1637,Major,Robert Schultheis,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Intermittent Segfault with zkpython in pyzoo_exists  ,2013-02-03T07:32:11.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.4.5', id='12321883'>]",1.0
Michael K. Edwards,"[<JIRA Component: name='c client', id='12312380'>]",2013-01-31T03:54:46.000+0000,Thawan Kooburat,"deserialize_response for multi operation don't handle the case where the server fail to send back response. (Eg. when multi packet is too large) 

c-client will try to process completion of all sub-request as if the operation is successful and will eventually cause SIGSEGV","[<JIRA Version: name='3.4.15', id='12344988'>, <JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>]",Bug,ZOOKEEPER-1636,Critical,Thawan Kooburat,Fixed,2018-12-10T14:29:32.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,c-client crash when zoo_amulti failed ,2019-08-02T00:50:17.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",5.0
Alexander Shraer,"[<JIRA Component: name='server', id='12312382'>]",2013-01-30T19:14:03.000+0000,Alexander Shraer,"Currently the first message a server sends to another server includes just one field - the server's id (long). This is in QuorumCnxManager.java. This makes changes to the information passed during this initial connection very difficult. This patch will change the first field of the message to be a protocol version (a negative number that can't be a server id). The second field will be the server id. The third field is number of bytes in the remainder of the message. A 3.4 server will read the first field as before, but if this is a negative number it will read the second field to find the server id, and then remove the remainder of the message from the stream. This will not affect 3.4 since 3.4 and earlier servers send just the server id (so the code in the patch will not run unless there is a server > 3.4 trying to connect). This will, however, provide the necessary flexibility for future releases as well as an upgrade path from 3.4","[<JIRA Version: name='3.4.6', id='12323310'>]",Bug,ZOOKEEPER-1633,Major,Alexander Shraer,Fixed,2013-04-02T06:32:44.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Introduce a protocol version to connection initiation message,2014-03-13T18:16:54.000+0000,[],6.0
Flavio Paiva Junqueira,"[<JIRA Component: name='c client', id='12312380'>]",2013-01-29T22:49:00.000+0000,Colin McCabe,"Fix two memory leaks revealed by running:
{code}
valgrind --leak-check=full ./.libs/cli_st 127.0.0.1:2182
create /foo
quit
{code}","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1632,Minor,Colin McCabe,Fixed,2013-12-04T10:29:02.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,fix memory leaks in cli_st ,2014-03-13T18:17:10.000+0000,[],6.0
,[],2013-01-29T19:19:19.000+0000,Colin McCabe,"I tried running ""make run-check"" on the cppunit tests, and got the following error:

{code}
tests/TestOperations.cc:270: Assertion: equality assertion failed [Expected: 1, Actual : 0]
tests/TestOperations.cc:339: Assertion: assertion failed [Expression: timeMock==zh->last_recv]
tests/TestOperations.cc:407: Assertion: equality assertion failed [Expected: 1, Actual : 0]
tests/TestOperations.cc:212: Assertion: equality assertion failed [Expected: -7, Actual : 0]
{code}

I thought this might be an environment issue, but I was able to reproduce it on both Ubuntu 12.04 and OpenSUSE 12.1",[],Bug,ZOOKEEPER-1631,Minor,Colin McCabe,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,cppunit test TestOperations.cc fails,2013-01-29T19:19:19.000+0000,"[<JIRA Version: name='3.4.6', id='12323310'>]",1.0
Alexander Shraer,"[<JIRA Component: name='tests', id='12312427'>]",2013-01-25T01:10:42.000+0000,Alexander Shraer,"It seems that testTransactionLogCorruption is very flaky,for example fails here:

https://builds.apache.org/job/ZooKeeper-trunk-jdk7/500/
https://builds.apache.org/job/ZooKeeper-trunk-jdk7/502/
https://builds.apache.org/job/ZooKeeper-trunk-jdk7/503/#showFailuresLink

also fails for older builds (no longer on the website), for example all builds from 381 to 399.
","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1629,Major,Alexander Shraer,Fixed,2013-07-15T02:07:46.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,testTransactionLogCorruption occasionally fails,2014-03-13T18:17:04.000+0000,[],10.0
Gabriel Reid,"[<JIRA Component: name='documentation', id='12312422'>, <JIRA Component: name='java client', id='12312381'>]",2013-01-24T14:19:42.000+0000,Gabriel Reid,"The documented set of allowable characters in ZooKeeper node names in the Programmer's Guide is not entirely in line with the code.

The range of non-printable ASCII characters in the doc ends too early (i.e. 0x19 instead of 0x1F).

The range checking code in PathUtils also includes off-by-one errors, so that characters that are on the border of being unallowable are actually allowed by the code.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1628,Major,Gabriel Reid,Fixed,2013-01-25T07:09:06.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Documented list of allowable characters in ZK doc not in line with code,2013-01-25T07:09:06.000+0000,[],2.0
Alexander Shraer,"[<JIRA Component: name='scripts', id='12312384'>]",2013-01-19T21:33:56.000+0000,Alexander Shraer,"zkServer.sh is currently looking for ""clientPort"" entry in the static configuration file and uses it to contact the server. 

With ZOOKEEPER-1411 clientPort is part of the dynamic configuration, and may appear in the separate dynamic configuration file. The ""clientPort"" entry may no longer be in the static config file. 

With the proposed patch zkServer.sh first looks in the old (static) config file, then if clientPort is not there, it figures out the id of the server by looking at myid file, and then using that id finds the client port in the dynamic config file. ","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1625,Major,Alexander Shraer,Fixed,2013-01-23T02:56:22.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"zkServer.sh is looking for clientPort in config file, but it may no longer be there with ZK-1411",2013-01-23T19:24:32.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Thawan Kooburat,"[<JIRA Component: name='server', id='12312382'>]",2013-01-18T03:11:10.000+0000,Thawan Kooburat,"We found this issue when trying to issue multiple instances of the following multi-op concurrently

multi {
1. create sequential node /a- 
2. create node /b
}

The expected result is that only the first multi-op request should success and the rest of request should fail because /b is already exist

However, the reported result is that the subsequence multi-op failed because of sequential node creation failed which is not possible.

Below is the return code for each sub-op when issuing 3 instances of the above multi-op asynchronously

1. ZOK, ZOK
2. ZOK, ZNODEEXISTS,
3. ZNODEEXISTS, ZRUNTIMEINCONSISTENCY,

When I added more debug log. The cause is that PrepRequestProcessor rollback outstandingChanges of the second multi-op incorrectly causing sequential node name generation to be incorrect. Below is the sequential node name generated by PrepRequestProcessor

1. create /a-0001
2. create /a-0003
3. create /a-0001

The bug is getPendingChanges() method. In failed to copied ChangeRecord for the parent node (""/"").  So rollbackPendingChanges() cannot restore the right previous change record of the parent node when aborting the second multi-op

The impact of this bug is that sequential node creation on the same parent node may fail until the previous one is committed. I am not sure if there is other implication or not.  
","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1624,Critical,Thawan Kooburat,Fixed,2013-10-10T19:06:12.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,PrepRequestProcessor abort multi-operation incorrectly,2014-03-13T18:16:57.000+0000,[],6.0
,[],2013-01-16T16:46:51.000+0000,Christian Wuertz,"First of all, I'm just running some test and thus I don't wan't/need any authentication at all. So I didn't configured any. But running my Java client with an Oracle JVM (1.6.38) I run into the following problem:

`2013-01-16 17:40:30,659 [main] INFO  org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=192.168.2.28:2181 sessionTimeout=5000 watcher=master.Master@eb42cbf
2013-01-16 17:40:30,674 [main] DEBUG org.apache.zookeeper.ClientCnxn - zookeeper.disableAutoWatchReset is false
2013-01-16 17:40:30,698 [Thread-0] DEBUG master.Master - Master waits...
2013-01-16 17:40:30,701 [main-SendThread(Teots-PC:2181)] INFO  org.apache.zookeeper.ClientCnxn - Opening socket connection to server Teots-PC/192.168.2.28:2181. Will not attempt to authenticate using SASL (Unable to locate a login configuration)
2013-01-16 17:40:30,706 [main-SendThread(Teots-PC:2181)] INFO  org.apache.zookeeper.ClientCnxn - Socket connection established to Teots-PC/192.168.2.28:2181, initiating session
2013-01-16 17:40:30,708 [main-SendThread(Teots-PC:2181)] DEBUG org.apache.zookeeper.ClientCnxn - Session establishment request sent on Teots-PC/192.168.2.28:2181
2013-01-16 17:40:30,709 [main-SendThread(Teots-PC:2181)] DEBUG org.apache.zookeeper.client.ZooKeeperSaslClient - Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
2013-01-16 17:40:30,730 [main-SendThread(Teots-PC:2181)] INFO  org.apache.zookeeper.ClientCnxn - Session establishment complete on server Teots-PC/192.168.2.28:2181, sessionid = 0x13c44254fd70003, negotiated timeout = 5000
2013-01-16 17:40:30,732 [main-EventThread] DEBUG master.Master - Master recieved an event: None
2013-01-16 17:40:30,732 [main-SendThread(Teots-PC:2181)] DEBUG org.apache.zookeeper.client.ZooKeeperSaslClient - Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
2013-01-16 17:40:30,732 [main-EventThread] DEBUG master.Master - Master's state: SyncConnected
2013-01-16 17:40:30,732 [main-SendThread(Teots-PC:2181)] DEBUG org.apache.zookeeper.client.ZooKeeperSaslClient - Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration`

This does not happen with an OpenJDK JVM.",[],Bug,ZOOKEEPER-1623,Major,Christian Wuertz,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Authentication using SASL,2013-10-22T18:39:03.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",3.0
Eric C. Newton,[],2013-01-16T16:29:23.000+0000,Eric C. Newton,"Someone decided to use a large number for their myid file.  This cause session ids to go negative, and our software (Apache Accumulo) did not handle this very well.  While diagnosing the problem, I noticed this in SessionImpl:

{noformat}
   public static long initializeNextSession(long id) {
        long nextSid = 0;
        nextSid = (System.currentTimeMillis() << 24) >> 8;
        nextSid =  nextSid | (id <<56);
        return nextSid;
    }
{noformat}

When the 40th bit in System.currentTimeMillis() is a one, sign extension will fill the upper 8 bytes of nextSid, and id will not make the session id unique.  I recommend changing the right shift to the logical shift:

{noformat}
   public static long initializeNextSession(long id) {
        long nextSid = 0;
        nextSid = (System.currentTimeMillis() << 24) >>> 8;
        nextSid =  nextSid | (id <<56);
        return nextSid;
    }
{noformat}

But, we have until the year 2022 before we have to worry about it.
","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1622,Trivial,Eric C. Newton,Fixed,2013-12-16T06:30:20.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,session ids will be negative in the year 2022,2014-03-13T18:16:55.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",5.0
Michi Mutsuzaki,"[<JIRA Component: name='server', id='12312382'>]",2013-01-16T15:24:08.000+0000,David Arthur,"The disk that ZooKeeper was using filled up. During a snapshot write, I got the following exception

2013-01-16 03:11:14,098 - ERROR [SyncThread:0:SyncRequestProcessor@151] - Severe unrecoverable error, exiting
java.io.IOException: No space left on device
        at java.io.FileOutputStream.writeBytes(Native Method)
        at java.io.FileOutputStream.write(FileOutputStream.java:282)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at org.apache.zookeeper.server.persistence.FileTxnLog.commit(FileTxnLog.java:309)
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.commit(FileTxnSnapLog.java:306)
        at org.apache.zookeeper.server.ZKDatabase.commit(ZKDatabase.java:484)
        at org.apache.zookeeper.server.SyncRequestProcessor.flush(SyncRequestProcessor.java:162)
        at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:101)

Then many subsequent exceptions like:

2013-01-16 15:02:23,984 - ERROR [main:Util@239] - Last transaction was partial.
2013-01-16 15:02:23,985 - ERROR [main:ZooKeeperServerMain@63] - Unexpected exception, exiting abnormally
java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:375)
        at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
        at org.apache.zookeeper.server.persistence.FileHeader.deserialize(FileHeader.java:64)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.inStreamCreated(FileTxnLog.java:558)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.createInputArchive(FileTxnLog.java:577)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.goToNextLog(FileTxnLog.java:543)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:625)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.init(FileTxnLog.java:529)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.<init>(FileTxnLog.java:504)
        at org.apache.zookeeper.server.persistence.FileTxnLog.read(FileTxnLog.java:341)
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:130)
        at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:223)
        at org.apache.zookeeper.server.ZooKeeperServer.loadData(ZooKeeperServer.java:259)
        at org.apache.zookeeper.server.ZooKeeperServer.startdata(ZooKeeperServer.java:386)
        at org.apache.zookeeper.server.NIOServerCnxnFactory.startup(NIOServerCnxnFactory.java:138)
        at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:112)
        at org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:86)
        at org.apache.zookeeper.server.ZooKeeperServerMain.main(ZooKeeperServerMain.java:52)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:116)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)


It seems to me that writing the transaction log should be fully atomic to avoid such situations. Is this not the case?

","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-1621,Major,David Arthur,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,ZooKeeper does not recover from crash when disk was full,2022-02-03T08:36:22.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",28.0
Thawan Kooburat,"[<JIRA Component: name='server', id='12312382'>]",2013-01-15T04:18:52.000+0000,Alexander Shraer,"New code (committed in ZK-1504) opens selectors but doesn't close them.
Specifically AbstractSelectThread in its constructor does 

this.selector = Selector.open();

But possibly also elsewhere. Tests fail for me with the following message:

java.io.IOException: Too many open files
	at sun.nio.ch.EPollArrayWrapper.epollCreate(Native Method)
	at sun.nio.ch.EPollArrayWrapper.<init>(EPollArrayWrapper.java:69)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:52)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:18)
	at java.nio.channels.Selector.open(Selector.java:209)
	at org.apache.zookeeper.server.NIOServerCnxnFactory$AbstractSelectThread.<init>(NIOServerCnxnFactory.java:128)
	at org.apache.zookeeper.server.NIOServerCnxnFactory$AcceptThread.<init>(NIOServerCnxnFactory.java:177)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.configure(NIOServerCnxnFactory.java:663)
	at org.apache.zookeeper.server.ServerCnxnFactory.createFactory(ServerCnxnFactory.java:127)
	at org.apache.zookeeper.server.quorum.QuorumPeer.<init>(QuorumPeer.java:709)
	at org.apache.zookeeper.test.QuorumBase.startServers(QuorumBase.java:177)
	at org.apache.zookeeper.test.QuorumBase.setUp(QuorumBase.java:113)
	at org.apache.zookeeper.test.QuorumBase.setUp(QuorumBase.java:71)
	at org.apache.zookeeper.test.ReconfigTest.setUp(ReconfigTest.java:56)
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1620,Major,Alexander Shraer,Fixed,2013-01-25T06:47:12.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,NIOServerCnxnFactory (new code introduced in ZK-1504) opens selectors but never closes them,2013-05-02T02:30:00.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
,[],2013-01-09T10:46:51.000+0000,wangwei,"    2012-12-31 10:51:41,562-[TS] INFO main-EventThread org.I0Itec.zkclient.ZkClient - zookeeper state changed (Disconnected)  
    2012-12-31 10:51:43,008-[TS] INFO main-SendThread(17.22.17.1:2181) org.apache.zookeeper.ClientCnxn - Opening socket connection to server /17.22.17.1:2181. Will not attempt to authenticate using SASL (unknown error)  
    2012-12-31 10:51:43,009-[TS] INFO main-SendThread(17.22.17.1:2181) org.apache.zookeeper.ClientCnxn - Socket connection established to /17.22.17.1:2181, initiating session  
    2012-12-31 10:51:43,011-[TS] WARN main-SendThread(17.22.17.1:2181) org.apache.zookeeper.ClientCnxnSocket - Connected to an old server; r-o mode will be unavailable  
    2012-12-31 10:51:43,011-[TS] INFO main-SendThread(17.22.17.1:2181) org.apache.zookeeper.ClientCnxn - Session establishment complete on server /17.22.17.1:2181, sessionid = 0x13b8a23254100be, negotiated timeout = 6000  
    2012-12-31 10:51:43,012-[TS] INFO main-EventThread org.I0Itec.zkclient.ZkClient - zookeeper state changed (SyncConnected)  
    2012-12-31 10:51:47,012-[TS] INFO main-SendThread(17.22.17.1:2181) org.apache.zookeeper.ClientCnxn - Client session timed out, have not heard from server in 4002ms for sessionid 0x13b8a23254100be, closing socket connection and attempting reconnect  


zookeeper client is 3.4.4
zookeeper server is 3.3.4

user 3.4.4 client connection 3.3.4 server",[],Bug,ZOOKEEPER-1617,Major,wangwei,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zookeeper version error log info ?,2013-01-22T22:40:08.000+0000,[],2.0
,[],2013-01-09T00:35:47.000+0000,Todd Lipcon,"We recently had an issue with ZooKeeper sessions acting strangely due to a bad NTP setup on a set of hosts. Looking at the code, ZK seems to use System.currentTimeMillis to measure durations or intervals in many places. This is bad since that time can move backwards or skip ahead by several minutes. Instead, it should use System.nanoTime (or a wrapper such as Guava's Stopwatch class)",[],Bug,ZOOKEEPER-1616,Major,Todd Lipcon,Duplicate,2015-04-11T21:44:32.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,time calculations should use a monotonic clock,2015-04-11T21:44:32.000+0000,[],9.0
,"[<JIRA Component: name='c client', id='12312380'>]",2013-01-02T23:36:55.000+0000,Richard Dermer,"The windows C MultiThreaded client crashes when usng the zoo_multi APis.  The underlying is that the mutex and condition variables need to be initialized with pthread_cond_init and pthread_mutex_init.

Attached are the files I've modified to make this work.  In the modified files I've added a ""multi"" command to cli that when Cli.exe (mt build) is run on window's without the rest of the fixes will crash.",[],Bug,ZOOKEEPER-1614,Major,Richard Dermer,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zoo_multi c MT client windows crash,2013-01-02T23:42:30.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",1.0
Edward Ribeiro,"[<JIRA Component: name='documentation', id='12312422'>]",2012-12-30T23:47:09.000+0000,Edward Ribeiro,While fiddling with docbook to solve the broken links of ZOOKEEPER-1488 I noted that all the documentation's copyright notice still has the year 2008 only. I am submitting a patch a fix this.,"[<JIRA Version: name='3.3.7', id='12321882'>, <JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1613,Trivial,Edward Ribeiro,Fixed,2013-01-25T07:29:33.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,The documentation still points to 2008 in the copyright notice,2014-03-13T18:16:55.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",3.0
,[],2012-12-27T06:57:06.000+0000,suja s,"Once zookeeper data dir disk becomes full, the process gets shut down.
{noformat}
2012-12-14 13:22:26,959 [myid:2] - ERROR [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:2181:ZooKeeperServer@276] - Severe unrecoverable error, exiting
java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:282)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
	at java.util.zip.CheckedOutputStream.write(CheckedOutputStream.java:56)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at java.io.FilterOutputStream.write(FilterOutputStream.java:80)
	at org.apache.jute.BinaryOutputArchive.writeBuffer(BinaryOutputArchive.java:119)
	at org.apache.zookeeper.server.DataNode.serialize(DataNode.java:168)
	at org.apache.jute.BinaryOutputArchive.writeRecord(BinaryOutputArchive.java:123)
	at org.apache.zookeeper.server.DataTree.serializeNode(DataTree.java:1115)
	at org.apache.zookeeper.server.DataTree.serializeNode(DataTree.java:1130)
	at org.apache.zookeeper.server.DataTree.serializeNode(DataTree.java:1130)
	at org.apache.zookeeper.server.DataTree.serialize(DataTree.java:1179)
	at org.apache.zookeeper.server.util.SerializeUtils.serializeSnapshot(SerializeUtils.java:138)
	at org.apache.zookeeper.server.persistence.FileSnap.serialize(FileSnap.java:213)
	at org.apache.zookeeper.server.persistence.FileSnap.serialize(FileSnap.java:230)
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.save(FileTxnSnapLog.java:242)
	at org.apache.zookeeper.server.ZooKeeperServer.takeSnapshot(ZooKeeperServer.java:274)
	at org.apache.zookeeper.server.quorum.Learner.syncWithLeader(Learner.java:407)
	at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:82)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:759)
{noformat}

Later disk space is cleared and zk started again. Startup of zk fails as it is not able to read snapshot properly. (Since load from disk failed it is not able to join peers in the quorum and get a snapshot diff)
{noformat}

2012-12-14 16:20:31,489 [myid:2] - INFO  [main:FileSnap@83] - Reading snapshot ../dataDir/version-2/snapshot.1000000042
2012-12-14 16:20:31,564 [myid:2] - ERROR [main:QuorumPeer@472] - Unable to load database on disk
java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:375)
	at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
	at org.apache.zookeeper.server.persistence.FileHeader.deserialize(FileHeader.java:64)
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.inStreamCreated(FileTxnLog.java:558)
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.createInputArchive(FileTxnLog.java:577)
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.goToNextLog(FileTxnLog.java:543)
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:625)
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.init(FileTxnLog.java:529)
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.<init>(FileTxnLog.java:504)
	at org.apache.zookeeper.server.persistence.FileTxnLog.read(FileTxnLog.java:341)
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:132)
	at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:223)
	at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:436)
	at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:428)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:152)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:111)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)
2012-12-14 16:20:31,566 [myid:2] - ERROR [main:QuorumPeerMain@89] - Unexpected exception, exiting abnormally
java.lang.RuntimeException: Unable to run quorum server 
	at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:473)
	at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:428)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:152)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:111)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:375)
	at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
	at org.apache.zookeeper.server.persistence.FileHeader.deserialize(FileHeader.java:64)
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.inStreamCreated(FileTxnLog.java:558)
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.createInputArchive(FileTxnLog.java:577)
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.goToNextLog(FileTxnLog.java:543)
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:625)
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.init(FileTxnLog.java:529)
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.<init>(FileTxnLog.java:504)
	at org.apache.zookeeper.server.persistence.FileTxnLog.read(FileTxnLog.java:341)
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:132)

 {noformat}


",[],Bug,ZOOKEEPER-1612,Major,suja s,Duplicate,2013-01-16T18:37:45.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper unable to recover and start once datadir disk is full and disk space cleared,2013-01-16T18:37:45.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",3.0
,[],2012-12-27T06:23:16.000+0000,prabhu sharma,,[],Bug,ZOOKEEPER-1611,Major,prabhu sharma,Invalid,2012-12-27T08:57:46.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,cbcfhbf vfgbfb,2012-12-27T08:57:46.000+0000,[],2.0
Edward Ribeiro,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='quorum', id='12312379'>]",2012-12-26T17:31:12.000+0000,Edward Ribeiro,"The classes org.apache.zookeeper.client.ZooKeeperSaslClient.java and 
org.apache.zookeeper.server.quorum.flexible.QuorumHierarchical.java compare Strings and/or Longs using referential equality.

Usually, this is not a problem because the Longs are cached and Strings are interned, but I myself  had problems with those kind of comparisons in the past because one production JVM didn't reused the objects.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1610,Critical,Edward Ribeiro,Fixed,2013-10-11T19:19:59.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Some classes are using == or != to compare Long/String objects instead of .equals(),2014-03-13T18:17:03.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.5.0', id='12316644'>]",4.0
lixiaofeng,"[<JIRA Component: name='tests', id='12312427'>]",2012-12-21T22:19:46.000+0000,Patrick D. Hunt,"ZkDatabaseCorruptionTest is failing intermittently on jenkins with:

""Error Message: the last server is not the leader""

Seeing this on jdk7/openjdk7/solaris - 3 times in the last month.

https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper-trunk-openjdk7/2/testReport/junit/org.apache.zookeeper.test/ZkDatabaseCorruptionTest/testCorruption/","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1606,Major,Patrick D. Hunt,Fixed,2013-02-19T08:19:46.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,intermittent failures in ZkDatabaseCorruptionTest on jenkins,2014-03-13T18:17:12.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.5.0', id='12316644'>]",4.0
Flavio Paiva Junqueira,"[<JIRA Component: name='tests', id='12312427'>]",2012-12-16T08:27:08.000+0000,Patrick D. Hunt,"StaticHostProviderTest method testUpdateClientMigrateOrNot hangs forever.

On my laptop getHostName for 10.10.10.* takes 5+ seconds per call. As a result this method effectively runs forever.

Every time I run this test it hangs. Consistent.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1603,Blocker,Patrick D. Hunt,Fixed,2013-09-26T21:16:35.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,StaticHostProviderTest testUpdateClientMigrateOrNot hangs,2014-03-13T18:17:06.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
Alexander Shraer,"[<JIRA Component: name='server', id='12312382'>]",2012-12-15T00:40:58.000+0000,Patrick D. Hunt,"The following patch broke an API that's in use by HBase. Otherwise current trunk compiles fine when used by hbase:

bq. ZOOKEEPER-1411. Consolidate membership management, distinguish between static and dynamic configuration parameters (Alex Shraer via breed)

Considering it a blocker even though it's not really a ""public"" API. If possible we should add back ""getServers"" method on QuorumPeerConfig to reduce friction for the hbase team.
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1602,Blocker,Patrick D. Hunt,Fixed,2012-12-16T06:44:34.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,a change to QuorumPeerConfig's API broke compatibility with HBase,2012-12-16T11:04:20.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",3.0
Patrick D. Hunt,[],2012-12-10T06:43:48.000+0000,Deepa Muthunoori,"Closing of session is not deleting all the ephemeral nodes. 

(Eg: From the log, session Id:0x23b6ad21d160000 creates two ephemerals(/CONFIGNODE/NP2147483647 and /ACTIVE/192.168.11.94) but when the session expires, only /CONFIGNODE/NP2147483647 is getting deleted)",[],Bug,ZOOKEEPER-1600,Major,Deepa Muthunoori,Not A Problem,2013-02-16T01:21:43.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Ephemeral node not getting deleted,2013-02-16T01:21:43.000+0000,[],2.0
Skye Wanderman-Milne,"[<JIRA Component: name='quorum', id='12312379'>]",2012-12-07T18:54:16.000+0000,Skye Wanderman-Milne,"When a 3.3 server attempts to join an existing quorum lead by a 3.4 server, the 3.3 server is disconnected while trying to download the leader's snapshot. The 3.3 server restarts and starts the process over again, but is never able to join the quorum.

3.3 server log:
{code}
2012-12-07 10:44:34,582 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2183:Learner@294] - Getting a snapshot from leader
2012-12-07 10:44:34,582 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2183:Learner@325] - Setting leader epoch 12
2012-12-07 10:44:54,604 - WARN  [QuorumPeer:/0:0:0:0:0:0:0:0:2183:Follower@82] - Exception when following the leader
java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:392)
        at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
        at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:84)
        at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:108)
        at org.apache.zookeeper.server.quorum.Learner.readPacket(Learner.java:148)
        at org.apache.zookeeper.server.quorum.Learner.syncWithLeader(Learner.java:332)
        at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:75)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:645)
2012-12-07 10:44:54,605 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2183:Follower@165] - shutdown called
java.lang.Exception: shutdown Follower
        at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:165)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:649)
{code}

3.4 leader log:
{code}
2012-12-07 10:51:35,178 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection$Messenger$WorkerReceiver@273] - Backward compatibility mode, server id=3
2012-12-07 10:51:35,178 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection@542] - Notification: 3 (n.leader), 0x1100000000 (n.zxid), 0x2 (n.round), LOOKING (n.state), 3 (n.sid), 0x11 (n.peerEPoch), LEADING (my state)
2012-12-07 10:51:35,182 [myid:2] - INFO  [LearnerHandler-/127.0.0.1:37654:LearnerHandler@263] - Follower sid: 3 : info : org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@262f4873
2012-12-07 10:51:35,182 [myid:2] - INFO  [LearnerHandler-/127.0.0.1:37654:LearnerHandler@318] - Synchronizing with Follower sid: 3 maxCommittedLog=0x0 minCommittedLog=0x0 peerLastZxid=0x1100000000
2012-12-07 10:51:35,182 [myid:2] - INFO  [LearnerHandler-/127.0.0.1:37654:LearnerHandler@395] - Sending SNAP
2012-12-07 10:51:35,183 [myid:2] - INFO  [LearnerHandler-/127.0.0.1:37654:LearnerHandler@419] - Sending snapshot last zxid of peer is 0x1100000000  zxid of leader is 0x1200000000sent zxid of db as 0x1200000000
2012-12-07 10:51:55,204 [myid:2] - ERROR [LearnerHandler-/127.0.0.1:37654:LearnerHandler@562] - Unexpected exception causing shutdown while sock still open
java.net.SocketTimeoutException: Read timed out
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:150)
        at java.net.SocketInputStream.read(SocketInputStream.java:121)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
        at java.io.DataInputStream.readInt(DataInputStream.java:387)
        at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
        at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:83)
        at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:108)
        at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:450)
2012-12-07 10:51:55,205 [myid:2] - WARN  [LearnerHandler-/127.0.0.1:37654:LearnerHandler@575] - ******* GOODBYE /127.0.0.1:37654 ********
{code}","[<JIRA Version: name='3.4.6', id='12323310'>]",Bug,ZOOKEEPER-1599,Blocker,Skye Wanderman-Milne,Not A Problem,2013-09-17T22:30:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,3.3 server cannot join 3.4 quorum,2014-03-13T18:17:04.000+0000,"[<JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.4.5', id='12321883'>]",7.0
Michi Mutsuzaki,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='c client', id='12312380'>]",2012-12-04T08:28:32.000+0000,Alexander Shraer,"Seems to be related to C client changes done for ZK-1355.
We're not sure why these build failures happen on Windows.

###################################################################################
########################## LAST 60 LINES OF THE CONSOLE ###########################
[...truncated 376 lines...]
  .\src\zookeeper.c(768): error C2224: left of '.count' must have struct/union type
  .\src\zookeeper.c(768): error C2065: 'i' : undeclared identifier
  .\src\zookeeper.c(770): error C2065: 'resolved' : undeclared identifier
  .\src\zookeeper.c(770): error C2224: left of '.data' must have struct/union type
  .\src\zookeeper.c(770): error C2065: 'i' : undeclared identifier
  .\src\zookeeper.c(773): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(774): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(780): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(781): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(788): error C2143: syntax error : missing ';' before 'type'
  .\src\zookeeper.c(789): error C2143: syntax error : missing ';' before 'type'
  .\src\zookeeper.c(792): error C2065: 'num_old' : undeclared identifier
  .\src\zookeeper.c(792): error C2065: 'num_new' : undeclared identifier
  .\src\zookeeper.c(794): error C2065: 'found_current' : undeclared identifier
  .\src\zookeeper.c(797): error C2065: 'num_old' : undeclared identifier
  .\src\zookeeper.c(797): error C2065: 'num_new' : undeclared identifier
  .\src\zookeeper.c(814): error C2065: 'found_current' : undeclared identifier
  .\src\zookeeper.c(819): error C2065: 'num_old' : undeclared identifier
  .\src\zookeeper.c(819): error C2065: 'num_old' : undeclared identifier
  .\src\zookeeper.c(819): error C2065: 'num_new' : undeclared identifier
  .\src\zookeeper.c(819): error C2065: 'num_old' : undeclared identifier
  .\src\zookeeper.c(819): error C2065: 'num_new' : undeclared identifier
  .\src\zookeeper.c(819): error C2065: 'num_old' : undeclared identifier
  .\src\zookeeper.c(825): error C2065: 'resolved' : undeclared identifier
  .\src\zookeeper.c(825): error C2440: '=' : cannot convert from 'int' to 'addrvec_t'
  .\src\zookeeper.c(843): error C2065: 'resolved' : undeclared identifier
  .\src\zookeeper.c(843): error C2224: left of '.data' must have struct/union type
  .\src\zookeeper.c(845): error C2065: 'resolved' : undeclared identifier
  .\src\zookeeper.c(848): error C2065: 'hosts' : undeclared identifier
  .\src\zookeeper.c(849): error C2065: 'hosts' : undeclared identifier
  .\src\zookeeper.c(850): error C2065: 'hosts' : undeclared identifier
  .\src\zookeeper.c(853): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(1177): error C2143: syntax error : missing ';' before 'const'
  .\src\zookeeper.c(1179): error C2065: 'endpoint_info' : undeclared identifier
  .\src\zookeeper.c(1883): error C2143: syntax error : missing ';' before 'type'
  .\src\zookeeper.c(1884): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(1885): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(1916): error C2143: syntax error : missing ';' before 'type'
  .\src\zookeeper.c(1920): error C2143: syntax error : missing ';' before 'type'
  .\src\zookeeper.c(1927): error C2065: 'ssoresult' : undeclared identifier
  .\src\zookeeper.c(1927): error C2065: 'enable_tcp_nodelay' : undeclared identifier
  .\src\zookeeper.c(1927): error C2065: 'enable_tcp_nodelay' : undeclared identifier
  .\src\zookeeper.c(1928): error C2065: 'ssoresult' : undeclared identifier
  .\src\zookeeper.c(1944): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(1949): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(1962): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(1963): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(2004): error C2065: 'rc' : undeclared identifier
  .\src\zookeeper.c(2004): fatal error C1003: error count exceeds 100; stopping compilation

    38 Warning(s)
    102 Error(s)","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1597,Major,Alexander Shraer,Fixed,2013-11-17T11:42:59.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Windows build failing,2014-03-13T18:17:00.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",6.0
Enis Soztutar,[],2012-12-03T23:34:34.000+0000,Enis Soztutar,"Zab1_0Test fails on windows with: 
{code}
java.io.IOException: Could not rename temporary file C:\Users\ADMINI~1\AppData\Local\Temp\2\test6831881113551099349dir\version-2\acceptedEpoch.tmp to C:\Users\A
DMINI~1\AppData\Local\Temp\2\test6831881113551099349dir\version-2\acceptedEpoch
        at org.apache.zookeeper.common.AtomicFileOutputStream.close(AtomicFileOutputStream.java:82)
        at org.apache.zookeeper.server.quorum.QuorumPeer.writeLongToFile(QuorumPeer.java:1121)
        at org.apache.zookeeper.server.quorum.QuorumPeer.setAcceptedEpoch(QuorumPeer.java:1148)
        at org.apache.zookeeper.server.quorum.Learner.registerWithLeader(Learner.java:281)
        at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:72)
        at org.apache.zookeeper.server.quorum.Zab1_0Test$1.run(Zab1_0Test.java:450)
{code}

The file handlers currentEpoch and acceptedEpoch are not closed, so delete fails on windows. ","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1596,Major,Enis Soztutar,Fixed,2012-12-11T08:20:36.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Zab1_0Test should ensure that the file is closed,2014-03-13T18:17:06.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.5.0', id='12316644'>]",4.0
Marshall McMullen,"[<JIRA Component: name='c client', id='12312380'>]",2012-11-30T05:27:13.000+0000,Marshall McMullen,"We've seen an intermittent failure in one of the C client tests TestReconfig which was committed as part of ZOOKEEPER-1355.

The test that is failing is failing *before* any rebalancing algorithm is invoked. After inspecting this we've concluded it is a failure to properly seed the random number generator properly. This same problem was seen and solved on the Java client side so we just need to do something similar on the C client side.

The assertion:

Build/trunk/src/c/tests/TestReconfig.cc:571: Assertion: assertion failed [Expression: numClientsPerHost.at(i) >= lowerboundClientsPerServer(numClients, numServers)]
[exec] [exec] Failures !!!
[exec] [exec] Run: 38 Failure total: 1 Failures: 1 Errors: 0
[exec] [exec] make: *** [run-check] Error 1
[exec] 
[exec] BUILD FAILED
[exec] /home/jenkins/jenkins-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build.xml:1262: The following error occurred while executing this line:
[exec] /home/jenkins/jenkins-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build.xml:1272: exec returned: 2

Also this one:

From the latest build logs:
[exec] Zookeeper_watchers::testChildWatcher2 : elapsed 54 : OK
[exec] /home/jenkins/jenkins-slave/workspace/ZooKeeper-trunk/trunk/src/c/tests/TestReconfig.cc:183: Assertion: equality assertion failed [Expected: 1, Actual : 0]
[exec] Failures !!!
[exec] Run: 67 Failure total: 1 Failures: 1 Errors: 0
[exec] FAIL: zktest-mt
[exe
",[],Bug,ZOOKEEPER-1594,Major,Marshall McMullen,Duplicate,2016-11-05T17:30:50.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,TestReconfig intermittently fails,2016-11-05T17:30:50.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",6.0
Marshall McMullen,"[<JIRA Component: name='c client', id='12312380'>]",2012-11-27T23:32:12.000+0000,Michi Mutsuzaki,"addrvec.h includes inttypes.h, but it is not present in the windows build environment.

https://builds.apache.org/job/ZooKeeper-trunk-WinVS2008/596/console

f:\hudson\hudson-slave\workspace\zookeeper-trunk-winvs2008\trunk\src\c\src\addrvec.h(22): fatal error C1083: Cannot open include file: 'inttypes.h': No such file or directory ","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1591,Major,Michi Mutsuzaki,Fixed,2012-11-30T20:44:20.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Windows build is broken because inttypes.h doesn't exist,2012-12-01T11:03:32.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Flavio Paiva Junqueira,[],2012-11-27T20:08:26.000+0000,Flavio Paiva Junqueira,"Here is the related output of jenkins:

{noformat}
validate-xdocs:
     [exec] /home/jenkins/jenkins-slave/workspace/ZooKeeper-trunk/trunk/src/docs/src/documentation/content/xdocs/zookeeperProgrammers.xml:578:5: The element type ""para"" must be terminated by the matching end-tag ""</para>"".
     [exec] 
     [exec] BUILD FAILED
     [exec] /home/jenkins/tools/forrest/latest/main/targets/validate.xml:135: Could not validate document /home/jenkins/jenkins-slave/workspace/ZooKeeper-trunk/trunk/src/docs/src/documentation/content/xdocs/zookeeperProgrammers.xml
     [exec] 
{noformat}","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1590,Blocker,Flavio Paiva Junqueira,Fixed,2012-11-28T07:18:22.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Patch to add zk.updateServerList(newServerList) broke the build,2012-11-28T11:07:22.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",3.0
Mahadev Konar,[],2012-11-22T21:07:31.000+0000,Flavio Paiva Junqueira,"Check the version numbers of the documentation links on the project front page:

{noformat}
    Release 3.4.5(stable)
    Release 3.4.5(current)
{noformat}",[],Bug,ZOOKEEPER-1589,Minor,Flavio Paiva Junqueira,Invalid,2012-11-28T06:41:31.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Documentation list has wrong numbering,2012-11-28T06:41:31.000+0000,[],1.0
Raúl Gutiérrez Segalés,"[<JIRA Component: name='contrib-zkfuse', id='12312644'>]",2012-11-19T07:14:34.000+0000,Raúl Gutiérrez Segalés,,[],Bug,ZOOKEEPER-1586,Major,Raúl Gutiérrez Segalés,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,tarballs for zkfuse don't compile out of tree,2013-10-09T06:41:11.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",1.0
Raúl Gutiérrez Segalés,"[<JIRA Component: name='c client', id='12312380'>]",2012-11-19T06:04:33.000+0000,Raúl Gutiérrez Segalés,make dist from trunk is failing because of a wrong reference to src/zookeeper_log.h (which exists in include/). ,"[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1585,Major,Raúl Gutiérrez Segalés,Fixed,2012-11-27T01:37:16.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,make dist for src/c broken in trunk,2016-03-03T01:34:38.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
,[],2012-11-13T09:28:56.000+0000,Yanming Zhou,"1.download zookeeper-3.4.4.tar.gz and unzip
2.rename conf/zoo_sample.cfg to zoo.cfg
3.click zkServer.cmd
4.click zkCli.cmd

zkCli can not connect to zkServer,it blocked
zkServer console print

2012-11-13 17:28:05,302 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@349] - caught end of stream exception
EndOfStreamException: Unable to read additional data from client sessionid 0x13af9131eee0000, likely client has closed socket
        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:220)
        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
        at java.lang.Thread.run(Thread.java:722)
2012-11-13 17:28:05,308 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1001] - Closed socket connection for client /127.0.0.1:54810 which had sessionid 0x13af9131eee0000 
",[],Bug,ZOOKEEPER-1582,Blocker,Yanming Zhou,Duplicate,2012-12-14T19:28:00.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,EndOfStreamException: Unable to read additional data from client,2018-11-21T12:19:03.000+0000,[],19.0
Benjamin Reed,"[<JIRA Component: name='build', id='12312383'>]",2012-11-08T14:56:01.000+0000,Benjamin Reed,it's 2012 so the copyright in notice.txt should end with 2012,"[<JIRA Version: name='3.3.7', id='12321882'>, <JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1581,Major,Benjamin Reed,Fixed,2012-12-12T07:00:58.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,change copyright in notice to 2012,2014-03-13T18:17:10.000+0000,[],4.0
Ling Mao,[],2012-11-08T11:13:09.000+0000,Flavio Paiva Junqueira,"setRunning is a public method and a search did not indicate that it is used anywhere, not even in tests. In fact, I believe we should not change ""running"" freely and we should only do it when calling shutdown. ","[<JIRA Version: name='3.5.4', id='12340141'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-1580,Minor,Flavio Paiva Junqueira,Fixed,2018-01-30T07:07:32.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,QuorumPeer.setRunning is not used,2018-01-30T10:44:13.000+0000,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.4.11', id='12339207'>, <JIRA Version: name='3.6.0', id='12326518'>]",5.0
Michelle Chen,[],2012-11-08T04:06:09.000+0000,Michelle Chen,"zookeeper invokes getOpenFileDescriptorCount() function in  com.sun.management.UnixOperatingSystemMXBean, which only exists in SUN JDK, and open JDK did not implement this function.

    [javac] /root/zookeeper-3.3.4/src/java/test/org/apache/zookeeper/test/ClientBase.java:57: package com.sun.management does not exist
    [javac] import com.sun.management.UnixOperatingSystemMXBean;
    [javac]                          ^
    [javac] /root/zookeeper-3.3.4/src/java/test/org/apache/zookeeper/test/QuorumBase.java:39: package com.sun.management does not exist
    [javac] import com.sun.management.UnixOperatingSystemMXBean;
    [javac]                          ^
    [javac] /root/zookeeper-3.3.4/src/java/test/org/apache/zookeeper/test/ClientTest.java:48: package com.sun.management does not exist
    [javac] import com.sun.management.UnixOperatingSystemMXBean;
    [javac]                          ^
    [javac] /root/zookeeper-3.3.4/src/java/test/org/apache/zookeeper/test/QuorumUtil.java:39: package com.sun.management does not exist
    [javac] import com.sun.management.UnixOperatingSystemMXBean;",[],Bug,ZOOKEEPER-1579,Major,Michelle Chen,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Compile error of UnixOperationSystemMXBean with open JDK,2013-10-31T16:22:35.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.3', id='12319288'>]",7.0
Michelle Chen,[],2012-11-08T03:55:08.000+0000,Michelle Chen,"org.apache.zookeeper.server.quorum.Zab1_0Test was failed both with SUN JDK and open JDK.

    [junit] Running org.apache.zookeeper.server.quorum.Zab1_0Test
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 18.334 sec
    [junit] Test org.apache.zookeeper.server.quorum.Zab1_0Test FAILED 


Zab1_0Test log:
Zab1_0Test log:
2012-07-11 23:17:15,579 [myid:] - INFO  [main:Leader@427] - Shutdown called
java.lang.Exception: shutdown Leader! reason: end of test
        at org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:427)
        at org.apache.zookeeper.server.quorum.Zab1_0Test.testLastAcceptedEpoch(Zab1_0Test.java:211)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:48)


2012-07-11 23:17:15,584 [myid:] - ERROR [main:Leader@139] - Couldn't bind to port 33556
java.net.BindException: Address already in use
        at java.net.PlainSocketImpl.bind(PlainSocketImpl.java:402)
        at java.net.ServerSocket.bind(ServerSocket.java:328)
        at java.net.ServerSocket.bind(ServerSocket.java:286)
        at org.apache.zookeeper.server.quorum.Leader.<init>(Leader.java:137)
        at org.apache.zookeeper.server.quorum.Zab1_0Test.createLeader(Zab1_0Test.java:810)
        at org.apache.zookeeper.server.quorum.Zab1_0Test.testLeaderInElectingFollowers(Zab1_0Test.java:224)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

2012-07-11 23:17:20,202 [myid:] - ERROR [LearnerHandler-bdvm039.svl.ibm.com/9.30.122.48:40153:LearnerHandler@559] - Unex
pected exception causing shutdown while sock still open
java.net.SocketTimeoutException: Read timed out
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
        at java.io.DataInputStream.readInt(DataInputStream.java:370)
        at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
        at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:83)
        at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:108)
        at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:291)
2012-07-11 23:17:20,203 [myid:] - WARN  [LearnerHandler-bdvm039.svl.ibm.com/9.30.122.48:40153:LearnerHandler@569] - ****
*** GOODBYE bdvm039.svl.ibm.com/9.30.122.48:40153 ********
2012-07-11 23:17:20,204 [myid:] - INFO  [Thread-20:Leader@421] - Shutting down
2012-07-11 23:17:20,204 [myid:] - INFO  [Thread-20:Leader@427] - Shutdown called
java.lang.Exception: shutdown Leader! reason: lead ended

this failure seems 33556 port is already used, but it is not in use with command check in fact. There is a hard code in unit test, we can improve it with code patch.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1578,Major,Michelle Chen,Fixed,2012-12-17T07:13:02.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,org.apache.zookeeper.server.quorum.Zab1_0Test failed due to hard code with 33556 port,2014-03-13T18:17:15.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",6.0
Edward Ribeiro,"[<JIRA Component: name='server', id='12312382'>]",2012-11-07T11:11:10.000+0000,Tally Tsabary,"Using a cluster of three 3.4.3 zookeeper servers.
All the servers are up, but on the client machine, the firewall is blocking one of the  servers.
The following exception is happening, and the client is not connected to any of the other cluster members.

The exception:Nov 02, 2012 9:54:32 PM com.netflix.curator.framework.imps.CuratorFrameworkImpl logError
SEVERE: Background exception was not retry-able or retry gave up
java.net.UnknownHostException: scnrmq003.myworkday.com
at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
at java.net.InetAddress$1.lookupAllHostAddr(Unknown Source)
at java.net.InetAddress.getAddressesFromNameService(Unknown Source)
at java.net.InetAddress.getAllByName0(Unknown Source)
at java.net.InetAddress.getAllByName(Unknown Source)
at java.net.InetAddress.getAllByName(Unknown Source)
at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:60)
at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:440)
at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:375)

The code at the org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:60) is :
public StaticHostProvider(Collection<InetSocketAddress> serverAddresses) throws UnknownHostException {
for (InetSocketAddress address : serverAddresses) {
InetAddress resolvedAddresses[] = InetAddress.getAllByName(address
.getHostName());
for (InetAddress resolvedAddress : resolvedAddresses) { this.serverAddresses.add(new InetSocketAddress(resolvedAddress .getHostAddress(), address.getPort())); }
}
......

The for-loop is not trying to resolve the rest of the servers on the list if there is an UnknownHostException at the InetAddress.getAllByName(address.getHostName()); 
and it fails the client connection creation.


I was expecting the connection will be created for the other members of the cluster. 
Also, InetAddress is a blocking command, and if it takes very long time,  (longer than the defined timeout) - that also should allow us to continue to try and connect to the other servers on the list.
Assuming this will be fixed, and we will get connection to the current available servers, I think the zookeeper should continue to retry to connect to the not-connected server of the cluster, so it will be able to use it later when it is back.
If one of the servers on the list is not available during the connection creation, then it should be retried every x time despite the fact that we 

","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1576,Major,Tally Tsabary,Fixed,2014-06-28T15:52:57.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper cluster - failed to connect to cluster if one of the provided IPs causes java.net.UnknownHostException,2018-12-19T11:43:05.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",18.0
Raja Aluri,[],2012-11-07T01:31:26.000+0000,Raja Aluri,adding .gitattributes to prevent CRLF and LF mismatches for source and text files,"[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1575,Major,Raja Aluri,Fixed,2014-04-04T01:36:03.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,adding .gitattributes to prevent CRLF and LF mismatches for source and text files,2014-04-04T11:12:05.000+0000,[],4.0
Vinayakumar B,"[<JIRA Component: name='server', id='12312382'>]",2012-11-01T23:13:14.000+0000,Thawan Kooburat,"While replaying txnlog on data tree, the server has a code to detect missing parent node. This code block was last modified as part of ZOOKEEPER-1333. In our production, we found a case where this check is return false positive.

The sequence of txns is as follows:

zxid 1:  create /prefix/a
zxid 2:  create /prefix/a/b
zxid 3:  delete /prefix/a/b
zxid 4:  delete /prefix/a

The server start capturing snapshot at zxid 1. However, by the time it traversing the data tree down to /prefix, txn 4 is already applied and /prefix have no children. 

When the server restore from snapshot, it process txnlog starting from zxid 2. This txn generate missing parent error and the server refuse to start up.

The same check allow me to discover bug in ZOOKEEPER-1551, but I don't know if we have any option beside removing this check to solve this issue.  ","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1573,Critical,Thawan Kooburat,Fixed,2014-02-10T20:53:16.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Unable to load database due to missing parent node,2014-03-13T18:17:07.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.5.0', id='12316644'>]",12.0
Jakub Lekstan,"[<JIRA Component: name='scripts', id='12312384'>]",2012-10-17T18:34:40.000+0000,Jakub Lekstan,"zkServer.sh looks for JMX variables before ""including"" zkEnv.sh, this way you can not disable JMX with scripts which zkEnv.sh ""includes"".

Patch included.",[],Bug,ZOOKEEPER-1567,Major,Jakub Lekstan,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,JMX can't be disabled with zkEnv.sh,2012-10-17T19:09:17.000+0000,"[<JIRA Version: name='3.4.4', id='12319841'>]",1.0
,[],2012-10-17T10:09:35.000+0000,Zhou wenjian,"2012-10-17 15:04:28,006 - WARN  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Follower@116] - Got  0x3800000002 expected 0x3800000001
2012-10-17 15:04:28,007 - WARN  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Follower@116] - Got zxid 0x3800000001 expected 0x3800000003
2012-10-17 15:04:28,007 - WARN  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Follower@116] - Got zxid 0x3800000003 expected 0x3800000002
2012-10-17 15:04:28,009 - FATAL [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FollowerZooKeeperServer@112] - Committing zxid 0x3800000003 but next pending txn 0x3800000001",[],Bug,ZOOKEEPER-1566,Major,Zhou wenjian,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,progress  quits duo to zxid  not in order,2012-10-17T10:11:52.000+0000,[],1.0
,"[<JIRA Component: name='c client', id='12312380'>]",2012-10-13T14:55:44.000+0000,Jakub Lekstan,"When I try to open zookeeper.sln the VS wants me to convert the project. While the convertion is taking place I'm getting a message:

""A file with the name: ""[path]\zookeeper.vcxproj"" already exists on disk.
Do you want to overwrite the project and its imported property sheets""

And after it I get next message with same text but it is about Cli.vxproj

No matter If I click yes or no the coverting process fails, both projects (Cli and zookeeper) are marked as unavailable.

If I close VS and open the zookeeper.sln once again it wants me to convert but now if I answer yes the projects are again unavailable but if I answer no the projects are available but are empty.",[],Bug,ZOOKEEPER-1563,Major,Jakub Lekstan,Fixed,2012-12-28T12:35:51.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Wrong solution - unable to build under Windows with Visual Studio,2012-12-28T12:35:51.000+0000,"[<JIRA Version: name='3.4.4', id='12319841'>]",3.0
Deepak Jagtap,"[<JIRA Component: name='c client', id='12312380'>]",2012-10-13T01:03:48.000+0000,Deepak Jagtap,"Valgrind is reporting memory leak for zoo_multi operations.

==4056== 2,240 (160 direct, 2,080 indirect) bytes in 1 blocks are definitely lost in loss record 18 of 24
==4056==    at 0x4A04A28: calloc (vg_replace_malloc.c:467)
==4056==    by 0x504D822: create_completion_entry (zookeeper.c:2322)
==4056==    by 0x5052833: zoo_amulti (zookeeper.c:3141)
==4056==    by 0x5052A8B: zoo_multi (zookeeper.c:3240)

It looks like completion entries for individual operations in multiupdate transaction are not getting freed. My observation is that memory leak size depends on the number of operations in single mutlipupdate transaction","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1562,Trivial,Deepak Jagtap,Fixed,2013-02-03T06:42:17.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Memory leaks in zoo_multi API,2014-03-13T18:16:54.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.4.4', id='12319841'>]",6.0
,"[<JIRA Component: name='java client', id='12312381'>]",2012-10-11T07:43:57.000+0000,Jacky007,"In the doIO method of ClientCnxnSocketNIO
{noformat}
 if (p != null) {
                    outgoingQueue.removeFirstOccurrence(p);
                    updateLastSend();
                    if ((p.requestHeader != null) &&
                            (p.requestHeader.getType() != OpCode.ping) &&
                            (p.requestHeader.getType() != OpCode.auth)) {
                        p.requestHeader.setXid(cnxn.getXid());
                    }
                    p.createBB();
                    ByteBuffer pbb = p.bb;
                    sock.write(pbb);
                    if (!pbb.hasRemaining()) {
                        sentCount++;
                        if (p.requestHeader != null
                                && p.requestHeader.getType() != OpCode.ping
                                && p.requestHeader.getType() != OpCode.auth) {
                            pending.add(p);
                        }
                    }
{noformat}
When the sock.write(pbb) method throws an exception, the packet will not be cleanup(not in outgoingQueue nor in pendingQueue). If the client wait for it, it will wait forever...","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1561,Major,Jacky007,Duplicate,2012-12-24T03:54:00.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper client may hang on a server restart,2012-12-24T04:18:07.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",3.0
Skye Wanderman-Milne,"[<JIRA Component: name='java client', id='12312381'>]",2012-10-10T23:45:05.000+0000,Igor Motov,"To reproduce, try creating a node with 0.5M of data using java client. The test will hang waiting for a response from the server. See the attached patch for the test that reproduces the issue.

It seems that ZOOKEEPER-1437 introduced a few issues to {{ClientCnxnSocketNIO.doIO}} that prevent {{ClientCnxnSocketNIO}} from sending large packets that require several invocations of {{SocketChannel.write}} to complete. The first issue is that the call to {{outgoingQueue.removeFirstOccurrence(p);}} removes the packet from the queue even if the packet wasn't completely sent yet.  It looks to me that this call should be moved under {{if (!pbb.hasRemaining())}} The second issue is that {{p.createBB()}} is reinitializing {{ByteBuffer}} on every iteration, which confuses {{SocketChannel.write}}. And the third issue is caused by extra calls to {{cnxn.getXid()}} that increment xid on every iteration and confuse the server.
","[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1560,Major,Igor Motov,Fixed,2012-10-31T18:44:12.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper client hangs on creation of large nodes,2012-10-31T23:00:47.000+0000,"[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",12.0
Eugene Joseph Koontz,"[<JIRA Component: name='server', id='12312382'>, <JIRA Component: name='tests', id='12312427'>]",2012-10-04T23:03:52.000+0000,Patrick D. Hunt,"Failure of testBadSaslAuthNotifiesWatch on the jenkins jdk7 job:

https://builds.apache.org/job/ZooKeeper-trunk-jdk7/407/

haven't seen this before.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1557,Major,Patrick D. Hunt,Fixed,2013-10-24T01:10:23.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,jenkins jdk7 test failure in testBadSaslAuthNotifiesWatch,2014-03-13T18:17:05.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.5.0', id='12316644'>]",6.0
,"[<JIRA Component: name='c client', id='12312380'>]",2012-10-03T19:10:24.000+0000,André Martin,"Valgrind reports the following memory leak when using the c-client (mt):

==11674== 18 bytes in 9 blocks are indirectly lost in loss record 14 of 173
==11674==    at 0x4C2B6CD: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==11674==    by 0xC8064A: ia_deserialize_string (recordio.c:271)
==11674==    by 0xC81F2E: deserialize_String_vector (zookeeper.jute.c:247)
==11674==    by 0xC842F9: deserialize_GetChildrenResponse (zookeeper.jute.c:874)
==11674==    by 0xC7E9F0: zookeeper_process (zookeeper.c:1904)
==11674==    by 0xC7FE5B: do_io (mt_adaptor.c:439)
==11674==    by 0x4E39E99: start_thread (pthread_create.c:308)
==11674==    by 0x5FA6DBC: clone (clone.S:112)
==11674== 
==11674== 90 (72 direct, 18 indirect) bytes in 49 blocks are definitely lost in loss record 139 of 173
==11674==    at 0x4C29DB4: calloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==11674==    by 0xC81EEE: deserialize_String_vector (zookeeper.jute.c:245)
==11674==    by 0xC842F9: deserialize_GetChildrenResponse (zookeeper.jute.c:874)
==11674==    by 0xC7E9F0: zookeeper_process (zookeeper.c:1904)
==11674==    by 0xC7FE5B: do_io (mt_adaptor.c:439)
==11674==    by 0x4E39E99: start_thread (pthread_create.c:308)
==11674==    by 0x5FA6DBC: clone (clone.S:112)",[],Bug,ZOOKEEPER-1556,Minor,André Martin,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Memory leak reported by valgrind mt version,2017-10-24T04:39:16.000+0000,"[<JIRA Version: name='3.4.4', id='12319841'>]",5.0
,[],2012-10-03T15:59:10.000+0000,Guillaume Nodet,Any session can delete nodes with restricted ACLs.,[],Bug,ZOOKEEPER-1555,Critical,Guillaume Nodet,Not A Problem,2012-10-03T16:08:26.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ACLs are not respected for node deletion,2012-10-03T16:08:26.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",0.0
,[],2012-10-03T15:35:13.000+0000,Guillaume Nodet,"The ZooKeeperSaslClient correctly detects that it should not use SASL when nothing is configured, however the SendThread waits forever because clientTunneledAuthenticationInProgress() returns true instead of false.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1554,Blocker,Guillaume Nodet,Fixed,2013-10-30T04:21:35.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Can't use zookeeper client without SASL,2014-03-13T18:17:14.000+0000,"[<JIRA Version: name='3.4.4', id='12319841'>]",10.0
Sean Busbey,"[<JIRA Component: name='build', id='12312383'>]",2012-10-01T22:00:22.000+0000,Sean Busbey,"While updating the findbugs configuration to account for a change in log4j versions I noticed findbugs complaining about access to the netty and slf4j classes.

Steps to reproduce:

# install findbugs to $FINDBUGS_HOME
# run ant -Dfindbugs.home=""$FINDBUGS_HOME"" findbugs

","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1553,Minor,Sean Busbey,Fixed,2012-12-12T08:03:29.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Findbugs configuration is missing some dependencies,2014-03-13T18:16:56.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Thawan Kooburat,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2012-10-01T00:57:32.000+0000,Thawan Kooburat,"In Learner.java, txns which comes after the learner has taken the snapshot (after NEWLEADER packet) are stored in packetsNotCommitted. The follower has special logic to apply these txns at the end of syncWithLeader() method. However, the observer will ignore these txns completely, causing data inconsistency. ","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1551,Blocker,Thawan Kooburat,Fixed,2013-10-08T16:34:35.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Observers ignore txns that come after snapshot and UPTODATE ,2014-03-13T18:17:01.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",8.0
Eugene Joseph Koontz,"[<JIRA Component: name='java client', id='12312381'>]",2012-09-26T16:32:37.000+0000,Robert Macomber,"On OpenJDK, {{javax.security.auth.login.Configuration.getConfiguration}} does not throw an exception.  {{ZooKeeperSaslClient.clientTunneledAuthenticationInProgress}} uses an exception from that method as a proxy for ""this client is not configured to use SASL"" and as a result no commands can be sent, since it is still waiting for auth to complete.

[Link to mailing list discussion|http://comments.gmane.org/gmane.comp.java.zookeeper.user/2667]

The relevant bit of logs from OpenJDK and Oracle versions of 'connect and do getChildren(""/"")':

{code:title=OpenJDK}
INFO [main] 2012-09-25 14:02:24,545 com.socrata.Main Waiting for connection...
DEBUG [main] 2012-09-25 14:02:24,548 com.socrata.zookeeper.ZooKeeperProvider Waiting for connected-state...
INFO [main-SendThread(mike.local:2181)] 2012-09-25 14:02:24,576 org.apache.zookeeper.ClientCnxn Opening socket connection to server mike.local/10.0.2.106:2181. Will not attempt to authenticate using SASL (unknown error)
INFO [main-SendThread(mike.local:2181)] 2012-09-25 14:02:24,584 org.apache.zookeeper.ClientCnxn Socket connection established to mike.local/10.0.2.106:2181, initiating session
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:24,586 org.apache.zookeeper.ClientCnxn Session establishment request sent on mike.local/10.0.2.106:2181
INFO [main-SendThread(mike.local:2181)] 2012-09-25 14:02:24,600 org.apache.zookeeper.ClientCnxn Session establishment complete on server mike.local/10.0.2.106:2181, sessionid = 0x139ff2e85b60005, negotiated timeout = 40000
DEBUG [main-EventThread] 2012-09-25 14:02:24,614 com.socrata.zookeeper.ZooKeeperProvider ConnectionStateChanged(Connected)
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:24,636 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,12  replyHeader:: 0,0,0 request:: '/,F  response:: v{} until SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:37,923 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,12  replyHeader:: 0,0,0 request:: '/,F  response:: v{} until SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:37,924 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,12  replyHeader:: 0,0,0 request:: '/,F  response:: v{} until SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:37,924 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:null serverPath:null finished:false header:: -2,11  replyHeader:: null request:: null response:: nulluntil SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:51,260 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,12  replyHeader:: 0,0,0 request:: '/,F  response:: v{} until SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:51,260 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:null serverPath:null finished:false header:: -2,11  replyHeader:: null request:: null response:: nulluntil SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:51,261 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,12  replyHeader:: 0,0,0 request:: '/,F  response:: v{} until SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:51,261 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:null serverPath:null finished:false header:: -2,11  replyHeader:: null request:: null response:: nulluntil SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:51,261 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:null serverPath:null finished:false header:: -2,11  replyHeader:: null request:: null response:: nulluntil SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:51,265 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,12  replyHeader:: 0,0,0 request:: '/,F  response:: v{} until SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:51,265 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:null serverPath:null finished:false header:: -2,11  replyHeader:: null request:: null response:: nulluntil SASL authentication completes.
DEBUG [main-SendThread(mike.local:2181)] 2012-09-25 14:02:51,266 org.apache.zookeeper.ClientCnxnSocketNIO deferring non-priming packet: clientPath:null serverPath:null finished:false header:: -2,11  replyHeader:: null request:: null response:: nulluntil SASL authentication completes.
INFO [main-SendThread(mike.local:2181)] 2012-09-25 14:02:51,266 org.apache.zookeeper.ClientCnxn Client session timed out, have not heard from server in 26668ms for sessionid 0x139ff2e85b60005, closing socket connection and attempting reconnect
DEBUG [main-EventThread] 2012-09-25 14:02:51,377 com.socrata.zookeeper.ZooKeeperProvider ConnectionStateChanged(Disconnected)
{code}

{code:title=Oracle}
INFO [main] 2012-09-25 14:03:16,315 com.socrata.Main Waiting for connection...
DEBUG [main] 2012-09-25 14:03:16,319 com.socrata.zookeeper.ZooKeeperProvider Waiting for connected-state...
INFO [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,335 org.apache.zookeeper.ClientCnxn Opening socket connection to server 10.0.2.106/10.0.2.106:2181. Will not attempt to authenticate using SASL (Unable to locate a login configuration)
INFO [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,344 org.apache.zookeeper.ClientCnxn Socket connection established to 10.0.2.106/10.0.2.106:2181, initiating session
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,346 org.apache.zookeeper.ClientCnxn Session establishment request sent on 10.0.2.106/10.0.2.106:2181
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,347 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,351 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
INFO [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,368 org.apache.zookeeper.ClientCnxn Session establishment complete on server 10.0.2.106/10.0.2.106:2181, sessionid = 0x139ff2e85b60006, negotiated timeout = 40000
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,371 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,371 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
DEBUG [main-EventThread] 2012-09-25 14:03:16,385 com.socrata.zookeeper.ZooKeeperProvider ConnectionStateChanged(Connected)
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,417 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,417 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,417 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,418 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,418 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,431 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,438 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,443 org.apache.zookeeper.ClientCnxn Reading reply sessionid:0x139ff2e85b60006, packet:: clientPath:/ serverPath:/ finished:false header:: 1,12  replyHeader:: 1,8292982,0  request:: '/,F  response:: v{'ro,'row-index,'zkbtest,'consumers,'reindex,'hotstandby,'bigdir,'vs,'orestes,'eurybates,'shardedcly,'row-locks,'id-counter,'zookeeper,'cly,'locks,'rwlocks,'tickets,'brokers},s{0,0,0,0,0,61,0,0,0,19,8292893}
DEBUG [main-SendThread(10.0.2.106:2181)] 2012-09-25 14:03:16,444 org.apache.zookeeper.client.ZooKeeperSaslClient Could not retrieve login configuration: java.lang.SecurityException: Unable to locate a login configuration
OK(Set(cly, row-locks, hotstandby, locks, tickets, bigdir, zkbtest, row-index, reindex, id-counter, eurybates, vs, rwlocks, shardedcly, brokers, consumers, zookeeper, orestes, ro),0,0,0,0,0,61,0,0,0,19,8292893)
{code}","[<JIRA Version: name='3.4.5', id='12321883'>]",Bug,ZOOKEEPER-1550,Blocker,Robert Macomber,Fixed,2012-09-28T17:06:59.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZooKeeperSaslClient does not finish anonymous login on OpenJDK,2013-01-16T19:00:21.000+0000,"[<JIRA Version: name='3.4.4', id='12319841'>]",6.0
Flavio Paiva Junqueira,"[<JIRA Component: name='quorum', id='12312379'>]",2012-09-10T07:58:55.000+0000,Jacky007,"the trunc code (from ZOOKEEPER-1154?) cannot work correct if the snapshot is not correct.
here is scenario(similar to 1154):
Initial Condition
1.	Lets say there are three nodes in the ensemble A,B,C with A being the leader
2.	The current epoch is 7. 
3.	For simplicity of the example, lets say zxid is a two digit number, with epoch being the first digit.
4.	The zxid is 73
5.	All the nodes have seen the change 73 and have persistently logged it.
Step 1
Request with zxid 74 is issued. The leader A writes it to the log but there is a crash of the entire ensemble and B,C never write the change 74 to their log.
Step 2
A,B restart, A is elected as the new leader,  and A will load data and take a clean snapshot(change 74 is in it), then send diff to B, but B died before sync with A. A died later.
Step 3
B,C restart, A is still down
B,C form the quorum
B is the new leader. Lets say B minCommitLog is 71 and maxCommitLog is 73
epoch is now 8, zxid is 80
Request with zxid 81 is successful. On B, minCommitLog is now 71, maxCommitLog is 81
Step 4
A starts up. It applies the change in request with zxid 74 to its in-memory data tree
A contacts B to registerAsFollower and provides 74 as its ZxId
Since 71<=74<=81, B decides to send A the diff. 
Problem:
The problem with the above sequence is that after truncate the log, A will load the snapshot again which is not correct.

In 3.3 branch, FileTxnSnapLog.restore does not call listener(ZOOKEEPER-874), the leader will send a snapshot to follower, it will not be a problem.",[],Bug,ZOOKEEPER-1549,Major,Jacky007,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Data inconsistency when follower is receiving a DIFF with a dirty snapshot,2022-02-03T08:50:14.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",21.0
,"[<JIRA Component: name='leaderElection', id='12312378'>]",2012-09-07T21:51:48.000+0000,Alan Horn,"Hi,

We have a five node cluster, recently upgraded from 3.3.5 to 3.4.3. Was running fine for a few weeks after the upgrade, then the following sequence of events occurred :

1. All servers stopped responding to 'ruok' at the same time
2. Our local supervisor process restarted all of them at the same time 
(yes, this is bad, we didn't expect it to fail this way :)
3. The cluster would not serve requests after this. Appeared to be unable to complete an election.

We tried various things at this point, none of which worked :

* Moved around the restart order of the nodes (e.g. 4 thru 0, instead of 0 thru 4)
* Reduced number of running nodes from 5 -> 3 to simplify the quorum, by only starting up 0, 1 & 2, in one test, and  0, 2 & 4 in the other
* Removed the *Epoch files from version-2/ snapshot directory
* Put the same version2/snapshot.xxxxx file on each server in the cluster
* Added the (same on all nodes) last txlog onto each cluster
* Kept only the last snapshot plus txlog unique on each server
* Moved leaderServes=no to leaderServes=yes
* Removed all files and started up with empty data as a control. This worked, but of course isn't terribly useful :)

Finally, I brought the data up on a single node running in standalone and this worked (yay!) So at this point we brought the single node back into service and have kept the other four available to debug why the election is failing.

We downgraded the four nodes to 3.3.5, and then they completed the election and started serving as expected.
We did a rolling upgrade to 3.4.3, and everything was fine until we restarted the leader, whereupon we encountered the same re-election loop as before.

We're a bit out of ideas at this point, so I was hoping someone from this list might have some useful input.

Output from two followers and a leader during this condition are attached.

Cheers,

Al
","[<JIRA Version: name='3.4.6', id='12323310'>]",Bug,ZOOKEEPER-1548,Major,Alan Horn,Duplicate,2013-08-29T14:22:03.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Cluster fails election loop in new and interesting way,2014-03-13T18:16:53.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",6.0
,"[<JIRA Component: name='server', id='12312382'>]",2012-09-04T09:36:25.000+0000,Erik Forsberg,"One of my zookeeper servers in a quorum of 3 froze (probably due to underlying hardware problems). When restarting, zookeeper fails to start with the following in zookeeper.log:

{noformat}
2012-09-04 09:02:35,300 - INFO  [main:QuorumPeerConfig@90] - Reading configuration from: /etc/zookeeper/zoo.cfg
2012-09-04 09:02:35,316 - INFO  [main:QuorumPeerConfig@310] - Defaulting to majority quorums
2012-09-04 09:02:35,333 - INFO  [main:QuorumPeerMain@119] - Starting quorum peer
2012-09-04 09:02:35,358 - INFO  [main:NIOServerCnxn$Factory@143] - binding to port 0.0.0.0/0.0.0.0:2181
2012-09-04 09:02:35,379 - INFO  [main:QuorumPeer@819] - tickTime set to 2000
2012-09-04 09:02:35,380 - INFO  [main:QuorumPeer@830] - minSessionTimeout set to -1
2012-09-04 09:02:35,380 - INFO  [main:QuorumPeer@841] - maxSessionTimeout set to -1
2012-09-04 09:02:35,386 - INFO  [main:QuorumPeer@856] - initLimit set to 10
2012-09-04 09:02:35,523 - INFO  [main:FileSnap@82] - Reading snapshot /var/zookeeper/version-2/snapshot.500017240
2012-09-04 09:02:38,944 - ERROR [main:FileTxnSnapLog@226] - Failed to increment parent cversion for: /osp/production/scheduler/waitfordeps_tasks/per_period-3092724ef4d611e18411525400fff018-bulkload_histograms
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /osp/production/scheduler/waitfordeps_tasks/per_period-3092724ef4d611e18411525400fff018-bulkload_histograms
        at org.apache.zookeeper.server.DataTree.incrementCversion(DataTree.java:1218)
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.processTransaction(FileTxnSnapLog.java:224)
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:152)
        at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:222)
        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:398)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:143)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:103)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:76)
2012-09-04 09:02:38,945 - FATAL [main:QuorumPeer@400] - Unable to load database on disk
java.io.IOException: Failed to process transaction type: 2 error: KeeperErrorCode = NoNode for /osp/production/scheduler/waitfordeps_tasks/per_period-3092724ef4d611e18411525400fff018-bulkload_histograms
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:154)
        at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:222)
        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:398)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:143)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:103)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:76)
2012-09-04 09:02:38,946 - FATAL [main:QuorumPeerMain@87] - Unexpected exception, exiting abnormally
java.lang.RuntimeException: Unable to run quorum server 
        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:401)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:143)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:103)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:76)
Caused by: java.io.IOException: Failed to process transaction type: 2 error: KeeperErrorCode = NoNode for /osp/production/scheduler/waitfordeps_tasks/per_period-3092724ef4d611e18411525400fff018-bulkload_histograms
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:154)
        at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:222)
        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:398)
        ... 3 more

{noformat}

Removing data from /var/zookeeper/version-2 then restart seems to ""fix"" the problem (it gets a snapshot from one of the other nodes in the quorum). 

This is Zookeeper 3.3.5+19.5-1~squeeze-cdh3, i.e. from Cloudera's distribution. ",[],Bug,ZOOKEEPER-1546,Major,Erik Forsberg,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"""Unable to load database on disk"" when restarting after node freeze",2015-06-04T18:55:11.000+0000,"[<JIRA Version: name='3.3.5', id='12319081'>]",4.0
,"[<JIRA Component: name='java client', id='12312381'>]",2012-09-04T06:20:15.000+0000,L.J.W,"if I deploy two application(both use zookeeper) to same tomcat,zookeeper in one app will inexplicable disconnect when tomcat startup.

following is my code,it is very simple:

public class ZKTester implements InitializingBean, Watcher {

    private ZooKeeper hZooKeeper;

    public void afterPropertiesSet() throws Exception {
        hZooKeeper = new ZooKeeper(""localhost:2181"", 300000, this);
    }

    public void process(WatchedEvent event) {
        System.out.println(""**************"" + event);
    }

and the spring config file:

<bean id=""zooTester"" class=""com.abc.framework.cluster.ZKTester""/>

And following is tomcat's startup log:

...
**************WatchedEvent state:Disconnected type:None path:null
**************WatchedEvent state:Expired type:None path:null
...
",[],Bug,ZOOKEEPER-1545,Major,L.J.W,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,very odd issue about zookeeper when deploy two web application in one tomcat,2019-02-23T08:06:42.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",2.0
,[],2012-09-03T14:01:32.000+0000,Dawid Weiss,"We have a test framework at Lucene/Solr which attempts to interrupt threads that leak out of a single class (suite) scope. The problem we're facing is that ZooKeeper's SyncThread is doing this:
{code}
>>             LOG.fatal(""Severe unrecoverable error, exiting"", t);
>>             System.exit(11);
{code}

Is this terminating the JVM really needed here? Could it be made optional with a system property or even removed entirely? Currently it aborts the entire JUnit runner and prevents successive tests from continuing.

",[],Bug,ZOOKEEPER-1544,Trivial,Dawid Weiss,Duplicate,2012-09-03T16:03:07.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,System.exit() calls on interrupted SyncThread,2012-09-03T16:03:07.000+0000,"[<JIRA Version: name='3.3.6', id='12320172'>]",2.0
,"[<JIRA Component: name='scripts', id='12312384'>]",2012-08-24T18:53:55.000+0000,Ryu Umayahara,"zkServer.sh

     99     nohup $JAVA ""-Dzookeeper.log.dir=${ZOO_LOG_DIR}"" ""-Dzookeeper.root.logger=${ZOO_LOG4J_PROP}"" \
    100     -cp ""$CLASSPATH"" $JVMFLAGS $ZOOMAIN ""$ZOOCFG"" > ""$_ZOO_DAEMON_OUT"" 2>&1 < /dev/null &

Cannot capture exit status of a background process.

    101     if [ $? -eq 0 ]
    102     then
    103       if /bin/echo -n $! > ""$ZOOPIDFILE""
    104       then
    105         sleep 1
    106         echo STARTED
    107       else
    108         echo FAILED TO WRITE PID
    109         exit 1
    110       fi
    111     else
    112       echo SERVER DID NOT START
    113       exit 1
    114     fi
",[],Bug,ZOOKEEPER-1542,Major,Ryu Umayahara,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zkServer.sh start fails but exit status 0,2012-08-24T19:00:45.000+0000,"[<JIRA Version: name='3.3.6', id='12320172'>]",1.0
,[],2012-08-24T05:09:46.000+0000,Yuta Okamoto,"I can't download zookeeper distribution because of ""404 Not Found"".

http://www.apache.org/dist/zookeeper/
",[],Bug,ZOOKEEPER-1541,Critical,Yuta Okamoto,Cannot Reproduce,2013-10-09T06:19:13.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper distributions are not available.,2013-10-09T06:19:13.000+0000,[],2.0
Andrew Ferguson,[],2012-08-23T17:51:57.000+0000,Andrew Ferguson,"There is a one-line bug in ZOOKEEPER-1411 which breaks backwards compatibility for sites which are using separate configuration files for each server. The bug is with the handling of the clientPort option.

One line fix to follow shortly.

thanks!
Andrew","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1540,Major,Andrew Ferguson,Fixed,2012-09-25T05:33:18.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZOOKEEPER-1411 breaks backwards compatibility,2016-03-03T01:34:54.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
,"[<JIRA Component: name='tests', id='12312427'>]",2012-08-22T04:59:48.000+0000,Alexander Shraer,"Consider the following test:

@Test
public void newTest() throws Exception {
    QuorumUtil qu = new QuorumUtil(3);
    qu.startAll();
}

Although it doesn't seem like we're checking anything at all here, this test actually fails. There is a JMXEnv.ensureAll test invoked from startAll(). It passes for QuorumUtil(1) or QuorumUtil(2) servers but fails for any larger number. Besides the fact that there's a bug in the tests, I think we should call the function differently if we want to invoke tests in it, or alternatively remove these tests or make them optional using some parameter.
",[],Bug,ZOOKEEPER-1539,Minor,Alexander Shraer,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Tests in QuorumUtil.startAll() and JMXenv,2012-09-23T01:49:43.000+0000,[],1.0
Andrew Ferguson,[],2012-08-22T00:00:11.000+0000,Andrew Ferguson,"Running `bin/zkServer.sh start` from a freshly-built copy of trunk fails if the source code is checked-out to a directory with spaces in the name. I'll include a small fix to fix this problem.

thanks!","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1538,Trivial,Andrew Ferguson,Fixed,2012-09-07T06:23:20.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Improve space handling in zkServer.sh and zkEnv.sh,2013-06-25T18:07:24.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",6.0
,"[<JIRA Component: name='c client', id='12312380'>]",2012-08-17T14:20:18.000+0000,mohammad taher,"1.Type zookeeper URL in the address bar to go to home page of it.
2.For new users, click on ""new user"" and it will open a registration form.
3.Give your full name in capital letters as mentioned.
4.Even though I give capital letters it is not accepting and is giving an error message as ""PLEASE TYPE CAPITAL LETTERS""",[],Bug,ZOOKEEPER-1537,Minor,mohammad taher,Incomplete,2012-08-30T05:39:02.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,registration page not accepting capital letters,2012-08-30T05:39:02.000+0000,"[<JIRA Version: name='3.3.5', id='12319081'>]",3.0
brooklin,"[<JIRA Component: name='c client', id='12312380'>]",2012-08-16T03:13:13.000+0000,brooklin,"At line 99 in winport.c, use windows API ""InitializeCriticalSection"" but never call ""DeleteCriticalSection""","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1536,Major,brooklin,Fixed,2012-08-30T20:38:28.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,c client : memory leak in winport.c,2012-08-31T11:02:19.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",5.0
Edward Ribeiro,"[<JIRA Component: name='scripts', id='12312384'>]",2012-08-14T23:56:40.000+0000,Stu Hood,"In the ZK 3.4.3 release's version of zkCli.sh, the last command that was executed is *re*-executed when you {{ctrl+d}} out of the shell. In the snippet below, {{ls}} is executed, and then {{ctrl+d}} is triggered (inserted below to illustrate), the output from {{ls}} appears again, due to the command being re-run. 
{noformat}
[zk: zookeeper.example.com:2181(CONNECTED) 0] ls /blah
[foo]
[zk: zookeeper.example.com:2181(CONNECTED) 1] <ctrl+d> [foo]
$
{noformat}","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1535,Major,Stu Hood,Fixed,2012-12-31T03:21:28.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZK Shell/Cli re-executes last command on exit,2017-05-20T23:07:10.000+0000,[],6.0
,"[<JIRA Component: name='server', id='12312382'>]",2012-08-13T08:55:44.000+0000,Tally Tsabary,"Server side: zookeeper 3.4.3 with patch ZOOKEEPER-1437.patch 22/Jun/12 00:24
Client side: java, Curator 1.1.15, zookeeper 3.4.3 with patch ZOOKEEPER-1437.patch 22/Jun/12 00:24

Environment configured to use Sasl authentication.
While the authenticatiion is successful, everything works fine.
In case of authentication failue, it seems that the zk server catch the SaslException and close the socket without sending any additional notification to the client, so despite the client has an implementation to handle Sasl authentication failure, it is never used…
 
Details:
=========
 
 
zk server log:
{noformat}
2012-08-10 11:00:46,730 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@213] - Accepted socket connection from /127.0.0.1:50208
2012-08-10 11:00:46,731 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@780] - Session establishment request from client /127.0.0.1:50208 client's lastZxid is 0x0
2012-08-10 11:00:46,731 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@838] - Client attempting to establish new session at /127.0.0.1:50208
2012-08-10 11:00:46,733 [myid:] - DEBUG [SyncThread:0:FinalRequestProcessor@88] - Processing request:: sessionid:0x1390fd2ee630004 type:createSession cxid:0x0 zxid:0x26b txntype:-10 reqpath:n/a
2012-08-10 11:00:46,733 [myid:] - DEBUG [SyncThread:0:FinalRequestProcessor@160] - sessionid:0x1390fd2ee630004 type:createSession cxid:0x0 zxid:0x26b txntype:-10 reqpath:n/a
2012-08-10 11:00:46,734 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@604] - Established session 0x1390fd2ee630004 with negotiated timeout 40000 for client /127.0.0.1:50208
2012-08-10 11:00:46,736 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@919] - Responding to client SASL token.
2012-08-10 11:00:46,736 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@923] - Size of client SASL token: 0
2012-08-10 11:00:46,736 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@954] - Size of server SASL response: 101
2012-08-10 11:00:46,740 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@919] - Responding to client SASL token.
2012-08-10 11:00:46,741 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@923] - Size of client SASL token: 272
2012-08-10 11:00:46,741 [myid:] - DEBUG [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:SaslServerCallbackHandler@106] - client supplied realm: zk-sasl-md5
2012-08-10 11:00:46,741 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@939] - Client failed to SASL authenticate: javax.security.sasl.SaslException: DIGEST-MD5: digest response format violation. Mismatched response.
2012-08-10 11:00:46,742 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@945] - Closing client connection due to SASL authentication failure.
2012-08-10 11:00:46,742 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1000] - Closed socket connection for client /127.0.0.1:50208 which had sessionid 0x1390fd2ee630004
2012-08-10 11:00:46,743 [myid:] - ERROR [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@180] - Unexpected Exception: 
java.nio.channels.CancelledKeyException
               at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:55)
               at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:59)
               at org.apache.zookeeper.server.NIOServerCnxn.sendBuffer(NIOServerCnxn.java:153)
               at org.apache.zookeeper.server.NIOServerCnxn.sendResponse(NIOServerCnxn.java:1075)
               at org.apache.zookeeper.server.ZooKeeperServer.processPacket(ZooKeeperServer.java:906)
               at org.apache.zookeeper.server.NIOServerCnxn.readRequest(NIOServerCnxn.java:365)
               at org.apache.zookeeper.server.NIOServerCnxn.readPayload(NIOServerCnxn.java:202)
               at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:236)
               at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:224)
               at java.lang.Thread.run(Thread.java:662)
 {noformat}
 
At the corresponding source: org.apache.zookeeper.server.ZooKeeperServer
 
{noformat}
private Record processSasl(ByteBuffer incomingBuffer, ServerCnxn cnxn) throws IOException {
        LOG.debug(""Responding to client SASL token."");
        GetSASLRequest clientTokenRecord = new GetSASLRequest();
        ByteBufferInputStream.byteBuffer2Record(incomingBuffer,clientTokenRecord);
        byte[] clientToken = clientTokenRecord.getToken();
        LOG.debug(""Size of client SASL token: "" + clientToken.length);
        byte[] responseToken = null;
        try {
            ZooKeeperSaslServer saslServer  = cnxn.zooKeeperSaslServer;
            try {
                // note that clientToken might be empty (clientToken.length == 0):
                // if using the DIGEST-MD5 mechanism, clientToken will be empty at the beginning of the
                // SASL negotiation process.
                responseToken = saslServer.evaluateResponse(clientToken);
                if (saslServer.isComplete() == true) {
                    String authorizationID = saslServer.getAuthorizationID();
                    LOG.info(""adding SASL authorization for authorizationID: "" + authorizationID);
                    cnxn.addAuthInfo(new Id(""sasl"",authorizationID));
                }
            }
            catch (SaslException e) {
                LOG.warn(""Client failed to SASL authenticate: "" + e);
                if ((System.getProperty(""zookeeper.allowSaslFailedClients"") != null)
                  &&
                  (System.getProperty(""zookeeper.allowSaslFailedClients"").equals(""true""))) {
                    LOG.warn(""Maintaining client connection despite SASL authentication failure."");
                } else {
                    LOG.warn(""Closing client connection due to SASL authentication failure."");
                    cnxn.close();   Tally: at this stage the socket is closed without sending any notification to the client
                }
            }
        }
        catch (NullPointerException e) {
            LOG.error(""cnxn.saslServer is null: cnxn object did not initialize its saslServer properly."");
        }
        if (responseToken != null) {
            LOG.debug(""Size of server SASL response: "" + responseToken.length);
        }
        // wrap SASL response token to client inside a Response object.
        return new SetSASLResponse(responseToken);
    }
{noformat}
 
 
The client log shows that the client identified the socket closer and just retry to connect as if the zk server just went down..
{noformat}
[10-Aug-2012 11:00:44.558 IST] INFO <org.apache.zookeeper.ClientCnxn$SendThread> Opening socket connection to server 127.0.0.1/127.0.0.1:2181
[10-Aug-2012 11:00:44.559 IST] INFO <org.apache.zookeeper.client.ZooKeeperSaslClient> Found Login Context section 'Client': will use it to attempt to SASL-authenticate.
[10-Aug-2012 11:00:44.560 IST] INFO <org.apache.zookeeper.client.ZooKeeperSaslClient> Client will use DIGEST-MD5 as SASL mechanism.
[10-Aug-2012 11:00:44.561 IST] INFO <org.apache.zookeeper.ClientCnxn$SendThread> Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session
[10-Aug-2012 11:00:44.563 IST] DEBUG <org.apache.zookeeper.ClientCnxn$SendThread> Session establishment request sent on 127.0.0.1/127.0.0.1:2181
[10-Aug-2012 11:00:44.564 IST] DEBUG <org.apache.zookeeper.ClientCnxnSocketNIO> deferring non-priming packet: clientPath:null serverPath:null finished:false header:: 0,3  replyHeader:: 0,0,0  request:: '/dev,F  response::  until SASL authentication completes.
[10-Aug-2012 11:00:44.566 IST] DEBUG <org.apache.zookeeper.ClientCnxnSocketNIO> deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,9  replyHeader:: 0,0,0  request:: '/  response::  until SASL authentication completes.
[10-Aug-2012 11:00:44.568 IST] INFO <org.apache.zookeeper.ClientCnxn$SendThread> Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x1390fd2ee630003, negotiated timeout = 40000
[10-Aug-2012 11:00:44.569 IST] INFO <com.netflix.curator.framework.state.ConnectionStateManager> State change: RECONNECTED
[10-Aug-2012 11:00:44.569 IST] DEBUG <org.apache.zookeeper.ClientCnxnSocketNIO> deferring non-priming packet: clientPath:null serverPath:null finished:false header:: 0,3  replyHeader:: 0,0,0  request:: '/dev,F  response::  until SASL authentication completes.
[10-Aug-2012 11:00:44.572 IST] DEBUG <org.apache.zookeeper.ClientCnxnSocketNIO> deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,9  replyHeader:: 0,0,0  request:: '/  response::  until SASL authentication completes.
[10-Aug-2012 11:00:44.574 IST] DEBUG <org.apache.zookeeper.ClientCnxnSocketNIO> deferring non-priming packet: clientPath:null serverPath:null finished:false header:: 0,3  replyHeader:: 0,0,0  request:: '/dev,F  response::  until SASL authentication completes.
[10-Aug-2012 11:00:44.576 IST] DEBUG <org.apache.zookeeper.ClientCnxnSocketNIO> deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,9  replyHeader:: 0,0,0  request:: '/  response::  until SASL authentication completes.
[10-Aug-2012 11:00:44.578 IST] DEBUG <org.apache.zookeeper.client.ZooKeeperSaslClient> ClientCnxn:sendSaslPacket:length=0
[10-Aug-2012 11:00:44.579 IST] DEBUG <org.apache.zookeeper.ClientCnxnSocketNIO> deferring non-priming packet: clientPath:null serverPath:null finished:false header:: 0,3  replyHeader:: 0,0,0  request:: '/dev,F  response::  until SASL authentication completes.
[10-Aug-2012 11:00:44.581 IST] DEBUG <org.apache.zookeeper.ClientCnxnSocketNIO> deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,9  replyHeader:: 0,0,0  request:: '/  response::  until SASL authentication completes.
[10-Aug-2012 11:00:44.583 IST] DEBUG <org.apache.zookeeper.ClientCnxnSocketNIO> deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,9  replyHeader:: 0,0,0  request:: '/  response::  until SASL authentication completes.
[10-Aug-2012 11:00:44.585 IST] DEBUG <org.apache.zookeeper.ClientCnxnSocketNIO> deferring non-priming packet: clientPath:null serverPath:null finished:false header:: 0,3  replyHeader:: 0,0,0  request:: '/dev,F  response::  until SASL authentication completes.
[10-Aug-2012 11:00:44.587 IST] DEBUG <org.apache.zookeeper.ClientCnxnSocketNIO> deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,9  replyHeader:: 0,0,0  request:: '/  response::  until SASL authentication completes.
[10-Aug-2012 11:00:44.589 IST] DEBUG <org.apache.zookeeper.ClientCnxnSocketNIO> deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,9  replyHeader:: 0,0,0  request:: '/  response::  until SASL authentication completes.
[10-Aug-2012 11:00:44.591 IST] DEBUG <org.apache.zookeeper.client.ZooKeeperSaslClient$2> saslClient.evaluateChallenge(len=101)
[10-Aug-2012 11:00:44.592 IST] DEBUG <org.apache.zookeeper.client.ZooKeeperSaslClient> ClientCnxn:sendSaslPacket:length=272
[10-Aug-2012 11:00:44.593 IST] DEBUG <org.apache.zookeeper.ClientCnxnSocketNIO> deferring non-priming packet: clientPath:null serverPath:null finished:false header:: 0,3  replyHeader:: 0,0,0  request:: '/dev,F  response::  until SASL authentication completes.
[10-Aug-2012 11:00:44.596 IST] DEBUG <org.apache.zookeeper.ClientCnxnSocketNIO> deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,9  replyHeader:: 0,0,0  request:: '/  response::  until SASL authentication completes.
[10-Aug-2012 11:00:44.598 IST] DEBUG <org.apache.zookeeper.ClientCnxnSocketNIO> deferring non-priming packet: clientPath:/ serverPath:/ finished:false header:: 0,9  replyHeader:: 0,0,0  request:: '/  response::  until SASL authentication completes.
[10-Aug-2012 11:00:44.600 IST] INFO <org.apache.zookeeper.ClientCnxn$SendThread> Unable to read additional data from server sessionid 0x1390fd2ee630003, likely server has closed socket, closing socket connection and attempting reconnect
[10-Aug-2012 11:00:44.701 IST] ERROR <com.netflix.curator.framework.imps.CuratorFrameworkImpl> Background operation retry gave up
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
               at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
               at com.netflix.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:438)
               at com.netflix.curator.framework.imps.BackgroundSyncImpl$1.processResult(BackgroundSyncImpl.java:49)
               at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:606)
               at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:495)
[10-Aug-2012 11:00:44.706 IST] INFO <com.netflix.curator.framework.state.ConnectionStateManager> State change: LOST
[10-Aug-2012 11:00:44.708 IST] WARN <com.netflix.curator.framework.state.ConnectionStateManager> ConnectionStateManager queue full - dropping events to make room
[10-Aug-2012 11:00:44.710 IST] INFO <com.netflix.curator.framework.state.ConnectionStateManager> State change: SUSPENDED
{noformat}",[],Bug,ZOOKEEPER-1534,Major,Tally Tsabary,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper server do not send Sal authentication failure notification to the client,2018-02-14T20:46:15.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",5.0
Warren Turkal,"[<JIRA Component: name='documentation', id='12312422'>]",2012-08-13T06:50:23.000+0000,Warren Turkal,Small doc fix in the JavaExample doc.,"[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1533,Minor,Warren Turkal,Fixed,2012-08-14T23:11:35.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Correct the documentation of the args for the JavaExample doc.,2016-03-03T01:35:43.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>, <JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.4.1', id='12318650'>, <JIRA Version: name='3.4.2', id='12319196'>, <JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.3.5', id='12319081'>, <JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",4.0
,[],2012-08-09T23:13:19.000+0000,Warren Turkal,Small doc fix.,"[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1531,Major,Warren Turkal,Duplicate,2013-09-03T06:53:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Correct the documentation of the args for the JavaExample doc.,2013-09-03T06:53:56.000+0000,[],1.0
,[],2012-08-09T23:13:00.000+0000,Warren Turkal,Small doc fix.,[],Bug,ZOOKEEPER-1530,Major,Warren Turkal,Duplicate,2013-09-03T06:54:15.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Correct the documentation of the args for the JavaExample doc.,2013-09-03T06:54:15.000+0000,[],0.0
,[],2012-08-09T20:51:59.000+0000,Warren Turkal,Correct the documentation of the args for the JavaExample doc.,[],Bug,ZOOKEEPER-1529,Minor,Warren Turkal,Duplicate,2013-09-03T06:54:38.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Correct the documentation of the args for the JavaExample doc.,2013-09-03T06:54:38.000+0000,[],0.0
,[],2012-08-09T20:50:55.000+0000,Warren Turkal,I added another listitem documenting the filename arg of the JavaExample code.,[],Bug,ZOOKEEPER-1528,Minor,Warren Turkal,Duplicate,2013-09-03T06:55:01.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Correct the documentation of the args for the JavaExample doc.,2013-09-03T06:55:01.000+0000,[],0.0
,[],2012-08-09T20:50:33.000+0000,Warren Turkal,I added another listitem documenting the filename arg of the JavaExample code.,[],Bug,ZOOKEEPER-1527,Trivial,Warren Turkal,Duplicate,2013-10-11T16:39:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Correct the documentation of the args for the JavaExample doc,2013-10-11T16:39:56.000+0000,[],0.0
,[],2012-08-09T20:50:17.000+0000,Warren Turkal,I added another listitem documenting the filename arg of the JavaExample code.,[],Bug,ZOOKEEPER-1526,Trivial,Warren Turkal,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Correct the documentation of the args for the JavaExample doc,2012-08-09T20:50:17.000+0000,[],0.0
Patrick D. Hunt,"[<JIRA Component: name='tests', id='12312427'>]",2012-07-30T22:27:53.000+0000,Patrick D. Hunt,"The jdk7 test job on jenkins is failing intermittently with 

{noformat}
java.lang.NullPointerException
	at org.apache.zookeeper.server.quorum.Zab1_0Test.recursiveDelete(Zab1_0Test.java:917)
	at org.apache.zookeeper.server.quorum.Zab1_0Test.recursiveDelete(Zab1_0Test.java:918)
	at org.apache.zookeeper.server.quorum.Zab1_0Test.recursiveDelete(Zab1_0Test.java:918)
	at org.apache.zookeeper.server.quorum.Zab1_0Test.testPopulatedLeaderConversation(Zab1_0Test.java:419)
	at org.apache.zookeeper.server.quorum.Zab1_0Test.testUnnecessarySnap(Zab1_0Test.java:483)
{noformat}

Seems to not be handling the case where the file is deleted out from under. Also the recursive deletes should be at the very end of the finally I would think.","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1522,Major,Patrick D. Hunt,Fixed,2012-08-01T15:57:01.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,intermittent failures in Zab test due to NPE in recursiveDelete test function,2012-08-01T21:12:00.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.5.0', id='12316644'>]",3.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2012-07-26T15:44:38.000+0000,Patrick D. Hunt,"branch 3.3: The leader is expecting the follower to initialize in syncLimit time rather than initLimit. In LearnerHandler run line 395 (branch33) we look for the ack from the follower with a timeout of syncLimit.

branch 3.4+: seems like ZOOKEEPER-1136 introduced a regression while attempting to fix the problem. It sets the timeout as initLimit however it never sets the timeout to syncLimit once the ack is received.","[<JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1521,Critical,Patrick D. Hunt,Fixed,2012-07-29T05:08:06.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,LearnerHandler initLimit/syncLimit problems specifying follower socket timeout limits,2012-07-29T11:02:03.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.3.5', id='12319081'>, <JIRA Version: name='3.5.0', id='12316644'>]",9.0
Bill Bridge,"[<JIRA Component: name='server', id='12312382'>]",2012-07-25T18:42:28.000+0000,Bill Bridge,In Util.readTxnBytes() the sentinel is compared with 0x42 and if it does not match then the record is considered partially written and thus the EOF. However if it is a partial record the sentinel should be 0x00 since that is what the log is initialized with. Any other value would indicate corruption and should throw an IOException rather than indicate EOF. See [ZOOKEEPER-1453|https://issues.apache.org/jira/browse/ZOOKEEPER-1453] for a related issue. ,[],Bug,ZOOKEEPER-1520,Minor,Bill Bridge,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,A txn log record with a corrupt sentinel byte looks like EOF,2022-02-03T08:50:25.000+0000,"[<JIRA Version: name='3.3.5', id='12319081'>]",4.0
Daniel Lescohier,"[<JIRA Component: name='c client', id='12312380'>]",2012-07-25T17:34:57.000+0000,Mark Gius,"zoo_acreate() and zoo_aset() take a char * argument for data and prepare a call to zookeeper.  This char * doesn't seem to be duplicated at any point, making it possible that the caller of the asynchronous function might potentially free() the char * argument before the zookeeper library completes its request.  This is unlikely to present a real problem unless the freed memory is re-used before zookeeper consumes it.  I've been unable to reproduce this issue using pure C as a result.

However, ZKPython is a whole different story.  Consider this snippet:

  ok = zookeeper.acreate(handle, path, json.dumps(value), 
                         acl, flags, callback)
  assert ok == zookeeper.OK

In this snippet, json.dumps() allocates a string which is passed into the acreate().  When acreate() returns, the zookeeper request has been constructed with a pointer to the string allocated by json.dumps().  Also when acreate() returns, that string is now referenced by 0 things (ZKPython doesn't bump the refcount) and the string is eligible for garbage collection and re-use.  The Zookeeper request now has a pointer to dangerous freed memory.

I've been seeing odd behavior in our development environments for some time now, where it appeared as though two separate JSON payloads had been joined together.  Python has been allocating a new JSON string in the middle of the old string that an incomplete zookeeper async call had not yet processed.

I am not sure if this is a behavior that should be documented, or if the C binding implementation needs to be updated to create copies of the data payload provided for aset and acreate.",[],Bug,ZOOKEEPER-1519,Major,Mark Gius,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper Async calls can reference free()'d memory,2022-02-03T08:50:28.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.3.6', id='12320172'>]",8.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2012-07-25T09:28:26.000+0000,Kiran BC,"Mailing List link under Miscellaneous section from the Zookeeper documentation is broken.
Following is the link:
http://zookeeper.apache.org/mailing_lists.html",[],Bug,ZOOKEEPER-1518,Major,Kiran BC,Fixed,2012-08-01T19:09:42.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Mailing List link is broken in the Zookeeper documentation,2012-08-01T19:09:42.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",2.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2012-07-24T12:52:45.000+0000,liuli,"I have Hadoop and Zookeeper installed 

the zoo.cfg is :

tickTime=2000
dataDir=/home/hduser/zookeeper/conf
clientPort=2181
initLimit=10
syncLimit=5
server.1=rsmm-master:2888:3888
server.2=rsmm-slave-1:2888:3888
server.3=rsmm-slave-2:2888:3888
server.4=rsmm-slave-3:2888:3888
server.5=rsmm-slave-4:2888:3888

=====================================
I tried to start zookeeper, 
./zkServer.sh start
./zkServer.sh status

JMX enabled by default
Using config: /home/hduser/zookeeper/bin/../conf/zoo.cfg
Mode: follower



The follower (rsmm-slave-4) logs complain: 

012-07-24 20:29:35,903 - WARN  [Thread-9:QuorumCnxManager$RecvWorker@727] - Connection broken for id 5, my id = 2, error = java.io.IOException: Channel eof
2012-07-24 20:29:35,904 - WARN  [Thread-9:QuorumCnxManager$RecvWorker@730] - Interrupting SendWorker
2012-07-24 20:29:35,905 - WARN  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Follower@82] - Exception when following the leader
java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
	at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:84)
	at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:108)
	at org.apache.zookeeper.server.quorum.Learner.readPacket(Learner.java:148)
	at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:78)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:645)
2012-07-24 20:29:35,905 - WARN  [Thread-8:QuorumCnxManager$SendWorker@633] - Interrupted while waiting for message on queue
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2017)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:370)
	at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:622)
2012-07-24 20:29:35,907 - WARN  [Thread-8:QuorumCnxManager$SendWorker@642] - Send worker leaving thread
2012-07-24 20:29:35,907 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Follower@165] - shutdown called
java.lang.Exception: shutdown Follower
	at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:165)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:649)
2012-07-24 20:29:35,913 - INFO  [FollowerRequestProcessor:2:FollowerRequestProcessor@93] - FollowerRequestProcessor exited loop!
2012-07-24 20:29:35,914 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FinalRequestProcessor@370] - shutdown of request processor complete
2012-07-24 20:29:35,914 - INFO  [CommitProcessor:2:CommitProcessor@148] - CommitProcessor exited loop!
2012-07-24 20:29:35,915 - INFO  [SyncThread:2:SyncRequestProcessor@151] - SyncRequestProcessor exited!
2012-07-24 20:29:35,916 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 1 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 1 (n.sid), FOLLOWING (my state)
2012-07-24 20:29:35,916 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@621] - LOOKING
2012-07-24 20:29:35,918 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FileSnap@82] - Reading snapshot /home/hduser/zookeeper/conf/version-2/snapshot.100000000
2012-07-24 20:29:35,919 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@663] - New election. My id =  2, Proposed zxid = 4294967296
2012-07-24 20:29:35,919 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 2 (n.sid), LOOKING (my state)
2012-07-24 20:29:35,920 - WARN  [WorkerSender Thread:QuorumCnxManager@384] - Cannot open channel to 5 at election address rsmm-slave-4/109.123.121.27:3888
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:592)
	at sun.nio.ch.SocketAdaptor.connect(SocketAdaptor.java:118)
	at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:371)
	at org.apache.zookeeper.server.quorum.QuorumCnxManager.toSend(QuorumCnxManager.java:340)
	at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.process(FastLeaderElection.java:360)
	at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.run(FastLeaderElection.java:333)
	at java.lang.Thread.run(Thread.java:679)
2012-07-24 20:29:35,920 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 3 (n.leader), 0 (n.zxid), 2 (n.round), LOOKING (n.state), 3 (n.sid), LOOKING (my state)
2012-07-24 20:29:35,922 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 1 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 3 (n.sid), LOOKING (my state)
2012-07-24 20:29:35,926 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 4 (n.leader), 0 (n.zxid), 2 (n.round), LOOKING (n.state), 4 (n.sid), LOOKING (my state)
2012-07-24 20:29:35,928 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 4 (n.sid), LOOKING (my state)
2012-07-24 20:29:35,932 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 1 (n.sid), LOOKING (my state)
2012-07-24 20:29:35,936 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 3 (n.sid), LOOKING (my state)
2012-07-24 20:29:36,137 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@655] - LEADING
2012-07-24 20:29:36,141 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Leader@55] - TCP NoDelay set to: true
2012-07-24 20:29:36,143 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:ZooKeeperServer@154] - Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 datadir /home/hduser/zookeeper/conf/version-2 snapdir /home/hduser/zookeeper/conf/version-2
2012-07-24 20:29:36,147 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FileSnap@82] - Reading snapshot /home/hduser/zookeeper/conf/version-2/snapshot.100000000
2012-07-24 20:29:36,148 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FileTxnSnapLog@254] - Snapshotting: 100000000
2012-07-24 20:29:37,149 - INFO  [LearnerHandler-/109.123.121.26:34087:LearnerHandler@249] - Follower sid: 4 : info : org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@1c74f37
2012-07-24 20:29:37,150 - INFO  [LearnerHandler-/109.123.121.26:34087:LearnerHandler@273] - Synchronizing with Follower sid: 4 maxCommittedLog =0 minCommittedLog = 0 peerLastZxid = 0
2012-07-24 20:29:37,151 - INFO  [LearnerHandler-/109.123.121.26:34087:LearnerHandler@357] - Sending snapshot last zxid of peer is 0x0  zxid of leader is 0x200000000sent zxid of db as 0x100000000
2012-07-24 20:29:37,152 - INFO  [LearnerHandler-/109.123.121.23:41659:LearnerHandler@249] - Follower sid: 1 : info : org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@a17083
2012-07-24 20:29:37,153 - INFO  [LearnerHandler-/109.123.121.23:41659:LearnerHandler@273] - Synchronizing with Follower sid: 1 maxCommittedLog =0 minCommittedLog = 0 peerLastZxid = 100000000
2012-07-24 20:29:37,154 - INFO  [LearnerHandler-/109.123.121.23:41659:LearnerHandler@357] - Sending snapshot last zxid of peer is 0x100000000  zxid of leader is 0x200000000sent zxid of db as 0x100000000
2012-07-24 20:29:37,156 - INFO  [LearnerHandler-/109.123.121.25:54707:LearnerHandler@249] - Follower sid: 3 : info : org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@16fe0f4
2012-07-24 20:29:37,156 - INFO  [LearnerHandler-/109.123.121.25:54707:LearnerHandler@273] - Synchronizing with Follower sid: 3 maxCommittedLog =0 minCommittedLog = 0 peerLastZxid = 0
2012-07-24 20:29:37,157 - INFO  [LearnerHandler-/109.123.121.25:54707:LearnerHandler@357] - Sending snapshot last zxid of peer is 0x0  zxid of leader is 0x200000000sent zxid of db as 0x100000000
2012-07-24 20:29:37,159 - WARN  [LearnerHandler-/109.123.121.26:34087:Leader@492] - Commiting zxid 0x200000000 from /109.123.121.24:2888 not first!
2012-07-24 20:29:37,160 - WARN  [LearnerHandler-/109.123.121.26:34087:Leader@494] - First is 0
2012-07-24 20:29:37,172 - INFO  [LearnerHandler-/109.123.121.26:34087:Leader@518] - Have quorum of supporters; starting up and setting last processed zxid: 8589934592
2012-07-24 20:30:40,397 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 5 (n.leader), 0 (n.zxid), 1 (n.round), LOOKING (n.state), 5 (n.sid), LEADING (my state)
2012-07-24 20:30:40,397 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 1 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 5 (n.sid), LEADING (my state)
2012-07-24 20:30:40,398 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 5 (n.sid), LEADING (my state)
2012-07-24 20:30:40,400 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 5 (n.sid), LEADING (my state)
2012-07-24 20:30:40,641 - INFO  [LearnerHandler-/109.123.121.27:34526:LearnerHandler@249] - Follower sid: 5 : info : org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@15663a2
2012-07-24 20:30:40,642 - INFO  [LearnerHandler-/109.123.121.27:34526:LearnerHandler@273] - Synchronizing with Follower sid: 5 maxCommittedLog =0 minCommittedLog = 0 peerLastZxid = 0
2012-07-24 20:30:40,642 - INFO  [LearnerHandler-/109.123.121.27:34526:LearnerHandler@357] - Sending snapshot last zxid of peer is 0x0  zxid of leader is 0x200000000sent zxid of db as 0x200000000

2012-07-24 20:30:37,768 - INFO  [main:QuorumPeerConfig@90] - Reading configuration from: /home/hduser/zookeeper/bin/../conf/zoo.cfg
2012-07-24 20:30:37,774 - INFO  [main:QuorumPeerConfig@310] - Defaulting to majority quorums
2012-07-24 20:30:37,792 - INFO  [main:QuorumPeerMain@119] - Starting quorum peer
2012-07-24 20:30:37,820 - INFO  [main:NIOServerCnxn$Factory@143] - binding to port 0.0.0.0/0.0.0.0:2181
2012-07-24 20:30:37,845 - INFO  [main:QuorumPeer@819] - tickTime set to 2000
2012-07-24 20:30:37,845 - INFO  [main:QuorumPeer@830] - minSessionTimeout set to -1
2012-07-24 20:30:37,846 - INFO  [main:QuorumPeer@841] - maxSessionTimeout set to -1
2012-07-24 20:30:37,846 - INFO  [main:QuorumPeer@856] - initLimit set to 10
2012-07-24 20:30:37,863 - INFO  [main:FileSnap@82] - Reading snapshot /home/hduser/zookeeper/conf/version-2/snapshot.0
2012-07-24 20:30:37,895 - INFO  [Thread-1:QuorumCnxManager$Listener@473] - My election bind port: 3888
2012-07-24 20:30:37,909 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@621] - LOOKING
2012-07-24 20:30:37,912 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@663] - New election. My id =  5, Proposed zxid = 0
2012-07-24 20:30:37,923 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 5 (n.leader), 0 (n.zxid), 1 (n.round), LOOKING (n.state), 1 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,923 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 1 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 1 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,924 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 1 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,924 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), FOLLOWING (n.state), 1 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,925 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@721] - Updating proposal
2012-07-24 20:30:37,928 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 5 (n.leader), 0 (n.zxid), 1 (n.round), LOOKING (n.state), 5 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,929 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 1 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 5 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,929 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 5 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,931 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 1 (n.round), LOOKING (n.state), 2 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,932 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 2 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,932 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LEADING (n.state), 2 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,933 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), FOLLOWING (n.state), 1 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,933 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 5 (n.leader), 0 (n.zxid), 1 (n.round), LOOKING (n.state), 3 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,934 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 5 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,935 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), FOLLOWING (n.state), 1 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,935 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 3 (n.leader), 0 (n.zxid), 2 (n.round), LOOKING (n.state), 3 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,936 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 1 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 3 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,937 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LEADING (n.state), 2 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,937 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 3 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,938 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), FOLLOWING (n.state), 3 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,938 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LEADING (n.state), 2 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,938 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), FOLLOWING (n.state), 3 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,939 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), FOLLOWING (n.state), 3 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,939 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LEADING (n.state), 2 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,939 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 5 (n.leader), 0 (n.zxid), 1 (n.round), LOOKING (n.state), 4 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,940 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 4 (n.leader), 0 (n.zxid), 2 (n.round), LOOKING (n.state), 4 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,941 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), FOLLOWING (n.state), 3 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,941 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 4 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,941 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), FOLLOWING (n.state), 4 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,942 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), FOLLOWING (n.state), 4 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,942 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), FOLLOWING (n.state), 4 (n.sid), LOOKING (my state)
2012-07-24 20:30:37,942 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), FOLLOWING (n.state), 4 (n.sid), LOOKING (my state)
2012-07-24 20:30:38,143 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@643] - FOLLOWING
2012-07-24 20:30:38,150 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Learner@80] - TCP NoDelay set to: true
2012-07-24 20:30:38,157 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:zookeeper.version=3.3.5-1301095, built on 03/15/2012 19:48 GMT
2012-07-24 20:30:38,157 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:host.name=rsmm-slave-4
2012-07-24 20:30:38,158 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:java.version=1.6.0_23
2012-07-24 20:30:38,158 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:java.vendor=Sun Microsystems Inc.
2012-07-24 20:30:38,158 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:java.home=/usr/lib/jvm/java-6-openjdk/jre
2012-07-24 20:30:38,159 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:java.class.path=/home/hduser/zookeeper/bin/../build/classes:/home/hduser/zookeeper/bin/../build/lib/*.jar:/home/hduser/zookeeper/bin/../zookeeper-3.3.5.jar:/home/hduser/zookeeper/bin/../lib/log4j-1.2.15.jar:/home/hduser/zookeeper/bin/../lib/jline-0.9.94.jar:/home/hduser/zookeeper/bin/../src/java/lib/*.jar:/home/hduser/zookeeper/bin/../conf:
2012-07-24 20:30:38,159 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:java.library.path=/usr/lib/jvm/java-6-openjdk/jre/lib/i386/client:/usr/lib/jvm/java-6-openjdk/jre/lib/i386:/usr/lib/jvm/java-6-openjdk/jre/../lib/i386:/usr/java/packages/lib/i386:/usr/lib/jni:/lib:/usr/lib
2012-07-24 20:30:38,159 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:java.io.tmpdir=/tmp
2012-07-24 20:30:38,159 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:java.compiler=<NA>
2012-07-24 20:30:38,160 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:os.name=Linux
2012-07-24 20:30:38,160 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:os.arch=i386
2012-07-24 20:30:38,160 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:os.version=3.0.0-12-generic
2012-07-24 20:30:38,160 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:user.name=hduser
2012-07-24 20:30:38,160 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:user.home=/home/hduser
2012-07-24 20:30:38,161 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:user.dir=/home/hduser/zookeeper/bin
2012-07-24 20:30:38,162 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:ZooKeeperServer@154] - Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 datadir /home/hduser/zookeeper/conf/version-2 snapdir /home/hduser/zookeeper/conf/version-2
2012-07-24 20:30:38,175 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Learner@294] - Getting a snapshot from leader
2012-07-24 20:30:38,179 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Learner@326] - Setting leader epoch 2
2012-07-24 20:30:38,180 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FileTxnSnapLog@254] - Snapshotting: 200000000
2012-07-24 20:30:46,564 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn$Factory@251] - Accepted socket connection from /127.0.0.1:41116
2012-07-24 20:30:46,569 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1237] - Processing srvr command from /127.0.0.1:41116
2012-07-24 20:30:46,573 - INFO  [Thread-10:NIOServerCnxn@1435] - Closed socket connection for client /127.0.0.1:41116 (no session established for client)
2012-07-24 20:33:27,407 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn$Factory@251] - Accepted socket connection from /127.0.0.1:41118
2012-07-24 20:33:27,408 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1237] - Processing srvr command from /127.0.0.1:41118
2012-07-24 20:33:27,411 - INFO  [Thread-11:NIOServerCnxn@1435] - Closed socket connection for client /127.0.0.1:41118 (no session established for client)
2012-07-24 20:47:21,659 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn$Factory@251] - Accepted socket connection from /127.0.0.1:41126
2012-07-24 20:47:21,660 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1237] - Processing srvr command from /127.0.0.1:41126
2012-07-24 20:47:21,663 - INFO  [Thread-12:NIOServerCnxn@1435] - Closed socket connection for client /127.0.0.1:41126 (no session established for client)
==================================

while the leader 's log shows

2012-07-24 20:22:33,769 - INFO  [main:QuorumPeerConfig@90] - Reading configuration from: /home/hduser/zookeeper/bin/../conf/zoo.cfg
2012-07-24 20:22:33,776 - INFO  [main:QuorumPeerConfig@310] - Defaulting to majority quorums
2012-07-24 20:22:33,795 - INFO  [main:QuorumPeerMain@119] - Starting quorum peer
2012-07-24 20:22:33,827 - INFO  [main:NIOServerCnxn$Factory@143] - binding to port 0.0.0.0/0.0.0.0:2181
2012-07-24 20:22:33,854 - INFO  [main:QuorumPeer@819] - tickTime set to 2000
2012-07-24 20:22:33,854 - INFO  [main:QuorumPeer@830] - minSessionTimeout set to -1
2012-07-24 20:22:33,855 - INFO  [main:QuorumPeer@841] - maxSessionTimeout set to -1
2012-07-24 20:22:33,855 - INFO  [main:QuorumPeer@856] - initLimit set to 10
2012-07-24 20:22:33,874 - INFO  [main:FileSnap@82] - Reading snapshot /home/hduser/zookeeper/conf/version-2/snapshot.100000000
2012-07-24 20:22:33,905 - INFO  [Thread-1:QuorumCnxManager$Listener@473] - My election bind port: 3888
2012-07-24 20:22:33,923 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@621] - LOOKING
2012-07-24 20:22:33,926 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@663] - New election. My id =  2, Proposed zxid = 4294967296
2012-07-24 20:22:33,935 - INFO  [WorkerSender Thread:QuorumCnxManager@183] - Have smaller server identifier, so dropping the connection: (3, 2)
2012-07-24 20:22:33,935 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 1 (n.round), LOOKING (n.state), 2 (n.sid), LOOKING (my state)
2012-07-24 20:22:33,936 - INFO  [WorkerSender Thread:QuorumCnxManager@183] - Have smaller server identifier, so dropping the connection: (4, 2)
2012-07-24 20:22:33,937 - INFO  [WorkerSender Thread:QuorumCnxManager@183] - Have smaller server identifier, so dropping the connection: (5, 2)
2012-07-24 20:22:33,938 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 5 (n.leader), 0 (n.zxid), 1 (n.round), LOOKING (n.state), 1 (n.sid), LOOKING (my state)
2012-07-24 20:22:33,939 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 5 (n.leader), 0 (n.zxid), 1 (n.round), FOLLOWING (n.state), 1 (n.sid), LOOKING (my state)
2012-07-24 20:22:33,941 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 5 (n.leader), 0 (n.zxid), 1 (n.round), FOLLOWING (n.state), 3 (n.sid), LOOKING (my state)
2012-07-24 20:22:33,941 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 5 (n.leader), 0 (n.zxid), 1 (n.round), FOLLOWING (n.state), 4 (n.sid), LOOKING (my state)
2012-07-24 20:22:33,942 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 5 (n.leader), 0 (n.zxid), 1 (n.round), FOLLOWING (n.state), 4 (n.sid), LOOKING (my state)
2012-07-24 20:22:33,943 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 5 (n.leader), 0 (n.zxid), 1 (n.round), FOLLOWING (n.state), 3 (n.sid), LOOKING (my state)
2012-07-24 20:22:33,945 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 5 (n.leader), 0 (n.zxid), 1 (n.round), LEADING (n.state), 5 (n.sid), LOOKING (my state)
2012-07-24 20:22:33,945 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 5 (n.leader), 0 (n.zxid), 1 (n.round), LEADING (n.state), 5 (n.sid), FOLLOWING (my state)
2012-07-24 20:22:33,946 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@643] - FOLLOWING
2012-07-24 20:22:33,952 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Learner@80] - TCP NoDelay set to: true
2012-07-24 20:22:33,959 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:zookeeper.version=3.3.5-1301095, built on 03/15/2012 19:48 GMT
2012-07-24 20:22:33,960 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:host.name=rsmm-slave-1
2012-07-24 20:22:33,960 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:java.version=1.6.0_23
2012-07-24 20:22:33,960 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:java.vendor=Sun Microsystems Inc.
2012-07-24 20:22:33,961 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:java.home=/usr/lib/jvm/java-6-openjdk/jre
2012-07-24 20:22:33,961 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:java.class.path=/home/hduser/zookeeper/bin/../build/classes:/home/hduser/zookeeper/bin/../build/lib/*.jar:/home/hduser/zookeeper/bin/../zookeeper-3.3.5.jar:/home/hduser/zookeeper/bin/../lib/log4j-1.2.15.jar:/home/hduser/zookeeper/bin/../lib/jline-0.9.94.jar:/home/hduser/zookeeper/bin/../src/java/lib/*.jar:/home/hduser/zookeeper/bin/../conf:
2012-07-24 20:22:33,961 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:java.library.path=/usr/lib/jvm/java-6-openjdk/jre/lib/i386/client:/usr/lib/jvm/java-6-openjdk/jre/lib/i386:/usr/lib/jvm/java-6-openjdk/jre/../lib/i386:/usr/java/packages/lib/i386:/usr/lib/jni:/lib:/usr/lib
2012-07-24 20:22:33,961 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:java.io.tmpdir=/tmp
2012-07-24 20:22:33,962 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:java.compiler=<NA>
2012-07-24 20:22:33,962 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:os.name=Linux
2012-07-24 20:22:33,962 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:os.arch=i386
2012-07-24 20:22:33,962 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:os.version=3.0.0-12-generic
2012-07-24 20:22:33,962 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:user.name=hduser
2012-07-24 20:22:33,963 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:user.home=/home/hduser
2012-07-24 20:22:33,963 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Environment@97] - Server environment:user.dir=/home/hduser/zookeeper/bin
2012-07-24 20:22:33,965 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:ZooKeeperServer@154] - Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 datadir /home/hduser/zookeeper/conf/version-2 snapdir /home/hduser/zookeeper/conf/version-2
2012-07-24 20:22:33,977 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Learner@291] - Getting a diff from the leader 0x100000000
2012-07-24 20:22:33,981 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Learner@326] - Setting leader epoch 1
2012-07-24 20:22:33,983 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FileTxnSnapLog@254] - Snapshotting: 100000000
2012-07-24 20:22:40,102 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn$Factory@251] - Accepted socket connection from /127.0.0.1:41400
2012-07-24 20:22:40,106 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1237] - Processing srvr command from /127.0.0.1:41400
2012-07-24 20:22:40,109 - INFO  [Thread-10:NIOServerCnxn@1435] - Closed socket connection for client /127.0.0.1:41400 (no session established for client)
2012-07-24 20:29:35,903 - WARN  [Thread-9:QuorumCnxManager$RecvWorker@727] - Connection broken for id 5, my id = 2, error = java.io.IOException: Channel eof
2012-07-24 20:29:35,904 - WARN  [Thread-9:QuorumCnxManager$RecvWorker@730] - Interrupting SendWorker
2012-07-24 20:29:35,905 - WARN  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Follower@82] - Exception when following the leader
java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
	at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:84)
	at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:108)
	at org.apache.zookeeper.server.quorum.Learner.readPacket(Learner.java:148)
	at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:78)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:645)
2012-07-24 20:29:35,905 - WARN  [Thread-8:QuorumCnxManager$SendWorker@633] - Interrupted while waiting for message on queue
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2017)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:370)
	at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:622)
2012-07-24 20:29:35,907 - WARN  [Thread-8:QuorumCnxManager$SendWorker@642] - Send worker leaving thread
2012-07-24 20:29:35,907 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Follower@165] - shutdown called
java.lang.Exception: shutdown Follower
	at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:165)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:649)
2012-07-24 20:29:35,913 - INFO  [FollowerRequestProcessor:2:FollowerRequestProcessor@93] - FollowerRequestProcessor exited loop!
2012-07-24 20:29:35,914 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FinalRequestProcessor@370] - shutdown of request processor complete
2012-07-24 20:29:35,914 - INFO  [CommitProcessor:2:CommitProcessor@148] - CommitProcessor exited loop!
2012-07-24 20:29:35,915 - INFO  [SyncThread:2:SyncRequestProcessor@151] - SyncRequestProcessor exited!
2012-07-24 20:29:35,916 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 1 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 1 (n.sid), FOLLOWING (my state)
2012-07-24 20:29:35,916 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@621] - LOOKING
2012-07-24 20:29:35,918 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FileSnap@82] - Reading snapshot /home/hduser/zookeeper/conf/version-2/snapshot.100000000
2012-07-24 20:29:35,919 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@663] - New election. My id =  2, Proposed zxid = 4294967296
2012-07-24 20:29:35,919 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 2 (n.sid), LOOKING (my state)
2012-07-24 20:29:35,920 - WARN  [WorkerSender Thread:QuorumCnxManager@384] - Cannot open channel to 5 at election address rsmm-slave-4/109.123.121.27:3888
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:592)
	at sun.nio.ch.SocketAdaptor.connect(SocketAdaptor.java:118)
	at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:371)
	at org.apache.zookeeper.server.quorum.QuorumCnxManager.toSend(QuorumCnxManager.java:340)
	at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.process(FastLeaderElection.java:360)
	at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.run(FastLeaderElection.java:333)
	at java.lang.Thread.run(Thread.java:679)
2012-07-24 20:29:35,920 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 3 (n.leader), 0 (n.zxid), 2 (n.round), LOOKING (n.state), 3 (n.sid), LOOKING (my state)
2012-07-24 20:29:35,922 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 1 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 3 (n.sid), LOOKING (my state)
2012-07-24 20:29:35,926 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 4 (n.leader), 0 (n.zxid), 2 (n.round), LOOKING (n.state), 4 (n.sid), LOOKING (my state)
2012-07-24 20:29:35,928 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 4 (n.sid), LOOKING (my state)
2012-07-24 20:29:35,932 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 1 (n.sid), LOOKING (my state)
2012-07-24 20:29:35,936 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 3 (n.sid), LOOKING (my state)
2012-07-24 20:29:36,137 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@655] - LEADING
2012-07-24 20:29:36,141 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Leader@55] - TCP NoDelay set to: true
2012-07-24 20:29:36,143 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:ZooKeeperServer@154] - Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 datadir /home/hduser/zookeeper/conf/version-2 snapdir /home/hduser/zookeeper/conf/version-2
2012-07-24 20:29:36,147 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FileSnap@82] - Reading snapshot /home/hduser/zookeeper/conf/version-2/snapshot.100000000
2012-07-24 20:29:36,148 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FileTxnSnapLog@254] - Snapshotting: 100000000
2012-07-24 20:29:37,149 - INFO  [LearnerHandler-/109.123.121.26:34087:LearnerHandler@249] - Follower sid: 4 : info : org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@1c74f37
2012-07-24 20:29:37,150 - INFO  [LearnerHandler-/109.123.121.26:34087:LearnerHandler@273] - Synchronizing with Follower sid: 4 maxCommittedLog =0 minCommittedLog = 0 peerLastZxid = 0
2012-07-24 20:29:37,151 - INFO  [LearnerHandler-/109.123.121.26:34087:LearnerHandler@357] - Sending snapshot last zxid of peer is 0x0  zxid of leader is 0x200000000sent zxid of db as 0x100000000
2012-07-24 20:29:37,152 - INFO  [LearnerHandler-/109.123.121.23:41659:LearnerHandler@249] - Follower sid: 1 : info : org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@a17083
2012-07-24 20:29:37,153 - INFO  [LearnerHandler-/109.123.121.23:41659:LearnerHandler@273] - Synchronizing with Follower sid: 1 maxCommittedLog =0 minCommittedLog = 0 peerLastZxid = 100000000
2012-07-24 20:29:37,154 - INFO  [LearnerHandler-/109.123.121.23:41659:LearnerHandler@357] - Sending snapshot last zxid of peer is 0x100000000  zxid of leader is 0x200000000sent zxid of db as 0x100000000
2012-07-24 20:29:37,156 - INFO  [LearnerHandler-/109.123.121.25:54707:LearnerHandler@249] - Follower sid: 3 : info : org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@16fe0f4
2012-07-24 20:29:37,156 - INFO  [LearnerHandler-/109.123.121.25:54707:LearnerHandler@273] - Synchronizing with Follower sid: 3 maxCommittedLog =0 minCommittedLog = 0 peerLastZxid = 0
2012-07-24 20:29:37,157 - INFO  [LearnerHandler-/109.123.121.25:54707:LearnerHandler@357] - Sending snapshot last zxid of peer is 0x0  zxid of leader is 0x200000000sent zxid of db as 0x100000000
2012-07-24 20:29:37,159 - WARN  [LearnerHandler-/109.123.121.26:34087:Leader@492] - Commiting zxid 0x200000000 from /109.123.121.24:2888 not first!
2012-07-24 20:29:37,160 - WARN  [LearnerHandler-/109.123.121.26:34087:Leader@494] - First is 0
2012-07-24 20:29:37,172 - INFO  [LearnerHandler-/109.123.121.26:34087:Leader@518] - Have quorum of supporters; starting up and setting last processed zxid: 8589934592
2012-07-24 20:30:40,397 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 5 (n.leader), 0 (n.zxid), 1 (n.round), LOOKING (n.state), 5 (n.sid), LEADING (my state)
2012-07-24 20:30:40,397 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 1 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 5 (n.sid), LEADING (my state)
2012-07-24 20:30:40,398 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 5 (n.sid), LEADING (my state)
2012-07-24 20:30:40,400 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 2 (n.leader), 4294967296 (n.zxid), 2 (n.round), LOOKING (n.state), 5 (n.sid), LEADING (my state)
2012-07-24 20:30:40,641 - INFO  [LearnerHandler-/109.123.121.27:34526:LearnerHandler@249] - Follower sid: 5 : info : org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@15663a2
2012-07-24 20:30:40,642 - INFO  [LearnerHandler-/109.123.121.27:34526:LearnerHandler@273] - Synchronizing with Follower sid: 5 maxCommittedLog =0 minCommittedLog = 0 peerLastZxid = 0
2012-07-24 20:30:40,642 - INFO  [LearnerHandler-/109.123.121.27:34526:LearnerHandler@357] - Sending snapshot last zxid of peer is 0x0  zxid of leader is 0x200000000sent zxid of db as 0x200000000
2012-07-24 20:49:19,788 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn$Factory@251] - Accepted socket connection from /127.0.0.1:41403
2012-07-24 20:49:19,789 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1237] - Processing srvr command from /127.0.0.1:41403
2012-07-24 20:49:19,791 - INFO  [Thread-18:NIOServerCnxn@1435] - Closed socket connection for client /127.0.0.1:41403 (no session established for client)","[<JIRA Version: name='3.3.5', id='12319081'>]",Bug,ZOOKEEPER-1517,Major,liuli,Invalid,2012-08-03T00:17:18.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zookeeper follower closed,2012-08-03T00:30:28.000+0000,"[<JIRA Version: name='3.3.5', id='12319081'>]",1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='quorum', id='12312379'>]",2012-07-20T00:52:33.000+0000,Patrick D. Hunt,"In the following case we have a 3 server ensemble.

Initially all is well, zk3 is the leader.

However zk3 fails, restarts, and rejoins the quorum as the new leader (was the old leader, still the leader after re-election)

The existing two followers, zk1 and zk2 rejoin the new quorum again as followers of zk3.

zk1 then fails, the datadirectory is deleted (so it has no state whatsoever) and restarted. However zk1 can never rejoin the quorum (even after an hour). During this time zk2 and zk3 are serving properly.

Later all three servers are later restarted and properly form a functional quourm.


Here are some interesting log snippets. Nothing else of interest was seen in the logs during this time:

zk3. This is where it becomes the leader after failing initially (as the leader). Notice the ""round"" is ahead of zk1 and zk2:

{noformat}
2012-07-18 17:19:35,423 - INFO  [QuorumPeer:/0.0.0.0:2181:FastLeaderElection@663] - New election. My id =  3, Proposed zxid = 77309411648
2012-07-18 17:19:35,423 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 3 (n.leader), 77309411648 (n.zxid), 832 (n.round), LOOKING (n.state), 3 (n.sid), LOOKING (my state)
2012-07-18 17:19:35,424 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 3 (n.leader), 73014444480 (n.zxid), 831 (n.round), FOLLOWING (n.state), 2 (n.sid), LOOKING (my state)
2012-07-18 17:19:35,424 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 3 (n.leader), 73014444480 (n.zxid), 831 (n.round), FOLLOWING (n.state), 1 (n.sid), LOOKING (my state)
2012-07-18 17:19:35,424 - INFO  [QuorumPeer:/0.0.0.0:2181:QuorumPeer@655] - LEADING
{noformat}

zk1 which won't come back. Notice that zk3 is reporting the round as 831, while zk2 thinks that the round is 832:

{noformat}
2012-07-18 17:31:12,015 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 1 (n.leader), 77309411648 (n.zxid), 1 (n.round), LOOKING (n.state), 1 (n.sid), LOOKING (my state)
2012-07-18 17:31:12,016 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 3 (n.leader), 73014444480 (n.zxid), 831 (n.round), LEADING (n.state), 3 (n.sid), LOOKING (my state)
2012-07-18 17:31:12,017 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 3 (n.leader), 77309411648 (n.zxid), 832 (n.round), FOLLOWING (n.state), 2 (n.sid), LOOKING (my state)
2012-07-18 17:31:15,219 - INFO  [QuorumPeer:/0.0.0.0:2181:FastLeaderElection@697] - Notification time out: 6400
{noformat}
","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1514,Critical,Patrick D. Hunt,Fixed,2012-08-02T22:29:52.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,FastLeaderElection - leader ignores the round information when joining a quorum,2012-08-03T10:55:21.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>]",6.0
Skye Wanderman-Milne,"[<JIRA Component: name='server', id='12312382'>]",2012-07-20T00:38:33.000+0000,Patrick D. Hunt,"The server is allowing a client to set data larger than the server can then later read:

{noformat}
2012-07-18 14:28:12,555 - FATAL [main:QuorumPeer@400] - Unable to load database on disk 
java.io.IOException: Unreasonable length = 1048583 
at org.apache.jute.BinaryInputArchive.readBuffer(BinaryInputArchive.java:100) 
at org.apache.zookeeper.server.persistence.Util.readTxnBytes(Util.java:232) 
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:602) 
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.init(FileTxnLog.java:529) 
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.<init>(FileTxnLog.java:504) 
at org.apache.zookeeper.server.persistence.FileTxnLog.read(FileTxnLog.java:341) 
at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:131) 
at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:222) 
at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:398) 
at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:143) 
at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:103) 
at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:76) 
2012-07-18 14:28:12,555 - FATAL [main:QuorumPeerMain@87] - Unexpected exception, exiting abnormally 
java.lang.RuntimeException: Unable to run quorum server 
at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:401) 
at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:143) 
at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:103) 
at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:76) 
Caused by: java.io.IOException: Unreasonable length = 1048583 
at org.apache.jute.BinaryInputArchive.readBuffer(BinaryInputArchive.java:100) 
at org.apache.zookeeper.server.persistence.Util.readTxnBytes(Util.java:232) 
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:602) 
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.init(FileTxnLog.java:529) 
at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.<init>(FileTxnLog.java:504) 
at org.apache.zookeeper.server.persistence.FileTxnLog.read(FileTxnLog.java:341) 
at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:131) 
at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:222) 
at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:398) 
... 3 more
{noformat}

Notice the size is 0x100007 - 7 bytes beyond.

The SetDataTxn contains the client data + a couple extra fields. On ingest the server is applying the jute.maxbuffer size to the data (expected) but not handling the fact that the data plus these extra fields may exceed the jute.maxbuffer check when reading from disk.

Workaround was simple here: set the jute.maxbuffer size a bit higher (and fix the mis-behaving client, expectation was not that the data would grow this large).
","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1513,Major,Patrick D. Hunt,Fixed,2012-12-12T06:52:29.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"""Unreasonable length"" exception while starting a server.",2014-03-13T18:16:53.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>]",10.0
,"[<JIRA Component: name='java client', id='12312381'>]",2012-07-18T18:26:46.000+0000,Micah Whitacre,"When running the Java client you frequently get messages like the following:

org.apache.zookeeper.client.ZooKeeperSaslClient SecurityException: java.lang.SecurityException: Unable to locate a login configuration occurred when trying to find JAAS configuration.

In cases where we don't want this configuration enabled, the logs get spammed with this message.  It's scope should lowered to debug/trace to prevent flooding logs.",[],Bug,ZOOKEEPER-1512,Major,Micah Whitacre,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Reduce log level of missing ZookeeperSaslClient Security Exception,2014-05-23T12:00:10.000+0000,[],5.0
Brian Sutherland,"[<JIRA Component: name='contrib', id='12312700'>]",2012-07-04T17:14:54.000+0000,Brian Sutherland,Returning OK under such conditions is really not good...,"[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1501,Major,Brian Sutherland,Fixed,2012-09-07T06:30:53.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Nagios plugin always returns OK when it cannot connect to zookeeper,2012-09-07T11:01:51.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",4.0
Brian Sutherland,"[<JIRA Component: name='contrib', id='12312700'>]",2012-07-04T17:10:34.000+0000,Brian Sutherland,"The plugin requires a difference between the warning and critical value for the checks to work. If the values are the same, OK is always returned.

I can't figure out how to attach a file to this ticket in JIRA, so here's a minimal inline patch that at least lets the admin know it's not working:

{noformat}
Index: src/contrib/monitoring/check_zookeeper.py
===================================================================
--- src/contrib/monitoring/check_zookeeper.py	(revision 1357335)
+++ src/contrib/monitoring/check_zookeeper.py	(working copy)
@@ -57,6 +57,10 @@
             print >>sys.stderr, 'Invalid values for ""warning"" and ""critical"".'
             return 2
 
+        if warning == critical:
+            print >>sys.stderr, '""warning"" and ""critical"" cannot have the same value.'
+            return 2
+
         if opts.key is None:
             print >>sys.stderr, 'You should specify a key name.'
             return 2
{noformat}
",[],Bug,ZOOKEEPER-1500,Minor,Brian Sutherland,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Nagios check always returns OK when the critical and warning values are the same,2012-07-31T22:37:28.000+0000,[],1.0
Alexander Shraer,"[<JIRA Component: name='server', id='12312382'>]",2012-07-03T22:33:02.000+0000,Camille Fournier,"With the new reconfig logic, clientPort=2181 in the zoo.cfg file no longer gets read, and clients can't connect without adding ;2181 to the end of their server lines. ","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1499,Blocker,Camille Fournier,Fixed,2013-10-24T05:21:48.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,clientPort config changes not backwards-compatible,2013-10-24T11:08:33.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",5.0
,"[<JIRA Component: name='server', id='12312382'>]",2012-07-03T21:25:43.000+0000,Camille Fournier,"In pre-Zab1.0, we would process the NEWLEADER packet in registerWithLeader. Now we only process it in syncWithLeader, and in certain circumstances (the first follower of a new leader) it seems like we get 2 of them, which causes 2 snapshots to be taken one right after another. Not sure whether we should ignore taking the snapshot the second time, or not send two packets, or what. ",[],Bug,ZOOKEEPER-1498,Minor,Camille Fournier,Duplicate,2013-01-04T02:29:39.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zab1.0 sends NEWLEADER packet twice,2013-01-04T02:29:39.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.5.0', id='12316644'>]",3.0
Rakesh Radhakrishnan,"[<JIRA Component: name='server', id='12312382'>]",2012-06-28T10:24:35.000+0000,suja s,"In one of the tests we performed, came across a case where the ephemeral node was not getting cleared from zookeeper though the client exited.

Zk version: 3.4.3

Ephemeral node still exists in Zookeeper: 

HOST-xx-xx-xx-55:/home/Jun25_LR/install/zookeeper/bin # date 

Tue Jun 26 16:07:04 IST 2012 
HOST-xx-xx-xx-55:/home/Jun25_LR/install/zookeeper/bin # ./zkCli.sh -server xx.xx.xx.55:2182 
Connecting to xx.xx.xx.55:2182 
Welcome to ZooKeeper! 
JLine support is enabled 
[zk: xx.xx.xx.55:2182(CONNECTING) 0] 
WATCHER:: 

WatchedEvent state:SyncConnected type:None path:null 

[zk: xx.xx.xx.55:2182(CONNECTED) 0] get /hadoop-ha/hacluster/ActiveStandbyElectorLock 

haclusternn2HOSt-xx-xx-xx-102 ï¿½ï¿½ 
cZxid = 0x200000075 
ctime = Tue Jun 26 13:10:19 IST 2012 
mZxid = 0x200000075 
mtime = Tue Jun 26 13:10:19 IST 2012 
pZxid = 0x200000075 
cversion = 0 
dataVersion = 0 
aclVersion = 0 
ephemeralOwner = 0x1382791d4e50004 
dataLength = 42 
numChildren = 0 
[zk: xx.xx.xx.55:2182(CONNECTED) 1] 

Grepped logs at ZK side for session ""0x1382791d4e50004"" - close session and later create coming before closesession processed. 

HOSt-xx-xx-xx-91:/home/Jun25_LR/install/zookeeper/logs # grep -E ""/hadoop-ha/hacluster/ActiveStandbyElectorLock|0x1382791d4e50004"" *|grep 0x200000074 
2012-06-26 13:10:18,834 [myid:3] - DEBUG [ProcessThread(sid:3 cport:-1)::CommitProcessor@171] - Processing request:: sessionid:0x1382791d4e50004 type:closeSession cxid:0x0 zxid:0x200000074 txntype:-11 reqpath:n/a 
2012-06-26 13:10:19,892 [myid:3] - DEBUG [ProcessThread(sid:3 cport:-1)::Leader@716] - Proposing:: sessionid:0x1382791d4e50004 type:closeSession cxid:0x0 zxid:0x200000074 txntype:-11 reqpath:n/a 
2012-06-26 13:10:19,919 [myid:3] - DEBUG [LearnerHandler-/xx.xx.xx.102:13846:CommitProcessor@161] - Committing request:: sessionid:0x1382791d4e50004 type:closeSession cxid:0x0 zxid:0x200000074 txntype:-11 reqpath:n/a 
2012-06-26 13:10:20,608 [myid:3] - DEBUG [CommitProcessor:3:FinalRequestProcessor@88] - Processing request:: sessionid:0x1382791d4e50004 type:closeSession cxid:0x0 zxid:0x200000074 txntype:-11 reqpath:n/a 

HOSt-xx-xx-xx-91:/home/Jun25_LR/install/zookeeper/logs # grep -E ""/hadoop-ha/hacluster/ActiveStandbyElectorLock|0x1382791d4e50004"" *|grep 0x200000075 
2012-06-26 13:10:19,893 [myid:3] - DEBUG [ProcessThread(sid:3 cport:-1)::CommitProcessor@171] - Processing request:: sessionid:0x1382791d4e50004 type:create cxid:0x2 zxid:0x200000075 txntype:1 reqpath:n/a 
2012-06-26 13:10:19,920 [myid:3] - DEBUG [ProcessThread(sid:3 cport:-1)::Leader@716] - Proposing:: sessionid:0x1382791d4e50004 type:create cxid:0x2 zxid:0x200000075 txntype:1 reqpath:n/a 
2012-06-26 13:10:20,278 [myid:3] - DEBUG [LearnerHandler-/xx.xx.xx.102:13846:CommitProcessor@161] - Committing request:: sessionid:0x1382791d4e50004 type:create cxid:0x2 zxid:0x200000075 txntype:1 reqpath:n/a 
2012-06-26 13:10:20,752 [myid:3] - DEBUG [CommitProcessor:3:FinalRequestProcessor@88] - Processing request:: sessionid:0x1382791d4e50004 type:create cxid:0x2 zxid:0x200000075 txntype:1 reqpath:n/a 


 Close session and create requests coming almost parallely. 


Env:
Hadoop setup.
We were using Namenode HA with bookkeeper as shared storage and auto failover enabled.
NN102 was active and NN55 was standby. 
FailoverController at 102 got shut down due to ZK connection error. 
The lock-ActiveStandbyElectorLock created (ephemeral node) by this failovercontroller is not cleared from ZK
","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1496,Critical,suja s,Fixed,2012-09-17T07:58:01.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Ephemeral node not getting cleared even after client has exited,2012-09-17T11:02:06.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",9.0
Nicolas Liochon,"[<JIRA Component: name='server', id='12312382'>]",2012-06-28T07:51:03.000+0000,Nicolas Liochon,"This happens for example when using zk#multi with a 3.4 client but a 3.3 server.

The issue seems to be on the server side: the servers drops the packets with an unknown OpCode in ZooKeeperServer#submitRequest
{noformat}
public void submitRequest(Request si) {
    // snip
    try {
        touch(si.cnxn);
        boolean validpacket = Request.isValid(si.type); // ===> Check on case OpCode.*
        if (validpacket) {
            // snip
        } else {
            LOG.warn(""Dropping packet at server of type "" + si.type);
            // if invalid packet drop the packet.
        }
    } catch (MissingSessionException e) {
        if (LOG.isDebugEnabled()) {
            LOG.debug(""Dropping request: "" + e.getMessage());
        }
    }
}
{noformat}

The solution discussed in ZOOKEEPER-1381 would be to get an exception on the client side then & close the session.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1495,Minor,Nicolas Liochon,Fixed,2013-01-25T01:37:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZK client hangs when using a function not available on the server.,2014-03-13T18:16:55.000+0000,"[<JIRA Version: name='3.4.2', id='12319196'>, <JIRA Version: name='3.3.5', id='12319081'>]",7.0
Michi Mutsuzaki,"[<JIRA Component: name='c client', id='12312380'>]",2012-06-22T19:56:13.000+0000,Michi Mutsuzaki,"In zookeeper_interest(), we set zk->fd to -1 without closing it when timeout happens. Instead we should let handle_socket_error_msg() function take care of closing the socket properly.

--Michi","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1494,Major,Michi Mutsuzaki,Fixed,2012-09-10T07:04:28.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,C client: socket leak after receive timeout in zookeeper_interest(),2012-09-10T11:01:44.000+0000,"[<JIRA Version: name='3.4.2', id='12319196'>, <JIRA Version: name='3.3.5', id='12319081'>]",5.0
Michi Mutsuzaki,"[<JIRA Component: name='c client', id='12312380'>]",2012-06-20T20:47:58.000+0000,Michi Mutsuzaki,"In ZOOKEEPER-804, we added a check in zookeeper_process() to see if zookeeper_close() has been called. This was to avoid calling assert(cptr) on a NULL pointer, as dequeue_completion() returns NULL if the sent_requests queue has been cleared by free_completion() from zookeeper_close(). However, we should still call the completion if it is not NULL. ","[<JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1493,Major,Michi Mutsuzaki,Fixed,2012-07-29T05:35:34.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,C Client: zookeeper_process doesn't invoke completion callback if zookeeper_close has been called,2012-11-21T10:12:57.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.3.5', id='12319081'>]",7.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2012-06-20T12:51:21.000+0000,gaoxiao,"When a follower leave the cluster, and the cluster cannot achieve a majority, the leader should get out from Leading stat and get into Looking state, but if the there are some observers, the leader will not get away and the client cannot use the cluster.

eg:

The servers config:

server.1=z1:2888:3888
server.2=z2:2888:3888
server.3=z3:2888:3888:observer

At first, 1,2,3 are all started, it's all ok, 2 is the leader, but at this time, if 1 is stopped, 2 will not leave the Leading state, and client cannot connect to cluster.

I think the problem is:
(Leader.java  method:lead)

Line 388-407
                syncedSet.add(self.getId());
                synchronized (learners) {
                    for (LearnerHandler f : learners) {
                        if (f.synced()) {
                            syncedCount++;
                            syncedSet.add(f.getSid());
                        }
                        f.ping();
                    }
                }
              if (!tickSkip && !self.getQuorumVerifier().containsQuorum(syncedSet)) {
                //if (!tickSkip && syncedCount < self.quorumPeers.size() / 2) {
                    // Lost quorum, shutdown
                  // TODO: message is wrong unless majority quorums used
                    shutdown(""Only "" + syncedCount + "" followers, need ""
                            + (self.getVotingView().size() / 2));
                    // make sure the order is the same!
                    // the leader goes to looking
                    return;
              } 

The code add all learners' ping to syncedSet, and I think at this place, only followers should be added to syncedSet, so the method 'containsQuorum' can figure out the majority.",[],Bug,ZOOKEEPER-1492,Critical,gaoxiao,Duplicate,2012-06-20T14:18:21.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,leader cannot switch to LOOKING state when lost the majority,2012-06-20T14:18:21.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",2.0
,[],2012-06-19T18:59:21.000+0000,Keith Turner,"When I type help the shell, I see the following for the create command.

{noformat}
create [-s] [-e] path data acl
{noformat}

However, the ACL is optional.  So I think the usage message should look like the following.

{noformat}
create [-s] [-e] path data [acl]
{noformat}

",[],Bug,ZOOKEEPER-1491,Major,Keith Turner,Duplicate,2012-12-13T22:19:51.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Help for create command in zkCli is misleading,2012-12-13T22:19:51.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",2.0
suja s,"[<JIRA Component: name='scripts', id='12312384'>]",2012-06-19T04:10:41.000+0000,suja s,"if the configured log directory does not exists zookeeper will not start. Better to create the directory and start
in zkEnv.sh we can change as follows

if [ ""x${ZOO_LOG_DIR}"" = ""x"" ]
then
    ZOO_LOG_DIR="".""
   else
    if [ ! -w ""$ZOO_LOG_DIR"" ] ; then
        mkdir -p ""$ZOO_LOG_DIR""
    fi
fi
 

","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1490,Minor,suja s,Fixed,2012-06-30T06:08:31.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0, If the configured log directory does not exist zookeeper will not start. Better to create the directory and start,2012-06-30T11:01:10.000+0000,[],8.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2012-06-18T09:09:37.000+0000,Christian Ziech,"The truncate method on the transaction log in the class org.apache.zookeeper.server.persistence.FileTxnLog will reduce the file size to the required amount without either closing or re-positioning the logStream (which could also be dangerous since the truncate method is not synchronized against concurrent writes to the log).

This causes the next append to that log to create a small ""hole"" in the file which java would interpret as binary zeroes when reading it. This then causes to the FileTxnIterator.next() implementation to detect the end of the log file too early.

I'll attach a small maven project with one junit test which can be used to reproduce the issue. Due to the blackbox nature of the test it will run for roughly 50 seconds unfortunately. 

Steps to reproduce:
- Start an ensemble of zookeeper servers with at least 3 participants
- Create one entry and the remove one of the servers from the ensemble temporarily (e.g. zk-2)
- Create another entry which is hence only reflected on zk-1 and zk-3
- Take zk-1 out of the ensemble without shutting it down (that is important, I did that by interrupting the network connection to that node) and clean zk-3
- Bring back zk-2 and zk-3 so that they form a quorum
- Allow zk-1 to connect again
- zk-1 will receive a TRUNC message from zk-2 since zk-1 is now a minority knowing about that second node creation event
- Create a third node
- Force zk-1 to become master somehow
- That third node will be gone
","[<JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1489,Blocker,Christian Ziech,Fixed,2012-07-17T21:29:26.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Data loss after truncate on transaction log,2012-07-18T11:01:52.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.3.5', id='12319081'>]",9.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2012-06-15T06:13:26.000+0000,Kiran BC,"There are some internal link errors in the Zookeeper documentation. The list is as follows:
docs\zookeeperAdmin.html -> tickTime and datadir
docs\zookeeperOver.html -> fg_zkComponents, fg_zkPerfReliability and fg_zkPerfRW
docs\zookeeperStarted.html -> Logging",[],Bug,ZOOKEEPER-1488,Minor,Kiran BC,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Some links are not working in the Zookeeper Documentation,2015-07-01T23:53:26.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2012-06-15T05:57:02.000+0000,Surendra Singh Lilhore,"In [ZOOKEEPER-980|https://issues.apache.org/jira/browse/ZOOKEEPER-980] for log4j.properties provide some  properties that may be overridden using system properties. 
For example
JVMFLAGS=""-Dzookeeper.root.logger=DEBUG,CONSOLE,ROLLINGFILE -Dzookeeper.console.threshold=DEBUG"" bin/zkServer.sh start
But if we not override these property using system properties then zookeeper not able to create log file means these property not taking default value.  
",[],Bug,ZOOKEEPER-1487,Major,Surendra Singh Lilhore,Invalid,2012-06-18T14:07:11.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,if log4j.properties configuration parameters is not override  by system properties then zookeeper not able to  create log file.,2012-06-18T14:07:11.000+0000,[],2.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2012-06-12T21:07:44.000+0000,Dmitri Perelman,"Hi, 
There are two problems with the barrier example code in the tutorial:

1) A znode created by a process in the function enter() is created with SEQUENTIAL suffix, however, the name of a znode deleted in the function leave() doesn't have this suffix. Actually, the leave() function tries to delete a nonexistent node => a KeeperException is thrown, which is caught silently => the process terminates without waiting for the barrier. 

2) It seems that the very idea of leaving the barrier by deleting ephemeral nodes is problematic. Consider the following scenario: there are two clients: C1 and C2. 
 - C1 enters the barrier, creates a znode /b1/C1, checks that it's alone and starts waiting for the second client to come. 
 - C2 enters the barrier and creates a znode /b1/C2 - the notification to C1 is sent but still not delivered.
 - C2 observes that there are enough children to /b1, enters the barrier, executes its own operations and invokes leave() procedure.
 - during the leave() procedure C2 removes its znode /b1/C2 and exits.
 - when the notification about C2's arrival finally arrives to C1, C1 checks the children of /b1 and doesn't find C2's znode: C1 is stuck. 
The solution to this data race would be to create special znodes for leaving the barrier, similarly to the way they are created for entering the barrier. 

Thanks,
Dima",[],Bug,ZOOKEEPER-1486,Minor,Dmitri Perelman,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,A couple of bugs in the tutorial code,2012-07-20T22:11:33.000+0000,[],2.0
Martin Kuchta,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>]",2012-06-12T18:32:31.000+0000,Michi Mutsuzaki,"Both Java and C clients use signed 32-bit int as XIDs. XIDs are assumed to be non-negative, and zookeeper uses some negative values as special XIDs (e.g. -2 for ping, -4 for auth). However, neither Java nor C client ensures the XIDs it generates are non-negative, and the server doesn't reject negative XIDs.

Pat had some suggestions on how to fix this:

- (bin-compat) Expire the session when the client sends a negative XID.
- (bin-incompat) In addition to expiring the session, use 64-bit int for XID so that overflow will practically never happen.

--Michi",[],Bug,ZOOKEEPER-1485,Major,Michi Mutsuzaki,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,client xid overflow is not handled,2016-07-08T16:29:42.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.3.5', id='12319081'>]",10.0
Thawan Kooburat,"[<JIRA Component: name='server', id='12312382'>]",2012-06-11T21:36:58.000+0000,Thawan Kooburat,"We noticed that one of the follower fail to restart due to missing parent node

{noformat}
2012-05-29 15:44:41,037 [myid:9] - INFO [main:FileSnap@83] - Reading snapshot /var/facebook/zeus-server/data/global-ropt.0/version-2/snapshot.3d001f19c9
2012-05-29 15:44:43,300 [myid:9] - ERROR [main:FileTxnSnapLog@220] - Parent /phpunittest/1862297546 missing for /phpunittest/1862297546/dir1
2012-05-29 15:44:43,302 [myid:9] - ERROR [main:QuorumPeer@488] - Unable to load database on disk
java.io.IOException: Failed to process transaction type: 1 error: KeeperErrorCode = NoNode for /phpunittest/1862297546
{noformat}

We believed that the root cause is due to bugs in follower sync-up logic. Due to race condition, the follower may miss some proposals. The log below show that the follower see the commit message but it haven't seen this proposal before
{noformat}
2012-05-15 15:11:27,449 [myid:13] - WARN [QuorumPeer[myid=13]/0.0.0.0:2182:Learner@378] - Got zxid 0x3c00282dc9 expected 0x3c00282dca
{noformat}

I can reproduce this by keep running FollowerResyncConcurrencyTest until failure occurs. I suspected that the root caused is due to how we handle toBeApplied and outstandingProposals in the leader. 

1. In-flight proposals is removed from outstandingProposal before it is added to toBeApplied. Most of the problem I seen so far seem to caused by this gap.
2. startForwarding() iterate through outstandingProposal without locking PrepRequestProcessor properly, so there is possibility of missing in-flight proposal. 

",[],Bug,ZOOKEEPER-1484,Critical,Thawan Kooburat,Invalid,2012-06-16T02:04:34.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Missing znode found in the follower,2012-06-16T02:04:34.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",0.0
Michi Mutsuzaki,"[<JIRA Component: name='documentation', id='12312422'>]",2012-06-11T21:09:01.000+0000,Ankur Bansal,"The leader election recipe documentation suggest that to avoid the herd effect a client process volunteering for leadership via child znode [i] under the leader election path [/leader] must only watch the the SMALLEST znode [j] from a different client process such that [j < i]. 

This will NOT avoid the herd effect as many clients will end up watching the same znode[j] where j is the next-in-sequence number greater than the number of the current leader.

Specifically in Step 3 of the Election procedure here http://zookeeper.apache.org/doc/trunk/recipes.html#sc_leaderElection

This ""where j is the SMALLEST sequence number"" should be changed to this
""where j is the LARGEST sequence number""
","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1483,Major,Ankur Bansal,Fixed,2012-09-14T07:34:45.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Fix leader election recipe documentation,2012-12-14T22:11:14.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",5.0
Leader Ni,"[<JIRA Component: name='java client', id='12312381'>]",2012-06-06T02:00:00.000+0000,Leader Ni,"　　When zookeeper occur an unexpected error( Not SessionExpiredException, SessionTimeoutException and EndOfStreamException), ClientCnxn(1161) will log such as the formart ""Session 0x for server null, unexpected error, closing socket connection and attempting reconnect "". The log at line 1161 in zookeeper-3.3.3
　　We found that, zookeeper use ""((SocketChannel)sockKey.channel()).socket().getRemoteSocketAddress()"" to get zookeeper addr. But,Sometimes, it logs ""Session 0x for server null"", you know, if log null, developer can't determine the current zookeeper addr that client is connected or connecting.
　　I add a method in Class SendThread:InetSocketAddress org.apache.zookeeper.ClientCnxn.SendThread.getCurrentZooKeeperAddr().

　　Here:
/**
* Returns the address to which the socket is connected.
* 
* @return ip address of the remote side of the connection or null if not
*         connected
*/
@Override
SocketAddress getRemoteSocketAddress() {
   // a lot could go wrong here, so rather than put in a bunch of code
   // to check for nulls all down the chain let's do it the simple
   // yet bulletproof way 
.....
",[],Bug,ZOOKEEPER-1480,Major,Leader Ni,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"ClientCnxn(1161) can't get the current zk server add, so that - Session 0x for server null, unexpected error",2022-02-03T08:50:24.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",3.0
,"[<JIRA Component: name='c client', id='12312380'>]",2012-06-04T00:48:25.000+0000,Michi Mutsuzaki,"It can take up to sessionTimeout / 3 for the IO thread to send out the auth packet. The {{zoo_add_auth()}} function should call {{adaptor_send_queue(zh, 0)}} after {{calling send_last_auth_info(zh)}}.

--Michi",[],Bug,ZOOKEEPER-1479,Major,Michi Mutsuzaki,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,C Client: zoo_add_auth() doesn't wake up the IO thread,2022-02-03T08:50:12.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",3.0
Alexander Shraer,"[<JIRA Component: name='tests', id='12312427'>]",2012-06-03T03:03:39.000+0000,Alexander Shraer,"The following code appears in QuorumTest.testFollowersStartAfterLeader( ):

for (int i = 0; i < 30; i++) {
    try {
       zk.create(""/test"", ""test"".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE,
                 CreateMode.PERSISTENT);
       break;
     } catch(KeeperException.ConnectionLossException e) {
       Thread.sleep(1000);
     }
    // test fails if we still can't connect to the quorum after 30 seconds.
    Assert.fail(""client could not connect to reestablished quorum: giving up after 30+ seconds."");
}

From the comment it looks like the intention was to try to reconnect 30 times and only then trigger the Assert, but that's not what this does.
After we fail to connect once and Thread.sleep is executed, Assert.fail will be executed without retrying create. ","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1478,Minor,Alexander Shraer,Fixed,2012-12-13T07:19:27.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Small bug in QuorumTest.testFollowersStartAfterLeader( ),2014-03-13T18:16:58.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",6.0
,"[<JIRA Component: name='server', id='12312382'>, <JIRA Component: name='tests', id='12312427'>]",2012-06-01T19:52:19.000+0000,Diwaker Gupta,"I downloaded ZK 3.4.3 sources and ran {{ant test}}. Many of the tests failed, including ZooKeeperTest. A common symptom was spurious {{ConnectionLossException}}:

{code}
2012-06-01 12:01:23,420 [myid:] - INFO  [main:JUnit4ZKTestRunner$LoggedInvokeMethod@54] - TEST METHOD FAILED testDeleteRecursiveAsync
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1246)
        at org.apache.zookeeper.ZooKeeperTest.testDeleteRecursiveAsync(ZooKeeperTest.java:77)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
... (snipped)
{code}

As background, I was actually investigating some non-deterministic failures when using Netflix's Curator with Java 7 (see https://github.com/Netflix/curator/issues/79). After a while, I figured I should establish a clean ZK baseline first and realized it is actually a ZK issue, not a Curator issue.

We are trying to migrate to Java 7 but this is a blocking issue for us right now.",[],Bug,ZOOKEEPER-1477,Major,Diwaker Gupta,Not A Problem,2013-08-31T18:11:32.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Test failures with Java 7 on Mac OS X,2013-10-08T16:10:18.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",20.0
,[],2012-06-01T13:01:28.000+0000,Jilles van Gurp,"We observed a weird, random issue trying to create zookeeper client connections on osx. Sometimes it would work and sometimes it would fail. Also it is randomly very slow. It turns out both issues have the same cause.

My hosts file on osx (which is an unmodified default one), lists three entries for localhost:

127.0.0.1	localhost
::1             localhost 
fe80::1%lo0	localhost

We saw zookeeper trying to connect to fe80:0:0:0:0:0:0:1%1 sometimes, which is not listed (actually one in four times, it seems to round robin over the addresses). 

Whenever that happens, it sometimes works and sometimes fails. In both cases it's very slow. Reason: the reverse lookup for fe80:0:0:0:0:0:0:1%1 can't be resolved using the hosts file and it falls back to actually using the dns. Sometimes it actually works but other times it fails/times out after about 5 seconds. Probably a platform specific settings with dns setup hide this problem on linux. 

As a workaround, we preresolve localhost now: Inet4Address.getByName(""localhost""). This always resolves to 127.0.0.1 on my machine and works fast.

This fixes the issue for us. We're not sure where the fe80:0:0:0:0:0:0:1%1 address comes from though. I don't recall having this issue with other server side software so this might be a mix of platform setup, osx specific defaults, and zookeeper behavior.

I've seen one ticket that relates to ipv6 in zookeeper that might be related: ZOOKEEPER-667. Perhaps the workaround for that ticket introduced this problem? 


",[],Bug,ZOOKEEPER-1476,Minor,Jilles van Gurp,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ipv6 reverse dns related timeouts on OSX connecting to localhost,2014-07-03T22:34:33.000+0000,[],7.0
Paulo Ricardo Paz Vital,"[<JIRA Component: name='build', id='12312383'>]",2012-05-30T14:31:15.000+0000,Adalberto Medeiros,"zookeeper.server.NIOServerCnxn and zookeeper.server.NettyServerCnxn imports com.sun.management.UnixOperatingSystemMXBean . This OperatingSystemMXBean class is not implemented by IBM or open java. 

In my case, I need IBM Java so I can run zookeeper in Power ppc64 servers.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1474,Major,Adalberto Medeiros,Fixed,2012-11-28T07:46:11.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Cannot build Zookeeper with IBM Java: use of Sun MXBean classes,2014-03-13T18:17:09.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.4.5', id='12321883'>]",10.0
Thawan Kooburat,"[<JIRA Component: name='server', id='12312382'>]",2012-05-29T19:58:46.000+0000,Henry Robinson,"ZKDatabase.committedLog retains the past 500 transactions to enable fast catch-up. This works great, but it's using triple the memory it needs to by retaining three copies of the data part of any transaction.

* The first is in committedLog[i].request.request.hb - a heap-allocated {{ByteBuffer}}.
* The second is in committedLog[i].request.txn.data - a jute-serialised record of the transaction
* The third is in committedLog[i].packet.data - also jute-serialised, seemingly uninitialised data.

This means that a ZK-server could be using 1G of memory more than it should be in the worst case. We should use just one copy of the data, even if we really have to refer to it 3 times. 
",[],Bug,ZOOKEEPER-1473,Major,Henry Robinson,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Committed proposal log retains triple the memory it needs to,2022-02-03T08:50:18.000+0000,[],7.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2012-05-25T14:34:01.000+0000,David Nickerson,org.apache.zookeeper.WatchedEvent is missing from the 3.3.5 documentation.,[],Bug,ZOOKEEPER-1472,Minor,David Nickerson,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,WatchedEvent class missing from documentation,2012-05-25T14:34:01.000+0000,"[<JIRA Version: name='3.3.5', id='12319081'>]",2.0
Michi Mutsuzaki,"[<JIRA Component: name='jute', id='12312385'>]",2012-05-21T01:10:35.000+0000,Michi Mutsuzaki,"There are 2 issues with the current jute generated C++ code.

1. Variable declaration for JRecord is incorrect. It looks something like this:
{code} 
    Id id;
{code}
It should be like this instead:
{code} 
 org::apache::zookeeper::data::Id mid;
{code}

2. The header file declares all the variables (except for JRecord ones) with ""m"" prefix, but the .cc file doesn't use the prefix. ","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1471,Minor,Michi Mutsuzaki,Fixed,2012-06-30T06:44:42.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Jute generates invalid C++ code,2012-06-30T11:01:10.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",4.0
,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2012-05-20T17:00:34.000+0000,Paul Giannaros,"When calling zookeeper.close(handle), any connection watcher for the handle is not deleted. This is a source of memory leaks for applications that create and close lots of connections. Its damage can be mitigated to some degree by changing the watcher to some function that won't keep references to instances alive before calling close.

The fix is just to add a free_pywatcher(..) call in the close sequence. Alternatively you could allow set_watcher(handle, None) as a way of deleting the watcher, but it's probably best to take care of it on close too.",[],Bug,ZOOKEEPER-1470,Minor,Paul Giannaros,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zkpython: close() should delete any watcher,2012-05-20T18:31:43.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='quorum', id='12312379'>]",2012-05-15T17:21:53.000+0000,Patrick D. Hunt,org.apache.zookeeper.server.quorum.QuorumCnxManager.shutdown is not being synchronized even though it's accessed by multiple threads.,"[<JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1466,Blocker,Patrick D. Hunt,Fixed,2012-06-29T20:13:45.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,QuorumCnxManager.shutdown missing synchronization,2012-06-30T11:01:09.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.3.5', id='12319081'>, <JIRA Version: name='3.5.0', id='12316644'>]",4.0
Camille Fournier,"[<JIRA Component: name='leaderElection', id='12312378'>]",2012-05-10T14:47:11.000+0000,Alex Gvozdenovic,"When re-electing a new leader of a cluster, it takes a long time for the cluster to become available if the dataset is large

Test Data
----------
650mb snapshot size
20k nodes of varied size 
3 member cluster 

On 3.4.x branch (http://svn.apache.org/repos/asf/zookeeper/branches/branch-3.4?r=1244779)
------------------------------------------------------------------------------------------

Takes 3-4 minutes to bring up a cluster from cold 
Takes 40-50 secs to recover from a leader failure 
Takes 10 secs for a new follower to join the cluster 

Using the 3.3.5 release on the same hardware with the same dataset
-----------------------------------------------------------------

Takes 10-20 secs to bring up a cluster from cold 
Takes 10 secs to recover from a leader failure 
Takes 10 secs for a new follower to join the cluster 

I can see from the logs in 3.4.x that once a new leader is elected, it pushes a new snapshot to each of the followers who need to save it before they ack the leader who can then mark the cluster as available. 

The kit being used is a low spec vm so the times taken are not relevant per se - more the fact that a snapshot is always sent even through there is no difference between the persisted state on each peer.
No data is being added to the cluster while the peers are being restarted.






","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1465,Critical,Alex Gvozdenovic,Fixed,2012-07-05T16:50:09.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Cluster availability following new leader election takes a long time with large datasets - is correlated to dataset size,2012-07-18T00:33:19.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",12.0
Michael Hu,"[<JIRA Component: name='build', id='12312383'>]",2012-05-07T21:52:51.000+0000,Michael Hu,"There is a use of external inline function in zookeeper hashtable_itr.h file, which is not compatible with C99. This causes problem when compiling with other library like code coverage library.
---
hashtable_itr.h:37: error: 'cov_v_cab2c78b' is static but used in inline
function 'hashtable_iterator_key' which is not static
---

The easy fix would be put the following line in hashtable_itr.c which ignores this inline warning.
#pragma GCC diagnostic ignored ""-Winline""","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1463,Major,Michael Hu,Duplicate,2012-05-11T05:26:49.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,external inline function is not compatible with C99,2012-05-11T05:26:49.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.3.5', id='12319081'>]",0.0
Thawan Kooburat,"[<JIRA Component: name='server', id='12312382'>]",2012-05-03T01:37:48.000+0000,Thawan Kooburat,"Brief Description:
When a participant or observer get partitioned and restart as Read-only server. ZkDb doesn't get reinitialized. This causes the RO server to drop any incoming request with zxid > 0 

Error message:
Refusing session request for client /xx.xx.xx.xx:39875 
as it has seen zxid 0x2e00405fd9 our last zxid is 0x0 client must try another server

Steps to reproduce:
Start an RO-enabled observer connecting to an ensemble. Kill the ensemble and wait until the observer restart in RO mode. Zxid of this observer should be 0.

Description:
Before a server transition into LOOKING state, its database get closed as part of shutdown sequence. The database of leader, follower and observer get initialized as a side effect of participating in leader election protocol. (eg. observer will call registerWithLeader() and call getLastLoggedZxid() which initialize the db if not already).

However, RO server does not participate in this protocol so its DB doesn't get initialized properly
 ","[<JIRA Version: name='3.4.6', id='12323310'>]",Bug,ZOOKEEPER-1462,Critical,Thawan Kooburat,Fixed,2013-10-02T22:42:16.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Read-only server does not initialize database properly,2014-03-13T18:16:57.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",5.0
Joseph Walton,"[<JIRA Component: name='quorum', id='12312379'>]",2012-04-30T19:49:30.000+0000,Chris Dolan,"Via code inspection, I see that the ""server.nnn"" configuration key does not support literal IPv6 addresses because the property value is split on "":"". In v3.4.3, the problem is in QuorumPeerConfig:

{noformat}
String parts[] = value.split("":"");
InetSocketAddress addr = new InetSocketAddress(parts[0],
                        Integer.parseInt(parts[1]));
{noformat}

In the current trunk (http://svn.apache.org/viewvc/zookeeper/trunk/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeer.java?view=markup) this code has been refactored into QuorumPeer.QuorumServer, but the bug remains:

{noformat}
String serverClientParts[] = addressStr.split("";"");
String serverParts[] = serverClientParts[0].split("":"");
addr = new InetSocketAddress(serverParts[0],
                        Integer.parseInt(serverParts[1]));
{noformat}

This bug probably affects very few users because most will naturally use a hostname rather than a literal IP address. But given that IPv6 addresses are supported for clients via ZOOKEEPER-667 it seems that server support should be fixed too.","[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-1460,Major,Chris Dolan,Fixed,2016-06-23T20:21:13.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,IPv6 literal address not supported for quorum members,2016-07-21T20:18:19.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",19.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2012-04-30T01:00:30.000+0000,Andrey Kornev,"If I have a child delete op interleaving two child create ops, the second child create will nevertheless have the path suffix incremented only by 1 rather than by 2. Is this expected? The 3.3.5 version takes into account the delete and increments the sequence by 2.

PrepRequestProcessor uses the parent's cversion to generate the child's sequence suffix. However it appears that this particular cversion only counts ""create"" operations and it doesn't take into account the deletes. Strangely enough, the parent stats returned by getData() show the correct cversion with all the creates and deletes accounted for.

It looks like the first cversion comes from the ChangeRecord for the parent node stuck in ZooKeeperServer.outstandingChangesForPath map. And the second one (returned by getData(), that is) comes from the DataTree.

Here's a simple example that reproduces the situation.

zk.create(""/parent"", null, OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
Stat stat = new Stat();

zk.getData(""/parent"", false, stat);
stat.getCVersion(); // returns 0 -- expected;

String actualPath = zk.create(""/parent/child"", null, OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);
// actualPath is ""/parent/child0000000000"" -- expected.

zk.getData(""/parent"", false, stat);
stat.getCVersion(); // returns 1 -- expected;

zk.getData(actualPath, false, stat);
zk.delete(actualPath,stat.getVersion()); // delete the child node

zk.getData(""/parent"", false, stat);
stat.getCVersion(); // returns 2;

// create another child
actualPath = zk.create(""/parent/child"", null, OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);
// returned ""/parent/child0000000001"" but expected ""/parent/child0000000002""

zk.getData(""/parent"", false, stat);
stat.getCVersion(); // returns 3;",[],Bug,ZOOKEEPER-1458,Major,Andrey Kornev,Not A Problem,2012-04-30T23:10:10.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Parent's cversion doesn't match the sequence number that get assigned to a child node with the SEQUENTIAL flag on.,2012-04-30T23:10:10.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",0.0
,[],2012-04-26T19:25:43.000+0000,Neha Narkhede,"This week, we saw a potential bug with zookeeper 3.3.4. In an attempt to adding a separate disk for zookeeper transaction logs, our SysOps team threw new disks at all the zookeeper servers in our production cluster at around the same time. Right after this, we saw degraded performance on our zookeeper cluster. And yes, I agree that this degraded behavior is expected and we could've done a better job and upgraded one server at a time. Al though, the observed impact was that ephemeral nodes got deleted without session expiration on the zookeeper clients. 

Let me try and describe what I've observed from the Kafka and ZK server logs - Kafka client has a session established with ZK, say Session A, that it has been using successfully. At the time of the degraded ZK performance issue, Session A expires. Kafka's ZkClient tries to establish another session with ZK. After 9 seconds, it establishes a session, say Session B and tries to use it for creating a znode. This operation fails with a NodeExists error since another session, say session C, has created that znode. This is considered OK since ZkClient retries an operation transparently if it gets disconnected and sometimes you can get NodeExists. But then later, session C expires and hence the ephemeral node is deleted from ZK. This leads to unexpected errors in Kafka since its session, Session B, is still valid and hence it expects the znode to be there. The issue is that session C was established, created the znode and expired, without the zookeeper client on Kafka ever knowing about it. 

",[],Bug,ZOOKEEPER-1457,Major,Neha Narkhede,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Ephemeral node deleted for unexpired sessions,2012-04-30T19:00:34.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>]",5.0
,[],2012-04-26T17:17:16.000+0000,Patrick D. Hunt,When creating a session there is no way for a client to specify that they require a kerberos (via sasl) authenticated session. Similarly there is no way to request an unauthenticated session if kerberos has been configured at the jvm level.,[],Bug,ZOOKEEPER-1456,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,sessions cannot specify whether they require kerberos authenticated sessions or not,2012-04-27T14:42:49.000+0000,[],4.0
,[],2012-04-26T17:12:58.000+0000,Patrick D. Hunt,"The ZooKeeper interface provides no way to determine if the session is sasl authenticated or not. There is an event sent to the watcher when the sasl authentication completes, however there no way to determine if there is intent to negotiate via sasl. As a result the event cannot be used to wait to send messages until the authentication has completed. see HADOOP-8315",[],Bug,ZOOKEEPER-1455,Critical,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,there is no way to determine if a session is sasl authenticated or not,2013-09-25T09:53:51.000+0000,[],9.0
,"[<JIRA Component: name='server', id='12312382'>]",2012-04-24T22:05:10.000+0000,Patrick D. Hunt,"See ZOOKEEPER-1449 for background on this issue. The main problem is that during server recovery org.apache.zookeeper.server.persistence.FileTxnLog.FileTxnIterator.next() does not indicate if the available logs are valid or not. In some cases (say a truncated record and a single txnlog in the datadir) we will not detect that the file is corrupt, vs reaching the end of the file.",[],Bug,ZOOKEEPER-1453,Critical,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,corrupted logs may not be correctly identified by FileTxnIterator,2016-03-18T20:05:28.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",7.0
Stephen Tyree,"[<JIRA Component: name='c client', id='12312380'>]",2012-04-19T18:04:57.000+0000,Stephen Tyree,"This is how the code currently logs getaddrinfo errors:

{quote}
                errno = getaddrinfo_errno(rc);
                LOG_ERROR((""getaddrinfo: %s\n"", strerror(errno)));
{quote}

On Linux, specifically when using glibc, there is a better function for logging getaddrinfo errors called gai_strerror. An example:

{quote}
                LOG_ERROR((""getaddrinfo: %s\n"", gai_strerror(rc)));
{quote}

It doesn't miss a lot of cases like the errno based version does.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1451,Trivial,Stephen Tyree,Fixed,2012-04-25T20:29:44.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,C API improperly logs getaddrinfo failures on Linux when using glibc,2012-04-25T20:29:44.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",1.0
Patrick D. Hunt,[],2012-04-17T18:26:34.000+0000,Daniel Lord,"I've been running in to this situation in our labs fairly regularly where we'll get a single follower that will be in an inconsistent state with dangling ephemeral znodes.  Here is all of the information that I have right now.  Please ask if there is anything else that is useful.

Here is a quick snapshot of the state of the ensemble where you can see it is out of sync across several znodes: 

-bash-3.2$ echo srvr | nc il23n04sa-zk001 2181
Zookeeper version: 3.3.3-cdh3u2--1, built on 10/14/2011 05:17 GMT
Latency min/avg/max: 0/7/25802
Received: 64002
Sent: 63985
Outstanding: 0
Zxid: 0x500000a41
Mode: follower
Node count: 497

-bash-3.2$ echo srvr | nc il23n04sa-zk002 2181
Zookeeper version: 3.3.3-cdh3u2--1, built on 10/14/2011 05:17 GMT
Latency min/avg/max: 0/13/79032
Received: 74320
Sent: 74276
Outstanding: 0
Zxid: 0x500000a41
Mode: leader
Node count: 493

-bash-3.2$ echo srvr | nc il23n04sa-zk003 2181
Zookeeper version: 3.3.3-cdh3u2--1, built on 10/14/2011 05:17 GMT
Latency min/avg/max: 0/2/25234
Received: 187310
Sent: 187320
Outstanding: 0
Zxid: 0x500000a41
Mode: follower
Node count: 493

All of the zxids match up just fine but zk001 has 4 more nodes in its node count than the other two (including the leader..).  When I use a zookeeper client connect to connect directly to zk001 I can see the following znode that should no longer exist: 

[zk: localhost:2181(CONNECTED) 0] stat /siri/Douroucouli/clients/il23n04sa-app004.siri.apple.com:38096
cZxid = 0x40000001a
ctime = Mon Apr 16 11:00:47 PDT 2012
mZxid = 0x40000001a
mtime = Mon Apr 16 11:00:47 PDT 2012
pZxid = 0x40000001a
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x236bc504cb50002
dataLength = 0
numChildren = 0

This node does not exist using the client to connect to either of the other two members of the ensemble.

I searched through the logs for that session id and it looks like it was established and closed cleanly.  There were several leadership/quorum problems during the course of the session but it looks like it should have been shut down properly.  Neither the session nor the znode show up in the ""dump"" on the leader but the problem znode does show up in the ""dump"" on zk001.

2012-04-16 11:00:47,637 - INFO  [CommitProcessor:2:NIOServerCnxn@1580] - Established session 0x236bc504cb50002 with negotiated timeout 15000 for client /17.202.71.201:38971
2012-04-16 11:20:59,341 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@770] - Client attempting to renew session 0x236bc504cb50002 at /17.202.71.201:50841
2012-04-16 11:20:59,342 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1580] - Established session 0x236bc504cb50002 with negotiated timeout 15000 for client /17.202.71.201:50841
2012-04-16 11:21:09,343 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@634] - EndOfStreamException: Unable to read additional data from client sessionid 0x236bc504cb50002, likely client has closed socket
2012-04-16 11:21:09,343 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1435] - Closed socket connection for client /17.202.71.201:50841 which had sessionid 0x236bc504cb50002
2012-04-16 11:21:20,352 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:NIOServerCnxn@1435] - Closed socket connection for client /17.202.71.201:38971 which had sessionid 0x236bc504cb50002
2012-04-16 11:21:22,151 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@770] - Client attempting to renew session 0x236bc504cb50002 at /17.202.71.201:38166
2012-04-16 11:21:22,152 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:NIOServerCnxn@1580] - Established session 0x236bc504cb50002 with negotiated timeout 15000 for client /17.202.71.201:38166
2012-04-16 11:27:17,902 - INFO  [ProcessThread:-1:PrepRequestProcessor@387] - Processed session termination for sessionid: 0x236bc504cb50002
2012-04-16 11:27:17,904 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1435] - Closed socket connection for client /17.202.71.201:38166 which had sessionid 0x236bc504cb50002

The only way I've been able to recover from this situation is to shut down the problem follower, delete its snapshots and let it resync with the leader.

I'll attach the full log4j logs, the txn logs, the snapshots and the conf files for each member of the ensemble.  Please let me know what other information is useful.",[],Bug,ZOOKEEPER-1449,Major,Daniel Lord,Cannot Reproduce,2013-10-02T09:58:42.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Ephemeral znode not deleted after session has expired on one follower (quorum is in an inconsistent state) ,2013-10-02T16:16:46.000+0000,[],2.0
Flavio Paiva Junqueira,"[<JIRA Component: name='server', id='12312382'>]",2012-04-17T16:15:40.000+0000,Botond Hejj,"Hi,

I've found a bug in zookeeper related to quota creation which can shutdown zookeeper leader on startup.

Steps to reproduce:
1. create /quota_bug
2. setquota -n 10000 /quota_bug
3. stop the whole ensemble (the previous operations should be in the transaction log)
4. start all the servers
5. the elected leader will shutdown with an exception (Missing stat node for count /zookeeper/quota/quota_bug/zookeeper_
stats)

I've debugged a bit what happening and I found the following problem:
On startup each server loads the last snapshot and replays the last transaction log. While doing this it fills up the pTrie variable of the DataTree with the path of the nodes which have quota.
After the leader is elected the leader servers loads the snapshot and last transaction log but it doesn't clean up the pTrie variable. This means it still contains the ""/quota_bug"" path. Now when the ""create /quota_bug"" is processed from the transaction log the DataTree already thinks that the quota nodes (""/zookeeper/quota/quota_bug/zookeeper_limits"" and ""/zookeeper/quota/quota_bug/zookeeper_stats"") are created but those node creation actually comes later in the transaction log. This leads to the missing stat node exception.

I think clearing the pTrie should solve this problem.

","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1448,Critical,Botond Hejj,Fixed,2013-09-05T21:50:44.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Node+Quota creation in transaction log can crash leader startup,2014-03-13T18:17:08.000+0000,"[<JIRA Version: name='3.3.5', id='12319081'>]",7.0
,"[<JIRA Component: name='c client', id='12312380'>]",2012-04-12T19:59:57.000+0000,Stephen Tyree,"When using the C API, one might feel inclined to create a zookeeper_wait_until_connected method which waits for some amount for a connected state event to occur. The code might look like the following (didn't actually compile this):

//------
static pthread_mutex_t kConnectedMutex = PTHREAD_MUTEX_INITIALIZER;
static pthread_cond_t kConnectedCondvar = PTHREAD_COND_INITIALIZER;

int zookeeper_wait_until_connected(zhandle_t* zk, const struct timespec* timeout)
{
  struct timespec abstime;
  clock_gettime(TIMER_ABSTIME, &abstime);
  abstime->tv_sec += timeout->tv_sec;
  abstime->tv_nsec += timeout->tv_nsec;

  pthread_mutex_lock(&kConnectedMutex);
  if (zoo_state(zk) == ZOO_CONNECTED_STATE) {
    return 1;
  }

  pthread_cond_timedwait(&kConnectedCondvar, &kConnectedMutex, &abstime);
  int state = zoo_state(zk);
  return (state == ZOO_CONNECTED_STATE);
}

void zookeeper_session_callback(zhandle_t* zh, int type, int state, const char* path, void* arg)
{
  pthread_mutex_lock(&kConnectedMutex);
  if (type == ZOO_SESSION_EVENT && state == ZOO_CONNECTED_STATE) {
    pthread_cond_broadcast(&kConnectedCondvar);
  }
}
//-----

That would work fine (assuming I didn't screw anything up), except that pthread_cond_timedwait can spuriously wakeup, making you not actually wait the desired timeout. The solution to this is to loop until the condition is met, which might look like the following:

//---
  int state = zoo_state(zk);
  int result = 0;
  while ((state == ZOO_CONNECTING_STATE || state == ZOO_ASSOCIATING_STATE) && result != ETIMEDOUT) {
    result = pthread_cond_timedwait(&kConnectedCondvar, &kConnectedMutex, &abstime);
    state = zoo_state(zk);
  }
//---

That would work fine, except the state might be valid and connecting, yet not ZOO_CONNECTING_STATE or ZOO_ASSOCIATING_STATE, it might be 0 or, as implemented recently courtesy of zookeeper-1108, 999. Checking for those states causes your code to rely upon an implementation detail of zookeeper, a problem highlighted by that implementation detail changing recently. Is there any way this behavior can become documented (via a ZOO_INITIALIZED_STATE or something like that), or is there any way this behavior can be supported by the library itself?
",[],Bug,ZOOKEEPER-1446,Minor,Stephen Tyree,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,C API makes it difficult to implement a timed wait_until_connected method correctly,2012-05-30T20:30:29.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.3.5', id='12319081'>]",1.0
Jay Shrauner,"[<JIRA Component: name='server', id='12312382'>]",2012-04-10T01:08:24.000+0000,Jay Shrauner,"A socket connection to the server on which a session is not created will never time out. A misbehaving client that opens and leaks connections without creating sessions will hold open file descriptors on the server.

The existing timeout code is implemented at the session level, but the servers also should track and expire connections at the connection level. Proposed solution is to pull the timeout data structure handling code (hashmap of expiry time to sets of objects, simple monotonically incrementing nextExpirationTime) from SessionTrackerImpl into its own class in order to share it with connection level timeouts to be implemented in NIOServerCnxnFactory. Connections can be assigned a small initial timeout (proposing something small, like 3s) until a session is created, at which point the ServerCnxn session timeout can be used instead.",[],Bug,ZOOKEEPER-1444,Critical,Jay Shrauner,Duplicate,2012-07-27T05:17:55.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Idle session-less connections never time out,2012-07-27T05:17:55.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.5.0', id='12316644'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2012-04-04T23:05:36.000+0000,Michi Mutsuzaki,"The ""API Docs"" link is broken in trunk.

http://zookeeper.apache.org/doc/trunk/
http://zookeeper.apache.org/doc/trunk/api/index.html",[],Bug,ZOOKEEPER-1443,Major,Michi Mutsuzaki,Duplicate,2013-10-09T06:46:42.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,API docs for trunk returns 404,2013-10-09T06:46:42.000+0000,[],0.0
Jeremy Stribling,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2012-04-04T17:09:40.000+0000,Jeremy Stribling,"The uncaught exception handler registered in NIOServerCnxnFactory and ClientCnxn simply logs exceptions and lets the rest of ZooKeeper go on its merry way.  However, errors such as OutOfMemoryErrors should really crash the program, as they represent unrecoverable errors.  If the exception that gets to the uncaught exception handler is an instanceof a java.lang.Error, ZK should exit with an error code (in addition to logging the error).",[],Bug,ZOOKEEPER-1442,Minor,Jeremy Stribling,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Uncaught exception handler should exit on a java.lang.Error,2017-07-29T14:34:31.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.3.5', id='12319081'>]",3.0
Jordan Zimmerman,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2012-04-01T20:28:39.000+0000,Jordan Zimmerman,"When shutting down the QuroumPeer, ZK server logs unnecessary errors. See QuorumCnxManager.Listener.run() - ss.accept() will throw an exception when it is closed. The catch (IOException e) will log errors. It should first check the shutdown field to see if the Listener is being shutdown. If it is, the exception is correct and no errors should be logged.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1440,Minor,Jordan Zimmerman,Fixed,2014-03-12T21:45:35.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Spurious log error messages when QuorumCnxManager is shutting down,2014-03-12T23:32:22.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",5.0
Yubing Yin,"[<JIRA Component: name='c client', id='12312380'>]",2012-04-01T08:15:53.000+0000,Yubing Yin,"Man of getpwuid_r ""return a pointer to a passwd structure, or NULL if the matching entry is not found or an error occurs"",
""The getpwnam_r() and getpwuid_r() functions return zero on success."", it means entry may not be found when getpwuid_r success.

In log_env of zookeeper.c in c sdk:
  {{if (!getpwuid_r(uid, &pw, buf, sizeof(buf), &pwp)) {}}
    {{LOG_INFO((""Client environment:user.home=%s"", pw.pw_dir));}}
  {{}}}
pwp is not checked to ensure entry is found, pw.pw_dir is not initialized in this case, core happens in LOG_INFO.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1439,Major,Yubing Yin,Fixed,2012-04-26T10:59:52.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,c sdk: core in log_env for lack of checking the output argument *pwp* of getpwuid_r,2012-04-27T11:00:37.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.3.5', id='12319081'>]",1.0
,"[<JIRA Component: name='jmx', id='12312451'>]",2012-03-30T04:49:08.000+0000,Todd Lipcon,"I have a functional test that extends from ClientBase, which I'm using to stress test a piece of software that uses ZK underneath. In this test, I want to simulate disconnection events, so I fire up a thread which calls ""serverFactory.closeAll()"" every 50ms. The clients themselves churn through a lot of sessions as part of the test. When the test completes, the ClientBase teardown method fails, since it sees one or two MBeans ""left over"" from earlier elapsed sessions.",[],Bug,ZOOKEEPER-1438,Minor,Todd Lipcon,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,JMX MBeans for client connections can be orphaned,2012-03-30T04:49:08.000+0000,"[<JIRA Version: name='3.4.2', id='12319196'>]",2.0
Eugene Joseph Koontz,"[<JIRA Component: name='java client', id='12312381'>]",2012-03-29T02:09:53.000+0000,Thomas Weise,"Found issue in the context of hbase region server startup, but can be reproduced w/ zkCli alone.

getData may occur prior to SaslAuthenticated and fail with NoAuth. This is not expected behavior when the client is configured to use SASL.
","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1437,Major,Thomas Weise,Fixed,2012-09-09T18:24:53.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Client uses session before SASL authentication complete,2016-02-18T12:31:52.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",19.0
Hartmut Lang,"[<JIRA Component: name='java client', id='12312381'>]",2012-03-27T00:06:47.000+0000,Wing Yew Poon,"In the command line client (zkCli.sh), when I do

{noformat}
stat /non-existent
{noformat}

the client crashes with

{noformat}
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.zookeeper.ZooKeeperMain.printStat(ZooKeeperMain.java:130)
	at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:722)
	at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:581)
	at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:353)
	at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:311)
	at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:270)
{noformat}
",[],Bug,ZOOKEEPER-1434,Major,Wing Yew Poon,Won't Fix,2012-03-27T00:39:18.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zkCli crashes with NPE on stat of non-existent path,2012-03-27T00:41:27.000+0000,"[<JIRA Version: name='3.3.5', id='12319081'>]",0.0
Kapil Thangavelu,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2012-03-23T07:11:07.000+0000,johan rydberg,"I'm seeing a memory leakage when using the ""aget"" method.

It leaks tuples and dicts, both containing ""stats"".

","[<JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1431,Major,johan rydberg,Fixed,2012-06-19T00:24:24.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zkpython: async calls leak memory,2012-06-19T11:00:22.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",7.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2012-03-19T21:25:48.000+0000,Todd Lipcon,"Currently, the writeLongToFile() function opens the file for truncate, writes the new data, syncs, and then closes. If the process crashes after opening the file but before writing the new data, the file may be left empty, causing ZK to ""forget"" an earlier promise. Instead, it should use RandomAccessFile to avoid truncating.","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1427,Critical,Todd Lipcon,Fixed,2012-07-17T21:22:22.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Writing to local files is done non-atomically,2012-07-18T11:01:52.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",10.0
,"[<JIRA Component: name='server', id='12312382'>]",2012-03-16T21:15:06.000+0000,Mihai Claudiu Toader,"Hi all, 

While using zookeeper at midokura we hit an interesting bug in zookeeper. We did hit it sporadically 
while developing some functional tests so i had to build a test case for it. 

I finally created the test case and i think i narrowed down the conditions under which it happens. 
So i wanted to let you know my findings since they are somewhat troublesome. 

We need:
  - one running zookeeper server (didn't test that with a cluster)
      let's name this: server

  - one running zookeeper client that will create an ephemeral node under the tree created by the next client
      let's name this: the ephemeral client

  - one running zookeeper client that will create a persistent tree and try to delete that tree
      let's name this: the persistent client

What needs to happen is this:

 step 1. - the server starts
 step 2. - the persistent client connects and creates a tree
 step 3. - the ephemeral client connects and adds a ephemeral node under the tree created by the persistent client
 step 4. - the persistent client will try to delete the tree recursively (without including the ephemeral node in the multi op
 step 5. - the ephemeral client crashes hard (the equivalent of kill -9)
 step 6. - the persistent client will try to delete the tree recursively again (and fail with NoEmptyNode even if when we list the node we don't see any childrens)
    - the zookeeper server needs to be restarted in order for this to work. 

The step 4 is critical in the sense that if we don't have that (there is no previous error trying to remove a tree) then the nexts steps behave as we would expect them to behave (aka pass). 

Also no amount of fiddling with zookeeper connection timeouts (between zookeeper and ephemeral node) will help. 
 
If the ephemeral client is shutdown properly it seems like everything will behave properly (even with step 4). 

The test code is available here:
   https://github.com/mtoadermido/play

It needs an zookeepr 3.4.2 installed on the system (it uses the installed jars from the deb to spawn the zookeeper server).

The entry point is https://github.com/mtoadermido/play/blob/master/src/main/java/com/midokura/tests/zookeeper/BlockingBug.java

There is a lot of boiler plate since i didn't want it to be depending on stuff from midonet but the interesting part is the BlockingBug.main() method. 

It will launch a zookeeper process, an external ephemeral client process, and after that act as the second client. 

Available tweaks:
- the zookeeper client timeout for the ephemeral client here: 
  https://github.com/mtoadermido/play/blob/master/src/main/java/com/midokura/tests/zookeeper/BlockingBug.java#L56

- the step 4 here (set to true / false):
 https://github.com/mtoadermido/play/blob/master/src/main/java/com/midokura/tests/zookeeper/BlockingBug.java#L69

- the shutdown of the ephemeral client (soft aka clean shutdown, hard aka kill -9):
 https://github.com/mtoadermido/play/blob/master/src/main/java/com/midokura/tests/zookeeper/BlockingBug.java#L88

The result is displayed depending on the fact that the final recursive deletion succeeded or not:
   
We hit it !. The clear tree failed.
   https://github.com/mtoadermido/play/blob/master/src/main/java/com/midokura/tests/zookeeper/BlockingBug.java#L103

""No error :(""  
   https://github.com/mtoadermido/play/blob/master/src/main/java/com/midokura/tests/zookeeper/BlockingBug.java#L99


The conclusion is that the bug seems to be inside the zookeeper codebase and it's prone to being triggered by this 
particular usage of zookeeper combined with the misfortune of having to kill the ephemeral process hard. 


",[],Bug,ZOOKEEPER-1424,Major,Mihai Claudiu Toader,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper will not allow a client to delete a tree when it should allow it,2014-10-06T13:47:35.000+0000,"[<JIRA Version: name='3.4.2', id='12319196'>]",3.0
Flavio Paiva Junqueira,"[<JIRA Component: name='leaderElection', id='12312378'>]",2012-03-16T00:07:45.000+0000,Jeremy Stribling,"We have a situation where it seems to my untrained eye that leader election never finishes for a 5-node cluster.  In this test, all nodes are ZK 3.4.3 and running on the same server (listening on different ports, of course).  The nodes have server IDs of 0, 1, 2, 3, 4.  The test brings up the cluster in different configurations, adding in a new node each time.  We embed ZK in our application, so when we shut a node down and restart it with a new configuration, it all happens in a single JVM process.  Here's our server startup code (for the case where there's more than one node in the cluster):

{code}
if (servers.size() > 1) {
    _log.debug(""Starting Zookeeper server in quorum server mode"");

    _quorum_peer = new QuorumPeer();
    synchronized(_quorum_peer) {
        _quorum_peer.setClientPortAddress(clientAddr);
        _quorum_peer.setTxnFactory(log);
        _quorum_peer.setQuorumPeers(servers);
        _quorum_peer.setElectionType(_election_alg);
        _quorum_peer.setMyid(_server_id);
        _quorum_peer.setTickTime(_tick_time);
        _quorum_peer.setInitLimit(_init_limit);
        _quorum_peer.setSyncLimit(_sync_limit);
        QuorumVerifier quorumVerifier =
            new QuorumMaj(servers.size());
        _quorum_peer.setQuorumVerifier(quorumVerifier);
        _quorum_peer.setCnxnFactory(_cnxn_factory);
        _quorum_peer.setZKDatabase(new ZKDatabase(log));
        _quorum_peer.start();
    }
} else {
    _log.debug(""Starting Zookeeper server in single server mode"");
    _zk_server = new ZooKeeperServer();
    _zk_server.setTxnLogFactory(log);
    _zk_server.setTickTime(_tick_time);
    _cnxn_factory.startup(_zk_server);
}
{code}

And here's our shutdown code:

{code}
if (_quorum_peer != null) {
    synchronized(_quorum_peer) {
        _quorum_peer.shutdown();
        FastLeaderElection fle =
            (FastLeaderElection) _quorum_peer.getElectionAlg();
        fle.shutdown();
        try {
            _quorum_peer.getTxnFactory().commit();
        } catch (java.nio.channels.ClosedChannelException e) {
            // ignore
        }
    }
} else {
    _cnxn_factory.shutdown();
    _zk_server.getTxnLogFactory().commit();
}
{code}

The test steps through the following scenarios in quick succession:

Run 1: Start a 1-node cluster, servers=[0]
Run 2: Start a 2-node cluster, servers=[0,3]
Run 3: Start a 3-node cluster, servers=[0,1,3]
Run 4: Start a 4-node cluster, servers=[0,1,2,3]
Run 5: Start a 5-node cluster, servers=[0,1,2,3,4]

It appears that run 5 never elects a leader -- the nodes just keep spewing messages like this (example from node 0):

{noformat}
2012-03-14 16:23:12,775 13308 [WorkerSender[myid=0]] DEBUG org.apache.zookeeper.server.quorum.QuorumCnxManager  - There is a connection already for server 2
2012-03-14 16:23:12,776 13309 [QuorumPeer[myid=0]/127.0.0.1:2900] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Sending Notification: 3 (n.leader), 0x0 (n.zxid), 0x1 (n.round), 3 (recipient), 0 (myid), 0x2 (n.peerEpoch)
2012-03-14 16:23:12,776 13309 [WorkerSender[myid=0]] DEBUG org.apache.zookeeper.server.quorum.QuorumCnxManager  - There is a connection already for server 3
2012-03-14 16:23:12,776 13309 [QuorumPeer[myid=0]/127.0.0.1:2900] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Sending Notification: 3 (n.leader), 0x0 (n.zxid), 0x1 (n.round), 4 (recipient), 0 (myid), 0x2 (n.peerEpoch)
2012-03-14 16:23:12,776 13309 [WorkerSender[myid=0]] DEBUG org.apache.zookeeper.server.quorum.QuorumCnxManager  - There is a connection already for server 4
2012-03-14 16:23:12,776 13309 [WorkerReceiver[myid=0]] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Receive new notification message. My id = 0
2012-03-14 16:23:12,776 13309 [WorkerReceiver[myid=0]] INFO org.apache.zookeeper.server.quorum.FastLeaderElection  - Notification: 4 (n.leader), 0x0 (n.zxid), 0x1 (n.round), LOOKING (n.state), 1 (n.sid), 0x0 (n.peerEPoch), LOOKING (my state)
2012-03-14 16:23:12,776 13309 [WorkerReceiver[myid=0]] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Receive new notification message. My id = 0
2012-03-14 16:23:12,776 13309 [WorkerReceiver[myid=0]] INFO org.apache.zookeeper.server.quorum.FastLeaderElection  - Notification: 3 (n.leader), 0x0 (n.zxid), 0x1 (n.round), LOOKING (n.state), 2 (n.sid), 0x2 (n.peerEPoch), LOOKING (my state)
2012-03-14 16:23:12,776 13309 [QuorumPeer[myid=0]/127.0.0.1:2900] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Adding vote: from=1, proposed leader=3, proposed zxid=0x0, proposed election epoch=0x1
2012-03-14 16:23:12,776 13309 [QuorumPeer[myid=0]/127.0.0.1:2900] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - id: 3, proposed id: 3, zxid: 0x0, proposed zxid: 0x0
2012-03-14 16:23:12,776 13309 [QuorumPeer[myid=0]/127.0.0.1:2900] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - id: 3, proposed id: 3, zxid: 0x0, proposed zxid: 0x0
2012-03-14 16:23:12,776 13309 [QuorumPeer[myid=0]/127.0.0.1:2900] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - id: 3, proposed id: 3, zxid: 0x0, proposed zxid: 0x0
2012-03-14 16:23:12,776 13309 [QuorumPeer[myid=0]/127.0.0.1:2900] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - id: 4, proposed id: 3, zxid: 0x0, proposed zxid: 0x0
2012-03-14 16:23:12,776 13309 [QuorumPeer[myid=0]/127.0.0.1:2900] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - id: 4, proposed id: 3, zxid: 0x0, proposed zxid: 0x0
{noformat}

I'm guessing this means that nodes 3 and 4 are fighting over leadership, but I don't know enough about the leader election code to debug this any further.  Attaching a tarball with the logs for each run and the data directories for each node (though I don't think any data is being written to ZK during the test).","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1419,Blocker,Jeremy Stribling,Fixed,2012-03-20T01:19:02.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Leader election never settles for a 5-node cluster,2012-03-20T01:19:02.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.5.0', id='12316644'>]",1.0
Joe Gamache,"[<JIRA Component: name='documentation', id='12312422'>]",2012-03-15T21:11:54.000+0000,Joe Gamache,"When I ran the Queue example from here: http://zookeeper.apache.org/doc/trunk/zookeeperTutorial.html
The producer created entries of the form:  /app1/element0000000001...
but the consumer tried to consume of the form:  /app1/element1...

adding a patch with the file attached.",[],Bug,ZOOKEEPER-1418,Minor,Joe Gamache,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Just a bug in the tutorial code on the website,2012-03-22T14:49:10.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",0.0
Thawan Kooburat,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>]",2012-03-15T17:05:23.000+0000,Patrick D. Hunt,"In ZOOKEEPER-1412 it was identified that the c and java clients handle updating the last zxid seen a bit differently. ZOOKEEPER-1412 fixed a bug associated with this, however there are still some differences that should be investigated.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1417,Major,Patrick D. Hunt,Fixed,2013-06-06T16:54:19.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,investigate differences in client last zxid handling btw c and java clients,2013-06-06T17:21:47.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",5.0
,[],2012-03-14T19:06:41.000+0000,Stefano Ghio,Not setting the ecf.exported.containerfactoryargs property when publishing an OSGi service through Zookeeper results in the service being published under the host's hostname instead of its IP. This means that hosts not able to correctly resolve that hostname cannot connect to its ZooKeeper instance. It would be desirable to use the IP instead of the hostname when that property is purposely left blank e.g. when it is unknown where the application will be deployed.,[],Bug,ZOOKEEPER-1415,Minor,Stefano Ghio,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper broadcasts host's hostname instead of IP when ecf.exported.containerfactoryargs property is not set,2012-03-14T19:06:41.000+0000,[],1.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2012-03-12T09:13:24.000+0000,Botond Hejj,"I've observed an inconsistent behavior in java client watches. The inconsistency relates to the behavior after the client reconnects to the zookeeper ensemble.

After the client reconnects to the ensemble only those watches should trigger which should have been triggered also if the connections was not lost. This means if I watch for changes in node /foo and there is no change there than my watch should not be triggered on reconnecting to the ensemble.
This is not always the case in the java client.

I've debugged the issues and I could locate the case when the watch is always triggered on reconnect. This is consistently happening if I connect to a follower in the ensemble and I don't do any operation which goes through the leader.
Looking at the code I see that the client stores the lastzxid and sends that with its request. This is 0 on startup and will be updated everytime from the server replies. This lastzxid is also sent to the server after reconnect together with watches. The server decides which watch to trigger based on this lastzxid probably because that should mean the last known state of the client. If this lastzxid is 0 than all the watches are triggered.
I've checked why is this lastzxid 0. I thought it shouldn't be since there was already a request to the server to set the watch and in the reply the server could have sent back the zxid but it turns out that it sends just 0. Looking at the server code I see that for requests which doesn't go through the leader the follower server just sends back the same zxid that the client sent.","[<JIRA Version: name='3.3.5', id='12319081'>, <JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1412,Blocker,Botond Hejj,Fixed,2012-03-15T17:06:13.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,java client watches inconsistently triggered on reconnect,2012-06-04T23:33:58.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.4.1', id='12318650'>, <JIRA Version: name='3.4.2', id='12319196'>, <JIRA Version: name='3.4.3', id='12319288'>]",6.0
Chris Beauchamp,"[<JIRA Component: name='scripts', id='12312384'>]",2012-03-06T09:14:03.000+0000,Chris Beauchamp,"The included init.d script for dpkg creation doesn't restart.

It exits with the following error:

{quote}
\# /etc/init.d/zookeeper restart
/etc/init.d/zookeeper: 127: check_privsep_dir: not found
{quote}

Also the actual zkServer.sh line in restart has a path of .../bin/ rather than .../sbin/

","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1406,Major,Chris Beauchamp,Fixed,2012-03-18T06:50:37.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,dpkg init scripts don't restart - missing check_priv_sep_dir,2012-03-18T11:00:40.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",1.0
,"[<JIRA Component: name='recipes', id='12313246'>]",2012-03-05T13:42:15.000+0000,Robert Varga,"Since the process method is not synchronized in org.apache.zookeeper.recipes.election.LeaderElectionSupport, therefore there is a race condition where events coming in from the watch may overtake the events dispatched during the start method. 

A solution to ensure that events dispatched during the start method are handled before any watch-based events is to make the process method synchronized.",[],Bug,ZOOKEEPER-1405,Major,Robert Varga,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,leader election recipe sample code - dispatchEvent invocations can get out of order,2012-03-06T12:10:55.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",3.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2012-03-05T12:05:16.000+0000,Robert Varga,"The pseudo code for leader election in the recipes.html page of 3.4.3 documentation is the following...

{quote}
Let ELECTION be a path of choice of the application. To volunteer to be a leader: 

1.Create znode z with path ""ELECTION/guid-n_"" with both SEQUENCE and EPHEMERAL flags;

2.Let C be the children of ""ELECTION"", and i be the sequence number of z;

3.Watch for changes on ""ELECTION/guid-n_j"", where j is the {color:red}*smallest*{color} sequence number such that j < i and n_j is a znode in C;

Upon receiving a notification of znode deletion: 

1.Let C be the new set of children of ELECTION; 

2.If z is the smallest node in C, then execute leader procedure;

3.Otherwise, watch for changes on ""ELECTION/guid-n_j"", where j is the {color:red}*smallest*{color} sequence number such that j < i and n_j is a znode in C; 
{quote}


I think, in both third steps *highest* should appear instead of {color:red}*smallest*{color}.
",[],Bug,ZOOKEEPER-1404,Major,Robert Varga,Fixed,2012-12-14T22:11:14.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,leader election pseudo code probably incorrect,2012-12-14T22:11:14.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>]",4.0
James Page,"[<JIRA Component: name='scripts', id='12312384'>]",2012-03-02T11:12:37.000+0000,James Page,"The zkCli.sh script included with zookeeper doesn't quote its parameters
correctly when passing them on to the java program. 

This causes issues with arguments with spaces and such.
","[<JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1403,Minor,James Page,Fixed,2012-03-18T07:04:09.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zkCli.sh script quoting issue,2012-03-18T11:00:39.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.3', id='12319288'>]",1.0
,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='server', id='12312382'>]",2012-02-23T02:16:32.000+0000,Mike Lundy,"When you start up zookeeper using the jar in zookeeper-3.3.4.tar.gz, it prints a 3.3.3 version string:

server.ZooKeeperServer  - Server environment:zookeeper.version=3.3.3-1203054, built on 11/17/2011 05:47 GMT
server.ZooKeeperServer  - Server environment:java.class.path=/usr/lib/zookeeper/apache-rat-tasks-0.6.jar:/usr/lib/zookeeper/commons-lang-2.4.jar:/usr/lib/zookeeper/commons-cli-1.1.jar:/usr/lib/zookeeper/log4j-1.2.15.jar:/usr/lib/zookeeper/commons-collections-3.2.jar:/usr/lib/zookeeper/apache-rat-core-0.6.jar:/usr/lib/zookeeper/jline-0.9.94.jar:/usr/lib/zookeeper/zookeeper-3.3.4.jar:/etc/zookeeper

I assume this is due to a build problem of some form. (Rebuilding the jar from the tarball fixes the version).
",[],Bug,ZOOKEEPER-1399,Minor,Mike Lundy,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Binary Jar in zookeeper-3.3.4 displays wrong version when run,2012-02-23T02:16:32.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>]",0.0
Mike Lundy,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='contrib-bindings', id='12312860'>]",2012-02-22T19:10:02.000+0000,Mike Lundy,"If the session password contains a nul character (\0), it will be mutated as it is passed to python. zkpython currently uses the ParseArgs flag that stops on nul.",[],Bug,ZOOKEEPER-1398,Critical,Mike Lundy,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zkpython corrupts session passwords that contain nulls,2014-09-25T12:56:47.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>]",3.0
Mike Lundy,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='contrib-bindings', id='12312860'>]",2012-02-21T20:01:48.000+0000,Mike Lundy,"This is basically the same issue as ZOOKEEPER-888 and ZOOKEEPER-740 (the latter is open as I write this, but it was superseded by the fix that went in with 888). The problem still exists after the ZOOKEEPER-888 patch, however; it's just more difficult to trigger:

1) Zookeeper notices connection loss, schedules watcher_dispatch
2) Zookeeper notices session loss, schedules watcher_dispatch
3) watcher_dispatch runs for connection loss
4) pywatcher is freed due to is_unrecoverable being true
5) watcher_dispatch runs for session loss
6) PyObject_CallObject attempts to run freed pywatcher with varying bad results

The fix is easy, the dispatcher should act on the state it is given, not the state of the world when it runs. (Patch attached). Reliably triggering the crash is tricky due to the race, but it's not theoretical.","[<JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1395,Critical,Mike Lundy,Fixed,2012-03-30T22:31:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,node-watcher double-free redux,2012-04-25T23:37:13.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>]",1.0
wu wen,"[<JIRA Component: name='java client', id='12312381'>]",2012-02-21T13:26:57.000+0000,Herman Meerlo,"When close() is called on the ZooKeeper instance from a ContextListener (contextDestroyed) there is no way to synchronize with the fact that the EventThread and SendThread have actually finished their work. The problem lies in the SendThread which makes a call to ZooTrace when it exits, but that class has not been loaded yet. Because the ContextListener could not synchronize with the death of the threads the classloader has already disappeared, resulting in a ClassNotFoundException.
My personal opinion is that the close() method should probably wait until the event and send thread have actually died.",[],Bug,ZOOKEEPER-1394,Minor,Herman Meerlo,Not A Problem,2016-10-31T02:22:52.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ClassNotFoundException on shutdown of client,2017-02-17T13:44:52.000+0000,"[<JIRA Version: name='3.4.2', id='12319196'>]",5.0
,"[<JIRA Component: name='java client', id='12312381'>]",2012-02-15T19:11:15.000+0000,Gary Malouf,"I found it very misleading that the javadoc for the exists() calls that take a boolean or a Watcher state that 'The watch will be triggered by a successful operation that creates/delete the node or sets the data on the node.'

What I've seen from descriptions of bugs (older but this is this references it http://zookeeper-user.578899.n2.nabble.com/Exists-Watch-Triggered-by-Delete-td1490893.html) and my own personal usage is that watchers set on exists() are triggered when a non-existing node is now created or an existing node is changed.  They are NOT triggered when the node already exists and is deleted.

http://zookeeper.apache.org/doc/r3.4.3/api/index.html",[],Bug,ZOOKEEPER-1393,Minor,Gary Malouf,Invalid,2012-02-25T18:41:42.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZooKeeper client exists() javadoc incorrectly states watcher(s) will be triggered on node deletion,2012-02-25T18:41:42.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.2', id='12319196'>]",1.0
Bruce Gao,"[<JIRA Component: name='server', id='12312382'>]",2012-02-13T01:45:05.000+0000,Thomas Weise,"Not authorized to read, yet still able to list ACL:

[zk: localhost:2181(CONNECTED) 0] getAcl /sasltest/n4
'sasl,'notme@EXAMPLE.COM
: cdrwa
[zk: localhost:2181(CONNECTED) 1] get /sasltest/n4
Exception in thread ""main"" org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /sasltest/n4
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:113)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1131)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1160)
	at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:711)
	at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:593)
	at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:365)
	at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:323)
	at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:282)
","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.5.5', id='12343268'>, <JIRA Version: name='3.4.14', id='12343587'>]",Bug,ZOOKEEPER-1392,Major,Thomas Weise,Fixed,2019-02-06T14:40:17.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Should not allow to read ACL when not authorized to read node,2019-04-02T10:40:40.000+0000,"[<JIRA Version: name='3.4.2', id='12319196'>]",6.0
Hartmut Lang,"[<JIRA Component: name='java client', id='12312381'>]",2012-02-13T01:41:08.000+0000,Thomas Weise,"[zk: localhost:2181(CONNECTED) 1] create /sasltest/n4 c sasl:notme@EXAMPLE.COM:cdrwa
Created /sasltest/n4
[zk: localhost:2181(CONNECTED) 2] ls /sasltest/n4                                   
Exception in thread ""main"" org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /sasltest/n4
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:113)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1448)
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1476)
	at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:717)
	at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:593)
	at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:365)
	at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:323)
	at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:282)
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1391,Major,Thomas Weise,Duplicate,2012-04-26T08:47:45.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zkCli dies on NoAuth,2012-04-26T08:47:45.000+0000,"[<JIRA Version: name='3.4.2', id='12319196'>]",1.0
Rakesh Radhakrishnan,"[<JIRA Component: name='java client', id='12312381'>]",2012-02-07T05:17:29.000+0000,Rakesh Radhakrishnan,"Multi ops: Op.create(path,..), Op.delete(path, ..), Op.setData(path, ..), 
Op.check(path, ...) apis are not performing the client side path validation and the call will go to the server side and is throwing exception back to the client. 

It would be good to provide ZooKeeper client side path validation for the multi transaction apis. Presently its getting err codes from the server, which is also not properly conveying the cause.

For example: When specified invalid znode path in Op.create, it giving the following exception. This will not be useful to know the actual cause.
{code}
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:115)
	at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:1174)
	at org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:1115)
{code}","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1388,Major,Rakesh Radhakrishnan,Fixed,2013-12-17T16:57:54.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Client side 'PathValidation' is missing for the multi-transaction api.,2014-03-13T18:17:09.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",9.0
Benjamin Reed,"[<JIRA Component: name='quorum', id='12312379'>]",2012-02-06T05:57:41.000+0000,Benjamin Busjaeger,"It looks like line 443 in QuorumPeer [1] may need to change from:

writeLongToFile(CURRENT_EPOCH_FILENAME, acceptedEpoch);

to

writeLongToFile(ACCEPTED_EPOCH_FILENAME, acceptedEpoch);

I only noticed this reading the code, so I may be wrong and I don't know yet if/how this affects the runtime.

[1] https://github.com/apache/zookeeper/blob/trunk/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeer.java#L443","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1387,Minor,Benjamin Busjaeger,Fixed,2012-12-13T08:00:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Wrong epoch file created,2014-03-13T18:16:59.000+0000,"[<JIRA Version: name='3.4.2', id='12319196'>]",4.0
Eugene Joseph Koontz,"[<JIRA Component: name='documentation', id='12312422'>]",2012-02-03T19:14:37.000+0000,Eugene Joseph Koontz,"It seems that the current javadoc.link.java value, http://java.sun.com/javase/6/docs/api/, redirects (via HTTP 301) to http://download.oracle.com/javase/6/docs/api/. This redirect does not always work apparently, causing the URL fetch to fail. This causes an additional javadoc warning: 

javadoc: warning - Error fetching URL: http://java.sun.com/javase/6/docs/api/package-list

which can in turn cause Jenkins to give a -1 to an otherwise OK build (see e.g. https://issues.apache.org/jira/browse/ZOOKEEPER-1373?focusedCommentId=13199456&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13199456). 

","[<JIRA Version: name='3.3.5', id='12319081'>, <JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1386,Minor,Eugene Joseph Koontz,Fixed,2012-02-27T01:01:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"avoid flaky URL redirection in ""ant javadoc"" : replace ""http://java.sun.com/javase/6/docs/api/"" with ""http://download.oracle.com/javase/6/docs/api/"" ",2012-02-28T00:23:06.000+0000,[],1.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2012-02-01T17:27:52.000+0000,Flavio Paiva Junqueira,"API Docs gives a ""Not found"" message.",[],Bug,ZOOKEEPER-1385,Major,Flavio Paiva Junqueira,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zookeeper.apache.org/doc/trunk/ has broken pointers,2013-10-09T06:44:26.000+0000,[],0.0
Jay Shrauner,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='tests', id='12312427'>]",2012-02-01T00:51:33.000+0000,Jay Shrauner,"On Linux with gcc installed in /usr/local and the libs in /usr/local/lib64, test-core-cppunit fails because zktest-st is unable to find the right libstdc++.

build.xml is overriding the environment LD_LIBRARY_PATH instead of appending to it. This should be changed to match the treatment of PATH by appending the desired extra path.","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1384,Minor,Jay Shrauner,Fixed,2012-03-19T06:17:06.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,test-cppunit overrides LD_LIBRARY_PATH and fails if gcc is in non-standard location,2012-03-19T11:00:02.000+0000,"[<JIRA Version: name='3.4.2', id='12319196'>]",1.0
Germán Blanco,"[<JIRA Component: name='server', id='12312382'>]",2012-01-31T01:06:39.000+0000,Neha Narkhede,"I've observed that zookeeper server holds onto expired session ids in the watcher data structures. The result is the wchp command reports session ids that cannot be found through cons/dump and those expired session ids sit there maybe until the server is restarted. Here are snippets from the client and the server logs that lead to this state, for one particular session id 0x134485fd7bcb26f -

There are 4 servers in the zookeeper cluster - 223, 224, 225 (leader), 226 and I'm using ZkClient to connect to the cluster

From the application log -

application.log.2012-01-26-325.gz:2012/01/26 04:56:36.177 INFO [ClientCnxn] [main-SendThread(223.prod:12913)] [application Session establishment complete on server 223.prod/172.17.135.38:12913, sessionid = 0x134485fd7bcb26f, negotiated timeout = 6000
application.log.2012-01-27.gz:2012/01/27 09:52:37.714 INFO [ClientCnxn] [main-SendThread(223.prod:12913)] [application] Client session timed out, have not heard from server in 9827ms for sessionid 0x134485fd7bcb26f, closing socket connection and attempting reconnect
application.log.2012-01-27.gz:2012/01/27 09:52:38.191 INFO [ClientCnxn] [main-SendThread(226.prod:12913)] [application] Unable to reconnect to ZooKeeper service, session 0x134485fd7bcb26f has expired, closing socket connection

On the leader zk, 225 -

zookeeper.log.2012-01-27-leader-225.gz:2012-01-27 09:52:34,010 - INFO  [SessionTracker:ZooKeeperServer@314] - Expiring session 0x134485fd7bcb26f, timeout of 6000ms exceeded
zookeeper.log.2012-01-27-leader-225.gz:2012-01-27 09:52:34,010 - INFO  [ProcessThread:-1:PrepRequestProcessor@391] - Processed session termination for sessionid: 0x134485fd7bcb26f

On the server, the client was initially connected to, 223 -

zookeeper.log.2012-01-26-223.gz:2012-01-26 04:56:36,173 - INFO  [CommitProcessor:1:NIOServerCnxn@1580] - Established session 0x134485fd7bcb26f with negotiated timeout 6000 for client /172.17.136.82:45020
zookeeper.log.2012-01-27-223.gz:2012-01-27 09:52:34,018 - INFO  [CommitProcessor:1:NIOServerCnxn@1435] - Closed socket connection for client /172.17.136.82:45020 which had sessionid 0x134485fd7bcb26f

Here are the log snippets from 226, which is the server, the client reconnected to, before getting session expired event -

2012-01-27 09:52:38,190 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12913:NIOServerCnxn@770] - Client attempting to renew session 0x134485fd7bcb26f at /172.17.136.82:49367
2012-01-27 09:52:38,191 - INFO  [QuorumPeer:/0.0.0.0:12913:NIOServerCnxn@1573] - Invalid session 0x134485fd7bcb26f for client /172.17.136.82:49367, probably expired
2012-01-27 09:52:38,191 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12913:NIOServerCnxn@1435] - Closed socket connection for client /172.17.136.82:49367 which had sessionid 0x134485fd7bcb26f

wchp output from 226, taken on 01/30 -

nnarkhed-ld:zk-cons-wchp-2012013000 nnarkhed$ grep 0x134485fd7bcb26f *226.*wchp* | wc -l
3

wchp output from 223, taken on 01/30 -

nnarkhed-ld:zk-cons-wchp-2012013000 nnarkhed$ grep 0x134485fd7bcb26f *223.*wchp* | wc -l
0

cons output from 223 and 226, taken on 01/30 -

nnarkhed-ld:zk-cons-wchp-2012013000 nnarkhed$ grep 0x134485fd7bcb26f *226.*cons* | wc -l
0

nnarkhed-ld:zk-cons-wchp-2012013000 nnarkhed$ grep 0x134485fd7bcb26f *223.*cons* | wc -l
0

So, what seems to have happened is that the client was able to re-register the watches on the new server (226), after it got disconnected from 223, inspite of having an expired session id. 


In NIOServerCnxn, I saw that after suspecting that a session is expired, a server removes the cnxn and its watches from its internal data structures. But before that it allows more requests to be processed even if the session is expired -

            // Now that the session is ready we can start receiving packets
            synchronized (this.factory) {
                sk.selector().wakeup();
                enableRecv();
            }
        } catch (Exception e) {
            LOG.warn(""Exception while establishing session, closing"", e);
            close();
        }

I wonder if the client somehow sneaked in the set watches, right after the server removed the connection through removeCnxn() API ?
","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1382,Critical,Neha Narkhede,Fixed,2013-12-11T19:18:31.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Zookeeper server holds onto dead/expired session ids in the watch data structures,2016-10-14T05:47:47.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",18.0
Botond Hejj,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2012-01-30T17:14:17.000+0000,Botond Hejj,"The doubly linked list of watches is not updated properly if a watch is taken out from the middle of the chain.
The item after the item which is taken out will receive null pointer for the previous element! This will make the doubly linked list inconsistent and can lead to segfault or infinite loop when the doubly linked list is iterated later.","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1380,Major,Botond Hejj,Fixed,2012-09-07T06:15:48.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zkperl: _zk_release_watch doesn't remove items properly from the watch list,2012-09-07T11:01:51.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.4.1', id='12318650'>, <JIRA Version: name='3.4.2', id='12319196'>]",3.0
Edward Ribeiro,"[<JIRA Component: name='java client', id='12312381'>]",2012-01-30T08:43:50.000+0000,kavita sharma,"while executing the commands:
'printwatches, redo, history and connect usage is getting print 
.basically we are printing usage if user has entered the command 
wrong but in these commands case every time usage is getting print.
eg
{noformat}
[zk: localhost:2181(CONNECTED) 0] printwatches
printwatches is on
ZooKeeper -server host:port cmd args
	connect host:port
	get path [watch]
	ls path [watch]
	set path data [version]
	delquota [-n|-b] path
	quit 
	printwatches on|off
	create [-s] [-e] path data acl
	stat path [watch]
	close 
	ls2 path [watch]
	history 
	listquota path
	setAcl path acl
	getAcl path
	sync path
	redo cmdno
	addauth scheme auth
	delete path [version]
	setquota -n|-b val path
{noformat}","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1379,Minor,kavita sharma,Fixed,2013-09-02T21:05:28.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"'printwatches, redo, history and connect '. client commands always print usage. This is not necessary",2014-03-13T18:17:01.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",4.0
Skye Wanderman-Milne,"[<JIRA Component: name='scripts', id='12312384'>]",2012-01-27T01:39:03.000+0000,Patrick D. Hunt,"It will always include it even if not defined, although not much harm.

if [ ""x$SERVER_JVMFLAGS"" ]
then
JVMFLAGS=""$SERVER_JVMFLAGS $JVMFLAGS""
fi

should use the std idiom.","[<JIRA Version: name='3.3.7', id='12321882'>, <JIRA Version: name='3.4.5', id='12321883'>]",Bug,ZOOKEEPER-1376,Minor,Patrick D. Hunt,Fixed,2012-09-21T23:05:36.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zkServer.sh does not correctly check for $SERVER_JVMFLAGS,2012-09-24T18:29:14.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.3.4', id='12316276'>]",3.0
,[],2012-01-25T08:43:26.000+0000,Rakesh Radhakrishnan,"After reviewing the ClientCnxn code, there is still chances of exiting the SendThread without intimating the users. Say if client throws OOMError and entered into the throwable block. Here again while sending the Disconnected event, its creating ""new WatchedEvent()"" object.This will throw OOMError and leads to exit the SendThread without any Disconnected event notification.

{noformat}
try{
    //...
} catch (Throwable e)
{
    //..
    cleanup();
   if(state.isAlive()){
        eventThread.queueEvent(
        new WatchedEvent(Event.EventType.None, Event.KeeperState.Disconnected, null) )
   }
   //....
}
{noformat}",[],Bug,ZOOKEEPER-1375,Major,Rakesh Radhakrishnan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,SendThread is exiting after OOMError,2013-09-12T22:47:57.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",5.0
James Page,"[<JIRA Component: name='c client', id='12312380'>]",2012-01-24T15:22:47.000+0000,James Page,"The multi-threaded test suite fails to build on ARM architectures:

g++ -DHAVE_CONFIG_H -I. -I./include -I./tests -I./generated -D_FORTIFY_SOURCE=2 -DUSE_STATIC_LIB -DTHREADED -DZKSERVER_CMD=""\""./tests/zkServer.sh\"""" -Wall -g -MT zktest_mt-ThreadingUtil.o -MD -MP -MF .deps/zktest_mt-ThreadingUtil.Tpo -c -o zktest_mt-ThreadingUtil.o `test -f 'tests/ThreadingUtil.cc' || echo './'`tests/ThreadingUtil.cc
/tmp/ccqJWQRC.s: Assembler messages:
/tmp/ccqJWQRC.s:373: Error: bad instruction `lock xaddl r4,[r3,#0]'
/tmp/ccqJWQRC.s:425: Error: bad instruction `lock xchgl r4,[r3,#0]'

gcc does provide alternative primitives (_sync_*) which provide better cross platform compatibility; but that does make the assumption that a) gcc is being used or b) the primitives are provided by alternative compilers.

Tracked in Ubuntu here: https://bugs.launchpad.net/ubuntu/+source/zookeeper/+bug/920871","[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1374,Minor,James Page,Fixed,2012-02-06T09:54:52.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,C client multi-threaded test suite fails to compile on ARM architectures.,2016-06-28T08:37:40.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>]",3.0
Eugene Joseph Koontz,"[<JIRA Component: name='java client', id='12312381'>]",2012-01-24T03:40:05.000+0000,Thomas Weise,"I'm trying to configure a process with Hadoop security (Hive metastore server) to talk to ZooKeeper 3.4.2 with Kerberos authentication. In this scenario Hadoop controls the SASL configuration (org.apache.hadoop.security.UserGroupInformation.HadoopConfiguration), instead of setting up the ZooKeeper ""Client"" loginContext via jaas.conf and system property 

{{-Djava.security.auth.login.config}}

Using the Hadoop configuration would work, except that ZooKeeper client code expects the loginContextName to be ""Client"" while Hadoop security will use  ""hadoop-keytab-kerberos"". I verified that by changing the name in the debugger the SASL authentication succeeds while otherwise the login configuration cannot be resolved and the connection to ZooKeeper is unauthenticated. 

To integrate with Hadoop, the following in ZooKeeperSaslClient would need to change to make the name configurable:

     {{login = new Login(""Client"",new ClientCallbackHandler(null));}}
","[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1373,Major,Thomas Weise,Fixed,2012-02-06T08:37:03.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Hardcoded SASL login context name clashes with Hadoop security configuration override,2013-05-02T02:29:48.000+0000,"[<JIRA Version: name='3.4.2', id='12319196'>]",4.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2012-01-24T01:33:15.000+0000,Patrick D. Hunt,"I started a 2 server ensemble, made some changes to znodes, then shutdown the cluster. 

I then removed the datadir from the original leader. 

I then restarted the entire ensemble. 

after this the new leader has a zxid of 0x400000000 while the follower reported a zxid of 0x300000007 (the last zxid of the old epoch). This was via stat. 

I then connected a client to the ensemble, subsequent to which the zxid was again in sync. The data all seemed fine, but stat was reporting invalid information until a client connected.

",[],Bug,ZOOKEEPER-1372,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,stat reports inconsistent zxids across servers after a leader change,2012-01-24T01:33:15.000+0000,"[<JIRA Version: name='3.4.2', id='12319196'>]",1.0
Mohammad Arshad,[],2012-01-24T00:30:06.000+0000,Mahadev Konar,"ZOOKEEPER-850 added slf4j to ZK. We still depend on log4j in our codebase. We should remove the dependency on log4j so that we can make logging pluggable.
","[<JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-1371,Major,Mahadev Konar,Fixed,2015-11-21T21:21:49.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Remove dependency on log4j in the source code.,2020-02-25T01:21:31.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.4.1', id='12318650'>, <JIRA Version: name='3.4.2', id='12319196'>, <JIRA Version: name='3.4.3', id='12319288'>]",24.0
Mahadev Konar,[],2012-01-24T00:28:23.000+0000,Mahadev Konar,,"[<JIRA Version: name='3.4.3', id='12319288'>]",Bug,ZOOKEEPER-1370,Major,Mahadev Konar,Fixed,2012-02-06T10:29:36.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Add logging changes in Release Notes needed for clients because of ZOOKEEPER-850.,2012-02-06T10:29:36.000+0000,[],1.0
,"[<JIRA Component: name='c client', id='12312380'>]",2012-01-22T04:06:11.000+0000,Marc Celani,"Although wget, awget, wexists, awexists, wgetchildren, awgetchildren will return ZBADARGUMENTS when zh is null, the get APIs will crash if you request a watch, as they dereference the zh without checking for null in order to get the watch function.",[],Bug,ZOOKEEPER-1368,Major,Marc Celani,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zookeeper c client get apis crash if zhandle is null,2012-01-23T20:11:12.000+0000,[],2.0
Benjamin Reed,"[<JIRA Component: name='server', id='12312382'>]",2012-01-20T18:48:01.000+0000,Jeremy Stribling,"In one of our tests, we have a cluster of three ZooKeeper servers.  We kill all three, and then restart just two of them.  Sometimes we notice that on one of the restarted servers, ephemeral nodes from previous sessions do not get deleted, while on the other server they do.  We are effectively running 3.4.2, though technically we are running 3.4.1 with the patch manually applied for ZOOKEEPER-1333 and a C client for 3.4.1 with the patches for ZOOKEEPER-1163.

I noticed that when I connected using zkCli.sh to the first node (90.0.0.221, zkid 84), I saw only one znode in a particular path:

{quote}
[zk: 90.0.0.221:2888(CONNECTED) 0] ls /election/zkrsm
[nominee0000000011]
[zk: 90.0.0.221:2888(CONNECTED) 1] get /election/zkrsm/nominee0000000011
90.0.0.222:7777 
cZxid = 0x400000027
ctime = Thu Jan 19 08:18:24 UTC 2012
mZxid = 0x400000027
mtime = Thu Jan 19 08:18:24 UTC 2012
pZxid = 0x400000027
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0xa234f4f3bc220001
dataLength = 16
numChildren = 0
{quote}

However, when I connect zkCli.sh to the second server (90.0.0.222, zkid 251), I saw three znodes under that same path:

{quote}
[zk: 90.0.0.222:2888(CONNECTED) 2] ls /election/zkrsm
nominee0000000006   nominee0000000010   nominee0000000011
[zk: 90.0.0.222:2888(CONNECTED) 2] get /election/zkrsm/nominee0000000011
90.0.0.222:7777 
cZxid = 0x400000027
ctime = Thu Jan 19 08:18:24 UTC 2012
mZxid = 0x400000027
mtime = Thu Jan 19 08:18:24 UTC 2012
pZxid = 0x400000027
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0xa234f4f3bc220001
dataLength = 16
numChildren = 0
[zk: 90.0.0.222:2888(CONNECTED) 3] get /election/zkrsm/nominee0000000010
90.0.0.221:7777 
cZxid = 0x30000014c
ctime = Thu Jan 19 07:53:42 UTC 2012
mZxid = 0x30000014c
mtime = Thu Jan 19 07:53:42 UTC 2012
pZxid = 0x30000014c
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0xa234f4f3bc220000
dataLength = 16
numChildren = 0
[zk: 90.0.0.222:2888(CONNECTED) 4] get /election/zkrsm/nominee0000000006
90.0.0.223:7777 
cZxid = 0x200000cab
ctime = Thu Jan 19 08:00:30 UTC 2012
mZxid = 0x200000cab
mtime = Thu Jan 19 08:00:30 UTC 2012
pZxid = 0x200000cab
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x5434f5074e040002
dataLength = 16
numChildren = 0
{quote}

These never went away for the lifetime of the server, for any clients connected directly to that server.  Note that this cluster is configured to have all three servers still, the third one being down (90.0.0.223, zkid 162).

I captured the data/snapshot directories for the the two live servers.  When I start single-node servers using each directory, I can briefly see that the inconsistent data is present in those logs, though the ephemeral nodes seem to get (correctly) cleaned up pretty soon after I start the server.

I will upload a tar containing the debug logs and data directories from the failure.  I think we can reproduce it regularly if you need more info.","[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.3.5', id='12319081'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1367,Blocker,Jeremy Stribling,Fixed,2012-01-31T06:56:13.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Data inconsistencies and unexpired ephemeral nodes after cluster restart,2013-08-28T22:20:04.000+0000,"[<JIRA Version: name='3.4.2', id='12319196'>]",9.0
Hongchao Deng,[],2012-01-19T07:00:43.000+0000,Ted Dunning,"If you want to wreak havoc on a ZK based system just do [date -s ""+1hour""] and watch the mayhem as all sessions expire at once.

This shouldn't happen.  Zookeeper could easily know handle elapsed times as elapsed times rather than as differences between absolute times.  The absolute times are subject to adjustment when the clock is set while a timer is not subject to this problem.  In Java, System.currentTimeMillis() gives you absolute time while System.nanoTime() gives you time based on a timer from an arbitrary epoch.

I have done this and have been running tests now for some tens of minutes with no failures.  I will set up a test machine to redo the build again on Ubuntu and post a patch here for discussion.","[<JIRA Version: name='3.5.1', id='12326786'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-1366,Critical,Ted Dunning,Fixed,2015-02-06T05:19:36.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper should be tolerant of clock adjustments,2017-05-11T20:34:32.000+0000,[],32.0
Henry Robinson,[],2012-01-13T17:43:38.000+0000,Henry Robinson,"This block:

{code}
HashSet<Long> followerSet = new HashSet<Long>();
for(LearnerHandler f : learners)
    followerSet.add(f.getSid());
{code}

is executed without holding the lock on learners, so if there were ever a condition where a new learner was added during the initial sync phase, I'm pretty sure we'd see a concurrent modification exception. Certainly other parts of the code are very careful to lock on learners when iterating. 

It would be nice to use a {{ConcurrentHashMap}} to hold the learners instead, but I can't convince myself that this wouldn't introduce some correctness bugs. For example the following:

Learners contains A, B, C, D
Thread 1 iterates over learners, and gets as far as B.
Thread 2 removes A, and adds E.
Thread 1 continues iterating and sees a learner view of A, B, C, D, E

This may be a bug if Thread 1 is counting the number of synced followers for a quorum count, since at no point was A, B, C, D, E a correct view of the quorum.

In practice, I think this is actually ok, because I don't think ZK makes any strong ordering guarantees on learners joining or leaving (so we don't need a strong serialisability guarantee on learners) but I don't think I'll make that change for this patch. Instead I want to clean up the locking protocols on the follower / learner sets - to avoid another easy deadlock like the one we saw in ZOOKEEPER-1294 - and to do less with the lock held; i.e. to copy and then iterate over the copy rather than iterate over a locked set. ","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1361,Major,Henry Robinson,Fixed,2012-09-17T05:04:15.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Leader.lead iterates over 'learners' set without proper synchronisation,2012-09-17T05:04:15.000+0000,"[<JIRA Version: name='3.4.2', id='12319196'>]",5.0
Abraham Fine,"[<JIRA Component: name='tests', id='12312427'>]",2012-01-12T07:44:11.000+0000,Henry Robinson,"After the apparently valid fix to ZOOKEEPER-1294, testNoLogBeforeLeaderEstablishment is failing for me about one time in four. While I'll investigate whether the patch is 1294 is ultimately to blame, reading the test brought to light a number of issues that appear to be bugs or in need of improvement:

* As part of QuorumTest, an ensemble is already established by the fixture setup code, but apparently unused by the test which uses QuorumUtil. 
* The test reads QuorumPeer.leader and QuorumPeer.follower without synchronization, which means that writes to those fields may not be published when we come to read them. 
* The return value of sem.tryAcquire is never checked.
* The progress of the test is based on ad-hoc timings (25 * 500ms sleeps) and inscrutable numbers of iterations through the main loop (e.g. the semaphore blocking the final asserts is released only after the 20000th of 50000 callbacks)
* The test as a whole takes ~30s to run

The first three are easy to fix (as part of fixing the second, I intend to hide all members of QuorumPeer behind getters and setters), the fourth and fifth need a slightly deeper understanding of what the test is trying to achieve.",[],Bug,ZOOKEEPER-1360,Major,Henry Robinson,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,QuorumTest.testNoLogBeforeLeaderEstablishment has several problems,2022-02-03T08:50:26.000+0000,"[<JIRA Version: name='3.4.2', id='12319196'>]",4.0
,"[<JIRA Component: name='java client', id='12312381'>]",2012-01-10T08:46:09.000+0000,kavita sharma,"In zkCli if we create a node without data then also node is getting created but if we will see in the commandMap 
it shows that
{noformat}
 commandMap.put(""create"", ""[-s] [-e] path data acl"");
{noformat}
that means data and acl parts are not optional .we need to change these parts as optional.",[],Bug,ZOOKEEPER-1359,Trivial,kavita sharma,Duplicate,2012-12-16T06:50:49.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZkCli create command data and acl parts should be optional.,2013-07-01T21:37:13.000+0000,[],5.0
Alexander Shraer,[],2012-01-10T01:46:31.000+0000,Alexander Shraer,"should check for something smaller, perhaps 1ms or 5ms","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1358,Trivial,Alexander Shraer,Fixed,2012-01-16T02:20:47.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"In StaticHostProviderTest.java, testNextDoesNotSleepForZero tests that hostProvider.next(0) doesn't sleep by checking that the latency of this call is less than 10sec",2012-01-16T03:56:46.000+0000,[],1.0
Alexander Shraer,"[<JIRA Component: name='tests', id='12312427'>]",2012-01-09T23:04:30.000+0000,Alexander Shraer,"Here's what I get:


Testcase: testLeaderInConnectingFollowers took 34.117 sec
Testcase: testLastAcceptedEpoch took 0.047 sec                    <----- new test added in ZK-1343
Testcase: testLeaderInElectingFollowers took 0.004 sec
        Caused an ERROR
Address already in use
java.net.BindException: Address already in use
        at java.net.PlainSocketImpl.socketBind(Native Method)
        at java.net.PlainSocketImpl.bind(PlainSocketImpl.java:383)
        at java.net.ServerSocket.bind(ServerSocket.java:328)
        at java.net.ServerSocket.<init>(ServerSocket.java:194)
        at java.net.ServerSocket.<init>(ServerSocket.java:106)
        at org.apache.zookeeper.server.quorum.Leader.<init>(Leader.java:220)
        at org.apache.zookeeper.server.quorum.Zab1_0Test.createLeader(Zab1_0Test.java:711)
        at org.apache.zookeeper.server.quorum.Zab1_0Test.testLeaderInElectingFollowers(Zab1_0Test.java:225)

Testcase: testNormalFollowerRun took 29.128 sec
Testcase: testNormalRun took 25.158 sec
Testcase: testLeaderBehind took 25.148 sec
Testcase: testAbandonBeforeACKEpoch took 34.029 sec


My guess is that testLastAcceptedEpoch doesn't properly close the connection before testLeaderInElectingFollowers starts.
I propose to add 

if (leadThread != null) {
                leadThread.interrupt();
                leadThread.join();
}       

to the test.


In addition, I propose to change the hard-wired ports in Zab1_0Test to use Portassignment.unique() as done in other tests. If I understand correctly the static counter used in unique() to assign ports is initialized once per test file, so it would also prevent the problem I'm seeing here of two tests in the same file trying to use the same port. 

The error can be reproduced using the attached patch (for some reason I don't see the problem in the trunk).

","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1357,Minor,Alexander Shraer,Fixed,2014-04-14T21:53:11.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"Zab1_0Test uses hard-wired port numbers. Specifically, it uses the same port for leader in two different tests. The second test periodically fails complaining that the port is still in use.",2014-04-14T22:31:59.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",4.0
Neha Narkhede,"[<JIRA Component: name='java client', id='12312381'>]",2012-01-09T22:42:06.000+0000,Neha Narkhede,"Relevant conversation on the dev mailing list - https://email.corp.linkedin.com/owa/redir.aspx?C=87f3d1e78c96438c8115e450f410d010&URL=http%3a%2f%2fmarkmail.org%2fmessage%2f3vzynx6rgurubf3p%3fq%3dPerforming%2bno%2bdowntime%2bhardware%2bchanges%2bto%2ba%2blive%2bzookeeper%2bcluster%2blist%3aorg%252Eapache%252Ehadoop%252Ezookeeper-dev

Basically, the client caches the list of server IPs internally and maintains that list for the entire lifetime of the client. This limits the ability to remove/change a server node from a zookeeper cluster, without having to restart every client. Also, two levels of IP caching, one in the JVM and one in the zookeeper client code seems unnecessar.

It would be ideal to provide a config option that would turn off this IP caching in the client and re-resolve the host names during the reconnect. ",[],Bug,ZOOKEEPER-1356,Major,Neha Narkhede,Duplicate,2012-01-10T16:39:14.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Avoid permanent caching of server IPs in the client ,2016-02-05T09:54:08.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.2', id='12319196'>]",3.0
Patrick D. Hunt,"[<JIRA Component: name='tests', id='12312427'>]",2012-01-07T01:10:24.000+0000,Patrick D. Hunt,"I'm seeing the following intermittent failure:

{noformat}
junit.framework.AssertionFailedError: Should have called my watcher expected:<1> but was:<0>
	at org.apache.zookeeper.test.AuthTest.testBadAuthThenSendOtherCommands(AuthTest.java:89)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
{noformat}

The following commit introduced this test:

bq. ZOOKEEPER-1152. Exceptions thrown from handleAuthentication can cause buffer corruption issues in NIOServer. (camille via breed)

+            Assert.assertEquals(""Should have called my watcher"",
+                    1, authFailed.get());

I think it's due to either a) the code is not waiting for the
notification to be propagated, or 2) the message doesn't make it back
from the server to the client prior to the socket or the clientcnxn
being closed.

What do you think, should I just wait for the notification to arrive? or do you think it's 2). ?

","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1354,Major,Patrick D. Hunt,Fixed,2012-03-02T03:05:38.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,AuthTest.testBadAuthThenSendOtherCommands fails intermittently,2012-03-02T03:21:33.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
Clint Byrum,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='tests', id='12312427'>]",2012-01-06T21:42:50.000+0000,Clint Byrum,"When the c client test suite, zktest-mt, is run, it fails with this:

tests/TestZookeeperInit.cc:233: Assertion: equality assertion failed [Expected: 2, Actual  : 22]

This was also reported in 3.3.1 here:

http://www.mail-archive.com/zookeeper-dev@hadoop.apache.org/msg08914.html

The C client tests are making some assumptions that are not valid. getaddrinfo may have, at one time, returned ENOENT instead of EINVAL for the host given in the test. The assertion should simply be that EINVAL | ENOENT are given, so that builds on platforms which return ENOENT for this are not broken.

","[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.3.5', id='12319081'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1353,Minor,Clint Byrum,Fixed,2012-02-06T08:00:08.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,C client test suite fails consistently,2012-02-06T10:58:04.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='tests', id='12312427'>]",2012-01-05T19:12:19.000+0000,Patrick D. Hunt,"InvalidSnapshotTest is using connection timeouts that are too short, see this false failure:
https://builds.apache.org/job/ZooKeeper_branch33_solaris/65/testReport/junit/org.apache.zookeeper.server/InvalidSnapshotTest/testInvalidSnapshot/

{noformat}
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /invalidsnap-0
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:643)
	at org.apache.zookeeper.server.InvalidSnapshotTest.testInvalidSnapshot(InvalidSnapshotTest.java:71)
{noformat}

Also in looking at the test itself it could use some cleanup (reuse features from ClientBase test utils)","[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.3.5', id='12319081'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1352,Major,Patrick D. Hunt,Fixed,2012-02-06T08:52:20.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,server.InvalidSnapshotTest is using connection timeouts that are too short,2012-02-06T10:58:04.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='tests', id='12312427'>]",2012-01-04T22:55:46.000+0000,Patrick D. Hunt,"tests such as org.apache.zookeeper.test.MultiTransactionTest.testWatchesTriggered() are incorrect. Two issues I see

1) zk.sync is async, there is no guarantee that the watcher will be called subsequent to sync returning

{noformat}
        zk.sync(""/"", null, null);
        assertTrue(watcher.triggered); /// incorrect assumption
{noformat}

The callback needs to be implemented, only once the callback is called can we verify the trigger.

2) trigger is not declared as volatile, even though it will be set in the context of a different thread (eventthread)

See https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper-trunk-solaris/91/testReport/junit/org.apache.zookeeper.test/MultiTransactionTest/testWatchesTriggered/
for an example of a false positive failure

{noformat}
junit.framework.AssertionFailedError
	at org.apache.zookeeper.test.MultiTransactionTest.testWatchesTriggered(MultiTransactionTest.java:236)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
{noformat}
","[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1351,Major,Patrick D. Hunt,Fixed,2012-01-16T02:35:27.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,invalid test verification in MultiTransactionTest,2012-01-16T03:56:46.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
Mahadev Konar,"[<JIRA Component: name='c client', id='12312380'>]",2011-12-31T02:58:03.000+0000,Marshall McMullen,"When running the 3.4.2 C client, it shows the following output:

Client environment:zookeeper.version=zookeeper C client 3.4.1

This should show ""3.4.2"" not ""3.4.1"". The problem looks to be caused by stale autoconf files in the C directory. 

grep -R ""zookeeper C client 3.4.1"" *                                                                                                                                                                     

autom4te.cache/output.0:@%:@ Generated by GNU Autoconf 2.59 for zookeeper C client 3.4.1.
autom4te.cache/output.0:PACKAGE_STRING='zookeeper C client 3.4.1'
autom4te.cache/output.0:\`configure' configures zookeeper C client 3.4.1 to adapt to many kinds of systems.
autom4te.cache/output.0:     short | recursive ) echo ""Configuration of zookeeper C client 3.4.1:"";;
autom4te.cache/output.1:@%:@ Generated by GNU Autoconf 2.59 for zookeeper C client 3.4.1.
autom4te.cache/output.1:PACKAGE_STRING='zookeeper C client 3.4.1'
autom4te.cache/output.1:\`configure' configures zookeeper C client 3.4.1 to adapt to many kinds of systems.
autom4te.cache/output.1:     short | recursive ) echo ""Configuration of zookeeper C client 3.4.1:"";;
config.h:#define PACKAGE_STRING ""zookeeper C client 3.4.1""
config.log:| #define PACKAGE_STRING ""zookeeper C client 3.4.1""
config.log:| #define PACKAGE_STRING ""zookeeper C client 3.4.1""
config.log:| #define PACKAGE_STRING ""zookeeper C client 3.4.1""
config.log:| #define PACKAGE_STRING ""zookeeper C client 3.4.1""
config.log:| #define PACKAGE_STRING ""zookeeper C client 3.4.1""
config.log:| #define PACKAGE_STRING ""zookeeper C client 3.4.1""
config.log:| #define PACKAGE_STRING ""zookeeper C client 3.4.1""
config.log:PACKAGE_STRING='zookeeper C client 3.4.1'
config.log:#define PACKAGE_STRING ""zookeeper C client 3.4.1""
config.status:s,@PACKAGE_STRING@,zookeeper C client 3.4.1,;t t
config.status:${ac_dA}PACKAGE_STRING${ac_dB}PACKAGE_STRING${ac_dC}""zookeeper C client 3.4.1""${ac_dD}
config.status:${ac_uA}PACKAGE_STRING${ac_uB}PACKAGE_STRING${ac_uC}""zookeeper C client 3.4.1""${ac_uD}
configure:# Generated by GNU Autoconf 2.59 for zookeeper C client 3.4.1.
configure:PACKAGE_STRING='zookeeper C client 3.4.1'
configure:\`configure' configures zookeeper C client 3.4.1 to adapt to many kinds of systems.
configure:     short | recursive ) echo ""Configuration of zookeeper C client 3.4.1:"";;
Binary file libzkmt_la-zookeeper.o matches
Makefile:PACKAGE_STRING = zookeeper C client 3.4.1
","[<JIRA Version: name='3.4.3', id='12319288'>]",Bug,ZOOKEEPER-1348,Major,Marshall McMullen,Fixed,2012-02-06T08:20:35.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper 3.4.2 C client incorrectly reports string version of 3.4.1,2012-02-06T08:20:35.000+0000,"[<JIRA Version: name='3.4.2', id='12319196'>]",1.0
Rakesh Radhakrishnan,"[<JIRA Component: name='java client', id='12312381'>]",2011-12-26T13:36:28.000+0000,Rakesh Radhakrishnan,"For example: 
I have created a ZooKeeper client with subtree as ""10.18.52.144:2179/apps/X"". Now just generated OP command for the creation of zNode ""/myId"". When the client creates the path ""/myid"", the ZooKeeper server is actually be creating the path as ""/myid"" instead of creating as ""/apps/X/myid""

Expected output: zNode has to be created as ""/apps/X/myid""","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1344,Critical,Rakesh Radhakrishnan,Fixed,2012-03-17T00:32:59.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZooKeeper client multi-update command is not considering the Chroot request,2012-03-18T04:54:23.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",3.0
Flavio Paiva Junqueira,[],2011-12-23T12:44:25.000+0000,Flavio Paiva Junqueira,"The following block in Leader.getEpochToPropose:

{noformat}
if (lastAcceptedEpoch > epoch) {
	epoch = lastAcceptedEpoch+1;
}
{noformat}

needs to be fixed, since it doesn't increment the epoch variable in the case epoch != -1 (initial value) and lastAcceptedEpoch is equal. The fix trivial and corresponds to changing > with >=. ","[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1343,Critical,Flavio Paiva Junqueira,Fixed,2012-01-04T00:28:18.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,getEpochToPropose should check if lastAcceptedEpoch is greater or equal than epoch,2012-01-09T22:50:23.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2011-12-22T19:55:48.000+0000,Patrick D. Hunt,"The handling of an invalid multi op in org.apache.zookeeper.server.DataTree.processTxn(TxnHeader, Record) is unusual, looks wrong to me.

In particular an IOException is thrown and then essentially ignored, it seems to me we should fail the operation properly instead. This will be more important if we add new op types going fwd.

Use of assert is a bit suspect as well, however perhaps it's fine... not sure. (we don't explicitly turn on assertions in our tests so not sure how useful it is regardless)

Also notice that the catch of IOException is ignoring the result. It seems to me that handling this exception should be localized to the multi block (separate it out to it's own method seems like a good idea).

We should add a test for this case.",[],Bug,ZOOKEEPER-1341,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,problem handling invalid multi op in processTxn,2011-12-22T19:55:48.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",0.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2011-12-22T18:03:07.000+0000,Patrick D. Hunt,"Multi operations run by users are generating ERROR level messages in the server log even though they are typical user level operations that are not in any way impacting the server, example:

{noformat}
2011-12-22 09:55:06,538 [myid:] - ERROR [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@545] - >>>> Got user-level KeeperException when processing sessionid:0x13466e9828c0000 type:multi cxid:0x3 zxid:0x2 txntype:2 reqpath:n/a Error Path:/nonexisting Error:KeeperErrorCode = NoNode for /nonexisting
2011-12-22 09:55:06,538 [myid:] - ERROR [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@549] - >>>> ABORTING remaing MultiOp ops
{noformat}

This is misleading. We should demote these messages to INFO level at the highest. (this is what we do for other such user operations, e.g. nonode)
","[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1340,Major,Patrick D. Hunt,Fixed,2012-02-06T09:50:06.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,multi problem - typical user operations are generating ERROR level messages in the server,2012-02-06T10:58:03.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
Eric Liang,"[<JIRA Component: name='c client', id='12312380'>]",2011-12-22T10:00:55.000+0000,Jakub Lekstan,"When I'm trying to build 3.4.1 c client with --enable-debug switch I'm getting following error:

{code}
make  all-am
make[1]: Entering directory `/home/jlekstan/zookeeper-3.4.1/src/c'
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O0 -D_GNU_SOURCE -MT zookeeper.lo -MD -MP -MF "".deps/zookeeper.Tpo"" -c -o zookeeper.lo `test -f 'src/zookeeper.c' || echo './'`src/zookeeper.c; \
	then mv -f "".deps/zookeeper.Tpo"" "".deps/zookeeper.Plo""; else rm -f "".deps/zookeeper.Tpo""; exit 1; fi
mkdir .libs
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c src/zookeeper.c  -fPIC -DPIC -o .libs/zookeeper.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c src/zookeeper.c -o zookeeper.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O0 -D_GNU_SOURCE -MT recordio.lo -MD -MP -MF "".deps/recordio.Tpo"" -c -o recordio.lo `test -f 'src/recordio.c' || echo './'`src/recordio.c; \
	then mv -f "".deps/recordio.Tpo"" "".deps/recordio.Plo""; else rm -f "".deps/recordio.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT recordio.lo -MD -MP -MF .deps/recordio.Tpo -c src/recordio.c  -fPIC -DPIC -o .libs/recordio.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT recordio.lo -MD -MP -MF .deps/recordio.Tpo -c src/recordio.c -o recordio.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O0 -D_GNU_SOURCE -MT zookeeper.jute.lo -MD -MP -MF "".deps/zookeeper.jute.Tpo"" -c -o zookeeper.jute.lo `test -f 'generated/zookeeper.jute.c' || echo './'`generated/zookeeper.jute.c; \
	then mv -f "".deps/zookeeper.jute.Tpo"" "".deps/zookeeper.jute.Plo""; else rm -f "".deps/zookeeper.jute.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT zookeeper.jute.lo -MD -MP -MF .deps/zookeeper.jute.Tpo -c generated/zookeeper.jute.c  -fPIC -DPIC -o .libs/zookeeper.jute.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT zookeeper.jute.lo -MD -MP -MF .deps/zookeeper.jute.Tpo -c generated/zookeeper.jute.c -o zookeeper.jute.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O0 -D_GNU_SOURCE -MT zk_log.lo -MD -MP -MF "".deps/zk_log.Tpo"" -c -o zk_log.lo `test -f 'src/zk_log.c' || echo './'`src/zk_log.c; \
	then mv -f "".deps/zk_log.Tpo"" "".deps/zk_log.Plo""; else rm -f "".deps/zk_log.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT zk_log.lo -MD -MP -MF .deps/zk_log.Tpo -c src/zk_log.c  -fPIC -DPIC -o .libs/zk_log.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT zk_log.lo -MD -MP -MF .deps/zk_log.Tpo -c src/zk_log.c -o zk_log.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O0 -D_GNU_SOURCE -MT zk_hashtable.lo -MD -MP -MF "".deps/zk_hashtable.Tpo"" -c -o zk_hashtable.lo `test -f 'src/zk_hashtable.c' || echo './'`src/zk_hashtable.c; \
	then mv -f "".deps/zk_hashtable.Tpo"" "".deps/zk_hashtable.Plo""; else rm -f "".deps/zk_hashtable.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT zk_hashtable.lo -MD -MP -MF .deps/zk_hashtable.Tpo -c src/zk_hashtable.c  -fPIC -DPIC -o .libs/zk_hashtable.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT zk_hashtable.lo -MD -MP -MF .deps/zk_hashtable.Tpo -c src/zk_hashtable.c -o zk_hashtable.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O0 -D_GNU_SOURCE -MT st_adaptor.lo -MD -MP -MF "".deps/st_adaptor.Tpo"" -c -o st_adaptor.lo `test -f 'src/st_adaptor.c' || echo './'`src/st_adaptor.c; \
	then mv -f "".deps/st_adaptor.Tpo"" "".deps/st_adaptor.Plo""; else rm -f "".deps/st_adaptor.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT st_adaptor.lo -MD -MP -MF .deps/st_adaptor.Tpo -c src/st_adaptor.c  -fPIC -DPIC -o .libs/st_adaptor.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT st_adaptor.lo -MD -MP -MF .deps/st_adaptor.Tpo -c src/st_adaptor.c -o st_adaptor.o >/dev/null 2>&1
/bin/bash ./libtool --tag=CC --mode=link gcc -Wall -Werror  -g -O0 -D_GNU_SOURCE   -o libzkst.la   zookeeper.lo recordio.lo zookeeper.jute.lo zk_log.lo zk_hashtable.lo st_adaptor.lo -lm 
ar cru .libs/libzkst.a .libs/zookeeper.o .libs/recordio.o .libs/zookeeper.jute.o .libs/zk_log.o .libs/zk_hashtable.o .libs/st_adaptor.o
ranlib .libs/libzkst.a
creating libzkst.la
(cd .libs && rm -f libzkst.la && ln -s ../libzkst.la libzkst.la)
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O0 -D_GNU_SOURCE -MT hashtable_itr.lo -MD -MP -MF "".deps/hashtable_itr.Tpo"" -c -o hashtable_itr.lo `test -f 'src/hashtable/hashtable_itr.c' || echo './'`src/hashtable/hashtable_itr.c; \
	then mv -f "".deps/hashtable_itr.Tpo"" "".deps/hashtable_itr.Plo""; else rm -f "".deps/hashtable_itr.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT hashtable_itr.lo -MD -MP -MF .deps/hashtable_itr.Tpo -c src/hashtable/hashtable_itr.c  -fPIC -DPIC -o .libs/hashtable_itr.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT hashtable_itr.lo -MD -MP -MF .deps/hashtable_itr.Tpo -c src/hashtable/hashtable_itr.c -o hashtable_itr.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O0 -D_GNU_SOURCE -MT hashtable.lo -MD -MP -MF "".deps/hashtable.Tpo"" -c -o hashtable.lo `test -f 'src/hashtable/hashtable.c' || echo './'`src/hashtable/hashtable.c; \
	then mv -f "".deps/hashtable.Tpo"" "".deps/hashtable.Plo""; else rm -f "".deps/hashtable.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT hashtable.lo -MD -MP -MF .deps/hashtable.Tpo -c src/hashtable/hashtable.c  -fPIC -DPIC -o .libs/hashtable.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT hashtable.lo -MD -MP -MF .deps/hashtable.Tpo -c src/hashtable/hashtable.c -o hashtable.o >/dev/null 2>&1
/bin/bash ./libtool --tag=CC --mode=link gcc -Wall -Werror  -g -O0 -D_GNU_SOURCE   -o libhashtable.la   hashtable_itr.lo hashtable.lo  
ar cru .libs/libhashtable.a .libs/hashtable_itr.o .libs/hashtable.o
ranlib .libs/libhashtable.a
creating libhashtable.la
(cd .libs && rm -f libhashtable.la && ln -s ../libhashtable.la libhashtable.la)
/bin/bash ./libtool --tag=CC --mode=link gcc -Wall -Werror  -g -O0 -D_GNU_SOURCE   -o libzookeeper_st.la -rpath /usr/local/lib -no-undefined -version-info 2 -export-symbols-regex '(zoo_|zookeeper_|zhandle|Z|format_log_message|log_message|logLevel|deallocate_|zerror|is_unrecoverable)'  libzkst.la libhashtable.la 
generating symbol list for `libzookeeper_st.la'
/usr/bin/nm -B   ./.libs/libzkst.a ./.libs/libhashtable.a | sed -n -e 's/^.*[ 	]\([ABCDGIRSTW][ABCDGIRSTW]*\)[ 	][ 	]*\([_A-Za-z][_A-Za-z0-9]*\)$/\1 \2 \2/p' | /bin/sed 's/.* //' | sort | uniq > .libs/libzookeeper_st.exp
grep -E -e ""(zoo_|zookeeper_|zhandle|Z|format_log_message|log_message|logLevel|deallocate_|zerror|is_unrecoverable)"" "".libs/libzookeeper_st.exp"" > "".libs/libzookeeper_st.expT""
mv -f "".libs/libzookeeper_st.expT"" "".libs/libzookeeper_st.exp""
echo ""{ global:"" > .libs/libzookeeper_st.ver
 cat .libs/libzookeeper_st.exp | sed -e ""s/\(.*\)/\1;/"" >> .libs/libzookeeper_st.ver
 echo ""local: *; };"" >> .libs/libzookeeper_st.ver
 gcc -shared  -Wl,--whole-archive ./.libs/libzkst.a ./.libs/libhashtable.a -Wl,--no-whole-archive  -lm  -Wl,-soname -Wl,libzookeeper_st.so.2 -Wl,-version-script -Wl,.libs/libzookeeper_st.ver -o .libs/libzookeeper_st.so.2.0.0
(cd .libs && rm -f libzookeeper_st.so.2 && ln -s libzookeeper_st.so.2.0.0 libzookeeper_st.so.2)
(cd .libs && rm -f libzookeeper_st.so && ln -s libzookeeper_st.so.2.0.0 libzookeeper_st.so)
rm -fr .libs/libzookeeper_st.lax
mkdir .libs/libzookeeper_st.lax
rm -fr .libs/libzookeeper_st.lax/libzkst.a
mkdir .libs/libzookeeper_st.lax/libzkst.a
(cd .libs/libzookeeper_st.lax/libzkst.a && ar x /home/jlekstan/zookeeper-3.4.1/src/c/./.libs/libzkst.a)
rm -fr .libs/libzookeeper_st.lax/libhashtable.a
mkdir .libs/libzookeeper_st.lax/libhashtable.a
(cd .libs/libzookeeper_st.lax/libhashtable.a && ar x /home/jlekstan/zookeeper-3.4.1/src/c/./.libs/libhashtable.a)
ar cru .libs/libzookeeper_st.a   .libs/libzookeeper_st.lax/libzkst.a/zookeeper.o .libs/libzookeeper_st.lax/libzkst.a/st_adaptor.o .libs/libzookeeper_st.lax/libzkst.a/recordio.o .libs/libzookeeper_st.lax/libzkst.a/zk_hashtable.o .libs/libzookeeper_st.lax/libzkst.a/zk_log.o .libs/libzookeeper_st.lax/libzkst.a/zookeeper.jute.o  .libs/libzookeeper_st.lax/libhashtable.a/hashtable_itr.o .libs/libzookeeper_st.lax/libhashtable.a/hashtable.o 
ranlib .libs/libzookeeper_st.a
rm -fr .libs/libzookeeper_st.lax
creating libzookeeper_st.la
(cd .libs && rm -f libzookeeper_st.la && ln -s ../libzookeeper_st.la libzookeeper_st.la)
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zookeeper.lo -MD -MP -MF "".deps/libzkmt_la-zookeeper.Tpo"" -c -o libzkmt_la-zookeeper.lo `test -f 'src/zookeeper.c' || echo './'`src/zookeeper.c; \
	then mv -f "".deps/libzkmt_la-zookeeper.Tpo"" "".deps/libzkmt_la-zookeeper.Plo""; else rm -f "".deps/libzkmt_la-zookeeper.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zookeeper.lo -MD -MP -MF .deps/libzkmt_la-zookeeper.Tpo -c src/zookeeper.c  -fPIC -DPIC -o .libs/libzkmt_la-zookeeper.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zookeeper.lo -MD -MP -MF .deps/libzkmt_la-zookeeper.Tpo -c src/zookeeper.c -o libzkmt_la-zookeeper.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-recordio.lo -MD -MP -MF "".deps/libzkmt_la-recordio.Tpo"" -c -o libzkmt_la-recordio.lo `test -f 'src/recordio.c' || echo './'`src/recordio.c; \
	then mv -f "".deps/libzkmt_la-recordio.Tpo"" "".deps/libzkmt_la-recordio.Plo""; else rm -f "".deps/libzkmt_la-recordio.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-recordio.lo -MD -MP -MF .deps/libzkmt_la-recordio.Tpo -c src/recordio.c  -fPIC -DPIC -o .libs/libzkmt_la-recordio.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-recordio.lo -MD -MP -MF .deps/libzkmt_la-recordio.Tpo -c src/recordio.c -o libzkmt_la-recordio.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zookeeper.jute.lo -MD -MP -MF "".deps/libzkmt_la-zookeeper.jute.Tpo"" -c -o libzkmt_la-zookeeper.jute.lo `test -f 'generated/zookeeper.jute.c' || echo './'`generated/zookeeper.jute.c; \
	then mv -f "".deps/libzkmt_la-zookeeper.jute.Tpo"" "".deps/libzkmt_la-zookeeper.jute.Plo""; else rm -f "".deps/libzkmt_la-zookeeper.jute.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zookeeper.jute.lo -MD -MP -MF .deps/libzkmt_la-zookeeper.jute.Tpo -c generated/zookeeper.jute.c  -fPIC -DPIC -o .libs/libzkmt_la-zookeeper.jute.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zookeeper.jute.lo -MD -MP -MF .deps/libzkmt_la-zookeeper.jute.Tpo -c generated/zookeeper.jute.c -o libzkmt_la-zookeeper.jute.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zk_log.lo -MD -MP -MF "".deps/libzkmt_la-zk_log.Tpo"" -c -o libzkmt_la-zk_log.lo `test -f 'src/zk_log.c' || echo './'`src/zk_log.c; \
	then mv -f "".deps/libzkmt_la-zk_log.Tpo"" "".deps/libzkmt_la-zk_log.Plo""; else rm -f "".deps/libzkmt_la-zk_log.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zk_log.lo -MD -MP -MF .deps/libzkmt_la-zk_log.Tpo -c src/zk_log.c  -fPIC -DPIC -o .libs/libzkmt_la-zk_log.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zk_log.lo -MD -MP -MF .deps/libzkmt_la-zk_log.Tpo -c src/zk_log.c -o libzkmt_la-zk_log.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zk_hashtable.lo -MD -MP -MF "".deps/libzkmt_la-zk_hashtable.Tpo"" -c -o libzkmt_la-zk_hashtable.lo `test -f 'src/zk_hashtable.c' || echo './'`src/zk_hashtable.c; \
	then mv -f "".deps/libzkmt_la-zk_hashtable.Tpo"" "".deps/libzkmt_la-zk_hashtable.Plo""; else rm -f "".deps/libzkmt_la-zk_hashtable.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zk_hashtable.lo -MD -MP -MF .deps/libzkmt_la-zk_hashtable.Tpo -c src/zk_hashtable.c  -fPIC -DPIC -o .libs/libzkmt_la-zk_hashtable.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zk_hashtable.lo -MD -MP -MF .deps/libzkmt_la-zk_hashtable.Tpo -c src/zk_hashtable.c -o libzkmt_la-zk_hashtable.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-mt_adaptor.lo -MD -MP -MF "".deps/libzkmt_la-mt_adaptor.Tpo"" -c -o libzkmt_la-mt_adaptor.lo `test -f 'src/mt_adaptor.c' || echo './'`src/mt_adaptor.c; \
	then mv -f "".deps/libzkmt_la-mt_adaptor.Tpo"" "".deps/libzkmt_la-mt_adaptor.Plo""; else rm -f "".deps/libzkmt_la-mt_adaptor.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-mt_adaptor.lo -MD -MP -MF .deps/libzkmt_la-mt_adaptor.Tpo -c src/mt_adaptor.c  -fPIC -DPIC -o .libs/libzkmt_la-mt_adaptor.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-mt_adaptor.lo -MD -MP -MF .deps/libzkmt_la-mt_adaptor.Tpo -c src/mt_adaptor.c -o libzkmt_la-mt_adaptor.o >/dev/null 2>&1
/bin/bash ./libtool --tag=CC --mode=link gcc -Wall -Werror  -g -O0 -D_GNU_SOURCE   -o libzkmt.la   libzkmt_la-zookeeper.lo libzkmt_la-recordio.lo libzkmt_la-zookeeper.jute.lo libzkmt_la-zk_log.lo libzkmt_la-zk_hashtable.lo libzkmt_la-mt_adaptor.lo -lm 
ar cru .libs/libzkmt.a .libs/libzkmt_la-zookeeper.o .libs/libzkmt_la-recordio.o .libs/libzkmt_la-zookeeper.jute.o .libs/libzkmt_la-zk_log.o .libs/libzkmt_la-zk_hashtable.o .libs/libzkmt_la-mt_adaptor.o
ranlib .libs/libzkmt.a
creating libzkmt.la
(cd .libs && rm -f libzkmt.la && ln -s ../libzkmt.la libzkmt.la)
/bin/bash ./libtool --tag=CC --mode=link gcc -Wall -Werror  -g -O0 -D_GNU_SOURCE   -o libzookeeper_mt.la -rpath /usr/local/lib -no-undefined -version-info 2 -export-symbols-regex '(zoo_|zookeeper_|zhandle|Z|format_log_message|log_message|logLevel|deallocate_|zerror|is_unrecoverable)'  libzkmt.la libhashtable.la -lpthread 
generating symbol list for `libzookeeper_mt.la'
/usr/bin/nm -B   ./.libs/libzkmt.a ./.libs/libhashtable.a | sed -n -e 's/^.*[ 	]\([ABCDGIRSTW][ABCDGIRSTW]*\)[ 	][ 	]*\([_A-Za-z][_A-Za-z0-9]*\)$/\1 \2 \2/p' | /bin/sed 's/.* //' | sort | uniq > .libs/libzookeeper_mt.exp
grep -E -e ""(zoo_|zookeeper_|zhandle|Z|format_log_message|log_message|logLevel|deallocate_|zerror|is_unrecoverable)"" "".libs/libzookeeper_mt.exp"" > "".libs/libzookeeper_mt.expT""
mv -f "".libs/libzookeeper_mt.expT"" "".libs/libzookeeper_mt.exp""
echo ""{ global:"" > .libs/libzookeeper_mt.ver
 cat .libs/libzookeeper_mt.exp | sed -e ""s/\(.*\)/\1;/"" >> .libs/libzookeeper_mt.ver
 echo ""local: *; };"" >> .libs/libzookeeper_mt.ver
 gcc -shared  -Wl,--whole-archive ./.libs/libzkmt.a ./.libs/libhashtable.a -Wl,--no-whole-archive  -lm -lpthread  -Wl,-soname -Wl,libzookeeper_mt.so.2 -Wl,-version-script -Wl,.libs/libzookeeper_mt.ver -o .libs/libzookeeper_mt.so.2.0.0
(cd .libs && rm -f libzookeeper_mt.so.2 && ln -s libzookeeper_mt.so.2.0.0 libzookeeper_mt.so.2)
(cd .libs && rm -f libzookeeper_mt.so && ln -s libzookeeper_mt.so.2.0.0 libzookeeper_mt.so)
rm -fr .libs/libzookeeper_mt.lax
mkdir .libs/libzookeeper_mt.lax
rm -fr .libs/libzookeeper_mt.lax/libzkmt.a
mkdir .libs/libzookeeper_mt.lax/libzkmt.a
(cd .libs/libzookeeper_mt.lax/libzkmt.a && ar x /home/jlekstan/zookeeper-3.4.1/src/c/./.libs/libzkmt.a)
rm -fr .libs/libzookeeper_mt.lax/libhashtable.a
mkdir .libs/libzookeeper_mt.lax/libhashtable.a
(cd .libs/libzookeeper_mt.lax/libhashtable.a && ar x /home/jlekstan/zookeeper-3.4.1/src/c/./.libs/libhashtable.a)
ar cru .libs/libzookeeper_mt.a   .libs/libzookeeper_mt.lax/libzkmt.a/libzkmt_la-zk_hashtable.o .libs/libzookeeper_mt.lax/libzkmt.a/libzkmt_la-zookeeper.o .libs/libzookeeper_mt.lax/libzkmt.a/libzkmt_la-zk_log.o .libs/libzookeeper_mt.lax/libzkmt.a/libzkmt_la-zookeeper.jute.o .libs/libzookeeper_mt.lax/libzkmt.a/libzkmt_la-recordio.o .libs/libzookeeper_mt.lax/libzkmt.a/libzkmt_la-mt_adaptor.o  .libs/libzookeeper_mt.lax/libhashtable.a/hashtable_itr.o .libs/libzookeeper_mt.lax/libhashtable.a/hashtable.o 
ranlib .libs/libzookeeper_mt.a
rm -fr .libs/libzookeeper_mt.lax
creating libzookeeper_mt.la
(cd .libs && rm -f libzookeeper_mt.la && ln -s ../libzookeeper_mt.la libzookeeper_mt.la)
if gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O0 -D_GNU_SOURCE -MT cli.o -MD -MP -MF "".deps/cli.Tpo"" -c -o cli.o `test -f 'src/cli.c' || echo './'`src/cli.c; \
	then mv -f "".deps/cli.Tpo"" "".deps/cli.Po""; else rm -f "".deps/cli.Tpo""; exit 1; fi
/bin/bash ./libtool --tag=CC --mode=link gcc -Wall -Werror  -g -O0 -D_GNU_SOURCE   -o cli_st  cli.o libzookeeper_st.la 
gcc -Wall -Werror -g -O0 -D_GNU_SOURCE -o .libs/cli_st cli.o  ./.libs/libzookeeper_st.so -lm 
./.libs/libzookeeper_st.so: undefined reference to `hashtable_iterator_value'
./.libs/libzookeeper_st.so: undefined reference to `hashtable_iterator_key'
collect2: ld returned 1 exit status
make[1]: *** [cli_st] Error 1
make[1]: Leaving directory `/home/jlekstan/zookeeper-3.4.1/src/c'
make: *** [all] Error 2
{code}","[<JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1339,Major,Jakub Lekstan,Fixed,2012-05-08T16:39:25.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,C clien doesn't build with --enable-debug,2012-05-08T18:04:59.000+0000,"[<JIRA Version: name='3.4.1', id='12318650'>]",3.0
Patrick D. Hunt,"[<JIRA Component: name='java client', id='12312381'>]",2011-12-22T00:30:25.000+0000,Patrick D. Hunt,There's a bug in ErrorResult and perhaps some of the other OpResult equals methods in multi.,"[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1338,Major,Patrick D. Hunt,Fixed,2012-02-06T10:16:09.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,class cast exceptions may be thrown by multi ErrorResult class (invalid equals),2012-02-06T10:58:03.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='java client', id='12312381'>]",2011-12-21T23:24:38.000+0000,Patrick D. Hunt,"There's this in org.apache.zookeeper.ZooKeeper.multi(Iterable<Op>)

{noformat}
     * Executes multiple Zookeeper operations or none of them.  On success, a list of results is returned.
     * On failure, only a single exception is returned.  If you want more details, it may be preferable to
     * use the alternative form of this method that lets you pass a list into which individual results are
     * placed so that you can zero in on exactly which operation failed and why.
{noformat}

What is the ""alternate form of this method"" that's being referred to? Seems like we should add this functionality, or at the very least update the javadoc. (I don't think this is referring to Transaction, although the docs there are pretty thin)

","[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1336,Major,Patrick D. Hunt,Fixed,2012-02-06T08:58:00.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"javadoc for multi is confusing, references functionality that doesn't seem to exist ",2012-02-06T10:58:04.000+0000,"[<JIRA Version: name='3.4.1', id='12318650'>]",1.0
Claus Ibsen,[],2011-12-20T16:31:37.000+0000,Claus Ibsen,"In Zookeeper 3.3.x you use log4j for logging, and the maven dep is

eg from 3.3.4
{code}
    <dependency>
      <groupId>log4j</groupId>
      <artifactId>log4j</artifactId>
      <version>1.2.15</version>
      <scope>compile</scope>
    </dependency>
{code}

Now in 3.4.0 or better you changed to use slf4j also/instead. The maven pom.xml now includes:
{code}
  <dependency>
      <groupId>org.slf4j</groupId>
      <artifactId>slf4j-api</artifactId>
      <version>1.6.1</version>
      <scope>compile</scope>
    </dependency>
    <dependency>
      <groupId>org.slf4j</groupId>
      <artifactId>slf4j-log4j12</artifactId>
      <version>1.6.1</version>
      <scope>compile</scope>
    </dependency>
    <dependency>
      <groupId>log4j</groupId>
      <artifactId>log4j</artifactId>
      <version>1.2.15</version>
      <scope>compile</scope>
    </dependency>
{code}

But the META-INF/MANIFEST.MF file in the distribution did not change to reflect this.

The 3.3.4 MANIFEST.MF, import packages
{code}
Import-Package: javax.management,org.apache.log4j,org.osgi.framework;v
 ersion=""[1.4,2.0)"",org.osgi.util.tracker;version=""[1.1,2.0)""
{code}

And the 3.4.1 MANIFEST.MF, import packages:
{code}
Import-Package: javax.management,org.apache.log4j,org.osgi.framework;v
 ersion=""[1.4,2.0)"",org.osgi.util.tracker;version=""[1.1,2.0)""
{code}

This makes using zookeeper 3.4.x in OSGi environments not possible, as we get NoClassDefFoundException for slf4j classes.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1334,Major,Claus Ibsen,Fixed,2012-12-19T08:05:11.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Zookeeper 3.4.x is not OSGi compliant - MANIFEST.MF is flawed,2014-10-08T15:55:46.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",12.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2011-12-20T01:24:33.000+0000,Andrew McNair,"I think a NPE was created in the fix for https://issues.apache.org/jira/browse/ZOOKEEPER-1269

Looking in DataTree.processTxn(TxnHeader header, Record txn) it seems likely that if rc.err != Code.OK then rc.path will be null. 

I'm currently working on a minimal test case for the bug, I'll attach it to this issue when it's ready.

java.lang.NullPointerException
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.processTransaction(FileTxnSnapLog.java:203)
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:150)
	at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:223)
	at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:418)
	at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:410)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:151)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:111)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)


","[<JIRA Version: name='3.4.2', id='12319196'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1333,Blocker,Andrew McNair,Fixed,2011-12-21T20:40:04.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,NPE in FileTxnSnapLog when restarting a cluster,2011-12-29T23:46:54.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",7.0
,"[<JIRA Component: name='server', id='12312382'>]",2011-12-19T09:32:11.000+0000,amith,"Please check the below mentioned scenario:-

1. Configure 3 zookeeper servers in quorum
2. Start zk1 (F) and zk2(L) from a java client create a node(client connect to zk2)
3. Stop the zk2 (L) 
4. Start the zk3, Now FLE is successful but zookeeper-3 is not having the node created 

In step 4 Zookeeper-3 is getting a diff from the leader

2011-12-19 20:15:59,379 [myid:3] - INFO  [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:2183:Environment@98] - Server environment:user.home=/root
2011-12-19 20:15:59,379 [myid:3] - INFO  [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:2183:Environment@98] - Server environment:user.dir=/home/amith/OpenSrc/zookeeper/zookeeper3/bin
2011-12-19 20:15:59,381 [myid:3] - INFO  [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:2183:ZooKeeperServer@168] - Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 datadir ../dataDir/version-2 snapdir ../dataDir/version-2
2011-12-19 20:15:59,382 [myid:3] - INFO  [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:2183:Follower@63] - FOLLOWING - LEADER ELECTION TOOK - 102
2011-12-19 20:15:59,403 [myid:3] - INFO  [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:2183:Learner@322] - Getting a diff from the leader 0x10000000a
2011-12-19 20:15:59,449 [myid:3] - WARN  [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:2183:Learner@372] - Got zxid 0x10000000a expected 0x1
2011-12-19 20:15:59,450 [myid:3] - INFO  [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:2183:FileTxnSnapLog@255] - Snapshotting: 10000000a

but in the diff all the required data is not obtained ...!

Here I think zookeeper-3 should get snapshot from leader and not Diff","[<JIRA Version: name='3.4.1', id='12318650'>]",Bug,ZOOKEEPER-1332,Major,amith,Duplicate,2011-12-19T12:03:11.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper data is not in sync with quorum in the mentioned scenario,2011-12-19T12:03:12.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",0.0
Andrew Ash,"[<JIRA Component: name='documentation', id='12312422'>]",2011-12-19T07:10:54.000+0000,Andrew Ash,Found this typo while reading docs.  Attaching SVN patch,"[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1331,Minor,Andrew Ash,Fixed,2011-12-28T00:39:41.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Typo in docs: acheive -> achieve,2011-12-28T10:58:22.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2011-12-19T05:21:12.000+0000,amith,"Have a cluster of 3 zookeepers
90 clients are connected to the server
leader got killed and started
the other 2 zookeeper started FLE and Leader was elected

But its taking nearly 10 sec for this server to server requests and saying ""ZooKeeperServer not running"" message..?

Why is this even after Leader election SERVER IS NOT RUNNING !!!!!!!!!!

2011-12-19 16:12:29,732 [myid:2] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2182:NIOServerCnxn@354] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
2011-12-19 16:12:29,733 [myid:2] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2182:NIOServerCnxn@1000] - Closed socket connection for client /10.18.47.148:51965 (no session established for client)
2011-12-19 16:12:29,753 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:2182:QuorumPeer@747] - LEADING
2011-12-19 16:12:29,762 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:2182:Leader@58] - TCP NoDelay set to: true
2011-12-19 16:12:29,765 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:2182:ZooKeeperServer@168] - Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 datadir ../dataDir/version-2 snapdir ../dataDir/version-2
2011-12-19 16:12:29,766 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:2182:Leader@294] - LEADING - LEADER ELECTION TOOK - 4663
2011-12-19 16:12:29,776 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:2182:FileSnap@83] - Reading snapshot ../dataDir/version-2/snapshot.100013661
2011-12-19 16:12:29,831 [myid:2] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2182:NIOServerCnxnFactory@213] - Accepted socket connection from /10.18.47.148:51982
2011-12-19 16:12:29,831 [myid:2] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2182:NIOServerCnxn@354] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
2011-12-19 16:12:29,832 [myid:2] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2182:NIOServerCnxn@1000] - Closed socket connection for client /10.18.47.148:51982 (no session established for client)
2011-12-19 16:12:29,884 [myid:2] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2182:NIOServerCnxnFactory@213] - Accepted socket connection from /10.18.47.148:51989
2011-12-19 16:12:29,884 [myid:2] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2182:NIOServerCnxn@354] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running",[],Bug,ZOOKEEPER-1330,Minor,amith,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper server not serving the client request even after completion of Leader election,2022-02-03T08:50:16.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",9.0
,"[<JIRA Component: name='recipes', id='12313246'>]",2011-12-16T00:04:48.000+0000,Evan McClure,"The lock recipe sorts sequenced children lexicographically.  When the sequence number wraps, a lexicographical comparison will always place 2147483648 ahead of -2147483649, place -2147483648 ahead of -2147483649, and place -1 ahead of -2.  Clearly, we want 2147483648 < -2147483649, -2147483649 < -2147483648, and -2 placed ahead of -1, since those sequence numbers were generated in that order.

I suggest that the sequence numbers be converted to unsigned numbers before being compared in the comparison functor that gets passed to qsort().

This leaves us with another issue.  When comparing unsigned sequence numbers, there is a slim chance that 4294967296 < 0.  So, I suggest that a fudge range be used, say, the number of nodes in the quorum * some fudge factor, in order to handle this comparison.

Please close this if I'm way off base here.",[],Bug,ZOOKEEPER-1329,Major,Evan McClure,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Lock recipe sorts sequenced children incorrectly,2011-12-16T00:05:12.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",2.0
Harsh J,[],2011-12-13T04:25:33.000+0000,Benjamin Reed,there are still hadoop urls and references to zookeeper lists under the hadoop project in the sources.,"[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1327,Major,Benjamin Reed,Fixed,2012-02-06T09:17:43.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,there are still remnants of hadoop urls,2012-02-06T10:58:04.000+0000,[],2.0
Michi Mutsuzaki,"[<JIRA Component: name='c client', id='12312380'>]",2011-12-09T01:28:16.000+0000,Michi Mutsuzaki,"EAI_NODATA and EAI_ADDRFAMILY have been deprecated in FreeBSD. I'm getting this error:

src/zookeeper.c: In function `getaddrinfo_errno':
src/zookeeper.c:446: error: `EAI_NODATA' undeclared (first use in this function)
src/zookeeper.c:446: error: (Each undeclared identifier is reported only once
src/zookeeper.c:446: error: for each function it appears in.)
src/zookeeper.c: In function `getaddrs':
src/zookeeper.c:581: error: `EAI_ADDRFAMILY' undeclared (first use in this function)

I'll submit a patch.

--Michi","[<JIRA Version: name='3.4.2', id='12319196'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1323,Major,Michi Mutsuzaki,Fixed,2011-12-14T23:19:38.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,c client doesn't compile on freebsd,2011-12-29T23:46:53.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
Patrick D. Hunt,[],2011-12-06T03:06:10.000+0000,Jeremy Stribling,"I've been trying to update to ZK 3.4.0 and have had some issues where some data become inaccessible after adding a node to a cluster.  My use case is a bit strange (as explained before on this list) in that I try to grow the cluster dynamically by having an external program automatically restart Zookeeper servers in a controlled way whenever the list of participating ZK servers needs to change.  This used to work just fine in 3.3.3 (and before), so this represents a regression.

The scenario I see is this:

1) Start up a 1-server ZK cluster (the server has ZK ID 0).
2) A client connects to the server, and makes a bunch of znodes, in particular a znode called ""/membership"".
3) Shut down the cluster.
4) Bring up a 2-server ZK cluster, including the original server 0 with its existing data, and a new server with ZK ID 1.
5) Node 0 has the highest zxid and is elected leader.
6) A client connecting to server 1 tries to ""get /membership"" and gets back a -101 error code (no such znode).
7) The same client then tries to ""create /membership"" and gets back a -110 error code (znode already exists).
8) Clients connecting to server 0 can successfully ""get /membership"".

I will attach a tarball with debug logs for both servers, annotating where steps #1 and #4 happen.  You can see that the election involves a proposal for zxid 110 from server 0, but immediately following the election server 1 has these lines:

2011-12-05 17:18:48,308 9299 [QuorumPeer[myid=1]/127.0.0.1:2901] WARN org.apache.zookeeper.server.quorum.Learner  - Got zxid 0x100000001 expected 0x1
2011-12-05 17:18:48,313 9304 [SyncThread:1] INFO org.apache.zookeeper.server.persistence.FileTxnLog  - Creating new log file: log.100000001

Perhaps that's not relevant, but it struck me as odd.  At the end of server 1's log you can see a repeated cycle of getData->create->getData as the client tries to make sense of the inconsistent responses.

The other piece of information is that if I try to use the on-disk directories for either of the servers to start a new one-node ZK cluster, all the data are accessible.

I haven't tried writing a program outside of my application to reproduce this, but I can do it very easily with some of my app's tests if anyone needs more information.","[<JIRA Version: name='3.4.1', id='12318650'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1319,Blocker,Jeremy Stribling,Fixed,2011-12-09T19:09:12.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Missing data after restarting+expanding a cluster,2011-12-17T01:33:57.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",4.0
Henry Robinson,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2011-12-04T19:06:03.000+0000,Jim Fulton,"In Python binding, get_children (and get and exists, and probably others) with expired session doesn't raise exception properly.


>>> zookeeper.state(h)
-112
>>> zookeeper.get_children(h, '/')
Traceback (most recent call last):
  File ""<console>"", line 1, in <module>
SystemError: error return without exception set

Let me know if you'd like me to work on a patch.","[<JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1318,Major,Jim Fulton,Fixed,2012-05-10T01:54:51.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"In Python binding, get_children (and get and exists, and probably others) with expired session doesn't raise exception properly",2012-05-11T11:00:44.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",5.0
Akira Kitada,"[<JIRA Component: name='c client', id='12312380'>]",2011-12-04T13:48:10.000+0000,Akira Kitada,"zookeeper_init does not check the return value of strdup(index_chroot).
When it returns NULL, it causes segfault when it try to strlen(zh->chroot).","[<JIRA Version: name='3.4.1', id='12318650'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1317,Minor,Akira Kitada,Fixed,2011-12-09T18:37:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Possible segfault in zookeeper_init,2011-12-17T01:33:59.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>]",1.0
Akira Kitada,"[<JIRA Component: name='c client', id='12312380'>]",2011-12-04T13:34:33.000+0000,Akira Kitada,"zookeeper_init does not free strdup'ed memory when chroot is just '/'.
","[<JIRA Version: name='3.4.1', id='12318650'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1316,Minor,Akira Kitada,Fixed,2011-12-08T22:30:51.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper_init leaks memory if chroot is just '/',2011-12-17T01:33:58.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>]",1.0
Akira Kitada,"[<JIRA Component: name='c client', id='12312380'>]",2011-12-04T10:31:17.000+0000,Akira Kitada,zookeeper_init always reports sessionPasswd=<hidden> even when it's empty.,"[<JIRA Version: name='3.4.1', id='12318650'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1315,Minor,Akira Kitada,Fixed,2011-12-09T00:21:55.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper_init always reports sessionPasswd=<hidden>,2011-12-17T01:33:58.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>]",1.0
,[],2011-12-01T00:26:27.000+0000,Daniel Lord,"When a zookeeper client receives a Packet that is over the jute max buffer limit the behavior that is exposed to the callers of the zookeeper client is misleading.  When the packet length exceeds the max size an IOException is thrown.  This is caught and handled by the SendThread by cleaning up the current connection and enqueueing a Disconnected event.  The immediate caller of zookeeper sees this as a ConnectionLossException with a Disconnected event on the main Watcher.  This state transition is a bit misleading because under many circumstances as soon as the SyncConnected event is received retrying the same operation will succeed.  However, in this case it is likely that the zookeeper client will reconnect immediately and if the operation is retried the same jute max buffer limit exception will be thrown which will trigger another disconnect and reconnect.

It would be great if the exception was exposed to the caller of the zookeeper client some how so that a more appropriate action can be taken.  For instance, it might be appropriate to fail completely or to attempt to establish a new session.",[],Bug,ZOOKEEPER-1313,Major,Daniel Lord,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Expose/create KeeperException for ""Packet len <x> is out of range!"" error when jute max buffer size is exceeded",2014-09-16T03:06:37.000+0000,[],3.0
Ivan Kelly,[],2011-11-30T18:04:06.000+0000,Ivan Kelly,"In http://repo1.maven.org/maven2/org/apache/zookeeper/zookeeper/3.4.0/ the test jar cannot be accessed by maven. 

There are two possible solutions to this. 
a) rename zookeeper-3.4.0-test.jar to zookeeper-3.4.0-tests.jar and remove zookeeper-3.4.0-test.pom*
With this, the maven can access the test jar with

{code}
     <dependency>
       <groupId>org.apache.zookeeper</groupId>
       <artifactId>zookeeper</artifactId>
       <version>3.4.0</version>
       <type>test-jar</type>
       <scope>test</scope>
     </dependency>
{code}

b) Alternatively, zookeeper test could be it's own submodule. To do this, it must be deployed in the following layout
{code}
./org/apache/zookeeper/zookeeper-test/3.4.0-BK-SNAPSHOT/zookeeper-test-3.4.0.jar
./org/apache/zookeeper/zookeeper-test/3.4.0-BK-SNAPSHOT/zookeeper-test-3.4.0.jar.md5
./org/apache/zookeeper/zookeeper-test/3.4.0-BK-SNAPSHOT/zookeeper-test-3.4.0.jar.sha1
./org/apache/zookeeper/zookeeper-test/3.4.0-BK-SNAPSHOT/zookeeper-test-3.4.0.pom
./org/apache/zookeeper/zookeeper-test/3.4.0-BK-SNAPSHOT/zookeeper-test-3.4.0.pom.md5
./org/apache/zookeeper/zookeeper-test/3.4.0-BK-SNAPSHOT/zookeeper-test-3.4.0.pom.sha1
{code}

This can then be accessed by maven with
{code}
     <dependency>
       <groupId>org.apache.zookeeper</groupId>
       <artifactId>zookeeper-test</artifactId>
       <version>3.4.0</version>
       <scope>test</scope>
     </dependency>
{code}


I think a) is the better solution.
","[<JIRA Version: name='3.4.1', id='12318650'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1311,Blocker,Ivan Kelly,Fixed,2011-12-01T07:14:23.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeper test jar is broken,2011-12-17T01:33:58.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
Daniel Lord,"[<JIRA Component: name='java client', id='12312381'>]",2011-11-29T02:03:52.000+0000,Daniel Lord,"If there is an IOException thrown by the constructor of ClientCnxn then file handles are leaked because of the initialization of the Selector which is never closed.

    final Selector selector = Selector.open();

If there is an abnormal exit from the constructor then the Selector is not closed and file handles are leaked.  You can easily see this by setting the hosts string to garbage (""qwerty"", ""asdf"", etc.) and then try to open a new ZooKeeper connection.  I've observed the same behavior in production when there were DNS issues where the host names of the ensemble can no longer be resolved and the application servers quickly run out of handles attempting to (re)connect to zookeeper.","[<JIRA Version: name='3.3.5', id='12319081'>]",Bug,ZOOKEEPER-1309,Critical,Daniel Lord,Fixed,2012-02-27T00:32:13.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Creating a new ZooKeeper client can leak file handles,2012-02-27T00:32:13.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>]",1.0
,"[<JIRA Component: name='recipes', id='12313246'>]",2011-11-24T13:43:45.000+0000,Mark Miller,"{code}
        public boolean execute() throws KeeperException, InterruptedException {
            do {
                if (id == null) {
                    long sessionId = zookeeper.getSessionId();
                    String prefix = ""x-"" + sessionId + ""-"";
                    // lets try look up the current ID if we failed 
                    // in the middle of creating the znode
                    findPrefixInChildren(prefix, zookeeper, dir);
                    idName = new ZNodeName(id);
                }
{code}

ZNodeName will throw an NPE if id is null.",[],Bug,ZOOKEEPER-1308,Minor,Mark Miller,Invalid,2011-11-24T13:48:24.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Guaranteed NPE in WriteLock recipe,2011-11-24T13:48:24.000+0000,[],0.0
kavita sharma,"[<JIRA Component: name='java client', id='12312381'>]",2011-11-23T09:07:13.000+0000,amith,"use consoleClient (zkCli.sh) and issue setAcl /temp abc
[zk: XX.XX.XX.XX:XXXX(CONNECTED) 17] setAcl /temp abc
abc does not have the form scheme:id:perm
Exception in thread ""main"" org.apache.zookeeper.KeeperException$InvalidACLException: KeeperErrorCode = InvalidACL
        at org.apache.zookeeper.ZooKeeper.setACL(ZooKeeper.java:1172)
        at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:717)
        at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:582)
        at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:354)
        at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:312)
        at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:271)
linux-xxx:/zookeeper1/bin #

if any InvalidACLException is thrown then client is exiting.
client should be able to handle this kind of issues

","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1307,Minor,amith,Fixed,2012-03-17T00:42:45.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zkCli.sh is exiting when an Invalid ACL exception is thrown from setACL command through client,2012-04-23T17:17:37.000+0000,[],3.0
Michael Lee,"[<JIRA Component: name='c client', id='12312380'>]",2011-11-19T07:02:45.000+0000,helei,"With patch ZOOKEEPER-981,  I saw another problem. Hang in zookeeper_close() again. here is the stack:
(gdb) bt
#0 0x000000302b80adfb in __lll_mutex_lock_wait () from /lib64/tls/libpthread.so.0
#1 0x000000302b1307a8 in main_arena () from /lib64/tls/libc.so.6
#2 0x000000302b910230 in stack_used () from /lib64/tls/libpthread.so.0
#3 0x000000302b808dde in pthread_cond_broadcast@@GLIBC_2.3.2 () from /lib64/tls/libpthread.so.0
#4 0x00000000006b4ce8 in adaptor_finish (zh=0x6902060) at src/mt_adaptor.c:217
#5 0x00000000006b0fd0 in zookeeper_close (zh=0x6902060) at src/zookeeper.c:2297
(gdb) p zh->ref_counter
$5 = 1
(gdb) p zh->close_requested
$6 = 1
(gdb) p *zh
$7 = {fd = 110112576, hostname = 0x6903620 """", addrs = 0x0, addrs_count = 1,
watcher = 0x62e5dc <doris::meta_register_mgr_t::register_mgr_watcher(_zhandle*, int, int, char const*, void*)>, last_recv = {tv_sec = 1321510694, tv_usec = 552835}, last_send = {tv_sec = 1321510694, tv_usec = 552886}, last_ping = {tv_sec = 1321510685, tv_usec = 774869}, next_deadline = { tv_sec = 1321510704, tv_usec = 547831}, recv_timeout = 30000, input_buffer = 0x0, to_process = {head = 0x0, last = 0x0, lock = {__m_reserved = 0,
__m_count = 0, __m_owner = 0x0, __m_kind = 0, __m_lock = {__status = 0, __spinlock = 0}}}, to_send = {head = 0x0, last = 0x0, lock = {
__m_reserved = 0, __m_count = 0, __m_owner = 0x0, __m_kind = 1, __m_lock = {__status = 0, __spinlock = 0}}}, sent_requests = {head = 0x0, last = 0x0,
cond = {__c_lock = {_status = 1, __spinlock = -1}, __c_waiting = 0x0, __padding = '\0' <repeats 15 times>, __align = 0}, lock = {_m_reserved = 0,
__m_count = 0, __m_owner = 0x0, __m_kind = 0, __m_lock = {__status = 0, __spinlock = 0}}}, completions_to_process = {head = 0x2aefbff800,
last = 0x2af0e05f40, cond = {__c_lock = {__status = 592705486850, __spinlock = -1}, __c_waiting = 0x45,
_padding = ""E\000\000\000\000\000\000\000\220\006\000\000\000"", __align = 296352743424}, lock = {_m_reserved = 1, __m_count = 0,
__m_owner = 0x1000026ca, __m_kind = 0, __m_lock = {__status = 0, __spinlock = 0}}}, connect_index = 0, client_id = {client_id = 86551148676999146, passwd = ""G懵擀\233\213\f闬202筴\002錪\034""}, last_zxid = 82057372, outstanding_sync = 0, primer_buffer = {buffer = 0x6902290 """", len = 40, curr_offset = 44, next = 0x0}, primer_storage = {len = 36, protocolVersion = 0, timeOut = 30000, sessionId = 86551148676999146, passwd_len = 16, passwd = ""G懵擀\233\213\f闬202筴\002錪\034""},
primer_storage_buffer = ""\000\000\000$\000\000\000\000\000\000u0\0013}惜薵闬000\000\000\020G懵擀\233\213\f闬202筴\002錪\034"", state = 0, context = 0x0,
auth_h = {auth = 0x0, lock = {__m_reserved = 0, __m_count = 0, __m_owner = 0x0, __m_kind = 0, __m_lock = {__status = 0, __spinlock = 0}}},
ref_counter = 1, close_requested = 1, adaptor_priv = 0x0, socket_readable = {tv_sec = 0, tv_usec = 0}, active_node_watchers = 0x6901520,
active_exist_watchers = 0x69015d0, active_child_watchers = 0x6902ef0, chroot = 0x0}
I think the ref_counter is suposed to be 2 or 3 or 4 here. it seems not correct. I think maybe we should increase the ref_counter before we set zh->close_request=1, otherwise the do_io thread and do_completion thread may release the handler just after we set zh->close_request and before we increase zh->ref_counter. Thanks again",[],Bug,ZOOKEEPER-1306,Major,helei,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,hang in zookeeper_close(),2011-11-29T21:52:06.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",0.0
Daniel Lescohier,"[<JIRA Component: name='c client', id='12312380'>]",2011-11-18T18:23:33.000+0000,Daniel Lescohier,"All the callers of the function prepend_string make a call to prepend_string before checking that zhandle_t *zh is not null. At the top of prepend_string, zh is dereferenced without checking for a null ptr:

static char* prepend_string(zhandle_t *zh, const char* client_path) {
    char *ret_str;
    if (zh->chroot == NULL)
        return (char *) client_path;

I propose fixing this by adding the check here in prepend_string:

static char* prepend_string(zhandle_t *zh, const char* client_path) {
    char *ret_str;
    if (zh==NULL || zh->chroot == NULL)
        return (char *) client_path;
","[<JIRA Version: name='3.4.1', id='12318650'>, <JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1305,Major,Daniel Lescohier,Fixed,2011-12-08T22:03:58.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper.c:prepend_string func can dereference null ptr,2012-05-03T02:06:44.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",3.0
,[],2011-11-17T23:37:26.000+0000,Daniel Kim,"**[Sorry. I don't know how to delete an issue that is already submitted. I just learned of the Bookkeeper jira, and I will submit this issue there instead. You can all ignore this issue.]


Since I couldn't finish building all hedwig components in CentOS, I built it successfully in Ubuntu, then I deployed it to CentOS (no ubuntu image in my company's cloud). I configured zookeeper, bookies and hubs as they were described in the documentations. First, I copied TestPubSubClient.java's publish and subscribe tests into my own test code. I also had to create another object that extends ClientConfiguration. I named it ""HedwigConf"", and overwrote getDefaultServerHedwigSocketAddress() method because the server was not on the same machine as the workstation. I targetted the right host and publish seemed to work. However, it throws me ServiceDownException for publish sometimes. I checked the logs of the hubs. They seem to have connected ok with the bookies. There was no error or warning there. However, the problem seemed to exist in bookies and zookeeper. This was found in the zookeeper log: ""Got user-level KeeperException when processing sessionid:0x----------- type:create cxid:0x5 zxid:0x29 txntype:-1 reqpath:n/a Error Path:/hedwig/standalone/topics Error:KeeperErrorCode = NoNode for /hedwig/standalone/topics"". Normally this znode path is created automatically. Also, some bookies complained this: ""WARN [NIOServerFactory] org.apache.bookkeeper.proto.NIOServerFactory - Exception in server socket loop: /0:0:0:0:0:0:0:0
java.lang.NullPointerException"". For some reason, this problem comes and goes. Sometimes everything just works and the new topic is saved in a new znode, and the message is saved in bookie(s). I spent hours trying to recreate this yesterday, but I couldn't. Now it is back again. Subscribe seems to have the similar issue.
",[],Bug,ZOOKEEPER-1304,Major,Daniel Kim,Duplicate,2013-10-10T00:09:16.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"[IGNORE THIS --- MOVING TO BOOKKEEPER JIRA] publish and subscribe methods get ServiceDownException even when the hubs, bookies, and zookeepers are running",2013-10-10T00:09:16.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",0.0
Ashish Mishra,"[<JIRA Component: name='scripts', id='12312384'>]",2011-11-17T23:32:18.000+0000,Ashish Mishra,"The Leader.removeLearnerHandler() call removes handlers from the forwardingFollowers and learners sets, but not from observingLearners.  This will cause a leak if observers are repeatedly connected and disconnected from the ensemble.","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1303,Minor,Ashish Mishra,Duplicate,2014-04-30T20:25:06.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Observer LearnerHandlers are not removed from Leader collection.,2014-04-30T20:25:06.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>]",4.0
Mahadev Konar,[],2011-11-16T19:43:45.000+0000,Mahadev Konar,"From phunt:

{noformat}
Note: I even tried upgrading to RAT 0.8 and this is the output: (same/similar)

[rat:report] 15 Unknown Licenses
[rat:report]
[rat:report] *******************************
[rat:report]
[rat:report] Unapproved licenses:
[rat:report]
[rat:report]   /home/phunt/Downloads/zookeeper-3.4.0/build/zookeeper-3.4.0/README_packaging.txt
[rat:report]   /home/phunt/Downloads/zookeeper-3.4.0/build/zookeeper-3.4.0/contrib/ZooInspector/licences/epl-v10.html
[rat:report]   /home/phunt/Downloads/zookeeper-3.4.0/build/zookeeper-3.4.0/src/c/include/winstdint.h
[rat:report]   /home/phunt/Downloads/zookeeper-3.4.0/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/log4j.properties
[rat:report]   /home/phunt/Downloads/zookeeper-3.4.0/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/date.format.js
[rat:report]   /home/phunt/Downloads/zookeeper-3.4.0/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/g.bar.js
[rat:report]   /home/phunt/Downloads/zookeeper-3.4.0/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/g.dot.js
[rat:report]   /home/phunt/Downloads/zookeeper-3.4.0/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/g.line.js
[rat:report]   /home/phunt/Downloads/zookeeper-3.4.0/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/g.pie.js
[rat:report]   /home/phunt/Downloads/zookeeper-3.4.0/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/g.raphael.js
[rat:report]   /home/phunt/Downloads/zookeeper-3.4.0/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/raphael.js
[rat:report]   /home/phunt/Downloads/zookeeper-3.4.0/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/yui-min.js
[rat:report]   /home/phunt/Downloads/zookeeper-3.4.0/build/zookeeper-3.4.0/src/contrib/monitoring/JMX-RESOURCES
[rat:report]   /home/phunt/Downloads/zookeeper-3.4.0/build/zookeeper-3.4.0/src/contrib/zooinspector/lib/log4j.properties
[rat:report]   /home/phunt/Downloads/zookeeper-3.4.0/build/zookeeper-3.4.0/src/contrib/zooinspector/licences/epl-v10.html
{noformat}","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1300,Major,Mahadev Konar,Duplicate,2014-07-21T20:50:51.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Rat complains about incosistent licenses in the src files.,2014-07-21T20:50:51.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",0.0
Mahadev Konar,[],2011-11-16T06:53:42.000+0000,Mahadev Konar,We need to add the winconfig.h to ignores in release audits.,"[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1299,Major,Mahadev Konar,Fixed,2011-11-16T07:19:47.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Add winconfig.h file to ignore in release audit.,2011-11-23T19:22:04.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
,"[<JIRA Component: name='c client', id='12312380'>]",2011-11-12T17:29:01.000+0000,Jim Fulton,"configure creates a working config.h.

On Snow leopard, after running configure:

  ls -l config.h
  -rw-r--r--  1 jim  jim  4437 Nov 12 12:16 config.h

which looks reasomnable.

Running make replaces config.h:

  make
  (CDPATH=""${ZSH_VERSION+.}:"" && cd . && /bin/sh /Users/jim/s/zookeeper-3.3.3/src/c/missing --run autoheader)
  /opt/local/bin/gm4: cannot open `configure.in': No such file or directory
  rm -f stamp-h1
  touch config.h.in
  cd . && /bin/sh ./config.status config.h
  config.status: creating config.h
  make  all-am
  /bin/sh ./libtool  --tag=CC   --mode=compile gcc -DHAVE_CONFIG_H -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O2 -D_GNU_SOURCE -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c -o zookeeper.lo `test -f 'src/zookeeper.c' || echo './'`src/zookeeper.c
  libtool: compile:  gcc -DHAVE_CONFIG_H -I. -I./include -I./tests -I./generated -Wall -Werror -g -O2 -D_GNU_SOURCE -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c src/zookeeper.c  -fno-common -DPIC -o .libs/zookeeper.o
  src/zookeeper.c: In function 'log_env':
  src/zookeeper.c:658: error: 'PACKAGE_STRING' undeclared (first use in this function)
  src/zookeeper.c:658: error: (Each undeclared identifier is reported only once
  src/zookeeper.c:658: error: for each function it appears in.)
  cc1: warnings being treated as errors
  src/zookeeper.c:647: warning: unused variable 'buf'
  make[1]: *** [zookeeper.lo] Error 1
  make: *** [all] Error 2


  ls -l config.h
  -rw-r--r--  1 jim  jim  137 Nov 12 12:17 config.h

config.h is empty, except for a comment.

If I make a copy of config.h after configure and restore it after
running the failed make, then I can run make again and the make
succeeds.

On a centos 5 vm, I can build just fine, but I suspect that has
something to do with it not being happy with autoconf:

  (CDPATH=""${ZSH_VERSION+.}:"" && cd . && /bin/sh /home/jim/s/zookeeper-3.3.3/src/c/missing --run autoheader)
  aclocal.m4:20: warning: this file was generated for autoconf 2.67.
  You have another version of autoconf.  It may work, but is not guaranteed to.
  If you have problems, you may need to regenerate the build system entirely.
  To do so, use the procedure documented by the package, typically `autoreconf'.
  configure.ac:21: error: Autoconf version 2.62 or higher is required
  aclocal.m4:8577: AM_INIT_AUTOMAKE is expanded from...
  configure.ac:21: the top level
  autom4te: /usr/bin/m4 failed with exit status: 63
  autoheader: /usr/bin/autom4te failed with exit status: 63
  WARNING: `autoheader' is probably too old.  You should only need it if
           you modified `acconfig.h' or `configure.ac'.  You might want
           to install the `Autoconf' and `GNU m4' packages.  Grab them
           from any GNU archive site.
  rm -f stamp-h1
  touch config.h.in
  cd . && /bin/sh ./config.status config.h
  config.status: creating config.h
  config.status: config.h is unchanged
  ...

I'm pretty clueless wrt autoconf.

I can work around this by touching config.h.in before running
configure. That seems to lead to a clean make, presumably by bypassing
the autoconf step.  I don't know if that matters. :)

My goal is to automate building on at least unix-like systems as part
of building a self-contained source distribution of the Python
extension that builds by just running it's setup script.
",[],Bug,ZOOKEEPER-1298,Major,Jim Fulton,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"config,h gets emptied by make, at least on mac os x 10.6.8",2011-11-12T17:29:01.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",0.0
Mohammad Arshad,"[<JIRA Component: name='documentation', id='12312422'>]",2011-11-09T19:23:41.000+0000,Daniel Lord,"The jute maxbuffer size is documented as being defaulted to 1 megabyte in the administrators guide.  I believe that this is true server side but it is not true client side.  On the client side the default is (at least in 3.3.2) this:

packetLen = Integer.getInteger(""jute.maxbuffer"", 4096 * 1024);

On the server side the documentation looks to be correct:
    private static int determineMaxBuffer() {
        String maxBufferString = System.getProperty(""jute.maxbuffer"");
        try {
            return Integer.parseInt(maxBufferString);
        } catch(Exception e) {
            return 0xfffff;
        }
        
    }

The documentation states this:
jute.maxbuffer:
(Java system property: jute.maxbuffer)

This option can only be set as a Java system property. There is no zookeeper prefix on it. It specifies the maximum size of the data that can be stored in a znode. The default is 0xfffff, or just under 1M. If this option is changed, the system property must be set on all servers and clients otherwise problems will arise. This is really a sanity check. ZooKeeper is designed to store data on the order of kilobytes in size.",[],Bug,ZOOKEEPER-1295,Major,Daniel Lord,Fixed,2016-05-28T17:00:40.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Documentation for jute.maxbuffer is not correct in ZooKeeper Administrator's Guide,2019-10-27T09:28:54.000+0000,"[<JIRA Version: name='3.5.2', id='12331981'>]",7.0
kavita sharma,"[<JIRA Component: name='server', id='12312382'>]",2011-11-09T09:54:27.000+0000,amith,"In zoo.cfg i have configured as
server.1 = XX.XX.XX.XX:65175:65173
server.2 = XX.XX.XX.XX:65185:65183
server.3 = XX.XX.XX.XX:65195:65193
server.4 = XX.XX.XX.XX:65205:65203:observer
server.5 = XX.XX.XX.XX:65215:65213:observer
server.6 = XX.XX.XX.XX:65225:65223:observer

Like above I have configured 3 PARTICIPANTS and 3 OBSERVERS
in the cluster of 6 zookeepers

Steps to reproduce the defect
1. Start all the 3 participant zookeeper
2. Stop all the participant zookeeper
3. Start zookeeper 1(Participant)
4. Start zookeeper 2(Participant)
5. Start zookeeper 4(Observer)
6. Create a persistent node with external client and close it
7. Stop the zookeeper 1(Participant neo quorum is unstable)
8. Create a new client and try to find the node created b4 using exists api (will fail since quorum not statisfied)
9. Start the Zookeeper 1 (Participant stabilise the quorum)

Now check the observer using 4 letter word (Server.4)
linux-216:/home/amith/CI/source/install/zookeeper/zookeeper2/bin # echo stat | netcat localhost 65200
Zookeeper version: 3.3.2-1031432, built on 11/05/2010 05:32 GMT
Clients:
 /127.0.0.1:46370[0](queued=0,recved=1,sent=0)

Latency min/avg/max: 0/0/0
Received: 1
Sent: 0
Outstanding: 0
Zxid: 0x100000003
Mode: observer
Node count: 5

check the participant 2 with 4 letter word

Latency min/avg/max: 22/48/83
Received: 39
Sent: 3
Outstanding: 35
Zxid: 0x100000003
Mode: leader
Node count: 5
linux-216:/home/amith/CI/source/install/zookeeper/zookeeper2/bin #

check the participant 1 with 4 letter word

linux-216:/home/amith/CI/source/install/zookeeper/zookeeper2/bin # echo stat | netcat localhost 65170
This ZooKeeper instance is not currently serving requests

We can see the participant1 logs filled with
2011-11-08 15:49:51,360 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:65170:NIOServerCnxn@642] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running


Problem here is participent1 is not responding / accepting any requests","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1294,Major,amith,Fixed,2012-01-14T00:08:54.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,One of the zookeeper server is not accepting any requests,2013-05-25T00:17:40.000+0000,[],7.0
Patrick D. Hunt,"[<JIRA Component: name='build', id='12312383'>]",2011-11-03T05:35:25.000+0000,Patrick D. Hunt,"I tried to compile 3.3.3 or the current 3.3 branch head, in both cases using ant 1.8.2 fails, however 1.7.0 is successful

here's the error:
{noformat}
Testsuite: org.apache.zookeeper.VerGenTest
Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.009 sec

Testcase: warning took 0.001 sec
	FAILED
Class org.apache.zookeeper.VerGenTest has no public constructor TestCase(String name) or TestCase()
junit.framework.AssertionFailedError: Class org.apache.zookeeper.VerGenTest has no public constructor TestCase(String name) or TestCase()
{noformat}
","[<JIRA Version: name='3.3.4', id='12316276'>]",Bug,ZOOKEEPER-1283,Blocker,Patrick D. Hunt,Fixed,2011-11-15T18:10:25.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,building 3.3 branch fails with Ant 1.8.2 (success with 1.7.1 though),2011-11-29T17:54:43.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2011-11-02T16:54:43.000+0000,Patrick D. Hunt,"When the lower 32bits of a zxid ""roll over"" (zxid is a 64 bit number, however the upper 32 are considered the epoch number) the epoch number (upper 32 bits) are incremented and the lower 32 start at 0 again.

This should work fine, however, afaict, in the current 3.4/3.5 the acceptedEpoch/currentEpoch files are not being updated for this case.

See ZOOKEEPER-335 for changes from 3.3 branch.",[],Bug,ZOOKEEPER-1278,Blocker,Patrick D. Hunt,Duplicate,2012-03-23T00:36:10.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,acceptedEpoch not handling zxid rollover in lower 32bits,2012-03-23T00:36:10.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",0.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2011-11-02T16:46:01.000+0000,Patrick D. Hunt,"When the lower 32bits of a zxid ""roll over"" (zxid is a 64 bit number, however the upper 32 are considered the epoch number) the epoch number (upper 32 bits) are incremented and the lower 32 start at 0 again.

This should work fine, however in the current 3.3 branch the followers see this as a NEWLEADER message, which it's not, and effectively stop serving clients. Attached clients seem to eventually time out given that heartbeats (or any operation) are no longer processed. The follower doesn't recover from this.

I've tested this out on 3.3 branch and confirmed this problem, however I haven't tried it on 3.4/3.5. It may not happen on the newer branches due to ZOOKEEPER-335, however there is certainly an issue with updating the ""acceptedEpoch"" files contained in the datadir. (I'll enter a separate jira for that)

","[<JIRA Version: name='3.3.5', id='12319081'>, <JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1277,Critical,Patrick D. Hunt,Fixed,2012-03-15T16:55:14.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,servers stop serving when lower 32bits of zxid roll over,2020-09-15T06:18:17.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",12.0
Chris Nauroth,"[<JIRA Component: name='server', id='12312382'>]",2011-10-31T07:06:50.000+0000,amith,"currently only data watchers (created by exists() and getdata() )are getting displayed with wchs,wchp,wchc 4 letter command command 

It would be useful to get the infomation related to childwatchers ( getChildren() ) also with 4 letter words.




",[],Bug,ZOOKEEPER-1274,Major,amith,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Support child watches to be displayed with 4 letter zookeeper commands (i.e. wchs, wchp and wchc)",2022-02-03T08:50:15.000+0000,[],7.0
Thomas Koch,"[<JIRA Component: name='tests', id='12312427'>]",2011-10-30T18:33:08.000+0000,Thomas Koch,"Probably caused by the usage of a legacy VCS a code duplication happened when you moved from Sourceforge to Apache (ZOOKEEPER-38). The following file can be deleted:
src/java/test/org/apache/zookeeper/server/DataTreeUnitTest.java

src/java/test/org/apache/zookeeper/test/DataTreeTest.java was an exact copy of the above until ZOOKEEPER-1046 added an additional test case only to the latter.

Do I need to upload a patch file for this?","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1273,Trivial,Thomas Koch,Fixed,2011-10-31T20:06:15.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Copy'n'pasted unit test,2011-11-01T10:57:28.000+0000,[],2.0
Thomas Koch,[],2011-10-29T13:36:00.000+0000,Thomas Koch,"The client API method Zookeeper.multi() promisses, that the KeeperException it throws in case of one of the multi ops failing, contains a list of individual results.

The method ZooKeeper.multiInternal() however throws a Keeperexception if the returned response header has an error code != 0. This should actually never happen if the server does not misbehave since the error code of a multi response is always zero, but I managed to trigger this code path with my refactorings.",[],Bug,ZOOKEEPER-1272,Minor,Thomas Koch,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper.multi() could violate API if server misbehaves,2011-10-30T17:11:43.000+0000,[],0.0
Mahadev Konar,"[<JIRA Component: name='java client', id='12312381'>]",2011-10-28T22:33:08.000+0000,Patrick D. Hunt,"See:
https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper_branch34_solaris/1/testReport/junit/org.apache.zookeeper.server.quorum/QuorumPeerMainTest/testEarlyLeaderAbandonment/

Notice that the clients attempt to connect before the servers have bound, then 30 seconds later, after seemingly no further client activity we see:

2011-10-28 21:40:56,828 [myid:] - INFO  [main-SendThread(localhost:11227):ClientCnxn$SendThread@1057] - Client session timed out, have not heard from server in 30032ms for sessionid 0x0, closing socket connection and attempting reconnect


I believe this is different from ZOOKEEPER-1270 because in the 1270 case it seems like the clients are attempting to connect but the servers are not accepting (notice the stat commands are being dropped due to no server running)","[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1271,Blocker,Patrick D. Hunt,Fixed,2011-11-02T21:59:00.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,testEarlyLeaderAbandonment failing on solaris - clients not retrying connection,2011-11-23T19:21:58.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",2.0
Flavio Paiva Junqueira,"[<JIRA Component: name='server', id='12312382'>]",2011-10-28T22:25:36.000+0000,Patrick D. Hunt,"Looks pretty serious - quorum is formed but no clients can attach. Will attach logs momentarily.

This test was introduced in the following commit (all three jira commit at once):
ZOOKEEPER-335. zookeeper servers should commit the new leader txn to their logs.
ZOOKEEPER-1081. modify leader/follower code to correctly deal with new leader
ZOOKEEPER-1082. modify leader election to correctly take into account current","[<JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1270,Blocker,Patrick D. Hunt,Fixed,2011-11-05T11:46:06.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"testEarlyLeaderAbandonment failing intermittently, quorum formed, no serving.",2011-11-23T19:22:04.000+0000,[],4.0
Camille Fournier,"[<JIRA Component: name='server', id='12312382'>]",2011-10-28T21:34:37.000+0000,Camille Fournier,"From the mailing list:

FileTxnSnapLog.restore contains a code block handling a NODEEXISTS failure during deserialization. The problem is explained there in a code comment. The code block however is only executed for a CREATE txn, not for a multiTxn containing a CREATE.

Even if the mentioned code block would also be executed for multi transactions, it needs adaption for multi transactions. What, if after the first failed transaction in a multi txn during deserialization, there would be subsequent transactions in the same multi that would also have failed?
We don't know, since the first failed transaction hides the information about the remaining transactions.
","[<JIRA Version: name='3.4.1', id='12318650'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1269,Major,Camille Fournier,Fixed,2011-12-09T22:25:16.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Multi deserialization issues,2011-12-17T01:33:58.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2011-10-28T18:01:05.000+0000,Patrick D. Hunt,"I'm having a lot problems testing the 3.4.0 release candidate (0). I'm seeing frequent failures in RO unit tests, also the solaris tests are broken on jenkins, some of which is due to RO mode:
https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper_trunk_solaris/30/#showFailuresLink

I'm also seeing ERROR level messages in the logs during test runs that are a result of attempting to start RO mode.

Given this is a new feature, one that could be very disruptive, I think we need to control whether the feature is enabled or not through a config option (system prop is fine), disabled by default.

I'll look at the RO mode tests to see if I can find the cause of the failures on solaris, but I may also turn off these tests for the time being. (I need to look at this further).


I'm marking this as a blocker for 3.4.0, Mahadev LMK if you feel similarly or whether I should be shooting for 3.4.1 with this. (or perhaps I'm just way off in general).

","[<JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1268,Blocker,Patrick D. Hunt,Fixed,2011-11-01T07:15:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"problems with read only mode, intermittent test failures and ERRORs in the log",2011-11-23T19:22:06.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",1.0
Thomas Koch,[],2011-10-28T13:35:46.000+0000,Thomas Koch,"As discussed on the list, it's probably an error that the ReadOnlyRequestProcessor does not have multi alongside the other write operations.
Adding check to the lists may not make a difference by now since the ZK client does not expose check as a first level request but only encapsulated inside a multi request. However from a logical view, change belongs in these lists.",[],Bug,ZOOKEEPER-1265,Major,Thomas Koch,Fixed,2011-10-28T16:38:33.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Normalize switch cases lists on request types,2011-10-29T10:56:49.000+0000,[],2.0
Camille Fournier,"[<JIRA Component: name='tests', id='12312427'>]",2011-10-28T04:23:28.000+0000,Patrick D. Hunt,"The FollowerResyncConcurrencyTest test is failing intermittently. 

saw the following on 3.4:
{noformat}
junit.framework.AssertionFailedError: Should have same number of
ephemerals in both followers expected:<11741> but was:<14001>
       at org.apache.zookeeper.test.FollowerResyncConcurrencyTest.verifyState(FollowerResyncConcurrencyTest.java:400)
       at org.apache.zookeeper.test.FollowerResyncConcurrencyTest.testResyncBySnapThenDiffAfterFollowerCrashes(FollowerResyncConcurrencyTest.java:196)
       at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
{noformat}
","[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1264,Blocker,Patrick D. Hunt,Fixed,2011-11-05T20:58:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,FollowerResyncConcurrencyTest failing intermittently,2011-11-23T19:22:41.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",6.0
Jordan Zimmerman,"[<JIRA Component: name='documentation', id='12312422'>]",2011-10-27T21:46:26.000+0000,Jordan Zimmerman,"The recipe for Locks documented here: http://zookeeper.apache.org/doc/trunk/recipes.html#sc_recipes_Locks doesn't deal with the problem of create() succeeding but the server crashing before the result is returned. As written, if the server crashes before the result is returned the client can never know what sequential node was created for it. The way to deal with this is to embed the session ID in the node name. The Lock implementation in the ZK distro does this. But, the documentation will lead implementors to write bad code.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1262,Major,Jordan Zimmerman,Fixed,2011-12-28T21:18:40.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Documentation for Lock recipe has major flaw,2011-12-28T21:18:40.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",2.0
Flavio Paiva Junqueira,"[<JIRA Component: name='tests', id='12312427'>]",2011-10-27T07:45:33.000+0000,Daniel Gómez Ferro,ClientPortBindTest is failing consistently on Mac OS X.,"[<JIRA Version: name='3.5.3', id='12335444'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-1256,Major,Daniel Gómez Ferro,Fixed,2016-07-29T23:05:46.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ClientPortBindTest is failing on Mac OS X,2017-05-18T03:43:55.000+0000,[],7.0
Peng Futian,"[<JIRA Component: name='c client', id='12312380'>]",2011-10-24T01:35:31.000+0000,Peng Futian," When I repeat add watcher , there are a memory leak. 
",[],Bug,ZOOKEEPER-1242,Major,Peng Futian,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Repeat add watcher, memory leak  ",2022-02-03T08:50:18.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",1.0
Jingguo Yao,"[<JIRA Component: name='documentation', id='12312422'>]",2011-10-23T15:10:38.000+0000,Jingguo Yao,"In ""if p is the lowest process node in L, wait on highest process node in P"", ""P"" should be ""L"".","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1241,Minor,Jingguo Yao,Fixed,2011-10-24T08:01:32.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Typo in ZooKeeper Recipes and Solutions documentation,2011-10-24T10:53:12.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",1.0
Peng Futian,"[<JIRA Component: name='c client', id='12312380'>]",2011-10-22T02:16:17.000+0000,Peng Futian,"When I compile zookeeper c client in my project, there are some error:
../../../include/zookeeper/recordio.h:70: error：expected unqualified-id before '__extension__'
../../../include/zookeeper/recordio.h:70: error：expected `)' before '__extension__'
../../.. /include/zookeeper/recordio.h:70: error：expected unqualified-id before ')' token
",[],Bug,ZOOKEEPER-1240,Minor,Peng Futian,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Compiler issue with redhat linux,2022-02-03T08:50:20.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",3.0
Skye Wanderman-Milne,"[<JIRA Component: name='server', id='12312382'>]",2011-10-20T16:58:21.000+0000,Patrick D. Hunt,"from NettyServerCnxn:

bq.         bootstrap.setOption(""child.soLinger"", 2);

See ZOOKEEPER-1049","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1238,Major,Patrick D. Hunt,Fixed,2014-01-12T21:36:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,when the linger time was changed for NIO the patch missed Netty,2014-03-13T18:17:10.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",5.0
,"[<JIRA Component: name='server', id='12312382'>]",2011-10-20T16:54:48.000+0000,Patrick D. Hunt,"After applying ZOOKEEPER-1049 to 3.3.3 (I believe the same problem exists in 3.4/3.5 but haven't tested this) I'm seeing the following exception more frequently:

{noformat}
Oct 19, 1:31:53 PM ERROR
Unexpected Exception:
java.nio.channels.CancelledKeyException
at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:55)
at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:59)
at org.apache.zookeeper.server.NIOServerCnxn.sendBuffer(NIOServerCnxn.java:418)
at org.apache.zookeeper.server.NIOServerCnxn.sendResponse(NIOServerCnxn.java:1509)
at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:367)
at org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:73)
{noformat}

This is a long standing problem where we try to send a response after the socket has been closed. Prior to ZOOKEEPER-1049 this issues happened much less frequently (2 sec linger), but I believe it was possible. The timing window is just wider now.

","[<JIRA Version: name='3.4.10', id='12338036'>]",Bug,ZOOKEEPER-1237,Major,Patrick D. Hunt,Duplicate,2017-01-24T23:58:58.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ERRORs being logged when queued responses are sent after socket has closed.,2018-05-31T00:16:03.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",39.0
Adalberto Medeiros,"[<JIRA Component: name='server', id='12312382'>]",2011-10-20T16:05:02.000+0000,Patrick D. Hunt,"See HADOOP-7211 - Recent kerberos integration resulted in the same issue in ZK.

{noformat}
    [javac] /home/phunt/dev/zookeeper/src/java/main/org/apache/zookeeper/server/auth/KerberosName.java:88: warning: sun.security.krb5.KrbException is Sun proprietary API and may be removed in a future release
    [javac]     } catch (KrbException ke) {
{noformat}","[<JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1236,Major,Patrick D. Hunt,Fixed,2012-06-30T06:31:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Security uses proprietary Sun APIs,2012-07-04T18:25:38.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.4.3', id='12319288'>]",5.0
Rakesh Radhakrishnan,"[<JIRA Component: name='recipes', id='12313246'>]",2011-10-13T10:36:33.000+0000,Rakesh Radhakrishnan,"Presently there is no state validation for the start() api, so one can invoke multiple times consecutively. The second or further invocation will makes the client node to become 'READY' state transition. Because there is an offer already got created during the first invocation of the start() api, the second invocation again makeOffer() and after determination will be chosen as READY state transitions. 

This makes the situation with no 'ELECTED' nodes present and the client (or the user of the election recipe) will be indefinitely waiting for the 'ELECTED' node.

Similarly, stop() api can be invoked and there is no state validation and this can dispatch unnecessary FAILED transition events.


IMO, LES recipe can have validation logic to avoid the successive start() and stop() invocations.","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-1225,Major,Rakesh Radhakrishnan,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Successive invocation of LeaderElectionSupport.start() will bring the ELECTED node to READY and cause no one in ELECTED state.,2022-02-03T08:36:26.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",2.0
Laxman,"[<JIRA Component: name='java client', id='12312381'>]",2011-10-13T04:54:14.000+0000,amith,"create a java client 
create a persistent node using that client
write data into the node  
like..
ZkClient zk = new ZkClient ( getZKServers () );
        zk.createPersistent ( ""/amith"" , true );
        zk.writeData ( ""/amith"", ""amith"" );
        Object readData = zk.readData ( ""/amith"" );
        LOGGER.logInfo (readData);

        zk.delete ( ""/amith"" );

and try to read the same using ZkCli.sh console client

[zk: XXX.XXX.XXX.XXX:XXXXX(CONNECTED) 2] get /amith
ï¿½ï¿½tamith
cZxid = 0x100000004
ctime = Wed Oct 12 10:13:15 CST 2011
mZxid = 0x100000005
mtime = Wed Oct 12 10:13:15 CST 2011
pZxid = 0x100000004
cversion = 0
dataVersion = 1
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 12
numChildren = 0

data is displayed as ï¿½ï¿½tamith
this include some unwanted char

 



","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1224,Minor,amith,Not A Problem,2011-10-18T08:09:58.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,problem across zookeeper clients when reading data written by other clients,2011-10-18T08:09:58.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",2.0
,"[<JIRA Component: name='recipes', id='12313246'>]",2011-10-13T03:51:31.000+0000,June Fang,"according to ZOOKEEPER-1033, headers will be installed into ""PREFIX/zookeeper"" directory.
i guess theses includes may also needed to be changed ? ",[],Bug,ZOOKEEPER-1223,Trivial,June Fang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,C recipes includes <zookeeper.h> instead of <zookeeper/zookeeper.h>,2011-12-29T16:17:04.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",1.0
Michi Mutsuzaki,"[<JIRA Component: name='java client', id='12312381'>]",2011-10-12T21:08:00.000+0000,Camille Fournier,"getACL(String, Stat) should allow the stat object to be null in the case that the user doesn't care about getting the stat back, as per other methods with similar syntax","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1222,Minor,Camille Fournier,Fixed,2014-07-08T18:33:39.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,getACL should only call DataTree.copyStat when passed in stat is not null,2014-07-08T21:17:24.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>]",5.0
kavita sharma,"[<JIRA Component: name='scripts', id='12312384'>]",2011-10-12T10:47:43.000+0000,kavita sharma,"Few problems while executing create command,
 
If we will give command like 
 
1)[zk: localhost:2181(CONNECTED) 0] create -s -e /node1
{noformat}
       Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:692)
	at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:593)
	at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:365)
	at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:323)
	at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:282)
{noformat}
      but actually it should create emphemeral sequential node.

2)[zk: localhost:2181(CONNECTED) 0] create -s -e
{noformat}
    Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 3
{noformat}
    here it should print the list of commands that is the default behaviour of zkCli for invalid/incomplete commands.

3)[zk: localhost:2181(CONNECTED) 3] create -s -e ""data""
{noformat}
     Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 4
{noformat}
     here command is wrong so it should print list of commnads. . 

4)[zk: localhost:2181(CONNECTED) 0] create /node1
    zkCli is treating it as a invalid command.because for args.length  check (3)is their but behaviour is 
    if user haven't given any of the option it should create persistent node.
	  {noformat}
	  if (cmd.equals(""create"") && args.length >= 3) {
            int first = 0;
            CreateMode flags = CreateMode.PERSISTENT;
{noformat}","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1220,Major,kavita sharma,Fixed,2011-12-14T22:59:01.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,./zkCli.sh 'create' command is throwing ArrayIndexOutOfBoundsException,2011-12-15T11:58:22.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",4.0
César Álvarez Núñez,"[<JIRA Component: name='quorum', id='12312379'>]",2011-10-05T09:56:36.000+0000,César Álvarez Núñez,"When a QuorumPeer thread dies, it is unregistering *all* ZKMBeanInfo MBeans previously registered on its java process; including those that has not been registered by itself.

It does not cause any side effect in production environment where each server is running on a separate java process; but fails when using ""org.apache.zookeeper.test.QuorumUtil"" to programmatically start up a zookeeper server ensemble and use its provided methods to force Disconnected, SyncConnected or SessionExpired events; in order to perform some basic/functional testing.

Scenario:
* QuorumUtil qU = new QuorumUtil(1); // It creates a 3 servers ensemble.
* qU.startAll(); // Startup all servers: 1 Leader + 2 Followers
* qU.shutdown\(i\); // i is a number from 1 to 3. It shutdown one server.

The last method causes that a QuorumPeer will die, invoking the MBeanRegistry.unregisterAll() method.
As a result, *all* ZKMBeanInfo MBeans are unregistered; including those belonging to the other QuorumPeer instances.

When trying to restart previous server (qU.restart\(i\)) an AssertionError is thrown at MBeanRegistry.register(ZKMBeanInfo bean, ZKMBeanInfo parent) method, causing the QuorumPeer thread dead.

To solve it:
* MBeanRegistry.unregisterAll() method has been removed.
* QuorumPeer only unregister its ZKMBeanInfo MBeans.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1214,Major,César Álvarez Núñez,Fixed,2014-05-18T03:49:28.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,QuorumPeer should unregister only its previsously registered MBeans instead of use MBeanRegistry.unregisterAll() method.,2014-05-20T11:09:12.000+0000,[],6.0
Roman Shaposhnik,"[<JIRA Component: name='scripts', id='12312384'>]",2011-10-03T20:24:24.000+0000,Roman Shaposhnik,"According to LSB Core para 20.2:
==================================================================================
Otherwise,  the exit  status shall  be non­zero,  as de­fined below. 
In addition to straightforward success, the following situations are
also to be considered successful: 
• restarting a service (instead of reloading it) with the force­reload argument
• running start on a service already running
• running stop on a service already stopped or not running
• running restart on a service already stopped or not running
• running try­restart on a service already stopped or not running
==================================================================================

Yet, zkServer.sh fails on stop if it can't find a PID file:

{noformat}
stop)
    echo -n ""Stopping zookeeper ... ""
    if [ ! -f ""$ZOOPIDFILE"" ]
    then
      echo ""error: could not find file $ZOOPIDFILE""
      exit 1
    else
      $KILL -9 $(cat ""$ZOOPIDFILE"")
      rm ""$ZOOPIDFILE""
      echo STOPPED
      exit 0
    fi
{noformat}","[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1212,Major,Roman Shaposhnik,Fixed,2011-10-19T06:42:03.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkServer.sh stop action is not conformat with LSB para 20.2 Init Script Actions,2011-11-23T19:22:15.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",1.0
,"[<JIRA Component: name='c client', id='12312380'>]",2011-09-30T09:07:25.000+0000,June Fang,"the package name of c client is ""c-client-src"",
which lead the include file to be installed to /usr/local/include/c-client-src.

it's a bit annoying since user need to manual rename it to zookeeper.

i think there are two fix,
1) change autoconf package name to ""zookeeper"", then the header will be installed to
   zookeeper subdir, which is consistent with the README;
2) change pkginclude_HEADER to include_HEADER, which will install headers to /usr/local/include.
",[],Bug,ZOOKEEPER-1211,Trivial,June Fang,Duplicate,2011-10-13T02:28:14.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,C client's package name,2011-10-13T02:28:14.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",0.0
Tadeusz Andrzej Kadłubowski,"[<JIRA Component: name='build', id='12312383'>]",2011-09-28T14:20:21.000+0000,Tadeusz Andrzej Kadłubowski,"I was trying to build the zookeeper RPM (basically, `ant rpm -Dskip.contrib=1`), using build scripts that were recently merged from the work on the ZOOKEEPER-999 issue.

The final stage, i.e. running rpmbuild failed. From what I understand it mixed BUILD and BUILDROOT subdirectories in /tmp/zookeeper_package_build_tkadlubo/, leaving BUILDROOT empty, and placing everything in BUILD.

The full build log is at http://pastebin.com/0ZvUAKJt (Caution: I cut out long file listings from running tar -xvvf).","[<JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.4.4', id='12319841'>]",Bug,ZOOKEEPER-1210,Minor,Tadeusz Andrzej Kadłubowski,Fixed,2012-06-30T06:19:27.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Can't build ZooKeeper RPM with RPM >= 4.6.0 (i.e. on RHEL 6 and Fedora >= 10),2012-06-30T11:01:10.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",6.0
Rakesh Radhakrishnan,"[<JIRA Component: name='recipes', id='12313246'>]",2011-09-28T06:53:03.000+0000,Rakesh Radhakrishnan,"*Case1-* N/w disconnection can bring both the client nodes to be in ELECTED state. Current LeaderElectionSupport(LES) f/w handles only 'NodeDeletion'.
 
Consider the scenario where ELECTED and READY nodes are running. Say ELECTED node's n/w got failed and is ""Disconnected"" from ZooKeeper. But it will behave as ELECTED as it is not getting any events from the LeaderElectionSupport(LES) framework.
After sessiontimeout, node in READY state will be notified by 'NodeDeleted' event and will go to ELECTED state.
*Problem:* 
Both the node becomes ELECTED and finally the user sees two Master (ELECTED) node and cause inconsistencies.


*Case2-* Also in this case, Let's say if user has started only one client node and becomes ELECTED. After sometime n/w has disconnected with the ZooKeeper server and the session got expired. 
*Problem:*
Still the client node will be in the ELECTED state. After sometime if user has started the second client node. Again both the nodes becomes ELECTED.
","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-1209,Major,Rakesh Radhakrishnan,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,"LeaderElection recipe doesn't handle the split-brain issue, n/w disconnection can bring both the client nodes to be in ELECTED",2022-02-03T08:36:25.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",5.0
Patrick D. Hunt,[],2011-09-28T04:35:47.000+0000,kishore gopalakrishna,"Copying from email thread.


We found our ZK server in a state where an ephemeral node still exists after
a client session is long gone. I used the cons command on each ZK host to
list all connections and couldn't find the ephemeralOwner id. We are using
ZK 3.3.3. Has anyone seen this problem?

I got the following information from the logs.

The node that still exists is /kafka-tracking/consumers/UserPerformanceEvent-<host>/owners/UserPerformanceEvent/529-7

I saw that the ephemeral owner is 86167322861045079 which is session id 0x13220b93e610550.

After searching in the transaction log of one of the ZK servers found that session expired

9/22/11 12:17:57 PM PDT session 0x13220b93e610550 cxid 0x74 zxid 0x601bd36f7 closeSession null

On digging further into the logs I found that there were multiple sessions created in quick succession and every session tried to create the same node. But i verified that the sessions were closed and opened in order
9/22/11 12:17:56 PM PDT session 0x13220b93e610550 cxid 0x0 zxid 0x601bd36b5 createSession 6000
9/22/11 12:17:57 PM PDT session 0x13220b93e610550 cxid 0x74 zxid 0x601bd36f7 closeSession null
9/22/11 12:17:58 PM PDT session 0x13220b93e610551 cxid 0x0 zxid 0x601bd36f8 createSession 6000
9/22/11 12:17:59 PM PDT session 0x13220b93e610551 cxid 0x74 zxid 0x601bd373a closeSession null
9/22/11 12:18:00 PM PDT session 0x13220b93e610552 cxid 0x0 zxid 0x601bd373e createSession 6000
9/22/11 12:18:01 PM PDT session 0x13220b93e610552 cxid 0x6c zxid 0x601bd37a0 closeSession null
9/22/11 12:18:02 PM PDT session 0x13220b93e610553 cxid 0x0 zxid 0x601bd37e9 createSession 6000
9/22/11 12:18:03 PM PDT session 0x13220b93e610553 cxid 0x74 zxid 0x601bd382b closeSession null
9/22/11 12:18:04 PM PDT session 0x13220b93e610554 cxid 0x0 zxid 0x601bd383c createSession 6000
9/22/11 12:18:05 PM PDT session 0x13220b93e610554 cxid 0x6a zxid 0x601bd388f closeSession null
9/22/11 12:18:06 PM PDT session 0x13220b93e610555 cxid 0x0 zxid 0x601bd3895 createSession 6000
9/22/11 12:18:07 PM PDT session 0x13220b93e610555 cxid 0x6a zxid 0x601bd38cd closeSession null
9/22/11 12:18:10 PM PDT session 0x13220b93e610556 cxid 0x0 zxid 0x601bd38d1 createSession 6000
9/22/11 12:18:11 PM PDT session 0x13220b93e610557 cxid 0x0 zxid 0x601bd38f2 createSession 6000
9/22/11 12:18:11 PM PDT session 0x13220b93e610557 cxid 0x51 zxid 0x601bd396a closeSession null

Here is the log output for the sessions that tried creating the same node

9/22/11 12:17:54 PM PDT session 0x13220b93e61054f cxid 0x42 zxid 0x601bd366b create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-7
9/22/11 12:17:56 PM PDT session 0x13220b93e610550 cxid 0x42 zxid 0x601bd36ce create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-7
9/22/11 12:17:58 PM PDT session 0x13220b93e610551 cxid 0x42 zxid 0x601bd3711 create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-7
9/22/11 12:18:00 PM PDT session 0x13220b93e610552 cxid 0x42 zxid 0x601bd3777 create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-7
9/22/11 12:18:02 PM PDT session 0x13220b93e610553 cxid 0x42 zxid 0x601bd3802 create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-7
9/22/11 12:18:05 PM PDT session 0x13220b93e610554 cxid 0x44 zxid 0x601bd385d create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-7
9/22/11 12:18:07 PM PDT session 0x13220b93e610555 cxid 0x44 zxid 0x601bd38b0 create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-7
9/22/11 12:18:11 PM PDT session 0x13220b93e610557 cxid 0x52 zxid 0x601bd396b create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-7

Let me know if you need additional information.
","[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1208,Blocker,kishore gopalakrishna,Fixed,2011-11-14T19:34:12.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Ephemeral node not removed after the client session is long gone,2011-11-23T19:22:11.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",12.0
,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2011-09-27T20:04:12.000+0000,Patrick D. Hunt,"I'm seeing a strange ERROR message when starting an ensemble:

{noformat}
2011-09-27 13:00:08,168 [myid:3] - ERROR [Thread-2:QuorumPeer$1@689] - FAILED to start ReadOnlyZooKeeperServer
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.zookeeper.server.quorum.QuorumPeer$1.run(QuorumPeer.java:684)
{noformat}

I did not specify ReadOnlyZooKeeperServer, also why is this at ERROR level? I'm not sure the expected behavior here. Is r/o turned on by default? Seems we should have this as a config option, off by default.
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1207,Critical,Patrick D. Hunt,Invalid,2014-04-25T19:28:24.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,strange ReadOnlyZooKeeperServer ERROR when starting ensemble,2014-04-25T19:28:24.000+0000,[],1.0
Mark Miller,"[<JIRA Component: name='server', id='12312382'>]",2011-09-27T16:26:16.000+0000,Mark Miller,"While I always expect to be able to parse a sequential node by looking for digits, under some locals you end up with non digits - for example: n_००००००००००

It looks like the problem is around line 236 in PrepRequestProcessor:

{code}
                if (createMode.isSequential()) {
                    path = path + String.format(""%010d"", parentCVersion);
                }
{code}

Instead we should pass Locale.ENGLISH to the format call.

{code}
                if (createMode.isSequential()) {
                    path = path + String.format(Locale.ENGLISH, ""%010d"", parentCVersion);
                }
{code}

Lucene/Solr tests with random Locales, and some of my tests that try and inspect the node name and order things expect to find digits - currently my leader election recipe randomly fails when the wrong locale pops up.","[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1206,Minor,Mark Miller,Fixed,2011-09-29T21:36:43.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Sequential node creation does not use always use digits in node name given certain Locales.,2011-11-23T19:22:12.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",1.0
Prashant Gokhale,"[<JIRA Component: name='tests', id='12312427'>]",2011-09-23T22:44:59.000+0000,Prashant Gokhale,"For running these tests, I am following instructions on https://github.com/apache/zookeeper/blob/trunk/src/java/systest/README.txt 

In Step 4, when I try to run java -jar build/contrib/fatjar/zookeeper-<version>-fatjar.jar systest org.apache.zookeeper.test.system.SimpleSysTest , it throws the following error,

Exception in thread ""main"" java.lang.NoClassDefFoundError: junit/framework/TestCase

The problem is that zookeeper-dev-fatjar.jar does not contain the TestCase class.

Patrick Hunt suggested that adding <zipgroupfileset dir=""${zk.root}/build/test/lib"" includes=""*.jar"" /> to fatjar/build.xml should solve the problem and it does.","[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1203,Major,Prashant Gokhale,Fixed,2011-09-29T18:27:46.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Zookeeper systest is missing Junit Classes ,2011-11-23T19:22:26.000+0000,[],1.0
Camille Fournier,"[<JIRA Component: name='server', id='12312382'>]",2011-09-21T14:37:39.000+0000,Camille Fournier,"When transferring a large amount of information from a 4 letter word, especially in interactive mode (telnet or nc) over a slower network link, the connection can be closed before all of the data has reached the client. This is due to the way we handle nc non-interactive mode, by cancelling the selector key. 

Instead of cancelling the selector key for 4-letter-words, we should instead flag the NIOServerCnxn to ignore detection of a close condition on that socket (CancelledKeyException, EndOfStreamException). Since the 4lw will close the connection immediately upon completion, this should be safe to do. 

See ZOOKEEPER-737 for more details
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1197,Critical,Camille Fournier,Won't Fix,2014-05-15T22:00:05.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Incorrect socket handling of 4 letter words for NIO,2014-05-15T22:00:05.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>]",3.0
Eugene Joseph Koontz,[],2011-09-20T15:15:41.000+0000,Eugene Joseph Koontz,"Tom Klonikowski writes:

    Hello developers,

    the SaslServerCallbackHandler in trunk changes the principal name
    service/host@REALM to service/service@REALM (i guess unintentionally).

    lines 131-133:
    if (!removeHost() && (kerberosName.getHostName() != null)) {
      userName += ""/"" + kerberosName.getServiceName();
    }

    Server Log:

    SaslServerCallbackHandler@115] - Successfully authenticated client:
    authenticationID=fetcher/ubook@QUINZOO;
    authorizationID=fetcher/ubook@QUINZOO.

    SaslServerCallbackHandler@137] - Setting authorizedID:
    fetcher/fetcher@QUINZOO

","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1195,Major,Eugene Joseph Koontz,Fixed,2011-09-29T07:42:51.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,SASL authorizedID being incorrectly set: should use getHostName() rather than getServiceName(),2013-05-02T02:29:44.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",2.0
Alexander Shraer,"[<JIRA Component: name='server', id='12312382'>]",2011-09-20T00:10:08.000+0000,Alexander Shraer,"Leader.getEpochToPropose() and Leader.waitForNewEpoch() act as barriers - they make sure that a leader/follower can return from calling the method only once connectingFollowers (or electingFollowers) contain a quorum. But these methods don't make sure that the leader itself is in connectingFollowers/electingFollowers. So the leader didn't necessarily reach the barrier when followers pass it. This can cause the following problems:

1. If the leader is not in connectingFollowers when a LearnerHandler returns from getEpochToPropose(), then the epoch sent by the leader to the follower might be smaller than the leader's own last accepted epoch.

2. If the leader is not in electingFollowers when LearnerHandler returns from waitForNewEpoch() then the leader will send a NEWLEADER message to followers, and the followers will respond, but it is possible that the NEWLEADER message is not in outstandingProposals when these NEWLEADER  acks arrive, which will cause the NEWLEADER acks to be dropped.


To fix this I propose to explicitly check that the leader is in connectingFollowers/electingFollowers before anyone can pass these barriers.




","[<JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1194,Major,Alexander Shraer,Fixed,2011-11-05T06:38:24.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Two possible race conditions during leader establishment,2011-11-23T19:22:46.000+0000,[],1.0
Alexander Shraer,"[<JIRA Component: name='server', id='12312382'>]",2011-09-19T02:00:24.000+0000,Alexander Shraer,"A follower/leader should block in Leader.waitForEpochAck() until either electingFollowers contains a quorum and electionFinished=true or until a timeout occurs. A timeout means that a quorum of followers didn't ack the epoch on time, which is an error. 

But the check in Leader.waitForEpochAck() is ""if (waitingForNewEpoch) throw..."" and this will never be triggered, even if the wait statement just timed out,  because Leader.getEpochToPropose() completes and sets waitingForNewEpoch to false before Leader.waitForEpochAck() is invoked.

Instead of ""if (waitingForNewEpoch) throw"" the condition in Leader.waitForEpochAck() should be ""if (!electionFinished) throw"".
The guarded block introduced in ZK-1191 should be checking !electionFinished.

","[<JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1192,Critical,Alexander Shraer,Fixed,2011-11-05T06:15:12.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Leader.waitForEpochAck() checks waitingForNewEpoch instead of checking electionFinished,2011-11-23T19:22:06.000+0000,[],2.0
Eric Yang,"[<JIRA Component: name='build', id='12312383'>]",2011-09-17T00:30:19.000+0000,Patrick D. Hunt,"run ""ant package"" and look in the build/zookeeper-<version>/bin directory. many of the bin scripts are missing.
","[<JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1190,Blocker,Patrick D. Hunt,Fixed,2011-10-07T20:48:26.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ant package is not including many of the bin scripts in the package (zkServer.sh for example),2011-11-23T19:22:49.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",2.0
Rakesh Radhakrishnan,"[<JIRA Component: name='server', id='12312382'>]",2011-09-16T14:29:05.000+0000,Rakesh Radhakrishnan,"When loading the snapshot, ZooKeeper will consider only the 'snapshots with atleast 10 bytes size'. Otherwsie it will ignore and just return without closing the RandomAccessFile.

{noformat}
Util.isValidSnapshot() having the following logic. 
       // Check for a valid snapshot
        RandomAccessFile raf = new RandomAccessFile(f, ""r"");
        // including the header and the last / bytes
        // the snapshot should be atleast 10 bytes
        if (raf.length() < 10) {
            return false;
        }
{noformat}

Since the snapshot file validation logic is outside try block, it won't go to the finally block and will be leaked.

Suggestion: Move the validation logic to the try/catch block.","[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1189,Major,Rakesh Radhakrishnan,Fixed,2011-09-27T01:11:20.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,For an invalid snapshot file(less than 10bytes size) RandomAccessFile stream is leaking.,2011-11-23T19:22:22.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",3.0
,"[<JIRA Component: name='java client', id='12312381'>]",2011-09-15T13:21:48.000+0000,Stepan Koltsov,"ZooKeeper client seems to hang quietly on OutOfMemoryError.

Look at code of ClientCnxn.SendThread.run:

{code}
void run() {
  while (zooKeeper.state.isAlive()) {
    try {
      ...
    } catch (Exception e) {
      // handle exception and restart
    }
  }
  ...
}
{code}

If OutOfMemoryError happens somewhere inside of try block, thread just exits and ZooKeeper hangs.

Client should handle any Throwable same way it handles Exception.",[],Bug,ZOOKEEPER-1186,Major,Stepan Koltsov,Duplicate,2011-11-01T16:06:45.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZooKeeper client seems to hang quietly on OutOfMemoryError,2011-11-01T16:06:45.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",0.0
Eugene Joseph Koontz,"[<JIRA Component: name='java client', id='12312381'>]",2011-09-15T01:11:09.000+0000,Eugene Joseph Koontz,"There are 3 places where ClientCnxn should queue a AuthFailed event if client fails to authenticate. Without sending this event, clients may be stuck watching for a SaslAuthenticated event that will never come (since the client failed to authenticate).

","[<JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1185,Major,Eugene Joseph Koontz,Fixed,2011-09-27T02:09:47.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Send AuthFailed event to client if SASL authentication fails,2013-05-02T02:29:44.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",2.0
Thomas Koch,"[<JIRA Component: name='build', id='12312383'>]",2011-09-14T22:48:04.000+0000,Patrick D. Hunt,"The change for ZOOKEEPER-96 has removed the generated files from SVN, it seems that these files should now live under build subdir? If this change is made be sure that the C/contrib/recipes environment is not broken...","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1184,Major,Patrick D. Hunt,Fixed,2011-09-17T00:25:14.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"jute generated files are not being cleaned up via ""ant clean""",2011-09-17T10:56:12.000+0000,"[<JIRA Version: name='3.5.0', id='12316644'>]",2.0
Eugene Joseph Koontz,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2011-09-13T23:38:55.000+0000,Eugene Joseph Koontz,"Currently, in Zookeeper trunk, there are two problems with Kerberos TGT renewal:

1. TGTs obtained from a keytab are not refreshed periodically. They should be, just as those from ticket cache are refreshed.

2. Ticket renewal should be retried if it fails. Ticket renewal might fail if two or more separate processes (different JVMs) running as the same user try to renew Kerberos credentials at the same time. 
","[<JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1181,Major,Eugene Joseph Koontz,Fixed,2011-10-24T06:47:23.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Fix problems with Kerberos TGT renewal,2013-05-02T02:29:44.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",3.0
Rakesh Radhakrishnan,"[<JIRA Component: name='server', id='12312382'>]",2011-09-13T16:20:56.000+0000,Camille Fournier,"When calling a 4-letter-word to a server configured to use NettyServerCnxnFactory, the factory will not properly cancel all the keys and close the socket after sending the response for the 4lw. The close request will throw this exception, and the thread will not shut down:
2011-09-13 12:14:17,546 - WARN  [New I/O server worker #1-1:NettyServerCnxnFactory$CnxnChannelHandler@117] - Exception caught [id: 0x009300cc, /1.1.1.1:38542 => /139.172.114.138:2181] EXCEPTION: java.io.IOException: A non-blocking socket operation could not be completed immediately
java.io.IOException: A non-blocking socket operation could not be completed immediately
	at sun.nio.ch.SocketDispatcher.close0(Native Method)
	at sun.nio.ch.SocketDispatcher.preClose(SocketDispatcher.java:44)
	at sun.nio.ch.SocketChannelImpl.implCloseSelectableChannel(SocketChannelImpl.java:684)
	at java.nio.channels.spi.AbstractSelectableChannel.implCloseChannel(AbstractSelectableChannel.java:201)
	at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)
	at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:593)
	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119)
	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76)
	at org.jboss.netty.channel.Channels.close(Channels.java:720)
	at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208)
	at org.apache.zookeeper.server.NettyServerCnxn.close(NettyServerCnxn.java:116)
	at org.apache.zookeeper.server.NettyServerCnxn.cleanupWriterSocket(NettyServerCnxn.java:241)
	at org.apache.zookeeper.server.NettyServerCnxn.access$0(NettyServerCnxn.java:231)
	at org.apache.zookeeper.server.NettyServerCnxn$CommandThread.run(NettyServerCnxn.java:314)
	at org.apache.zookeeper.server.NettyServerCnxn$CommandThread.start(NettyServerCnxn.java:305)
	at org.apache.zookeeper.server.NettyServerCnxn.checkFourLetterWord(NettyServerCnxn.java:674)
	at org.apache.zookeeper.server.NettyServerCnxn.receiveMessage(NettyServerCnxn.java:791)
	at org.apache.zookeeper.server.NettyServerCnxnFactory$CnxnChannelHandler.processMessage(NettyServerCnxnFactory.java:217)
	at org.apache.zookeeper.server.NettyServerCnxnFactory$CnxnChannelHandler.messageReceived(NettyServerCnxnFactory.java:141)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)
	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350)
	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201)
	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1179,Critical,Camille Fournier,Fixed,2014-02-12T01:18:49.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,NettyServerCnxn does not properly close socket on 4 letter word requests,2014-03-13T18:17:01.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",6.0
Ted Dunning,"[<JIRA Component: name='java client', id='12312381'>]",2011-09-08T18:47:42.000+0000,Ted Dunning,"In the socket connection logic there are several errors that result in bad behavior.  The basic problem is that a socket is registered with a selector unconditionally when there are nuances that should be dealt with.  First, the socket may connect immediately.  Secondly, the connect may throw an exception.  In either of these two cases, I don't think that the socket should be registered.

I will attach a test case that demonstrates the problem.  I have been unable to create a unit test that exhibits the problem because I would have to mock the low level socket libraries to do so.  It would still be good to do so if somebody can figure out a good way.","[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1174,Critical,Ted Dunning,Fixed,2011-09-30T22:02:40.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,FD leak when network unreachable,2011-11-23T19:22:48.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",2.0
Thomas Koch,"[<JIRA Component: name='server', id='12312382'>]",2011-09-07T14:47:52.000+0000,Thomas Koch,"The ACL stuff in DataTree.java reimplements a kind of reference system. The idea may have been to save memory for equal ACL lists. However there's no code that ever removes an ACL list that is not used anymore.

Related: 
- The ACL stuff could be in a separate class so that DataTree.java is not such a big beast anymore.
- It's risky to have mutable objects (list) as keys in a HashMap.

An idea to solve this: Have ACL lists as members of the datatree nodes. Lookup already existing ACL lists in a java.util.WeakHashMap.",[],Bug,ZOOKEEPER-1173,Major,Thomas Koch,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Server never forgets old ACL lists,2020-04-24T03:05:28.000+0000,[],2.0
Patrick D. Hunt,"[<JIRA Component: name='build', id='12312383'>]",2011-09-02T19:35:51.000+0000,Patrick D. Hunt,I tried testing out zk on java 7 (not yet officially supported) but I ran into a road block due to the build failing. Patch coming next.,"[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1171,Minor,Patrick D. Hunt,Fixed,2011-09-13T21:50:58.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,fix build for java 7,2011-11-23T19:22:48.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
Andrew Finnell,"[<JIRA Component: name='jmx', id='12312451'>]",2011-08-31T19:52:13.000+0000,Andrew Finnell,"OS: Windows 64-bit
JRE: IKVM 7.0.4258

IKVM 7.0.4258 does not support ManagementFactory.getPlatformMBeanServer(); It will throw a java.lang.Error.
","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1168,Major,Andrew Finnell,Fixed,2011-09-01T17:15:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeper fails to run with IKVM,2011-11-23T19:22:07.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
Marshall McMullen,"[<JIRA Component: name='c client', id='12312380'>]",2011-08-31T14:22:39.000+0000,Nicholas Harteau,"Reading through the source, the C API implements zoo_async() which is the zookeeper sync() method implemented in the multithreaded/asynchronous C API.  It doesn't implement anything equivalent in the non-multithreaded API.

I'm not sure if this was oversight or intentional, but it means that the non-multithreaded API can't guarantee consistent client views on critical reads.

The zkperl bindings depend on the synchronous, non-multithreaded API so also can't call sync() currently.",[],Bug,ZOOKEEPER-1167,Major,Nicholas Harteau,,,"This issue was once resolved, but the resolution was deemed incorrect. From here issues are either marked assigned or resolved.",Reopened,0.0,C api lacks synchronous version of sync() call.,2022-02-03T08:50:24.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.5.0', id='12316644'>]",7.0
Warren Turkal,"[<JIRA Component: name='tests', id='12312427'>]",2011-08-29T22:08:07.000+0000,Warren Turkal,"The Eclipse test runner tries to run tests from all classes that inherit from TestCase. However, this class is inherited by at least one class (org.apache.zookeeper.test.system.BaseSysTest) that has no test cases as it is used as infrastructure for other real test cases. This patch annotates that class with @Ignore, which causes the class to be Ignored. Also, due to the way annotations are not inherited by default, this patch will not affect classes that inherit from this class.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1165,Minor,Warren Turkal,Fixed,2011-09-02T17:02:49.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,better eclipse support in tests,2011-11-23T19:22:00.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",2.0
Anupam Chanda,"[<JIRA Component: name='c client', id='12312380'>]",2011-08-25T17:47:50.000+0000,Anupam Chanda,"zk_hashtable.c:do_insert_watcher_object() line number 193 calls add_to_list with clone flag set to 1.  This leaks memory, since the original watcher object was already allocated on the heap by activateWatcher() line 330.

I will upload a patch shortly.  The fix is to set clone flag to 0 in the call to add_to_list().","[<JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1163,Major,Anupam Chanda,Fixed,2012-06-25T18:09:01.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Memory leak in zk_hashtable.c:do_insert_watcher_object(),2016-03-03T01:36:10.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",3.0
Andor Molnar,"[<JIRA Component: name='java client', id='12312381'>]",2011-08-20T17:16:28.000+0000,Andrew Kyle Purtell,"ClientCnxn does not always propagate session expiration indication up to clients. If a reconnection attempt fails because the session has since expired, the KeeperCode is still Disconnected, but shouldn't it be set to Expired? Perhaps like so:

{code}
--- a/src/java/main/org/apache/zookeeper/ClientCnxn.java
+++ b/src/java/main/org/apache/zookeeper/ClientCnxn.java
@@ -1160,6 +1160,7 @@ public class ClientCnxn {
                     clientCnxnSocket.doTransport(to, pendingQueue, outgoingQueue);
 
                 } catch (Exception e) {
+                    Event.KeeperState eventState = Event.KeeperState.Disconnected;
                     if (closing) {
                         if (LOG.isDebugEnabled()) {
                             // closing so this is expected
@@ -1172,6 +1173,7 @@ public class ClientCnxn {
                         // this is ugly, you have a better way speak up
                         if (e instanceof SessionExpiredException) {
                             LOG.info(e.getMessage() + "", closing socket connection"");
+                            eventState = Event.KeeperState.Expired;
                         } else if (e instanceof SessionTimeoutException) {
                             LOG.info(e.getMessage() + RETRY_CONN_MSG);
                         } else if (e instanceof EndOfStreamException) {
@@ -1191,7 +1193,7 @@ public class ClientCnxn {
                         if (state.isAlive()) {
                             eventThread.queueEvent(new WatchedEvent(
                                     Event.EventType.None,
-                                    Event.KeeperState.Disconnected,
+                                    eventState,
                                     null));
                         }
                         clientCnxnSocket.updateNow();
{code}

This affects HBase. HBase master and region server processes will shut down by design if their session has expired, but will attempt to reconnect if they think they have been disconnected. The above prevents proper termination.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1159,Major,Andrew Kyle Purtell,Won't Fix,2018-05-08T20:42:59.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ClientCnxn does not propagate session expiration indication,2018-05-08T20:43:15.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",11.0
,"[<JIRA Component: name='tests', id='12312427'>]",2011-08-19T19:58:48.000+0000,Vishal Kathuria,"The following tests are consistently timing out for me, and sometimes they crash the JVM. We need to look at these tests and make sure they pass consistently, otherwise they provide no value.

org.apache.zookeeper.test.AsyncHammerTest
org.apache.zookeeper.test.FollowerResyncConcurrencyTest
org.apache.zookeeper.test.ObserverQuorumHammerTest
org.apache.zookeeper.test.QuorumHammerTest
org.apache.zookeeper.test.QuorumTest
",[],Bug,ZOOKEEPER-1157,Minor,Vishal Kathuria,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Some of the tests timeout or cause JVM crash,2012-06-29T17:11:46.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",1.0
Vishal Kathuria,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2011-08-18T17:48:07.000+0000,Vishal Kathuria,"The log truncation relies on position calculation for a particular zxid to figure out the new size of the log file. There is a bug in PositionInputStream implementation which skips counting the bytes in the log which have value 0. This can lead to underestimating the actual log size. The log records which should be there can get truncated, leading to data loss on the participant which is executing the trunc.

Clients can see different values depending on whether they connect to the node on which trunc was executed. ","[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1156,Blocker,Vishal Kathuria,Fixed,2011-09-05T20:04:35.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Log truncation truncating log too much - can cause data loss,2011-11-23T19:22:43.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",2.0
Vishal Kathuria,"[<JIRA Component: name='quorum', id='12312379'>]",2011-08-15T17:36:07.000+0000,Vishal Kathuria,"If a participant with the highest zxid (lets call it A) isn't present during leader election, a participant with a lower zxid (say B) might be chosen as a leader. When A comes up, it will replay the log with that higher zxid. The change that was in that higher zxid will only be visible to the clients connecting to the participant A, but not to other participants.

I was able to reproduce this problem by
1. connect debugger to B and C and suspend them, so they don't write anything
2. Issue an update to the leader A.
3. After a few seconds, crash all servers (A,B,C)
4. Start B and C, let the leader election take place
5. Start A.
6. You will find that the update done in step 2 is visible on A but not on B,C, hence the inconsistency.

Below is a more detailed analysis of what is happening in the code.


Initial Condition
1.	Lets say there are three nodes in the ensemble A,B,C with A being the leader
2.	The current epoch is 7. 
3.	For simplicity of the example, lets say zxid is a two digit number, with epoch being the first digit.
4.	The zxid is 73
5.	All the nodes have seen the change 73 and have persistently logged it.

Step 1
Request with zxid 74 is issued. The leader A writes it to the log but there is a crash of the entire ensemble and B,C never write the change 74 to their log.

Step 3
B,C restart, A is still down
B,C form the quorum
B is the new leader. Lets say  B minCommitLog is 71 and maxCommitLog is 73
epoch is now 8, zxid is 80
Request with zxid 81 is successful. On B, minCommitLog is now 71, maxCommitLog is 81

Step 4
A starts up. It applies the change in request with zxid 74 to its in-memory data tree
A contacts B to registerAsFollower and provides 74 as its ZxId
Since 71<=74<=81, B decides to send A the diff. B will send to A the proposal 81.


Problem:
The problem with the above sequence is that A's data tree has the update from request 74, which is not correct. Before getting the proposals 81, A should have received a trunc to 73. I don't see that in the code. If the maxCommitLog on B hadn't bumped to 81 but had stayed at 73, that case seems to be fine.
","[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1154,Blocker,Vishal Kathuria,Fixed,2011-09-05T20:04:21.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Data inconsistency when the node(s) with the highest zxid is not present at the time of leader election,2011-11-23T19:22:45.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",2.0
Camille Fournier,"[<JIRA Component: name='server', id='12312382'>]",2011-08-12T20:27:37.000+0000,Camille Fournier,"Exceptions thrown by an AuthenticationProvider's handleAuthentication method will not be caught, and can cause the buffers in the NIOServer to not read requests fully or properly. Any exceptions thrown here should be caught and treated as auth failure. ","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1152,Major,Camille Fournier,Fixed,2011-08-21T01:05:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Exceptions thrown from handleAuthentication can cause buffer corruption issues in NIOServer,2011-11-23T19:22:16.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>]",2011-08-04T17:21:13.000+0000,Patrick D. Hunt,"I tried running my latency tester against trunk, in so doing I noticed that the C/Python (not sure which yet) client performance has seriously degraded since 3.3.3.

The first run (below) is with released 3.3.3 client against a 3 server ensemble running released 3.3.3 server code. The second run is the exact same environment (same ensemble), however using trunk c/zkpython client.

Notice:

1) in the first run operations are approx 10ms/write, 0.25ms/read - which is pretty much what's expected.

2) however in the second run we are seeing 50ms/operation regardless of read or write.

{noformat}
[phunt@c0309 zk-smoketest-3.3.3]$ PYTHONPATH=lib.linux-x86_64-2.6/ LD_LIBRARY_PATH=lib.linux-x86_64-2.6/ python26 ./zk-latencies.py --servers ""c0309:2181,c0310:2181,c0311:2181"" --znode_size=100 --znode_count=100 --timeout=5000 --synchronous
Connecting to c0309:2181
Connected in 16 ms, handle is 0
Connecting to c0310:2181
Connected in 16 ms, handle is 1
Connecting to c0311:2181
Connected in 15 ms, handle is 2
Testing latencies on server c0309:2181 using syncronous calls
created     100 permanent znodes  in    959 ms (9.599378 ms/op 104.173415/sec)
set         100           znodes  in    933 ms (9.332101 ms/op 107.157002/sec)
get         100           znodes  in     27 ms (0.270889 ms/op 3691.551589/sec)
deleted     100 permanent znodes  in    881 ms (8.812950 ms/op 113.469388/sec)
created     100 ephemeral znodes  in    956 ms (9.564152 ms/op 104.557103/sec)
watched     100           znodes  in     26 ms (0.264361 ms/op 3782.707587/sec)
deleted     100 ephemeral znodes  in    881 ms (8.819292 ms/op 113.387792/sec)
notif       100           watches in    999 ms (9.994299 ms/op 100.057038/sec)
Testing latencies on server c0310:2181 using syncronous calls
created     100 permanent znodes  in    964 ms (9.640460 ms/op 103.729490/sec)
set         100           znodes  in    933 ms (9.332800 ms/op 107.148981/sec)
get         100           znodes  in     29 ms (0.299308 ms/op 3341.036650/sec)
deleted     100 permanent znodes  in    886 ms (8.864651 ms/op 112.807603/sec)
created     100 ephemeral znodes  in    958 ms (9.585140 ms/op 104.328161/sec)
watched     100           znodes  in     30 ms (0.300801 ms/op 3324.459240/sec)
deleted     100 ephemeral znodes  in    886 ms (8.865030 ms/op 112.802779/sec)
notif       100           watches in   1000 ms (10.000212 ms/op 99.997878/sec)
Testing latencies on server c0311:2181 using syncronous calls
created     100 permanent znodes  in    958 ms (9.582071 ms/op 104.361569/sec)
set         100           znodes  in    935 ms (9.359350 ms/op 106.845024/sec)
get         100           znodes  in     25 ms (0.252700 ms/op 3957.263893/sec)
deleted     100 permanent znodes  in    891 ms (8.913291 ms/op 112.192013/sec)
created     100 ephemeral znodes  in    958 ms (9.584489 ms/op 104.335246/sec)
watched     100           znodes  in     25 ms (0.251091 ms/op 3982.627356/sec)
deleted     100 ephemeral znodes  in    891 ms (8.915379 ms/op 112.165730/sec)
notif       100           watches in   1000 ms (10.000508 ms/op 99.994922/sec)
Latency test complete
[phunt@c0309 zk-smoketest-3.3.3]$ cd ../zk-smoketest-trunk/
[phunt@c0309 zk-smoketest-trunk]$ PYTHONPATH=lib.linux-x86_64-2.6/ LD_LIBRARY_PATH=lib.linux-x86_64-2.6/ python26 ./zk-latencies.py --servers ""c0309:2181,c0310:2181,c0311:2181"" --znode_size=100 --znode_count=100 --timeout=5000 --synchronous
Connecting to c0309:2181
Connected in 31 ms, handle is 0
Connecting to c0310:2181
Connected in 16 ms, handle is 1
Connecting to c0311:2181
Connected in 16 ms, handle is 2
Testing latencies on server c0309:2181 using syncronous calls
created     100 permanent znodes  in   5099 ms (50.999281 ms/op 19.608119/sec)
set         100           znodes  in   5066 ms (50.665429 ms/op 19.737324/sec)
get         100           znodes  in   4009 ms (40.093150 ms/op 24.941916/sec)
deleted     100 permanent znodes  in   5040 ms (50.404449 ms/op 19.839519/sec)
created     100 ephemeral znodes  in   5124 ms (51.249170 ms/op 19.512511/sec)
watched     100           znodes  in   4051 ms (40.514441 ms/op 24.682557/sec)
deleted     100 ephemeral znodes  in   5048 ms (50.484939 ms/op 19.807888/sec)
notif       100           watches in   1000 ms (10.004182 ms/op 99.958199/sec)
Testing latencies on server c0310:2181 using syncronous calls
created     100 permanent znodes  in   5115 ms (51.157510 ms/op 19.547472/sec)
set         100           znodes  in   5056 ms (50.568910 ms/op 19.774996/sec)
get         100           znodes  in   4099 ms (40.999382 ms/op 24.390612/sec)
deleted     100 permanent znodes  in   5041 ms (50.418010 ms/op 19.834182/sec)
created     100 ephemeral znodes  in   5083 ms (50.835850 ms/op 19.671157/sec)
watched     100           znodes  in   4100 ms (41.003261 ms/op 24.388304/sec)
deleted     100 ephemeral znodes  in   5058 ms (50.581930 ms/op 19.769906/sec)
notif       100           watches in   1000 ms (10.005081 ms/op 99.949219/sec)
Testing latencies on server c0311:2181 using syncronous calls
created     100 permanent znodes  in   5099 ms (50.992720 ms/op 19.610642/sec)
set         100           znodes  in   5091 ms (50.916569 ms/op 19.639972/sec)
get         100           znodes  in   4099 ms (40.996401 ms/op 24.392385/sec)
deleted     100 permanent znodes  in   5066 ms (50.669601 ms/op 19.735699/sec)
created     100 ephemeral znodes  in   5124 ms (51.249208 ms/op 19.512496/sec)
watched     100           znodes  in   4099 ms (40.999141 ms/op 24.390755/sec)
deleted     100 ephemeral znodes  in   5049 ms (50.498819 ms/op 19.802443/sec)
notif       100           watches in    999 ms (9.997852 ms/op 100.021486/sec)
Latency test complete
{noformat}","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1146,Blocker,Patrick D. Hunt,Fixed,2011-08-14T17:30:58.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,significant regression in client (c/python) performance,2011-11-23T19:22:40.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",2.0
Vishal Kher,[],2011-08-03T22:36:07.000+0000,Eugene Joseph Koontz,"Use the attached repeat.sh to run ObserverTest repeatedly by doing: 

src/repeat.sh ObserverTest

The test will will fail eventually after a few iterations; should be only a few minutes.

The line that fails in the test is: 

zk = new ZooKeeper(""127.0.0.1:"" + CLIENT_PORT_OBS,
                ClientBase.CONNECTION_TIMEOUT, this);

Attached as out.txt is the output showing a successful run, for comparison, followed by a failed run.


Note that in the seconds before the test fails, in the following lines, that there is a 24 second gap in time (between 22:13:02 and 22:13:26):

bq.
[junit] 2011-08-03 22:13:02,167 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11229:ZooKeeperServer@833] - Client attempting to establish new session at /127.0.0.1:46929
[junit] 2011-08-03 22:13:26,003 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11228:Leader@419] - Shutting down
[junit] 2011-08-03 22:13:26,003 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11228:Leader@425] - Shutdown called
[junit] java.lang.Exception: shutdown Leader! reason: Only 0 followers, need 1


","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1145,Blocker,Eugene Joseph Koontz,Duplicate,2011-08-15T01:02:59.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ObserverTest.testObserver fails at particular point after several runs of ant junt.run -Dtestcase=ObserverTest,2011-11-23T19:22:43.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",0.0
Vishal Kher,[],2011-08-03T22:35:49.000+0000,Vishal Kher,"I have found one problem that is causing QuorumPeerMainTest:testQuorum to fail. This test uses 2 ZK servers. 

The test is failing because leader is not starting ZooKeeperServer after leader election. so everything halts.

With the new changes, the server is now started in Leader.processAck() which is called from LeaderHandler. processAck() starts ZooKeeperServer if majority have acked NEWLEADER. The leader puts its ack in the the ackSet in Leader.lead(). Since processAck() is called from LearnerHandler it can happen that the learner's ack is processed before the leader is able to put its ack in the ackSet. When LearnerHandler invokes processAck(), the ackSet for newLeaderProposal will not have quorum (in this case 2). As a result, the ZooKeeperServer is never started on the Leader.

The leader needs to ensure that its ack is put in ackSet before starting LearnerCnxAcceptor or invoke processAck() itself after adding to ackSet. I haven't had time to go through the ZAB2 changes so I am not too familiar with the code. Can Ben/Flavio fix this?","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1144,Blocker,Vishal Kher,Fixed,2011-08-11T18:10:08.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeperServer not starting on leader due to a race condition,2011-11-23T19:22:46.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2011-08-03T00:29:06.000+0000,Patrick D. Hunt,"stat output seems to be missing some end of line:

{noformat}
echo stat |nc c0309 2181
Zookeeper version: 3.4.0--1, built on 08/02/2011 22:25 GMT
Clients:
 /172.29.81.91:33378[0](queued=0,recved=1,sent=0
Latency min/avg/max: 0/28/252
Received: 246844
Sent: 266737
Outstanding: 0
Zxid: 0x4000508c2
Mode: follower
Node count: 4
{noformat}

Multiple clients end up on the same line (missing newline)","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1142,Blocker,Patrick D. Hunt,Fixed,2011-08-11T06:00:40.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,incorrect stat output,2011-11-23T19:22:21.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2011-08-02T23:31:40.000+0000,Patrick D. Hunt,"""ant test"" under python 2.4 is failing due to a small issue in the test code - using a new feature introduced in 2.5.

I have a small patch which addresses this, after which I was able to compile and run the tests successfully under python 2.4.
","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1141,Major,Patrick D. Hunt,Fixed,2011-08-15T00:41:47.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkpython fails tests under python 2.4,2011-11-23T19:22:39.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
Laxman,"[<JIRA Component: name='server', id='12312382'>, <JIRA Component: name='tests', id='12312427'>]",2011-07-29T16:49:37.000+0000,Patrick D. Hunt,"Near the end of QuorumZxidSyncTest there are tons of threads running - 115 ""ProcessThread"" threads, similar numbers of SessionTracker.

Also I see ~100 ReadOnlyRequestProcessor - why is this running as a separate thread? (henry/flavio?)

","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1140,Blocker,Patrick D. Hunt,Fixed,2011-08-30T06:37:27.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,server shutdown is not stopping threads,2011-11-23T19:22:38.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",3.0
Patrick D. Hunt,[],2011-07-27T21:42:05.000+0000,Patrick D. Hunt,"cleanup jenkins report, currently 2 compiler warnings being reported.
","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1139,Minor,Patrick D. Hunt,Fixed,2011-07-28T23:23:57.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"jenkins is reporting two warnings, fix these",2011-11-23T19:22:41.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
Patrick D. Hunt,[],2011-07-27T20:04:23.000+0000,Patrick D. Hunt,"I'm seeing a number of problems in the release audit output for 3.4.0, these must be fixed before 3.4.0 release:

{noformat}
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/contrib/ZooInspector/config/defaultConnectionSettings.cfg
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/contrib/ZooInspector/config/defaultNodeVeiwers.cfg
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/contrib/ZooInspector/licences/epl-v10.html
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/c/Cli.vcproj
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/c/include/winconfig.h
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/c/include/winstdint.h
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/c/zookeeper.sln
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/c/zookeeper.vcproj
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/huebrowser/zkui/src/zkui/static/help/index.html
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/huebrowser/zkui/src/zkui/static/js/package.yml
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/log4j.properties
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/date.format.js
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/g.bar.js
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/g.dot.js
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/g.line.js
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/g.pie.js
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/g.raphael.js
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/raphael.js
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/yui-min.js
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/monitoring/JMX-RESOURCES
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/zooinspector/config/defaultConnectionSettings.cfg
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/zooinspector/config/defaultNodeVeiwers.cfg
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/zooinspector/lib/log4j.properties
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/zooinspector/licences/epl-v10.html
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/java/test/org/apache/zookeeper/MultiTransactionRecordTest.java
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/java/test/org/apache/zookeeper/server/quorum/LearnerTest.java
Lines that start with ????? in the release audit report indicate files that do not have an Apache license header.
{noformat}
","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1138,Blocker,Patrick D. Hunt,Fixed,2011-07-28T22:06:09.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,release audit failing for a number of new files,2011-11-23T19:22:28.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
,"[<JIRA Component: name='leaderElection', id='12312378'>]",2011-07-27T13:02:20.000+0000,Laxman,"AuthFLE is throwing NPE when servers are configured with different election ports.

*Configuration*
{noformat}
server.1 = 10.18.52.25:2888:3888
server.2 = 10.18.52.205:2889:3889
server.3 = 10.18.52.144:2899:3890
{noformat}

*Logs*
{noformat}
2011-07-22 16:06:22,404 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:65170:AuthFastLeaderElection@844] - Election tally
2011-07-22 16:06:29,483 - ERROR [WorkerSender Thread: 6:NIOServerCnxn$Factory$1@81] - Thread Thread[WorkerSender Thread: 6,5,main] died
java.lang.NullPointerException
	at org.apache.zookeeper.server.quorum.AuthFastLeaderElection$Messenger$WorkerSender.process(AuthFastLeaderElection.java:488)
	at org.apache.zookeeper.server.quorum.AuthFastLeaderElection$Messenger$WorkerSender.run(AuthFastLeaderElection.java:432)
	at java.lang.Thread.run(Thread.java:619)
2011-07-22 16:06:29,583 - ERROR [WorkerSender Thread: 1:NIOServerCnxn$Factory$1@81] - Thread Thread[WorkerSender Thread: 1,5,main] died
java.lang.NullPointerException
{noformat}

",[],Bug,ZOOKEEPER-1137,Critical,Laxman,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,AuthFLE is throwing NPE when servers are configured with different election ports.,2012-06-20T22:27:56.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",1.0
Benjamin Reed,[],2011-07-26T16:58:51.000+0000,Benjamin Reed,"the NEW_LEADER message was sent at the beginning of the sync phase in Zab pre1.0, but it must be at the end in Zab 1.0. if the protocol is 1.0 or greater we need to queue rather than send the packet.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1136,Blocker,Benjamin Reed,Fixed,2011-09-14T06:59:35.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,NEW_LEADER should be queued not sent to match the Zab 1.0 protocol on the twiki,2011-11-23T19:22:08.000+0000,[],2.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2011-07-22T22:18:55.000+0000,Patrick D. Hunt,Noticed string comparison using == rather than equals.,"[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1134,Critical,Patrick D. Hunt,Fixed,2011-07-25T21:32:33.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ClientCnxnSocket string comparison using == rather than equals,2011-11-23T19:22:08.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2011-07-22T12:56:55.000+0000,Will Johnson,"See http://markmail.org/thread/vyipodh6ar2b77a3

In addition, this other thread was mentioned as the culprit: http://markmail.org/thread/z5bt4o3quqil7r7t

There still seems to be no way to programmatically test SessionExipredExceptions based on these threads.  I'm not sure if that warrants a separate ticket or not.",[],Bug,ZOOKEEPER-1132,Major,Will Johnson,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper FAQ is out of date wrt testing SessionExpiredException,2011-07-22T12:56:55.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",1.0
,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='server', id='12312382'>]",2011-07-21T19:16:09.000+0000,Alexander Shraer,"Suppose we have 3 servers - A, B, C which have seen the same number of commits. 
- A is the leader and it sends out a new proposal.
- B doesn't receive the proposal, but A and C receive and ACK it
- A commits the proposal, but fails before anyone else sees the commit.
- B and C start leader election. 
- since both B and C saw the same number of commits, if B has a higher server-id than C, leader election will elect B. Then, the last transaction will be truncated from C's log, which is a bug since it was acked by a majority.
  
This happens since servers propose their last committed zxid in leader election, and not their last received / acked zxid (this is not being tracked, AFAIK). See method
FastLeaderElection.getInitLastLoggedZxid(), which calls QuorumPeer.getLastLoggedZxid(), which is supposed to return the last logged Zxid, but instead calls zkDb.getDataTreeLastProcessedZxid() which returns the last committed zxid.",[],Bug,ZOOKEEPER-1131,Major,Alexander Shraer,Not A Problem,2011-07-25T22:28:32.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Transactions can be dropped because leader election uses last committed zxid instead of last acknowledged/received zxid,2011-07-27T18:59:31.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
yynil,"[<JIRA Component: name='recipes', id='12313246'>]",2011-07-19T16:49:56.000+0000,yynil,"http://zookeeper.apache.org/doc/trunk/recipes.html
The current recipe for Lock has the wrong process.
Specifically, for the 
""4. The client calls exists( ) with the watch flag set on the path in the lock directory with the next lowest sequence number.""
It shouldn't be the ""the next lowest sequence number"". It should be the ""current lowest path"". 

If you're gonna use ""the next lowest sequence number"", you'll never wait for the lock possession.

The following is the test code:

{code:title=LockTest.java|borderStyle=solid}
        ACL acl = new ACL(Perms.ALL, new Id(""10.0.0.0/8"", ""1""));
        List<ACL> acls = new ArrayList<ACL>();
        acls.add(acl);
        String connectStr = ""localhost:2181"";
        final Semaphore sem = new Semaphore(0);
        ZooKeeper zooKeeper = new ZooKeeper(connectStr, 1000 * 30, new Watcher() {

            @Override
            public void process(WatchedEvent event) {
                System.out.println(""eventType:"" + event.getType());
                System.out.println(""keeperState:"" + event.getState());
                if (event.getType() == Event.EventType.None) {
                    if (event.getState() == Event.KeeperState.SyncConnected) {
                        sem.release();
                    }
                }
            }
        });
        System.out.println(""state:"" + zooKeeper.getState());
        System.out.println(""Waiting for the state to be connected"");
        try {
            sem.acquire();
        } catch (InterruptedException ex) {
            ex.printStackTrace();
        }
        System.out.println(""Now state:"" + zooKeeper.getState());

        String directory = ""/_locknode_"";
        Stat stat = zooKeeper.exists(directory, false);
        if (stat == null) {
            zooKeeper.create(directory, new byte[]{}, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
        }
        String prefix = directory + ""/lock-"";
        String path = zooKeeper.create(prefix, new byte[]{}, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);
        System.out.println(""Create the path for "" + path);
        while (true) {
            List<String> children = zooKeeper.getChildren(directory, false);
            Collections.sort(children);
            System.out.println(""The whole lock size is "" + children.size());
            String lowestPath = children.get(0);
            DecimalFormat df = new DecimalFormat(""0000000000"");
            String currentSuffix = lowestPath.substring(""lock-"".length());
            System.out.println(""CurrentSuffix is "" + currentSuffix);
            int intIndex = Integer.parseInt(currentSuffix);

            if (path.equals(directory + ""/"" + lowestPath)) {
                //I've got the lock and release it
                System.out.println(""I've got the lock at "" + new Date());
                System.out.println(""next index is "" + intIndex);
                Thread.sleep(10000);
                System.out.println(""After sleep 3 seconds, I'm gonna release the lock"");
                zooKeeper.delete(path, -1);
                break;
            }
            final Semaphore wakeupSem = new Semaphore(0);
            stat = zooKeeper.exists(directory + ""/"" + lowestPath, new Watcher() {

                @Override
                public void process(WatchedEvent event) {
                    System.out.println(""Event is "" + event.getType());
                    System.out.println(""State is "" + event.getState());
                    if (event.getType() == Event.EventType.NodeDeleted) {
                        wakeupSem.release();
                    }
                }
            });
            if (stat != null) {
                System.out.println(""Waiting for the delete of "");
                wakeupSem.acquire();
            } else {
                System.out.println(""Continue to seek"");
            }
        }
{code} ",[],Bug,ZOOKEEPER-1128,Major,yynil,Fixed,2011-07-28T01:21:03.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Recipe wrong for Lock process.,2016-03-03T01:35:29.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",1.0
,"[<JIRA Component: name='c client', id='12312380'>]",2011-07-18T18:50:48.000+0000,Dheeraj Agrawal,"When we get a auth response, every time we process any auth_response, we call ALL the auth completions (might be registered by different add_auth_info calls). we should be calling only the one that the request came from? I guess we dont know for which request the response corresponds to? If the requests are processed in FIFO and response are got in order then may be we can figure out which add_auth info request the response corresponds to.

Also , we never remove entries from the auth_list

Also the logging is misleading. 
<code>
  1206     if (rc) {
   1207         LOG_ERROR((""Authentication scheme %s failed. Connection closed."",
   1208                    zh->auth_h.auth->scheme));
   1209     }
   1210     else {
   1211         LOG_INFO((""Authentication scheme %s succeeded"", zh->auth_h.auth->scheme));
</code>
If there are multiple auth_info in the auth_list , we always print success/failure for ONLY the first one. So if I had two auths for scehmes, ABCD and EFGH and my auth scheme EFGH failed, the logs will still say ABCD failed
",[],Bug,ZOOKEEPER-1127,Critical,Dheeraj Agrawal,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Auth completion are called for every registered auth, and auths are never removed from the auth list. (even after they are processed).",2012-12-26T07:10:12.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",2.0
Dheeraj Agrawal,"[<JIRA Component: name='c client', id='12312380'>]",2011-07-18T18:47:05.000+0000,Dheeraj Agrawal,"In zoo_add_auth, we have following check.
  2954     // [ZOOKEEPER-800] zoo_add_auth should return ZINVALIDSTATE if
   2955     // the connection is closed.
   2956     if (zoo_state(zh) == 0) {
   2957         return ZINVALIDSTATE;

when we do zookeeper_init, the state is initialized to 0 and above we check if state = 0 then throw exception.
There is a race condition where the doIo thread is slow and has not changed the state to CONNECTING, then you end up returning back ZKINVALIDSTATE from zoo_add_auth.
The problem is we use 0 for CLOSED state and UNINITIALIZED state. in case of uninitialized case it should let it go through.
Is this intentional? In java we have the uninitialized state = null. 
If not we can initialize it to some other magic number.
",[],Bug,ZOOKEEPER-1126,Major,Dheeraj Agrawal,Duplicate,2011-07-18T21:19:25.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,state of zk_handle should NOT be initialized to 0 (CLOSED) in zokeeper_init. It should have a not initialized state.,2011-07-18T21:19:25.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",2.0
Vishal Kher,"[<JIRA Component: name='tests', id='12312427'>]",2011-07-13T20:33:23.000+0000,Vishal Kher,"Some of the tests are consistently failing for me and intermittently on hudson.

Posting discussion from mailing list below.

Vishal,
 Can you please open a jira for this and mark it as a blocker for 3.4
release? Looks like its transient:

https://builds.apache.org/job/ZooKeeper-trunk/

The latest build is passing.

thanks
mahadev
- Hide quoted text -

On Mon, Jul 11, 2011 at 12:49 PM, Vishal Kher <vishalmlst@gmail.com> wrote:
> Hi,
>
> ant test-core-java is consistently failing for me.
>
> The error seems to be either:
>
> Testcase: testFollowersStartAfterLeader took 35.577 sec
>    Caused an ERROR
> Did not connect
> java.util.concurrent.TimeoutException: Did not connect
>    at
> org.apache.zookeeper.test.ClientBase$CountdownWatcher.waitForConnected(ClientBase.java:124)
>    at
> org.apache.zookeeper.test.QuorumTest.testFollowersStartAfterLeader(QuorumTest.java:308)
>    at
> org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
>
> or
>
> Testcase: testNoLogBeforeLeaderEstablishment took 8.831 sec
>    Caused an ERROR
> KeeperErrorCode = ConnectionLoss for /blah
> org.apache.zookeeper.KeeperException$ConnectionLossException:
> KeeperErrorCode = ConnectionLoss for /blah
>    at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
>    at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
>    at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:761)
>    at
> org.apache.zookeeper.test.QuorumTest.testNoLogBeforeLeaderEstablishment(QuorumTest.java:385)
>    at
> org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
>
> Looks like the reason why the tests are failing for me is similar to why the
> tests failed on hudson:
>
> 2011-07-11 14:47:26,219 [myid:] - INFO  [QuorumPeer[myid=2]/0.0.0.0:11379
> :Leader@425] - Shutdown called
> java.lang.Exception: shutdown Leader! reason: Only 0 followers, need 1
>    at org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:425)
>    at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:400)
>    at
> org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:729)
> 2011-07-11 14:47:26,220 [myid:] - INFO  [QuorumPeer[myid=2]/0.0.0.0:11379
> :ZooKeeperServer@416] - shutting down
>
> The leader is not able to ping the followers. Has anyone seen this before?
>
> Thanks.
> -Vishal
>
> On Sun, Jul 10, 2011 at 6:52 AM, Apache Jenkins Server <
> jenkins@builds.apache.org> wrote:
>
>> See https://builds.apache.org/job/ZooKeeper-trunk/1239/
>>
>>
>> ###################################################################################
>> ########################## LAST 60 LINES OF THE CONSOLE
>> ###########################
>> [...truncated 242795 lines...]
>>    [junit] 2011-07-10 10:57:16,673 [myid:] - INFO
>>  [main:SessionTrackerImpl@206] - Shutting down
>>    [junit] 2011-07-10 10:57:16,673 [myid:] - INFO
>>  [main:PrepRequestProcessor@702] - Shutting down
>>    [junit] 2011-07-10 10:57:16,674 [myid:] - INFO
>>  [main:SyncRequestProcessor@170] - Shutting down
>>    [junit] 2011-07-10 10:57:16,674 [myid:] - INFO
>>  [SyncThread:0:SyncRequestProcessor@152] - SyncRequestProcessor exited!
>>    [junit] 2011-07-10 10:57:16,675 [myid:] - INFO
>>  [main:FinalRequestProcessor@423] - shutdown of request processor complete
>>    [junit] 2011-07-10 10:57:16,674 [myid:] - INFO  [ProcessThread(sid:0
>> cport:-1)::PrepRequestProcessor@133] - PrepRequestProcessor exited loop!
>>    [junit] 2011-07-10 10:57:16,676 [myid:] - INFO  [main:ClientBase@227] -
>> connecting to 127.0.0.1 11221
>>    [junit] ensureOnly:[]
>>    [junit] 2011-07-10 10:57:16,677 [myid:] - INFO  [main:ClientBase@428] -
>> STARTING server
>>    [junit] 2011-07-10 10:57:16,678 [myid:] - INFO
>>  [main:ZooKeeperServer@164] - Created server with tickTime 3000
>> minSessionTimeout 6000 maxSessionTimeout 60000 datadir
>> /grid/0/hudson/hudson-slave/workspace/ZooKeeper-trunk/trunk/build/test/tmp/test1139867753736175617.junit.dir/version-2
>> snapdir
>> /grid/0/hudson/hudson-slave/workspace/ZooKeeper-trunk/trunk/build/test/tmp/test1139867753736175617.junit.dir/version-2
>>    [junit] 2011-07-10 10:57:16,679 [myid:] - INFO
>>  [main:NIOServerCnxnFactory@94] - binding to port 0.0.0.0/0.0.0.0:11221
>>    [junit] 2011-07-10 10:57:16,680 [myid:] - INFO  [main:FileSnap@83] -
>> Reading snapshot
>> /grid/0/hudson/hudson-slave/workspace/ZooKeeper-trunk/trunk/build/test/tmp/test1139867753736175617.junit.dir/version-2/snapshot.b
>>    [junit] 2011-07-10 10:57:16,683 [myid:] - INFO  [main:FileTxnSnapLog@256]
>> - Snapshotting: b
>>    [junit] 2011-07-10 10:57:16,684 [myid:] - INFO  [main:ClientBase@227] -
>> connecting to 127.0.0.1 11221
>>    [junit] 2011-07-10 10:57:16,685 [myid:] - INFO  [NIOServerCxn.Factory:
>> 0.0.0.0/0.0.0.0:11221:NIOServerCnxnFactory@197] - Accepted socket
>> connection from /127.0.0.1:45122
>>    [junit] 2011-07-10 10:57:16,686 [myid:] - INFO  [NIOServerCxn.Factory:
>> 0.0.0.0/0.0.0.0:11221:NIOServerCnxn@815] - Processing stat command from /
>> 127.0.0.1:45122
>>    [junit] 2011-07-10 10:57:16,686 [myid:] - INFO
>>  [Thread-5:NIOServerCnxn$StatCommand@652] - Stat command output
>>    [junit] 2011-07-10 10:57:16,688 [myid:] - INFO
>>  [Thread-5:NIOServerCnxn@995] - Closed socket connection for client /
>> 127.0.0.1:45122 (no session established for client)
>>    [junit] ensureOnly:[InMemoryDataTree, StandaloneServer_port]
>>    [junit] expect:InMemoryDataTree
>>    [junit] found:InMemoryDataTree
>> org.apache.ZooKeeperService:name0=StandaloneServer_port-1,name1=InMemoryDataTree
>>    [junit] expect:StandaloneServer_port
>>    [junit] found:StandaloneServer_port
>> org.apache.ZooKeeperService:name0=StandaloneServer_port-1
>>    [junit] 2011-07-10 10:57:16,690 [myid:] - INFO
>>  [main:JUnit4ZKTestRunner$LoggedInvokeMethod@57] - FINISHED TEST METHOD
>> testQuota
>>    [junit] 2011-07-10 10:57:16,690 [myid:] - INFO  [main:ClientBase@465] -
>> tearDown starting
>>    [junit] 2011-07-10 10:57:16,754 [myid:] - INFO  [main:ZooKeeper@662] -
>> Session: 0x13113b1aca50000 closed
>>    [junit] 2011-07-10 10:57:16,754 [myid:] - INFO
>>  [main-EventThread:ClientCnxn$EventThread@495] - EventThread shut down
>>    [junit] 2011-07-10 10:57:16,754 [myid:] - INFO  [main:ClientBase@435] -
>> STOPPING server
>>    [junit] 2011-07-10 10:57:16,755 [myid:] - INFO  [NIOServerCxn.Factory:
>> 0.0.0.0/0.0.0.0:11221:NIOServerCnxnFactory@224] - NIOServerCnxn factory
>> exited run method
>>    [junit] 2011-07-10 10:57:16,755 [myid:] - INFO
>>  [main:ZooKeeperServer@416] - shutting down
>>    [junit] 2011-07-10 10:57:16,756 [myid:] - INFO
>>  [main:SessionTrackerImpl@206] - Shutting down
>>    [junit] 2011-07-10 10:57:16,756 [myid:] - INFO
>>  [main:PrepRequestProcessor@702] - Shutting down
>>    [junit] 2011-07-10 10:57:16,757 [myid:] - INFO
>>  [main:SyncRequestProcessor@170] - Shutting down
>>    [junit] 2011-07-10 10:57:16,760 [myid:] - INFO  [ProcessThread(sid:0
>> cport:-1)::PrepRequestProcessor@133] - PrepRequestProcessor exited loop!
>>    [junit] 2011-07-10 10:57:16,762 [myid:] - INFO
>>  [SyncThread:0:SyncRequestProcessor@152] - SyncRequestProcessor exited!
>>    [junit] 2011-07-10 10:57:16,762 [myid:] - INFO
>>  [main:FinalRequestProcessor@423] - shutdown of request processor complete
>>    [junit] 2011-07-10 10:57:16,763 [myid:] - INFO  [main:ClientBase@227] -
>> connecting to 127.0.0.1 11221
>>    [junit] ensureOnly:[]
>>    [junit] 2011-07-10 10:57:16,767 [myid:] - INFO  [main:ClientBase@493] -
>> fdcount after test is: 35 at start it was 24
>>    [junit] 2011-07-10 10:57:16,767 [myid:] - INFO  [main:ClientBase@495] -
>> sleeping for 20 secs
>>    [junit] 2011-07-10 10:57:16,768 [myid:] - INFO  [main:ZKTestCase$1@60]
>> - SUCCEEDED testQuota
>>    [junit] 2011-07-10 10:57:16,768 [myid:] - INFO  [main:ZKTestCase$1@55]
>> - FINISHED testQuota
>>    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.691 sec
>>
>> BUILD FAILED
>> /grid/0/hudson/hudson-slave/workspace/ZooKeeper-trunk/trunk/build.xml:959:
>> The following error occurred while executing this line:
>> /grid/0/hudson/hudson-slave/workspace/ZooKeeper-trunk/trunk/build.xml:870:
>> Tests failed!
>>
>> Total time: 19 minutes 0 seconds
>> [FINDBUGS] Skipping publisher since build result is FAILURE
>> [WARNINGS] Skipping publisher since build result is FAILURE
>> Recording fingerprints
>> Archiving artifacts
>> Recording test results
>> Publishing Javadoc
>> Publishing Clover coverage report...
>> No Clover report will be published due to a Build Failure
>> Email was triggered for: Failure
>> Sending email for trigger: Failure
>>
>>
>>
>>
>> ###################################################################################
>> ############################## FAILED TESTS (if any)
>> ##############################
>> 2 tests failed.
>> REGRESSION:  org.apache.zookeeper.test.ObserverTest.testObserver
>>
>> Error Message:
>> KeeperErrorCode = ConnectionLoss for /obstest
>>
>> Stack Trace:
>> org.apache.zookeeper.KeeperException$ConnectionLossException:
>> KeeperErrorCode = ConnectionLoss for /obstest
>>        at
>> org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
>>        at
>> org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
>>        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:761)
>>        at
>> org.apache.zookeeper.test.ObserverTest.testObserver(ObserverTest.java:101)
>>        at
>> org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
>>
>>
>> REGRESSION:  org.apache.zookeeper.test.ReadOnlyModeTest.testSeekForRwServer
>>
>> Error Message:
>> KeeperErrorCode = ConnectionLoss for /test
>>
>> Stack Trace:
>> org.apache.zookeeper.KeeperException$ConnectionLossException:
>> KeeperErrorCode = ConnectionLoss for /test
>>        at
>> org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
>>        at
>> org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
>>        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:761)
>>        at
>> org.apache.zookeeper.test.ReadOnlyModeTest.testSeekForRwServer(ReadOnlyModeTest.java:213)
>>        at
>> org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1125,Major,Vishal Kher,Not A Problem,2014-05-15T22:53:23.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Intermittent java core test failures,2014-05-15T22:53:23.000+0000,[],3.0
Marshall McMullen,"[<JIRA Component: name='server', id='12312382'>]",2011-07-13T16:17:58.000+0000,Marshall McMullen,"The new Multiop support added under zookeeper-965 fails every single time if the multiop is submitted to a non-leader in quorum mode. In standalone mode it always works properly and this bug only presents itself in quorum mode (with 2 or more nodes). After 12 hours of debugging (*sigh*) it turns out to be a really simple fix. There are a couple of missing case statements inside FollowerRequestProcessor.java and ObserverRequestProcessor.java to ensure that multiop is forwarded to the leader for commit. I've attached a patch that fixes this problem.

It's probably worth nothing that zookeeper-965 has already been committed to trunk. But this is a fatal flaw that will prevent multiop support from working properly and as such needs to get committed to 3.4.0 as well. Is there a way to tie these two cases together in some way?","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1124,Critical,Marshall McMullen,Fixed,2011-07-15T04:51:26.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Multiop submitted to non-leader always fails due to timeout,2011-11-23T19:22:26.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
,"[<JIRA Component: name='c client', id='12312380'>]",2011-07-12T11:03:06.000+0000,Tadeusz Andrzej Kadłubowski,"I have a C app that runs on Solaris and connects to ZooKeeper which I run on Linux (just a single server instance, that's just a development setup).

Upon calling zookeeper_init() I get logs that say connect() call fails. TCP-wise the client sends RST packet instead of the third part of the three-way handshake. Traced client syscalls below.

Sometimes the client is able to establish a connection - after half an hour of trying, or even longer. 

Logs
====

The client logs:

2011-07-11 16:20:22,954:13148(0xf):ZOO_ERROR@handle_socket_error_msg@1501: Socket [10.10.1.71:2181] zk retcode=-4, errno=0(Error 0): connect() call failed

The server logs:

2011-07-11 16:20:22,950 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn$Factory@251] - Accepted socket connection from /10.10.9.27:34017                                                           
2011-07-11 16:20:22,955 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@634] - EndOfStreamException: Unable to read additional data from client sessionid 0x0, likely client has closed socket     
2011-07-11 16:20:22,955 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1435] - Closed socket connection for client /10.10.9.27:34017 (no session established for client)           

Syscalls in the client:

/15:    3516.6191       so_socket(PF_INET, SOCK_STREAM, IPPROTO_IP, """", SOV_DEFAULT) = 11                 
/15:    3516.6192       setsockopt(11, tcp, TCP_NODELAY, 0xFD8A8ECC, 4, SOV_DEFAULT) = 0                  
/15:    3516.6193       fcntl(11, F_GETFL)                              = 2                               
/15:    3516.6194       fcntl(11, F_SETFL, FWRITE|FNONBLOCK)            = 0                               
/15:    3516.6194       connect(11, 0x0813BA30, 16, SOV_DEFAULT)        Err#150 EINPROGRESS               
/15:    3516.6195       write(2, "" 2 0 1 1 - 0 7 - 1 2   1"".., 23)      = 23                              
 <<< SNIP writing log message >>>                             
/15:    3516.6204       write(2, ""\n"", 1)                               = 1                               
/15:    3516.6205       close(11)                                       = 0 


What does work:
===============

Using Java client on the same Solaris machine works without any problems. Connecting to the Linux server using C client library on Linux works OK (though I tested it within one box, via loopback interface).",[],Bug,ZOOKEEPER-1123,Major,Tadeusz Andrzej Kadłubowski,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Can't connect to ZooKeeper server with the C client library from Solaris: connect() call fails.,2012-02-08T19:28:58.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='scripts', id='12312384'>]",2011-07-07T10:33:08.000+0000,Glen Mazza,"Hello, adding the following commented-out dataDir to the zoo.cfg file (keeping the default one provided active):

{noformat}
# the directory where the snapshot is stored.
# dataDir=test123/data
dataDir=/export/crawlspace/mahadev/zookeeper/server1/data
{noformat}

and then running sh zkServer.sh stop is showing that the program is incorrectly reading the commented-out dataDir:

{noformat}
gmazza@gmazza-work:~/dataExt3/apps/zookeeper-3.3.3/bin$ sh zkServer.sh stop
JMX enabled by default
Using config: /media/NewDriveExt3_/apps/zookeeper-3.3.3/bin/../conf/zoo.cfg
Stopping zookeeper ... 
error: could not find file test123/data
/export/crawlspace/mahadev/zookeeper/server1/data/zookeeper_server.pid
gmazza@gmazza-work:~/dataExt3/apps/zookeeper-3.3.3/bin$ 
{noformat}

If I change the commented-out line in zoo.cfg to ""test123456/data"" and run the stop command again I get:
error: could not find file test123456/data

showing that it's incorrectly doing a run-time read of the commented-out lines.  (Difficult to completely confirm, but this problem  doesn't appear to occur with the start command, only the stop one.)
","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1119,Major,Glen Mazza,Fixed,2011-07-25T22:22:04.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkServer stop command incorrectly reading comment lines in zoo.cfg,2011-11-23T19:22:44.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",1.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2011-07-06T02:56:31.000+0000,Kurt Young,"I think there is a bug when Follower try to sync data with Leader.
Assume there are some operations committed during one server had been crashed. When the server restart, it will receive a NEWLEADER packet which include the last zxid of leader and the server will set its own lastProcessZxid to the leader's. 
{code:title=Follower.java|borderStyle=solid}
void followLeader() throws InterruptedException {
    fzk.registerJMX(new FollowerBean(this, zk), self.jmxLocalPeerBean);
    try {
        InetSocketAddress addr = findLeader();
        try {
            connectToLeader(addr);
            long newLeaderZxid = registerWithLeader(Leader.FOLLOWERINFO);  // get the last zxid from leader
            //check to see if the leader zxid is lower than ours                                                                                          
            //this should never happen but is just a safety check                                                                                         
            long lastLoggedZxid = self.getLastLoggedZxid();
            if ((newLeaderZxid >> 32L) < (lastLoggedZxid >> 32L)) {
                LOG.fatal(""Leader epoch "" + Long.toHexString(newLeaderZxid >> 32L)
                        + "" is less than our epoch "" + Long.toHexString(lastLoggedZxid >> 32L));
                throw new IOException(""Error: Epoch of leader is lower"");
            }
            syncWithLeader(newLeaderZxid);   // set its own lastProcessZxid to leader's last zxid
{code}

Then, some COMMIT packets will be received by the server in order to sync the data with leader. And then, the leader will send an UPTODATE packet to server to take a snapshot. 
{code:title=Follower.java|borderStyle=solid}
protected void processPacket(QuorumPacket qp) throws IOException{
    switch (qp.getType()) {
    case Leader.PING:
        ping(qp);
        break;
    case Leader.PROPOSAL:
        TxnHeader hdr = new TxnHeader();
        BinaryInputArchive ia = BinaryInputArchive
        .getArchive(new ByteArrayInputStream(qp.getData()));
        Record txn = SerializeUtils.deserializeTxn(ia, hdr);
        if (hdr.getZxid() != lastQueued + 1) {
            LOG.warn(""Got zxid 0x""
                    + Long.toHexString(hdr.getZxid())
                    + "" expected 0x""
                    + Long.toHexString(lastQueued + 1));
        }
        lastQueued = hdr.getZxid();
        fzk.logRequest(hdr, txn);
        break;
    case Leader.COMMIT:
        fzk.commit(qp.getZxid());
        break;
    case Leader.UPTODATE:
        fzk.takeSnapshot();
        self.cnxnFactory.setZooKeeperServer(fzk);
        break;
    case Leader.REVALIDATE:
        revalidate(qp);
        break;
    case Leader.SYNC:
        fzk.sync();
        break;
    }
}
{code}
Notice the different way the Follower treat the COMMIT and the UPTODATE packets. When receives a COMMIT packet, the follower will give this to a processor to deal with. But if receives a UPTODATE packet, the follower will take a snapshot immediately. So it is possible that the server will take snapshot before it commits all the operations it missed. Then if the server crashed again and recovered， it will recover its data from the snapshot, so the date inconsistent with the leader now, but its last zxid is the same. ",[],Bug,ZOOKEEPER-1118,Critical,Kurt Young,Duplicate,2011-07-06T13:32:55.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Inconsistent data after server crashes several times,2011-07-07T01:11:17.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",0.0
James Page,"[<JIRA Component: name='c client', id='12312380'>]",2011-07-05T14:50:48.000+0000,James Page,"zookeeper 3.3.3 (and 3.3.1) fails to build on Debian and Ubuntu systems with gcc >= 4.6.1:

/bin/bash ./libtool  --tag=CC   --mode=compile gcc -DHAVE_CONFIG_H -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O2 -D_GNU_SOURCE -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c -o zookeeper.lo `test -f 'src/zookeeper.c' || echo './'`src/zookeeper.c
libtool: compile:  gcc -DHAVE_CONFIG_H -I. -I./include -I./tests -I./generated -Wall -Werror -g -O2 -D_GNU_SOURCE -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c src/zookeeper.c  -fPIC -DPIC -o .libs/zookeeper.o
src/zookeeper.c: In function 'getaddrs':
src/zookeeper.c:455:13: error: variable 'port' set but not used [-Werror=unused-but-set-variable]
cc1: all warnings being treated as errors

See http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=625441 for more information.","[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1117,Minor,James Page,Fixed,2011-08-26T07:52:15.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper 3.3.3 fails to build with gcc >= 4.6.1 on Debian/Ubuntu,2011-11-23T19:22:32.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>]",2.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2011-07-01T07:20:06.000+0000,helei,"exception causing shutdownthere are 5 members in the quorum. one follower can not sync with leader after restart. it seems leader has close the data connection with this follower because of read timeout. here is the key log in follower:
{noformat}
2011-06-30 22:14:45,069 - WARN  [Thread-17:QuorumCnxManager$RecvWorker@658] - Connection broken: 
java.nio.channels.ClosedChannelException
        at sun.nio.ch.SocketChannelImpl.ensureReadOpen(SocketChannelImpl.java:113)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:156)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:629)
2011-06-30 22:14:45,069 - INFO  [QuorumPeer:/0.0.0.0:2181:FastLeaderElection@689] - Notification: 3, 17198470148, 3, 3, LOOKING, LOOKING, 3
2011-06-30 22:14:45,070 - ERROR [Thread-16:QuorumCnxManager$SendWorker@559] - Failed to send last message. Shutting down thread.
java.nio.channels.ClosedChannelException
        at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:126)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.send(QuorumCnxManager.java:548)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:557)
2011-06-30 22:14:45,082 - INFO  [QuorumPeer:/0.0.0.0:2181:Learner@282] - Getting a diff from the leader 0x4011bd462
2011-06-30 22:14:45,083 - WARN  [Thread-18:QuorumCnxManager$SendWorker@589] - Send worker leaving thread
2011-06-30 22:14:45,085 - WARN  [QuorumPeer:/0.0.0.0:2181:Follower@116] - Got zxid 0x4011bd405 expected 0x1
2011-06-30 22:14:45,090 - INFO  [QuorumPeer:/0.0.0.0:2181:FileTxnSnapLog@208] - Snapshotting: 4011bd462
2011-06-30 22:14:53,397 - WARN  [SyncThread:3:SendAckRequestProcessor@63] - Closing connection to leader, exception during packet send
java.net.SocketException: Broken pipe
        at java.net.SocketOutputStream.socketWrite0(Native Method)
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at org.apache.zookeeper.server.quorum.Learner.writePacket(Learner.java:126)
        at org.apache.zookeeper.server.quorum.SendAckRequestProcessor.flush(SendAckRequestProcessor.java:61)
        at org.apache.zookeeper.server.SyncRequestProcessor.flush(SyncRequestProcessor.java:164)
        at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:98)
2011-06-30 22:14:53,398 - WARN  [QuorumPeer:/0.0.0.0:2181:Follower@82] - Exception when following the leader
java.net.SocketException: Socket closed
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:99)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at org.apache.zookeeper.server.quorum.Learner.writePacket(Learner.java:126)
        at org.apache.zookeeper.server.quorum.Learner.ping(Learner.java:358)
        at org.apache.zookeeper.server.quorum.Follower.processPacket(Follower.java:108)
        at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:79)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:634)
2011-06-30 22:14:53,398 - WARN  [SyncThread:3:SendAckRequestProcessor@63] - Closing connection to leader, exception during packet send
java.net.SocketException: Socket closed
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:99)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at org.apache.zookeeper.server.quorum.Learner.writePacket(Learner.java:126)
        at org.apache.zookeeper.server.quorum.SendAckRequestProcessor.flush(SendAckRequestProcessor.java:61)
        at org.apache.zookeeper.server.SyncRequestProcessor.flush(SyncRequestProcessor.java:164)
        at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:98)
2011-06-30 22:14:53,399 - INFO  [QuorumPeer:/0.0.0.0:2181:Follower@166] - shutdown called
java.lang.Exception: shutdown Follower
        at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:166)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:638)
and these are the leader's:
2011-06-30 22:14:35,943 - ERROR [LearnerHandler-/10.23.247.163:14975:LearnerHandler@444] - Unexpected exception causing shutdown while sock still open
java.net.SocketTimeoutException: Read timed out
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
        at java.io.DataInputStream.readInt(DataInputStream.java:370)
        at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
        at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:84)
        at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:108)
        at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:358)
2011-06-30 22:14:35,943 - WARN  [LearnerHandler-/10.23.247.163:14975:LearnerHandler@457] - ******* GOODBYE /10.23.247.163:14975 ********
2011-06-30 22:14:48,943 - ERROR [CommitProcessor:4:NIOServerCnxn@422] - Unexpected Exception: 
java.nio.channels.CancelledKeyException
        at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:55)
        at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:59)
        at org.apache.zookeeper.server.NIOServerCnxn.sendBuffer(NIOServerCnxn.java:395)
        at org.apache.zookeeper.server.NIOServerCnxn.sendResponse(NIOServerCnxn.java:1360)
        at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:367)
        at org.apache.zookeeper.server.quorum.Leader$ToBeAppliedRequestProcessor.processRequest(Leader.java:535)
        at org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:73)
2011-06-30 22:14:49,084 - ERROR [LearnerHandler-/10.23.247.163:14998:LearnerHandler@444] - Unexpected exception causing shutdown while sock still open
java.net.SocketTimeoutException: Read timed out
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
        at java.io.DataInputStream.readInt(DataInputStream.java:370)
        at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
        at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:84)
        at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:108)
        at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:358)
2011-06-30 22:14:49,084 - WARN  [LearnerHandler-/10.23.247.163:14998:LearnerHandler@457] - ******* GOODBYE /10.23.247.163:14998 ********
{noformat}",[],Bug,ZOOKEEPER-1115,Critical,helei,Not A Problem,2013-10-10T20:38:03.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,follower can not sync with leader,2013-10-22T03:06:06.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>, <JIRA Version: name='3.3.3', id='12315482'>]",6.0
Ivan Kelly,[],2011-06-30T08:51:00.000+0000,Ivan Kelly,"As stated in the title, org.apache.zookeeper.test.JMXEnv uses System.err.println to output traces. This makes for a lot of noise on the console when you run the tests. It has a logging object already, so it should use that instead.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1111,Major,Ivan Kelly,Fixed,2011-07-19T21:39:23.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,JMXEnv uses System.err instead of logging,2011-11-23T19:22:07.000+0000,[],1.0
,"[<JIRA Component: name='c client', id='12312380'>]",2011-06-29T05:30:27.000+0000,xiliu,"The correct step about close client is the client send CLOSE_OP to the server, wait for several seconds, the server will process the terminal request and close the fd.
But the zookeeper_close interface is wrong, because the adaptor_send_queue(zh, 3000) (line 2332), will first wait the timeout then send the request.
The right order is first send the request then wait the timeout. I change as follow:
$svn diff src/c/src/zookeeper.c
Index: src/c/src/zookeeper.c
===================================================================
--- src/c/src/zookeeper.c       (revision 1140451)
+++ src/c/src/zookeeper.c       (working copy)
@@ -2329,7 +2329,8 @@

         /* make sure the close request is sent; we set timeout to an arbitrary
          * (but reasonable) number of milliseconds since we want the call to block*/
-        rc=adaptor_send_queue(zh, 3000);
+        rc=adaptor_send_queue(zh, 0);
+        sleep(3);
     }else{
         LOG_INFO((""Freeing zookeeper resources for sessionId=%#llx\n"",
                 zh->client_id.client_id));
 ",[],Bug,ZOOKEEPER-1110,Major,xiliu,Invalid,2012-04-25T07:55:32.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,c interface zookeeper_close close fd too quickly.,2012-07-29T05:37:26.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",1.0
Laxman,"[<JIRA Component: name='quorum', id='12312379'>]",2011-06-24T04:48:18.000+0000,Laxman,"*Problem* Zookeeper is not shut down completely when dataDir disk space is full and ZK Cluster went into unserviceable state.
 

*Scenario*
If the leader zookeeper disk is made full, the zookeeper is trying to shutdown. But it is waiting indefinitely while shutting down the SyncRequestProcessor thread.

*Root Cause* 
this.join() is invoked in the same thread where System.exit(11) has been triggered.

When disk space full happens, It got the exception as follows 'No space left on device' and invoked System.exit(11) from the SyncRequestProcessor thread(The following logs shows the same). Before exiting JVM, ZK will execute the ShutdownHook of QuorumPeerMain and the flow comes to SyncRequestProcessor.shutdown(). Here this.join() is invoked in the same thread where System.exit(11) has been invoked.
","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1109,Critical,Laxman,Fixed,2011-07-25T21:01:11.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Zookeeper service is down when SyncRequestProcessor meets any exception.,2011-11-23T19:22:24.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>, <JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.3.3', id='12315482'>]",4.0
Dheeraj Agrawal,"[<JIRA Component: name='c client', id='12312380'>]",2011-06-23T21:02:34.000+0000,Dheeraj Agrawal,"3 issues:
In zoo_add_auth: there is a race condition:
   2940     // [ZOOKEEPER-800] zoo_add_auth should return ZINVALIDSTATE if
   2941     // the connection is closed.
   2942     if (zoo_state(zh) == 0) {
   2943         return ZINVALIDSTATE;
   2944     }
when we do zookeeper_init, the state is initialized to 0 and above we check if state = 0 then throw exception.
There is a race condition where the doIo thread is slow and has not changed the state to CONNECTING, then you end up returning back ZKINVALIDSTATE.
The problem is we use 0 for CLOSED state and UNINITIALIZED state. in case of uninitialized case it should let it go through.

2nd issue:

Another Bug: in send_auth_info, the check is not correct

while (auth->next != NULL) { //--BUG: in cases where there is only one auth in the list, this will never send that auth, as its next will be NULL 
   rc = send_info_packet(zh, auth); 
   auth = auth->next; 
}

FIX IS:
do { 
  rc = send_info_packet(zh, auth); 
  auth = auth->next; 
 } while (auth != NULL); //this will make sure that even if there is one auth ,that will get sent.

3rd issue:
   2965     add_last_auth(&zh->auth_h, authinfo);
   2966     zoo_unlock_auth(zh);
   2967
   2968     if(zh->state == ZOO_CONNECTED_STATE || zh->state == ZOO_ASSOCIATING_STATE)
   2969         return send_last_auth_info(zh);

if it is connected, we only send the last_auth_info, which may be different than the one we added, as we unlocked it before sending it.

","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1108,Blocker,Dheeraj Agrawal,Fixed,2011-09-09T02:27:39.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Various bugs in zoo_add_auth in C,2011-11-23T19:22:45.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",6.0
zhang yafei,"[<JIRA Component: name='c client', id='12312380'>]",2011-06-23T06:15:34.000+0000,jiang guangran,"in deserialize_CreateResponse
   rc = rc ? : in->deserialize_String(in, ""path"", &v->path);
   in deserialize_String
      len = -1
      so v->path is uninitialised, and free, so core

do_io thread
#0  0x00000039fb030265 in raise () from /lib64/libc.so.6
#1  0x00000039fb031d10 in abort () from /lib64/libc.so.6
#2  0x00000039fb06a84b in __libc_message () from /lib64/libc.so.6
#3  0x00000039fb0722ef in _int_free () from /lib64/libc.so.6
#4  0x00000039fb07273b in free () from /lib64/libc.so.6
#5  0x00002b0afd755dd1 in deallocate_String (s=0x5a490f40) at src/recordio.c:29
#6  0x00002b0afd754ade in zookeeper_process (zh=0x131e3870, events=<value optimized out>) at src/zookeeper.c:2071
#7  0x00002b0afd75b2ef in do_io (v=<value optimized out>) at src/mt_adaptor.c:310
#8  0x00000039fb8064a7 in start_thread () from /lib64/libpthread.so.0
#9  0x00000039fb0d3c2d in clone () from /lib64/libc.so.6

create_node thread
#0  0x00000039fb80ab99 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00002b0afd75af5c in wait_sync_completion (sc=0x131e4c90) at src/mt_adaptor.c:82
#2  0x00002b0afd751750 in zoo_create (zh=0x131e3870, path=0x13206fa8 ""/jsq/zr2/hb/10.250.8.139:8102"", 
    value=0x131e86a8 ""\n\021\061\060.250.8.139:8102\022\035/home/shaoqiang/workdir2/qrs/\030\001 \001*%\n\020\n"", 
    valuelen=102, acl=0x2b0afd961700, flags=1, path_buffer=0x0, path_buffer_len=0) at src/zookeeper.c:3028
",[],Bug,ZOOKEEPER-1106,Major,jiang guangran,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,mt c client core  when create node,2016-03-18T17:36:29.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",0.0
Mate Szalay-Beko,"[<JIRA Component: name='c client', id='12312380'>]",2011-06-23T06:05:56.000+0000,jiang guangran,"in zookeeper_close function,  do adaptor_finish before send CLOSE_OP request to server
so the CLOSE_OP request can not be sent to server

in server zookeeper.log have many
2011-06-22 00:23:02,323 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@634] - EndOfStreamException: Unable to read additional data from client sessionid 0x1305970d66d2224, likely client has closed socket
2011-06-22 00:23:02,324 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1435] - Closed socket connection for client /10.250.8.123:60257 which had sessionid 0x1305970d66d2224
2011-06-22 00:23:02,325 - ERROR [CommitProcessor:1:NIOServerCnxn@445] - Unexpected Exception:
java.nio.channels.CancelledKeyException
        at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:55)
        at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:59)
        at org.apache.zookeeper.server.NIOServerCnxn.sendBuffer(NIOServerCnxn.java:418)
        at org.apache.zookeeper.server.NIOServerCnxn.sendResponse(NIOServerCnxn.java:1509)
        at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:367)
        at org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:73)

and java client not have this problem","[<JIRA Version: name='3.6.0', id='12326518'>, <JIRA Version: name='3.7.0', id='12346617'>, <JIRA Version: name='3.5.7', id='12346098'>]",Bug,ZOOKEEPER-1105,Major,jiang guangran,Fixed,2020-02-05T08:33:41.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,c client zookeeper_close not send CLOSE_OP request to server,2020-02-14T15:23:34.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.3', id='12319288'>]",15.0
,[],2011-06-21T17:53:13.000+0000,Ted Dunning,"The new multi operation is undocumented as yet.  Clearly it needs some doc to cover:

1) the basic syntax

2) java code sample

3) C code sample
",[],Bug,ZOOKEEPER-1102,Major,Ted Dunning,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Need update for programmer manual to cover multi operation,2011-08-08T18:05:57.000+0000,[],0.0
Patrick D. Hunt,[],2011-06-21T17:15:16.000+0000,Ivan Kelly,"These are generated by ant package since ZOOKEEPER-1042, they just need to be pushed to a maven repo. Bookkeeper requires this package to build.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1101,Major,Ivan Kelly,Fixed,2011-08-01T18:31:57.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Upload zookeeper-test maven artifacts to maven repository.,2011-11-23T19:22:28.000+0000,[],1.0
Camille Fournier,"[<JIRA Component: name='java client', id='12312381'>]",2011-06-21T09:24:18.000+0000,Gunnar Wagenknecht,"After investigating an issues with [hanging threads|http://mail-archives.apache.org/mod_mbox/zookeeper-user/201106.mbox/%3Citpgb6$2mi$1@dough.gmane.org%3E] I noticed that any java.lang.Error might silently kill the SendThread. Without a SendThread any thread that wants to send something will hang forever. 

Currently nobody will recognize a SendThread that died. I think at least a state should be flipped (or flag should be set) that causes all further send attempts to fail or to re-spin the connection loop.
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1100,Major,Gunnar Wagenknecht,Fixed,2011-12-26T15:56:09.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Killed (or missing) SendThread will cause hanging threads,2016-03-03T01:37:24.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",5.0
Camille Fournier,"[<JIRA Component: name='server', id='12312382'>]",2011-06-16T14:07:44.000+0000,Camille Fournier,traverseNode in DataTree will never actually traverse the limit nodes properly.,"[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1097,Blocker,Camille Fournier,Fixed,2011-06-26T23:30:59.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Quota is not correctly rehydrated on snapshot reload,2011-11-23T19:22:06.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>]",1.0
Camille Fournier,"[<JIRA Component: name='server', id='12312382'>]",2011-06-13T20:05:51.000+0000,Camille Fournier,"/testing has quota on bytes but not node count. Count quota will always fire because it is set to -1 and will always fail comparison.

2011-06-13 16:01:53,492 - WARN  [CommitProcessor:3:DataTree@373] - Quota exceeded: /testing count=4 limit=-1

",[],Bug,ZOOKEEPER-1093,Major,Camille Fournier,Duplicate,2011-07-19T23:55:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZooKeeper quotas will always trigger if set on one criteria but not the other ,2011-07-19T23:55:56.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>]",0.0
,"[<JIRA Component: name='java client', id='12312381'>]",2011-06-10T03:21:23.000+0000,zhangyouming,"if the chrootPath of ClientCnxn is not null and the Watches of zooKeeper is not null; and then for some reason(like zookeeper server stop and start), the zookeeper client will primeConnection to server again and tell server the watcher path,but the path is wrong,it show be serverpath but not clientpath;if the wrong watcher clientPath is sended to server,
the exception will occurr, the exceptions:

2011-06-10 04:33:16,935 [pool-2-thread-30-SendThread(DB1-6:2181)] WARN  org.apache.zookeeper.ClientCnxn - Session 0x5302c4403a30232 for server DB1-6/192.168.1.6:2181, unexpected error, closing socket connection and attempting reconnect
java.lang.StringIndexOutOfBoundsException: String index out of range: -6
	at java.lang.String.substring(String.java:1937)
	at java.lang.String.substring(String.java:1904)
	at org.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:794)
	at org.apache.zookeeper.ClientCnxn$SendThread.doIO(ClientCnxn.java:881)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1130)
 ","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1091,Critical,zhangyouming,Duplicate,2011-10-16T18:21:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"when the chrootPath of ClientCnxn is not null and the Watches of zooKeeper is not null and the method primeConnection(SelectionKey k) of ClientCnxn Occurred again for some reason ,then the wrong watcher clientPath is sended to server",2011-11-23T19:22:30.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",3.0
Vishal Kher,"[<JIRA Component: name='server', id='12312382'>]",2011-06-09T14:24:06.000+0000,Vishal Kher,"I think I have found a bug in the snapshot mechanism.

The problem occurs because dt.lastProcessedZxid is not synchronized (or rather set before the data tree is modified):

FileTxnSnapLog:
{code}
    public void save(DataTree dataTree,
            ConcurrentHashMap<Long, Integer> sessionsWithTimeouts)
        throws IOException {
        long lastZxid = dataTree.lastProcessedZxid;
        LOG.info(""Snapshotting: "" + Long.toHexString(lastZxid));
        File snapshot=new File(
                snapDir, Util.makeSnapshotName(lastZxid));
        snapLog.serialize(dataTree, sessionsWithTimeouts, snapshot);   <=== the Datatree may not have the modification for lastProcessedZxid
    }
{code}

DataTree:
{code}
    public ProcessTxnResult processTxn(TxnHeader header, Record txn) {
        ProcessTxnResult rc = new ProcessTxnResult();

        String debug = """";
        try {
            rc.clientId = header.getClientId();
            rc.cxid = header.getCxid();
            rc.zxid = header.getZxid();
            rc.type = header.getType();
            rc.err = 0;
            if (rc.zxid > lastProcessedZxid) {
                lastProcessedZxid = rc.zxid;
            }
            [...modify data tree...]           
 }
{code}
The lastProcessedZxid must be set after the modification is done.

As a result, if server crashes after taking the snapshot (and the snapshot does not contain change corresponding to lastProcessedZxid) restore will not restore the data tree correctly:
{code}
public long restore(DataTree dt, Map<Long, Integer> sessions,
            PlayBackListener listener) throws IOException {
        snapLog.deserialize(dt, sessions);
        FileTxnLog txnLog = new FileTxnLog(dataDir);
        TxnIterator itr = txnLog.read(dt.lastProcessedZxid+1); <=== Assumes lastProcessedZxid is deserialized
 }
{code}


I have had offline discussion with Ben and Camille on this. I will be posting the discussion shortly.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1090,Critical,Vishal Kher,Fixed,2011-07-28T05:50:38.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Race condition while taking snapshot can lead to not restoring data tree correctly,2011-11-23T19:22:34.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",4.0
Roman Shaposhnik,"[<JIRA Component: name='scripts', id='12312384'>]",2011-06-09T13:57:16.000+0000,William Au,"The nc command used by zkServer.sh does not have the ""-q"" option on some linux versions ( I have checked RedHat/Fedora and FreeBSD).","[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.3.5', id='12319081'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1089,Major,William Au,Fixed,2011-12-28T06:08:29.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zkServer.sh status does not work due to invalid option of nc,2011-12-28T10:58:21.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>]",4.0
Camille Fournier,"[<JIRA Component: name='server', id='12312382'>]",2011-06-08T20:04:14.000+0000,Camille Fournier,"sequota -b 1000 /testing
delquota -b /testing
setquota -n 1024 /testing
Command failed: java.lang.IllegalArgumentException: /testing has a parent /zookeeper/quota/testing which has a quota
",[],Bug,ZOOKEEPER-1088,Major,Camille Fournier,Won't Fix,2011-06-13T20:04:16.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,delQuota does not remove the quota node and subesquent setquota calls for that path will fail,2011-11-17T06:05:53.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",1.0
Nate Putnam,"[<JIRA Component: name='scripts', id='12312384'>]",2011-06-06T20:09:54.000+0000,Ankit Patel,"Cannot use forceSync=no to asynchronously write transaction logs. This is a critical bug, please address it ASAP. More details:

The class org.apache.zookeeper.server.persistence.FileTxnLog initializes forceSync property in a static block. However, the static variable is defined after the static block with a default value of true. Therefore, the value of the variable can never be false. Please move the declaration of the variable before the static block.","[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1087,Blocker,Ankit Patel,Fixed,2011-06-21T05:34:27.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"ForceSync VM arguement not working when set to ""no""",2011-11-23T19:22:19.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",2.0
Ivan Kelly,[],2011-06-01T10:52:19.000+0000,Ivan Kelly,"The zookeeper test jar, (zookeeper-<version>-test.jar) depends on accessive.jar which is not available in maven. This is problematic for projects using the test jar (i.e. hedwig). ","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1086,Major,Ivan Kelly,Fixed,2011-10-19T06:56:15.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper test jar has non mavenised dependency.,2011-11-23T19:22:07.000+0000,[],2.0
Ivan Kelly,[],2011-05-31T16:23:44.000+0000,Ivan Kelly,See title.,"[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1083,Major,Ivan Kelly,Fixed,2011-06-13T17:25:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Javadoc for WatchedEvent not being generated,2011-11-23T19:22:01.000+0000,[],1.0
Chris Nauroth,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='c client', id='12312380'>]",2011-05-26T08:35:03.000+0000,Tadeusz Andrzej Kadłubowski,"Hello,

Some minor trouble with building ZooKeeper C client library on Sun^H^H^HOracle Solaris 5.10.

1. You need to link against ""-lnsl -lsocket""

2. ctime_r needs a buffer size. The signature is: ""char *ctime_r(const time_t *clock, char *buf, int buflen)""

3. In zk_log.c you need to manually cast pid_t to int (-Werror can be cumbersome ;) )

4. getpwuid_r()returns pointer to struct passwd, which works as the last parameter on Linux.

Solaris signature: struct passwd *getpwuid_r(uid_t  uid,  struct  passwd  *pwd, char *buffer, int  buflen); 
Linux signature: int getpwuid_r(uid_t uid, struct passwd *pwd, char *buf, size_t buflen, struct passwd **result);
","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-1077,Critical,Tadeusz Andrzej Kadłubowski,Fixed,2015-05-18T07:39:35.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,C client lib doesn't build on Solaris,2016-07-21T20:18:40.000+0000,"[<JIRA Version: name='3.3.4', id='12316276'>]",7.0
Patrick D. Hunt,"[<JIRA Component: name='tests', id='12312427'>]",2011-05-25T22:19:48.000+0000,Patrick D. Hunt,"Some tests are unnecessarily extending QuorumBase. Typically this is not a big issue, but it may cause more servers than necessary to be started (harder to debug a failing test in particular).
","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1076,Minor,Patrick D. Hunt,Fixed,2011-07-29T08:14:46.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,some quorum tests are unnecessarily extending QuorumBase,2011-11-23T19:22:29.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
,"[<JIRA Component: name='leaderElection', id='12312378'>]",2011-05-25T21:39:54.000+0000,Vishal Kathuria,"Here is the sequence of steps that reproduces the problem.
On a 3 server ensemble,
1. Bring up two servers (say 1 and 2). Lets say 1 is leading.
2. Bring down 2
3. Bring up 2. 
4. 2 gets a notification from 1 that it is leading but 2 doesn't accept it as a leader since it cannot find one other node that thinks 1 is the leader.


So the ensemble gets stuck where 2 isn't following. If at this point, 3 comes up, then one of 2 & 3 will become a leader and 1 will keep thinking it is the leader.


I am working on a patch to fix this issue.",[],Bug,ZOOKEEPER-1075,Major,Vishal Kathuria,Not A Problem,2011-05-26T18:19:02.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Zookeeper Server cannot join an existing ensemble if the existing ensemble doesn't already have a quorum,2011-05-27T07:07:02.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",4.0
Patrick D. Hunt,"[<JIRA Component: name='scripts', id='12312384'>]",2011-05-25T18:27:39.000+0000,Patrick D. Hunt,"zkServer.sh is missing nohup and ""sleep 1"" when starting the background daemon.

This is fine normally, however when running the server remotely via ssh this causes the process to not run successfully (it starts but immediately exits).

I'll be submitting a patch for this shortly.
","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1074,Major,Patrick D. Hunt,Fixed,2011-06-27T05:04:09.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"zkServer.sh is missing nohup/sleep, which are necessary for remote invocation",2011-11-23T19:22:12.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2011-05-25T17:52:08.000+0000,Patrick D. Hunt,"ZOOKEEPER-1030 updated the generated docs, not the source docs. I'll submit a patch to address in the src.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1073,Minor,Patrick D. Hunt,Fixed,2011-07-07T07:32:33.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,address a documentation issue in ZOOKEEPER-1030,2011-11-23T19:22:24.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
,"[<JIRA Component: name='scripts', id='12312384'>]",2011-05-24T17:20:56.000+0000,Roman Shaposhnik,"If one repeatedly invokes:

{noformat}
/usr/lib/zookeeper/bin/zkServer.sh start
{noformat}

after the initial start 2 bad things happen:

1. ZK reports that it got started where in reality it failed with the following:
{noformat}
2011-05-24 10:18:58,217 - INFO  [main:NIOServerCnxn$Factory@143] - binding to port 0.0.0.0/0.0.0.0:2181
2011-05-24 10:18:58,219 - FATAL [main:ZooKeeperServerMain@62] - Unexpected exception, exiting abnormally
java.net.BindException: Address already in use
{noformat}

2. It clobbers the zookeeper_server.pid file",[],Bug,ZOOKEEPER-1071,Major,Roman Shaposhnik,Duplicate,2011-05-26T17:48:36.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zkServer.sh script needs to track whether ZK is already running or not,2011-05-26T17:48:36.000+0000,[],0.0
Vishal Kher,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2011-05-23T23:53:31.000+0000,Jeremy Stribling,"I've only seen this happen once.  In order to restart Zookeeper with a new set of servers, we have a wrapper class that calls shutdown() on an existing QuorumPeer, and then starts a new one with a new set of servers.  Specifically, our shutdown code looks like this:

{code}
  synchronized(_quorum_peer) {
    _quorum_peer.shutdown();
    FastLeaderElection fle = (FastLeaderElection) _quorum_peer.getElectionAlg();
    fle.shutdown();  // I think this is unnecessary
    try {
      _quorum_peer.getTxnFactory().commit();
    } catch (java.nio.channels.ClosedChannelException e) {
      // ignore
    }
  }
{code}

One time, our wrapper class started one QuorumPeer, and then had to shut it down and start a new one very soon after the QuorumPeer transitioned into a FOLLOWING state.  When the new QuorumPeer tried to read in the latest log from disk, it encountered a bogus magic number of all zeroes:

{noformat}
2011-05-18 22:42:29,823 10467 [pool-1-thread-2] FATAL org.apache.zookeeper.server.quorum.QuorumPeer  - Unable to load database on disk
java.io.IOException: Transaction log: /var/cloudnet/data/zookeeper/version-2/log.700000001 has invalid magic number 0 != 1514884167
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.inStreamCreated(FileTxnLog.java:510)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.createInputArchive(FileTxnLog.java:527)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.goToNextLog(FileTxnLog.java:493)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:576)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.init(FileTxnLog.java:479)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.<init>(FileTxnLog.java:454)
        at org.apache.zookeeper.server.persistence.FileTxnLog.read(FileTxnLog.java:325)
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:126)
        at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:222)
        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:398)
...
2011-05-18 22:42:29,823 10467 [pool-1-thread-2] ERROR com.nicira.onix.zookeeper.Zookeeper  - Unexpected exception
java.lang.RuntimeException: Unable to run quorum server 
        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:401)
        at com.nicira.onix.zookeeper.Zookeeper.StartZookeeper(Zookeeper.java:198)
        at com.nicira.onix.zookeeper.Zookeeper.RestartZookeeper(Zookeeper.java:277)
        at com.nicira.onix.zookeeper.ZKRPCService.setServers(ZKRPC.java:83)
        at com.nicira.onix.zookeeper.Zkrpc$ZKRPCService.callMethod(Zkrpc.java:8198)
        at com.nicira.onix.rpc.RPC$10.run(RPC.java:534)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Transaction log: /var/cloudnet/data/zookeeper/version-2/log.700000001 has invalid magic number 0 != 1514884167
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.inStreamCreated(FileTxnLog.java:510)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.createInputArchive(FileTxnLog.java:527)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.goToNextLog(FileTxnLog.java:493)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:576)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.init(FileTxnLog.java:479)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.<init>(FileTxnLog.java:454)
        at org.apache.zookeeper.server.persistence.FileTxnLog.read(FileTxnLog.java:325)
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:126)
        at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:222)
        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:398)
        ... 8 more
{noformat}

I looked into the code a bit, and I believe the problem comes from the fact that QuorumPeer.shutdown() does not join() on this before returning.  Here's the scenario I think can happen:

# QuorumPeer.run() notices it is in the FOLLOWING state, makes a new Follower, and calls Follower.followLeader(), which starts connecting to the leader.
# In the main program thread, QuorumPeer.shutdown() is called.
# Through a complicated series of calls, this eventually leads to FollowerZooKeeperServer.shutdown() being called.
# This method calls SyncRequestProcess.shutdown(), which joins on this and returns.  However, it's possible that the SyncRequestProcessor thread hasn't yet been started because followLeader() hasn't yet called Learner.syncWithLeader(), which hasn't yet called ZooKeeperServer.startup(), which actually starts the thread.  Thus, the join would have no request, though a requestOfDeath is added to the queued requests list (possibly behind other requests).
# Back in the main thread, FileTxnSnapLog.commit() is called, which doesn't do much because the processor hasn't processed anything yet.
# Finally, ZooKeeperServer.startup is called in the QuorumPeer.run() thread, starting up the SyncRequestProcessor thread.
# That thread appends some request to the log.  The log doesn't exist yet, so it creates a new one, padding it with zeroes.
# Now either the SyncRequestProcessor hits the requestOfDeath or the whole QuorumPeer object is deleted.  It exits that thread without ever committing the log to disk (or the new QuorumPeer tries to read the log before the old thread gets to commit anything), and the log ends up with all zeroes instead of a proper magic number.

I haven't yet looked into whether there's an easy way to join() on the QuorumPeer thread from shutdown(), so that it won't go on to start the processor threads after it's been shutdown.  I wanted to check with the group first and see if anyone else agrees this could be a problem.

I marked this as minor since I think almost no one else uses Zookeeper this way, but it's pretty important to me personally.

I will upload a log file showing this behavior shortly.","[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1069,Critical,Jeremy Stribling,Fixed,2011-07-17T14:36:48.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Calling shutdown() on a QuorumPeer too quickly can lead to a corrupt log,2011-11-23T19:22:21.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",2.0
Roman Shaposhnik,"[<JIRA Component: name='documentation', id='12312422'>, <JIRA Component: name='scripts', id='12312384'>]",2011-05-23T23:08:12.000+0000,Roman Shaposhnik,"Documentation and default config suggest /var/zookeeper as a value for dataDir. This practice is, strictly speaking, incompatible with UNIX/Linux filesystem layout standards (e.g. http://www.s-gms.ms.edus.si/cgi-bin/man-cgi?filesystem+5 , http://tldp.org/LDP/Linux-Filesystem-Hierarchy/html/index.html  ). 

Even though Zookeeper use is not limited to UNIX-like OSes I'd recommend that we change references to /var/zookeeper to /var/lib/zookeeper","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1068,Minor,Roman Shaposhnik,Fixed,2011-06-21T17:24:41.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Documentation and default config suggest incorrect location for Zookeeper state,2011-11-23T19:22:45.000+0000,[],1.0
,"[<JIRA Component: name='c client', id='12312380'>]",2011-05-21T21:09:14.000+0000,Ted Dunning,"per the code review on ZOOKEEPER-965 it seems that multi should have an asynchronous version.

The semantics should be essentially identical.  The only difference is that the original caller shouldn't wait for the result.  Cloning existing multi-operations should be a decent implementation strategy.",[],Bug,ZOOKEEPER-1066,Major,Ted Dunning,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Multi should have an async version,2013-10-10T17:24:12.000+0000,[],2.0
,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2011-05-20T06:46:18.000+0000,Gunnar Wagenknecht,"I have an application that uses ZooKeeper. There is an ensemble in
production. But in order to simplify development the application will
start an embedded ZooKeeper server when started in development mode. We
are experiencing a timing issue with ZooKeeper 3.3.3 and I was wondering
if this is allowed to be happen or if we did something wrong when
starting the embedded server.


Basically, we have a watch registered using an #exists call and watch
code like the following.
{code}
@Override
public void process(final WatchedEvent event) {
  switch (event.getType()) {
    ...
    case NodeCreated:
      pathCreated(event.getPath());
      break;
    ...
  }
}

@Override
protected void pathCreated(final String path) {
  // process events only for this node
  if (!isMyPath(path))
    return;
  try {
    loadNode(); // calls zk.getData(String, Watcher, Stat)
  } catch (final Exception e) {
    // got NoNodeException here (but not when debugging)
    log(..., e)
  }
}
{code}


From inspecting the logs we noticed a NoNodeException. When setting
breakpoints on #loadNode and stepping through we don't get the
exception. But when setting a breakpoint on #log only we got a hit and
could confirm the issue this way.

The path is actually some levels deep. All the parent paths don't exist
either so they are created as well. However, no exception is thrown fro
them. The sequence is as follows.

{noformat}
/l1  --> watch triggered, getData, no exception
/l1/l2  --> watch triggered, getData, no exception
/l1/l2/l3  --> watch triggered, getData, no exception
/l1/l2/l3/l4  --> watch triggered, getData, no exception
/l1/l2/l3/l4/l5  --> watch triggered, getData, no exception
/l1/l2/l3/l4/l5/l6  --> watch triggered, getData, NoNodeException
{noformat}

The only difference is that all paths up to including l5 do not actually
have any data. Only l6 has some data. Could there be some latency issues?

For completeness, the embedded server is started as follows.
{code}
// disable LOG4J JMX stuff
System.setProperty(""zookeeper.jmx.log4j.disable"", Boolean.TRUE.toString());

// get directories
final File dataDir = new File(config.getDataLogDir());
final File snapDir = new File(config.getDataDir());

// clean old logs
PurgeTxnLog.purge(dataDir, snapDir, 3);

// create standalone server
zkServer = new ZooKeeperServer();
zkServer.setTxnLogFactory(new FileTxnSnapLog(dataDir, snapDir));
zkServer.setTickTime(config.getTickTime());
zkServer.setMinSessionTimeout(config.getMinSessionTimeout());
zkServer.setMaxSessionTimeout(config.getMaxSessionTimeout());

factory = new NIOServerCnxn.Factory(config.getClientPortAddress(),
config.getMaxClientCnxns());

// start server
LOG.info(""Starting ZooKeeper standalone server."");
try {
  factory.startup(zkServer);
} catch (final InterruptedException e) {
  LOG.warn(""Interrupted during server start."", e);
  Thread.currentThread().interrupt();
}
{code}",[],Bug,ZOOKEEPER-1065,Major,Gunnar Wagenknecht,Invalid,2011-05-20T18:30:05.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Possible timing issue in embedded server,2011-05-20T18:39:30.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",0.0
,[],2011-05-19T00:36:45.000+0000,Ted Dunning,"The zkServer.sh script kind of sort of implements the standard init.d style of interaction.

It lacks

- nice return codes

- status method

- standard output messages

See 

http://refspecs.freestandards.org/LSB_3.1.0/LSB-Core-generic/LSB-Core-generic/iniscrptact.html

and

http://refspecs.freestandards.org/LSB_3.1.0/LSB-Core-generic/LSB-Core-generic/iniscrptfunc.html

and

http://wiki.debian.org/LSBInitScripts

It is an open question how much zkServer should use these LSB scripts because that may impair portability.  I
think it should produce similar messages, however, and should return standardized error codes.  If lsb functions
are available, I think that they should be used so that ZK works as a first class citizen.


I will produce a proposed patch.",[],Bug,ZOOKEEPER-1064,Major,Ted Dunning,Implemented,2013-10-10T17:25:41.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Startup script needs more LSB compatability,2013-10-10T17:25:41.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",2.0
Yanick Dufresne,"[<JIRA Component: name='java client', id='12312381'>]",2011-05-17T21:34:09.000+0000,Yanick Dufresne,"Synchronization around dataWatches, existWatches and childWatches in Zookeeper is incorrect.
Synchronization around outgoingQueue and pendingQueue in ClientCnxnSocketNIO is incorrect.
Synchronization around selector and key sets in ClientCnxnSocketNIO seems odd.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1063,Critical,Yanick Dufresne,Fixed,2011-07-15T04:11:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Dubious synchronization in Zookeeper and ClientCnxnSocketNIO classes,2011-11-23T19:22:02.000+0000,[],2.0
Botond Hejj,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2011-05-13T05:50:15.000+0000,Patrick D. Hunt,"Reported by a user on the CDH user list (user reports that the listed fix addressed this issue for him): 

""Net::ZooKeeper consumes 100% cpu when ""wait"" is used. At my initial inspection, it seems to be related to implementation mistake in pthread_cond_timedwait.""

https://rt.cpan.org/Public/Bug/Display.html?id=61290
","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1062,Major,Patrick D. Hunt,Fixed,2014-05-16T22:33:23.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Net-ZooKeeper: Net::ZooKeeper consumes 100% cpu on wait,2014-05-20T11:09:13.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.4.6', id='12323310'>]",5.0
Ted Dunning,"[<JIRA Component: name='scripts', id='12312384'>]",2011-05-10T20:38:36.000+0000,Ted Dunning,"The zkServer.sh script doesn't check properly to see if a previously started
server is still running.  If you call start twice, the second invocation
will over-write the PID file with a process that then fails due to port
occupancy.

This means that stop will subsequently fail.

Here is a reference that describes how init scripts should normally work:

http://refspecs.freestandards.org/LSB_3.1.0/LSB-Core-generic/LSB-Core-generic/iniscrptact.html

","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1061,Major,Ted Dunning,Fixed,2011-05-16T17:12:48.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Zookeeper stop fails if start called twice,2017-03-30T14:27:23.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",4.0
Vishal Kher,"[<JIRA Component: name='quorum', id='12312379'>]",2011-05-10T19:32:58.000+0000,Vishal Kher,"This problem is seen only if you have ZooKeeper embedded in your application. QuorumPeerMain.initializeAndRun() does a quorumPeer.join() before exiting.

QuorumPeer.shutdown() tries to cleanup everything, but it does not interrupt itself. As a result, a if the peer is running FLE, it might be waiting to receive notifications (recvqueue.poll()) in FastLeaderElection. Therefore, quorumPeer.join() will wait until the peer wakes up from poll().

The fix is simple - call this.interrupt() in QuorumPeer.shutdown().","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1060,Minor,Vishal Kher,Fixed,2011-06-14T12:14:48.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,QuorumPeer takes a long time to shutdown,2011-11-23T19:22:37.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",2.0
Bhallamudi Venkata Siva Kamesh,"[<JIRA Component: name='java client', id='12312381'>]",2011-05-04T10:50:42.000+0000,Bhallamudi Venkata Siva Kamesh,"*stat* command issues on non existing zookeeper node,causes NPE to the client.
{noformat}
[zk: localhost:2181(CONNECTED) 2] stat /invalidPath
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.zookeeper.ZooKeeperMain.printStat(ZooKeeperMain.java:131)
        at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:723)
        at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:582)
        at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:354)
        at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:312)
        at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:271)

{noformat}","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1059,Major,Bhallamudi Venkata Siva Kamesh,Fixed,2011-05-16T17:39:15.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,stat command isses on non-existing node causes NPE ,2011-11-23T19:22:24.000+0000,[],1.0
Camille Fournier,[],2011-05-04T00:34:45.000+0000,Camille Fournier,fix Request getData to print that instead of getDate,"[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1058,Trivial,Camille Fournier,Fixed,2011-05-20T21:42:53.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,fix typo in opToString for getData,2011-11-23T19:22:07.000+0000,[],1.0
Michi Mutsuzaki,"[<JIRA Component: name='c client', id='12312380'>]",2011-05-03T01:16:21.000+0000,Woody Anderson,"Hello, I'm a contributor for the node.js zookeeper module: https://github.com/yfinkelstein/node-zookeeper
i'm using zk 3.3.3 for the purposes of this issue, but i have validated it fails on 3.3.1 and 3.3.2

i'm having an issue when trying to connect when one of my zookeeper servers is offline.
if the first server attempted is online, all is good.

if the offline server is attempted first, then the client is never able to connect to _any_ server.
inside zookeeper.c a connection loss (-4) is received, the socket is closed and buffers are cleaned up, it then attempts the next server in the list, creates a new socket (which gets the same fd as the previously closed socket) and connecting fails, and it continues to fail seemingly forever.
The nature of this ""fail"" is not that it gets -4 connection loss errors, but that zookeeper_interest doesn't find anything going on on the socket before the user provided timeout kicks things out. I don't want to have to wait 5 minutes, even if i could make myself.

this is the message that follows the connection loss:
2011-04-27 23:18:28,355:13485:ZOO_ERROR@handle_socket_error_msg@1530: Socket [127.0.0.1:5020] zk retcode=-7, errno=60(Operation timed out): connection timed out (exceeded timeout by 3ms)
2011-04-27 23:18:28,355:13485:ZOO_ERROR@yield@213: yield:zookeeper_interest returned error: -7 - operation timeout

While investigating, i decided to comment out close(zh->fd) in handle_error (zookeeper.c#1153)
now everything works (obviously i'm leaking an fd). Connection the the second host works immediately.
this is the behavior i'm looking for, though i clearly don't want to leak the fd, so i'm wondering why the fd re-use is causing this issue.
close() is not returning an error (i checked even though current code assumes success).

i'm on osx 10.6.7
i tried adding a setsockopt so_linger (though i didn't want that to be a solution), it didn't work.

full debug traces are included in issue here: https://github.com/yfinkelstein/node-zookeeper/issues/6
","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1057,Blocker,Woody Anderson,Fixed,2014-01-09T21:04:23.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"zookeeper c-client, connection to offline server fails to successfully fallback to second zk host",2014-03-13T18:17:05.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.3.3', id='12315482'>]",8.0
,"[<JIRA Component: name='c client', id='12312380'>]",2011-04-27T03:20:06.000+0000,Stephen Tyree,"Having been using the C client for a few months now, I thought I'd look through the code and see if anything could be improved and/or fixed in order to be a good citizen. Here are some observations and questions I was hoping people could elaborate on.

- There appears to be a bug in sub_string (zookeeper.c). The third argument being passed into strncmp is a conditional due to misplaced parenthesis, meaning the length is either 0 or 1. This likely leads to many, many false positives of chroots matching paths.
- There appears to be a bug in queue_session_event, where we check for cptr->buffer not being NULL after already dereferencing it
- In both queue_buffer and queue_completion_nolock, we assert a conditional that we just checked for
- What is the policy on whether the result of memory allocations are checked for, assert'd against or ignored? This is done inconsistently.
- What is the policy on whether pointers are checked/set against NULL versus 0? This is done inconsistently.
- Some functions, such as zoo_wget_children2_, exhibit needlessly high cyclomatic complexity
- What is the policy on line length restrictions? Some functions go through hurdles to enforce 80 characters while others do no such thing.
- What is the policy on indentation and spacing of if statements and blocks of code? This is done inconsistently.

If any or all of these turn out to be issues that need to be fixed I'd be more than happy to do so.",[],Bug,ZOOKEEPER-1056,Minor,Stephen Tyree,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Questions and Improvements for the C client codebase,2011-04-27T03:20:06.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",0.0
Eugene Joseph Koontz,[],2011-04-26T21:08:47.000+0000,Eugene Joseph Koontz,"actual result:


[zk: (CONNECTED) 0] create /test2 'test2' digest:test:test:cdrwa,digest:test:test:cdrwa
Created /test2
[zk: (CONNECTED) 1] getAcl /test2
'digest,'test:test
: cdrwa
'digest,'test:test
: cdrwa
[zk: (CONNECTED) 2]

but getAcl should only have a single entry.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1055,Major,Eugene Joseph Koontz,Fixed,2011-08-15T00:35:32.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,check for duplicate ACLs in addACL() and create(),2011-11-23T19:22:26.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
Flavio Paiva Junqueira,[],2011-04-24T13:45:34.000+0000,Flavio Paiva Junqueira,"{noformat}
REC 	Exception is caught when Exception is not thrown in org.apache.zookeeper.server.quorum.QuorumPeer$ResponderThread.run()
{noformat}","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1052,Major,Flavio Paiva Junqueira,Fixed,2011-05-03T17:58:31.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Findbugs warning in QuorumPeer.ResponderThread.run(),2011-11-23T19:22:37.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",1.0
Stephen Tyree,"[<JIRA Component: name='c client', id='12312380'>]",2011-04-21T13:56:15.000+0000,Stephen Tyree,"In libzookeeper_mt, if your process is going rather slowly (such as when running it in Valgrind's Memcheck) or you are using gdb with breakpoints, you can occasionally get SIGPIPE when trying to send a message to the cluster. For example:

==12788==
==12788== Process terminating with default action of signal 13 (SIGPIPE)
==12788==    at 0x3F5180DE91: send (in /lib64/libpthread-2.5.so)
==12788==    by 0x7F060AA: ??? (in /usr/lib64/libzookeeper_mt.so.2.0.0)
==12788==    by 0x7F06E5B: zookeeper_process (in /usr/lib64/libzookeeper_mt.so.2.0.0)
==12788==    by 0x7F0D38E: ??? (in /usr/lib64/libzookeeper_mt.so.2.0.0)
==12788==    by 0x3F5180673C: start_thread (in /lib64/libpthread-2.5.so)
==12788==    by 0x3F50CD3F6C: clone (in /lib64/libc-2.5.so)
==12788==

This is probably not the behavior we would like, since we handle server disconnections after a failed call to send. To fix this, there are a few options we could use. For BSD environments, we can tell a socket to never send SIGPIPE with send using setsockopt:

setsockopt(sd, SOL_SOCKET, SO_NOSIGPIPE, (void *)&set, sizeof(int));

For Linux environments, we can add a MSG_NOSIGNAL flag to every send call that says to not send SIGPIPE on a bad file descriptor.

For more information, see: http://stackoverflow.com/questions/108183/how-to-prevent-sigpipes-or-handle-them-properly","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1051,Minor,Stephen Tyree,Fixed,2011-08-30T07:02:07.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,SIGPIPE in Zookeeper 0.3.* when send'ing after cluster disconnection,2011-11-23T19:21:59.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>]",3.0
Will Johnson,"[<JIRA Component: name='contrib', id='12312700'>]",2011-04-21T00:46:22.000+0000,Chris Burroughs,"* zooInspector-dev.sh uses DOS line endings.  Dash at least chokes on this.
* zooInspector.sh has an errant ; in the classpath.

Also there really isn't a reason to hard code the zookeeper version needed in lib. Just use a glob.
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1050,Trivial,Chris Burroughs,Fixed,2012-01-06T01:23:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zooinspector shell scripts do not work,2012-01-06T10:57:24.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",3.0
Chang Song,"[<JIRA Component: name='server', id='12312382'>]",2011-04-16T03:42:55.000+0000,Chang Song,"Let's say we have 100 clients (group A) already connected to three-node ZK ensemble with session timeout of 15 second.  And we have 1000 clients (group B) already connected to the same ZK ensemble, all watching several nodes (with 15 second session timeout)

Consider a case in which All clients in group B suddenly hung or deadlocked (JVM OOME) all at the same time. 15 seconds later, all sessions in group B gets expired, creating session closing stampede. Depending on the number of this clients in group B, all request/response ZK ensemble should process get delayed up to 8 seconds (1000 clients we have tested).

This delay causes some clients in group A their sessions expired due to delay in getting heartbeat response. This causes normal servers to drop out of clusters. This is a serious problem in our installation, since some of our services running batch servers or CI servers creating the same scenario as above almost everyday.

I am attaching a graph showing ping response time delay.

I think ordering of creating/closing sessions and ping exchange isn't important (quorum state machine). at least ping request / response should be handle independently (different queue and different thread) to keep realtime-ness of ping.

As a workaround, we are raising session timeout to 50 seconds.
But this causes max. failover of cluster to significantly increased, thus initial QoS we promised cannot be met.







","[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1049,Critical,Chang Song,Fixed,2011-05-03T21:30:42.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Session expire/close flooding renders heartbeats to delay significantly,2011-11-23T19:22:20.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",6.0
allengao,"[<JIRA Component: name='c client', id='12312380'>]",2011-04-13T09:40:32.000+0000,allengao,"I can not operation a node with ACL by ""addauth"" when using cli_st. I have fixed this bug: 
original：else if (startsWith(line, ""addauth "")) {
      char *ptr;
      line += 8;
      ptr = strchr(line, ' ');
      if (ptr) {
        *ptr = '\0';
        ptr++;
      }
      zoo_add_auth(zh, line, ptr, ptr ? strlen(ptr) -1 : 0, NULL, NULL);
now: zoo_add_auth(zh, line, ptr, ptr ? strlen(ptr) : 0, NULL, NULL);
strlen(ptr) is just ok.","[<JIRA Version: name='3.3.6', id='12320172'>, <JIRA Version: name='3.4.4', id='12319841'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1048,Major,allengao,Fixed,2012-05-06T03:47:44.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,addauth command does not work in cli_mt/cli_st,2016-03-03T01:36:39.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2011-04-13T09:04:09.000+0000,Gunnar Wagenknecht,"When I shutdown a standalone ZooKeeper server (programmatically) I get the following exception logged. Occasionally, no exception is logged. 
{noformat}
10:32:43.353 [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181] WARN  o.a.zookeeper.server.NIOServerCnxn - Ignoring unexpected runtime exception
java.nio.channels.CancelledKeyException: null
	at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:55) ~[na:1.6.0_24]
	at sun.nio.ch.SelectionKeyImpl.readyOps(SelectionKeyImpl.java:69) ~[na:1.6.0_24]
	at org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:241) ~[na:na]
10:32:43.353 [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181] INFO  o.a.zookeeper.server.NIOServerCnxn - NIOServerCnxn factory exited run method
10:32:43.387 [SyncThread:0] INFO  o.a.z.server.SyncRequestProcessor - SyncRequestProcessor exited!
10:32:43.387 [ProcessThread:-1] INFO  o.a.z.server.PrepRequestProcessor - PrepRequestProcessor exited loop!
10:32:43.387 [app thread] INFO  o.a.z.server.FinalRequestProcessor - shutdown of request processor complete
{noformat}

Because it's logged with a WARN level, my assumption is that something is wrong on shutdown. However, I follow the exact same shutdown order than ZooKeeperMain, i.e. shutdown the {{NIOServerCnxn.Factory}} first and shutdown the {{ZooKeeperServer}} instance thereafter if its still running.

{noformat}
...
factory.shutdown();
factory = null;

if (zkServer.isRunning()) {
	zkServer.shutdown();
}
zkServer = null;
{noformat}


",[],Bug,ZOOKEEPER-1047,Major,Gunnar Wagenknecht,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZooKeeper Standalone does not shutdown cleanly,2011-04-13T09:04:09.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",1.0
Vishal Kher,"[<JIRA Component: name='server', id='12312382'>]",2011-04-12T22:24:19.000+0000,Jeremy Stribling,"On several occasions, I've seen a create() with the sequential flag set fail with a ZNODEEXISTS error, and I don't think that should ever be possible.  In past runs, I've been able to closely inspect the state of the system with the command line client, and saw that the parent znode's cversion is smaller than the sequential number of existing children znode under that parent.  In one example:

{noformat}
[zk:<ip:port>(CONNECTED) 3] stat /zkrsm
cZxid = 0x5
ctime = Mon Jan 17 18:28:19 PST 2011
mZxid = 0x5
mtime = Mon Jan 17 18:28:19 PST 2011
pZxid = 0x1d819
cversion = 120710
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 0
numChildren = 2955
{noformat}

However, the znode /zkrsm/000000000000002d_record0000120804 existed on disk.

In a recent run, I was able to capture the Zookeeper logs, and I will attach them to this JIRA.  The logs are named as nodeX.<zxid_prefixes>.log, and each new log represents an application process restart.

Here's the scenario:

# There's a cluster with nodes 1,2,3 using zxid 0x3.
# All three nodes restart, forming a cluster of zxid 0x4.
# Node 3 restarts, leading to a cluster of 0x5.

At this point, it seems like node 1 is the leader of the 0x5 epoch.  In its log (node1.0x4-0x5.log) you can see the first (of many) instances of the following message:

{noformat}
2011-04-11 21:16:12,607 16649 [ProcessThread:-1] INFO org.apache.zookeeper.server.PrepRequestProcessor  - Got user-level KeeperException when processing sessionid:0x512f466bd44e0002 type:create cxid:0x4da376ab zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a Error Path:/zkrsm/00000000000000b2_record0001761440 Error:KeeperErrorCode = NodeExists for /zkrsm/00000000000000b2_record0001761440
{noformat}

This then repeats forever as my application isn't expecting to ever get this error message on a sequential node create, and just continually retries.  The message even transfers over to node3.0x5-0x6.log once the 0x6 epoch comes into play.

I don't see anything terribly fishy in the transition between the epochs; the correct snapshots seem to be getting transferred, etc.  Unfortunately I don't have a ZK snapshot/log that exhibits the problem when starting with a fresh system.

Some oddities you might notice in these logs:
* Between epochs 0x3 and 0x4, the zookeeper IDs of the nodes changed due to a bug in our application code.  (They are assigned randomly, but are supposed to be consistent across restarts.)
* We manage node membership dynamically, and our application restarts the ZooKeeperServer classes whenever a new node wants to join (without restarting the entire application process).  This is why you'll see messages like the following in node1.0x4-0x5.log before a new election begins:
{noformat}
2011-04-11 21:16:00,762 4804 [QuorumPeer:/0.0.0.0:2888] INFO org.apache.zookeeper.server.quorum.Learner  - shutdown called
{noformat}
* There is in fact one of these dynamic membership changes in node1.0x4-0x5.log, just before the 0x4 epoch is formed.  I'm not sure how this would be related though, as no transactions are done during this period.","[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1046,Blocker,Jeremy Stribling,Fixed,2011-07-14T14:24:41.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Creating a new sequential node results in a ZNODEEXISTS error,2011-11-23T19:22:03.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.3.3', id='12315482'>]",3.0
,[],2011-04-04T15:34:27.000+0000,César Álvarez Núñez,"I'm sorry but I only have this log (which belongs to a ""follower"" node) and a previous message [Unexpected NodeCreated event after a reconnection.|http://mail-archives.apache.org/mod_mbox/zookeeper-user/201103.mbox/%3CAANLkTi=vmZ5v4W6FMhWg4XO6rJT89eGozGUE840bku0_@mail.gmail.com%3E] where I describe a potential side-effect at client side.

{noformat}
2011-04-04 09:31:09,608 - INFO  [Snapshot Thread:FileTxnSnapLog@208][] - Snapshotting: 1700527e36
2011-04-04 09:31:09,653 - INFO  [SyncThread:1:FileTxnLog@197][] - Creating new log file: log.1700527e38
2011-04-04 10:13:39,287 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2301:NIOServerCnxn$Factory@251][] - Accepted socket connection from /XXX.XXX.XXX.69:1093
2011-04-04 10:13:39,371 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2301:NIOServerCnxn@777][] - Client attempting to establish new session at /XXX.XXX.XXX.69:1093
2011-04-04 10:13:39,376 - INFO  [CommitProcessor:1:NIOServerCnxn@1580][] - Established session 0x12ee79c4a720022 with negotiated timeout 20000 for client /XXX.XXX.XXX.69:1093
2011-04-04 12:04:11,131 - INFO  [SyncThread:1:FileTxnLog@197][] - Creating new log file: log.170053bf15
2011-04-04 12:04:11,131 - INFO  [Snapshot Thread:FileTxnSnapLog@208][] - Snapshotting: 170053bf17
2011-04-04 12:13:10,779 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2301:NIOServerCnxn$Factory@251][] - Accepted socket connection from /XXX.XXX.XXX.63:1817
2011-04-04 12:13:10,790 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2301:NIOServerCnxn@777][] - Client attempting to establish new session at /XXX.XXX.XXX.63:1817
2011-04-04 12:13:10,794 - INFO  [CommitProcessor:1:NIOServerCnxn@1580][] - Established session 0x12ee79c4a720023 with negotiated timeout 20000 for client /XXX.XXX.XXX.63:1817
2011-04-04 12:13:10,814 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2301:NIOServerCnxn@634][] - EndOfStreamException: Unable to read additional data from client sessionid 0x12ee79c4a720023, likely client has closed socket
2011-04-04 12:13:10,816 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2301:NIOServerCnxn@1435][] - Closed socket connection for client /XXX.XXX.XXX.63:1817 which had sessionid 0x12ee79c4a720023
2011-04-04 12:13:10,839 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2301:NIOServerCnxn$Factory@251][] - Accepted socket connection from /XXX.XXX.XXX.63:1814
2011-04-04 12:13:10,840 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2301:NIOServerCnxn$Factory@274][] - Ignoring exception
java.net.SocketException: Invalid argument
        at sun.nio.ch.Net.setIntOption0(Native Method)
        at sun.nio.ch.Net.setIntOption(Unknown Source)
        at sun.nio.ch.SocketChannelImpl$1.setInt(Unknown Source)
        at sun.nio.ch.SocketOptsImpl.setBoolean(Unknown Source)
        at sun.nio.ch.SocketOptsImpl$IP$TCP.noDelay(Unknown Source)
        at sun.nio.ch.OptionAdaptor.setTcpNoDelay(Unknown Source)
        at sun.nio.ch.SocketAdaptor.setTcpNoDelay(Unknown Source)
        at org.apache.zookeeper.server.NIOServerCnxn.<init>(NIOServerCnxn.java:1367)
        at org.apache.zookeeper.server.NIOServerCnxn$Factory.createConnection(NIOServerCnxn.java:215)
        at org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:256)
2011-04-04 12:13:10,841 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2301:NIOServerCnxn$Factory@272][] - Ignoring unexpected runtime exception
java.lang.NullPointerException
        at org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:244)
2011-04-04 12:13:10,841 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2301:NIOServerCnxn$Factory@272][] - Ignoring unexpected runtime exception
java.lang.NullPointerException
        at org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:244)
2011-04-04 12:13:10,842 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2301:NIOServerCnxn$Factory@272][] - Ignoring unexpected runtime exception
java.lang.NullPointerException
        at org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:244)
...
...
...
2011-04-04 16:49:23,101 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2301:NIOServerCnxn$Factory@272][] - Ignoring unexpected runtime exception
java.lang.NullPointerException
        at org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:244)
{noformat}
",[],Bug,ZOOKEEPER-1043,Major,César Álvarez Núñez,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Looped NPE at org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:244),2015-08-28T20:03:45.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.6', id='12323310'>]",8.0
,"[<JIRA Component: name='server', id='12312382'>]",2011-03-29T00:46:05.000+0000,jiangwen wei,"1. current process
when leader fail, a new leader will be elected, followers will sync with the
new leader.
After synced, leader send UPTODATE to follower.

2. a corner case
but there is a corner case, things will go wrong.
suppose message M only exists on leader, after a follower synced with
leader, the client connected to the follower will see M.
but it only exists on two servers, not on a quorum of servers. If the new
leader and the follower failed, message M is lost, but M is already seen by
client.

3. one solution
So I think UPTODATE  can be sent to follower only when a quorum of server
synced with the leader.",[],Bug,ZOOKEEPER-1036,Major,jiangwen wei,Not A Problem,2011-03-31T20:26:30.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,send UPTODATE to follower until a quorum of servers synced with leader,2011-03-31T20:26:30.000+0000,[],0.0
,"[<JIRA Component: name='server', id='12312382'>]",2011-03-28T08:36:50.000+0000,Christian Ziech,"We were trying to use the predefined ACL ""Ids.CREATOR_ALL_ACL"" together with the default ip authentication. Unfortunately it seems that this cannot work due to the implementation of the PrepRequestProcessor.fixupACL() method checking the return value of the AuthenticationProvider.isAuthenticated() (the IPAuthenticationProvider in our case) method. 
Unfortunately this provider always returns false which results in the Ids.CREATOR_ALL_ACL to be always rejected.
",[],Bug,ZOOKEEPER-1035,Major,Christian Ziech,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,CREATOR_ALL_ACL does not work together with IPAuthenticationProvider,2011-09-07T14:49:26.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.3.2', id='12315108'>]",2.0
Nicholas Harteau,"[<JIRA Component: name='contrib', id='12312700'>]",2011-03-27T20:45:50.000+0000,Nicholas Harteau,"Installing Net::ZooKeeper from cpan or the zookeeper distribution tarballs will always fail due to not finding c-client header files.  In conjunction with ZOOKEEPER-1033 update perl bindings to look for c-client header files in INCDIR/zookeeper/

a.k.a. make installs of Net::ZooKeeper via cpan/cpanm/whatever *just work*, assuming you've already got the zookeeper c client installed.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1034,Minor,Nicholas Harteau,Fixed,2011-08-15T01:41:13.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,perl bindings should automatically find the zookeeper c-client headers,2011-11-23T19:22:05.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",2.0
Nicholas Harteau,"[<JIRA Component: name='c client', id='12312380'>]",2011-03-27T20:40:09.000+0000,Nicholas Harteau,"header files are installed into foo/include/c-client-src/, which doesn't indicate a relationship with zookeeper and doesn't correspond to foo/lib/libzookeeper*

header files should be installed into foo/include/zookeeper/ as this is the common practice.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1033,Minor,Nicholas Harteau,Fixed,2011-05-04T06:03:00.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"c client should install includes into INCDIR/zookeeper, not INCDIR/c-client-src",2011-11-23T19:22:14.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",2.0
Flavio Paiva Junqueira,"[<JIRA Component: name='c client', id='12312380'>]",2011-03-25T20:47:43.000+0000,Dheeraj Agrawal,"If you give invalid hostname to zookeeper_init method, it's not able to resolve it, and it tries to do the cleanup (free buffer/completion lists/etc) . The adaptor_init() is not called for this code path, so the lock,cond variables (for adaptor, completion lists) are not initialized.

As part of the cleanup it's trying to clean up some buffers and acquires locks and unlocks (where the locks have not yet been initialized, so unlocking fails) 
    lock_completion_list(&zh->sent_requests); - pthread_mutex/cond not initialized
    tmp_list = zh->sent_requests;
    zh->sent_requests.head = 0;
    zh->sent_requests.last = 0;
    unlock_completion_list(&zh->sent_requests);   trying to broadcast here on uninitialized cond

It should do error checking to see if locking succeeds before unlocking it. If Locking fails, then appropriate error handling has to be done.","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-1029,Blocker,Dheeraj Agrawal,Fixed,2015-12-11T20:15:56.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,C client bug in zookeeper_init (if bad hostname is given),2018-12-25T09:42:48.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",18.0
Chris Medaglia,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2011-03-24T20:26:04.000+0000,Chris Medaglia,"There is a small bug in the python bindings, specifically with the zookeeper.set2() call. This method should return a stat dictionary, but actually returns None. The fix is a one-character change to zookeeper.c such that the return value is '&stat' rather than 'stat'.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1028,Minor,Chris Medaglia,Fixed,2011-04-06T20:22:12.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"In python bindings, zookeeper.set2() should return a stat dict but instead returns None",2011-11-23T19:22:23.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",3.0
Thijs Terlouw,"[<JIRA Component: name='c client', id='12312380'>]",2011-03-24T05:06:47.000+0000,Thijs Terlouw,"I've recently started to use the chroot functionality (introduced in
3.2.0) as part of my connect string.It mostly works as expected, but
there is one case that is unexpected: when I create a path with
zoo_create() I can retrieve the created path. This is very useful when
you set the ZOO_SEQUENCE flag. Unfortunately the returned path
includes the chroot as part of the path. This was unexpected to me: I
expected that the chroot would be totally transparent. The
documentation for zoo_create() says:
""path_buffer : Buffer which will be filled with the path of the new
node (this might be different than the supplied path because of the
ZOO_SEQUENCE flag).""

This gave me the impression that this flag is the only reason the
returned path is different from the created path, but apparently it's
not. Is this a bug or intended behavior? 
I workaround this issue now by remembering the chroot in
my wrapper code and after a call to zoo_create() i check if the returned
path starts with the chroot. If it does, I remove it.

My use case is to create a path with a sequence number and then delete
this path later. Unfortunately I cannot delete the path because it has
the chroot prepended to it, and thus it will result in two chroots.

I believe this only affects the create functions.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1027,Critical,Thijs Terlouw,Fixed,2011-07-25T17:45:31.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,chroot not transparent in zoo_create(),2015-09-28T17:33:17.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",5.0
,"[<JIRA Component: name='server', id='12312382'>]",2011-03-22T17:16:03.000+0000,Jeremy Stribling,"I ran into a weird case where a Zookeeper server rejoins the cluster after missing several operations, and then a client creates a new sequential node that has a number earlier than the last node it created.  I don't have full logs, or a live system in this state, or any data directories, just some partial server logs and the evidence as seen by the client.  Haven't tried reproducing it yet, just wanted to see if anyone here had any ideas.  Here's the scenario (probably more info than necessary, but trying to be complete)

1) Initially (5:37:20): 3 nodes up, with ids 215, 126, and 37 (called nodes #1, #2, and #3 below):
2) Nodes periodically (and throughout this whole timeline) create sequential, non-ephemeral nodes under the /zkrsm parent node.
3) 5:46:57: Node #1 gets notified of /zkrsm/0000000000000000_record0000002116
4) 5:47:06: Node #1 restarts and rejoins
5) 5:49:26: Node #2 gets notified of /zkrsm/0000000000000000_record0000002708
6) 5:49:29: Node #2 restarts and rejoins
7) 5:52:01: Node #3 gets notified of /zkrsm/0000000000000000_record0000003291
8) 5:52:02: Node #3 restarts and begins the rejoining process
9) 5:52:08: Node #1 successfully creates /zkrsm/0000000000000000_record0000003348
10) 5:52:08: Node #2 dies after getting notified of /zkrsm/0000000000000000_record0000003348
11) 5:52:10ish: Node #3 is elected leader (the ZK server log doesn't have wallclock timestamps, so not exactly sure on the ordering of this step)
12) 5:52:15: Node #1 successfully creates /zkrsm/0000000000000000_record0000003292

Note that the node created in step #12 is lower than the one created in step #9, and is exactly one greater than the last node seen by node #3 before it restarted.

Here is the sequence of session establishments as seen from the C client of node #1 after its restart (the IP address of node #1=13.0.0.11, #2=13.0.0.12, #3=13.0.0.13):

2011-03-18 05:46:59,838:17454(0x7fc57d3db710):ZOO_INFO@check_events@1632: session establishment complete on server [13.0.0.13:2888], sessionId=0x252ec780a3020000, negotiated timeout=6000
2011-03-18 05:49:32,194:17454(0x7fc57cbda710):ZOO_INFO@check_events@1632: session establishment complete on server [13.0.0.13:2888], sessionId=0x252ec782f5100002, negotiated timeout=6000
2011-03-18 05:52:02,352:17454(0x7fc57d3db710):ZOO_INFO@check_events@1632: session establishment complete on server [13.0.0.12:2888], sessionId=0x7e2ec782ff5f0001, negotiated timeout=6000
2011-03-18 05:52:08,583:17454(0x7fc57d3db710):ZOO_INFO@check_events@1632: session establishment complete on server [13.0.0.11:2888], sessionId=0x7e2ec782ff5f0001, negotiated timeout=6000
2011-03-18 05:52:13,834:17454(0x7fc57cbda710):ZOO_INFO@check_events@1632: session establishment complete on server [13.0.0.11:2888], sessionId=0xd72ec7856d0f0001, negotiated timeout=6000

I will attach logs for all nodes after each of their restarts, and a partial log for node #3 from before its restart.",[],Bug,ZOOKEEPER-1026,Major,Jeremy Stribling,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Sequence number assignment decreases after old node rejoins cluster,2011-03-25T18:25:30.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",1.0
,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2011-03-19T10:20:00.000+0000,Botond Hejj,"If the add_auth method has a callback and we execute another command just after it than we can deadlock the python api.
Example:

def deadlock(a, b):
	pass

def watcher(zh, type, state, path):
	if(state == zookeeper.CONNECTED_STATE):
		zookeeper.add_auth(zh, 'test', 'test', deadlock)
		zookeeper.get_children(zh, '/')

zh = zookeeper.init(""host:port"", watcher)

Looking at the code the problem looks like the following:
get_children sync call is running on the main thread and have the GIL it blocks until the get_children finished. Meantime on the other thread the callback of add_auth is called and that tries to get the GIL to call the python callback. So this thread is waiting for the main thread to release the GIL but the main thread is waiting for the other thread to process the reply of get_children.

I am not an expert on python binding but I think it can be solved if the GIL would be release before synchronous c api calls.
",[],Bug,ZOOKEEPER-1023,Minor,Botond Hejj,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zkpython: add_auth can deadlock the interpreter,2022-02-03T08:50:27.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",0.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2011-03-15T09:59:38.000+0000,tom liu,"i use three node to deploy zkcluster. but follower node throws SocketException twice every day.
2011-03-15 14:15:48,260 - WARN  [QuorumPeer:/0:0:0:0:0:0:0:0:2181:Follower@90] - Exception when following the leader
java.net.SocketException: Broken pipe
        at java.net.SocketOutputStream.socketWrite0(Native Method)
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at org.apache.zookeeper.server.quorum.Learner.writePacket(Learner.java:126)
        at org.apache.zookeeper.server.quorum.Learner.ping(Learner.java:361)
        at org.apache.zookeeper.server.quorum.Follower.processPacket(Follower.java:116)
        at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:80)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:644)

i found the reason is that Follower do not reponse Leader's Ping just on time.
so, i add some logs. finnally, i found that, in org.apache.zookeeper.server.SyncRequestProcessor: 
{noformat}
    public void processRequest(Request request) {
        // request.addRQRec("">sync"");
        //TODO tom liu added
    	if(LOG.isDebugEnabled()) {
    		LOG.debug(""Processing request::"" + request);
    	}
        queuedRequests.add(request);
        //TODO tom liu added
    	if(LOG.isDebugEnabled()) {
    		LOG.debug(""Processing request::"" + request);
    	}
    }
{noformat}

that log is:
2011-03-15 14:15:34,515 - DEBUG [QuorumPeer:/0:0:0:0:0:0:0:0:2181:SyncRequestProcessor@189] - Processing request::sessionid:0x22e9907b5d50000 type:setData cxid:0x70b55 zxid:0xd50000a73f txntype:5 reqpath:n/a
2011-03-15 14:15:48,259 - DEBUG [QuorumPeer:/0:0:0:0:0:0:0:0:2181:SyncRequestProcessor@194] - Processing request::sessionid:0x22e9907b5d50000 type:setData cxid:0x70b55 zxid:0xd50000a73f txntype:5 reqpath:n/a

so: elapsed time=13744, LearnerHandler's ia.readRecord TimeOut on run method, then Leader shutdown, and re-elect Leader process.

my question is: why the queuedRequests.add statement take so long time?",[],Bug,ZOOKEEPER-1017,Major,tom liu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Follower.followLeader throws SocketException, then shutdown Follower",2011-03-15T09:59:38.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",1.0
Bill Havanki,"[<JIRA Component: name='server', id='12312382'>]",2011-03-13T03:00:39.000+0000,Xiaoming Shi,"In the file
{noformat} 
./zookeeper-3.3.2/src/java/main/org/apache/zookeeper/server/PurgeTxnLog.java line:103
{noformat}

DateFormat.getDateTimeInstance() is called many times in the for loop. We can cache the result and improve the performance

This is similar to the Apache bug https://issues.apache.org/bugzilla/show_bug.cgi?id=48778

Similar code can be found:
{noformat}
./zookeeper-3.3.2/src/java/main/org/apache/zookeeper/server/TraceFormatter.java
./zookeeper-3.3.2/src/java/main/org/apache/zookeeper/server/LogFormatter.java
{noformat}
",[],Bug,ZOOKEEPER-1015,Major,Xiaoming Shi,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,"DateFormat.getDateTimeInstance() is very expensive, we can cache it to improve performance",2016-03-03T02:44:55.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2011-03-12T17:42:07.000+0000,Xiaoming Shi,"In the file:
{noformat}
./zookeeper-3.3.2/src/java/main/org/apache/zookeeper/server/TraceFormatter.java
{noformat}
DateFormat.getDateTimeInstance() is called in the while loop. We can cache the return value, and improve performance.

This is similar to the  Apache Bug https://issues.apache.org/bugzilla/show_bug.cgi?id=48778 ",[],Bug,ZOOKEEPER-1014,Major,Xiaoming Shi,Duplicate,2011-03-15T03:51:36.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"DateFormat.getDateTimeInstance() is very expensive, we can cache it to improve performance",2011-03-15T03:51:36.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",0.0
Eugene Joseph Koontz,"[<JIRA Component: name='server', id='12312382'>]",2011-03-11T20:28:02.000+0000,Eugene Joseph Koontz,"currently the ""Usage"" message for zkServer shows:

  echo ""Usage: $0 {start|stop|restart|status}"" 

But it seems to me that it should show the other startup options as well, which are currently: start-foreground, upgrade, print-cmd.

","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1013,Trivial,Eugene Joseph Koontz,Fixed,2011-03-15T18:39:07.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkServer.sh usage message should mention all startup options,2011-11-23T19:22:16.000+0000,[],1.0
Ling Mao,"[<JIRA Component: name='documentation', id='12312422'>]",2011-03-09T09:49:50.000+0000,Semih Salihoglu,"There is a race condition in the Barrier example of the java doc: http://hadoop.apache.org/zookeeper/docs/current/zookeeperTutorial.html. It's in the enter() method. Here's the original example:
boolean enter() throws KeeperException, InterruptedException{
            zk.create(root + ""/"" + name, new byte[0], Ids.OPEN_ACL_UNSAFE,
                    CreateMode.EPHEMERAL_SEQUENTIAL);
            while (true) {
                synchronized (mutex) {
                    List<String> list = zk.getChildren(root, true);

                    if (list.size() < size) {
                        mutex.wait();
                    } else {
                        return true;
                    }
                }
            }
        }

Here's the race condition scenario:
Let's say there are two machines/nodes: node1 and node2 that will use this code to synchronize over ZK. Let's say the following steps take place:
node1 calls the zk.create method and then reads the number of children, and sees that it's 1 and starts waiting. 

node2 calls the zk.create method (doesn't call the zk.getChildren method yet, let's say it's very slow) 
node1 is notified that the number of children on the znode changed, it checks that the size is 2 so it leaves the barrier, it does its work and then leaves the barrier, deleting its node.

node2 calls zk.getChildren and because node1 has already left, it sees that the number of children is equal to 1. Since node1 will never enter the barrier again, it will keep waiting.

--- End of scenario ---

Here's Flavio's fix suggestions (copying from the email thread):
...
I see two possible action points out of this discussion:
	
1- State clearly in the beginning that the example discussed is not correct under the assumption that a process may finish the computation before another has started, and the example is there for illustration purposes;
2- Have another example following the current one that discusses the problem and shows how to fix it. This is an interesting option that illustrates how one could reason about a solution when developing with zookeeper.
...

We'll go with the 2nd option.",[],Bug,ZOOKEEPER-1011,Major,Semih Salihoglu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,fix Java Barrier Documentation example's race condition issue and polish up the Barrier Documentation,2019-01-20T12:12:23.000+0000,[],6.0
Jeremy Stribling,"[<JIRA Component: name='c client', id='12312380'>]",2011-03-04T21:42:58.000+0000,Jeremy Stribling,"On line 1957, zookeeper_process() returns without cleaning up the ""ia"" buffer that was previously allocated.  I don't know how often this code path is taken, but I thought it was worth reporting.  I will attach a simple patch shortly.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1007,Minor,Jeremy Stribling,Fixed,2011-03-15T20:42:44.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,iarchive leak in C client,2011-11-23T19:22:47.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='tests', id='12312427'>]",2011-03-03T17:38:18.000+0000,Patrick D. Hunt,"CnxManagerTest.testWorkerThreads 

See attachment, this is the first time I've seen this test fail, and it's failed 2 out of the last three test runs.

Notice (attachment) once this happens the port never becomes available.

{noformat}
2011-03-02 15:53:12,425 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11245:NIOServerCnxn$Factory@251] - Accepted socket connection from /172.29.6.162:51441
2011-03-02 15:53:12,430 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11245:NIOServerCnxn@639] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
2011-03-02 15:53:12,430 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11245:NIOServerCnxn@1435] - Closed socket connection for client /172.29.6.162:51441 (no session established for client)
2011-03-02 15:53:12,430 - WARN  [QuorumPeer:/0:0:0:0:0:0:0:0:11241:Follower@82] - Exception when following the leader
java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:375)
	at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
	at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:84)
	at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:108)
	at org.apache.zookeeper.server.quorum.Learner.readPacket(Learner.java:148)
	at org.apache.zookeeper.server.quorum.Learner.registerWithLeader(Learner.java:267)
	at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:66)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:645)
2011-03-02 15:53:12,431 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11241:Follower@165] - shutdown called
java.lang.Exception: shutdown Follower
	at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:165)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:649)
2011-03-02 15:53:12,432 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11241:QuorumPeer@621] - LOOKING
2011-03-02 15:53:12,432 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11241:FastLeaderElection@663] - New election. My id =  0, Proposed zxid = 0
2011-03-02 15:53:12,433 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 0 (n.leader), 0 (n.zxid), 2 (n.round), LOOKING (n.state), 0 (n.sid), LOOKING (my state)
2011-03-02 15:53:12,433 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 0 (n.leader), 0 (n.zxid), 2 (n.round), LOOKING (n.state), 0 (n.sid), LOOKING (my state)
2011-03-02 15:53:12,433 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 0 (n.leader), 0 (n.zxid), 2 (n.round), LOOKING (n.state), 0 (n.sid), LOOKING (my state)
2011-03-02 15:53:12,633 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 0 (n.leader), 0 (n.zxid), 2 (n.round), LOOKING (n.state), 0 (n.sid), LOOKING (my state)
2011-03-02 15:53:12,633 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11245:QuorumPeer@655] - LEADING
2011-03-02 15:53:12,636 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11245:Leader@54] - TCP NoDelay set to: true
2011-03-02 15:53:12,638 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11245:ZooKeeperServer@151] - Created server with tickTime 1000 minSessionTimeout 2000 maxSessionTimeout 20000 datadir /var/lib/hudson/workspace/CDH3-ZooKeeper-3.3.3_sles/build/test/tmp/test9001250572426375869.junit.dir/version-2 snapdir /var/lib/hudson/workspace/CDH3-ZooKeeper-3.3.3_sles/build/test/tmp/test9001250572426375869.junit.dir/version-2
2011-03-02 15:53:12,639 - ERROR [QuorumPeer:/0:0:0:0:0:0:0:0:11245:Leader@133] - Couldn't bind to port 11245
java.net.BindException: Address already in use
	at java.net.PlainSocketImpl.socketBind(Native Method)
	at java.net.PlainSocketImpl.bind(PlainSocketImpl.java:365)
	at java.net.ServerSocket.bind(ServerSocket.java:319)
	at java.net.ServerSocket.<init>(ServerSocket.java:185)
	at java.net.ServerSocket.<init>(ServerSocket.java:97)
	at org.apache.zookeeper.server.quorum.Leader.<init>(Leader.java:131)
	at org.apache.zookeeper.server.quorum.QuorumPeer.makeLeader(QuorumPeer.java:512)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:657)
{noformat}
","[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-1006,Minor,Patrick D. Hunt,Fixed,2011-07-27T17:21:18.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"QuorumPeer ""Address already in use"" -- regression in 3.3.3",2011-11-23T19:22:20.000+0000,"[<JIRA Version: name='3.3.3', id='12315482'>]",1.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2011-03-01T16:00:18.000+0000,Alexandre Hardy,"We were running 3 zookeeper servers, and simulated a failure on one of the servers. 

The one zookeeper node follows the other, but has trouble connecting. It looks like the following exception is the cause:
{noformat}
2011-03-01T14:02:29+02:00 e0-cb-4e-65-4d-60 INFO [zookeeper] --  [org.apache.zookeeper.server.quorum.QuorumPeer] FOLLOWING
2011-03-01T14:02:29+02:00 e0-cb-4e-65-4d-60 INFO [zookeeper] --  [org.apache.zookeeper.server.ZooKeeperServer] Created server
2011-03-01T14:02:29+02:00 e0-cb-4e-65-4d-60 INFO [zookeeper] --  [org.apache.zookeeper.server.quorum.Follower] Following zookeeper3/192.168.131.11:2888
2011-03-01T14:02:29+02:00 e0-cb-4e-65-4d-60 WARNING [zookeeper] --  [org.apache.zookeeper.server.quorum.Follower] Unexpected exception, tries=0
2011-03-01T14:02:29+02:00 e0-cb-4e-65-4d-60 WARNING java.net.ConnectException: --  Connection refused
2011-03-01T14:02:29+02:00 e0-cb-4e-65-4d-60 WARNING  --     at java.net.PlainSocketImpl.socketConnect(Native Method)
2011-03-01T14:02:29+02:00 e0-cb-4e-65-4d-60 WARNING  --     at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:310)
2011-03-01T14:02:29+02:00 e0-cb-4e-65-4d-60 WARNING  --     at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:176)
2011-03-01T14:02:29+02:00 e0-cb-4e-65-4d-60 WARNING  --     at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:163)
2011-03-01T14:02:29+02:00 e0-cb-4e-65-4d-60 WARNING  --     at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:384)
2011-03-01T14:02:29+02:00 e0-cb-4e-65-4d-60 WARNING  --     at java.net.Socket.connect(Socket.java:546)
2011-03-01T14:02:29+02:00 e0-cb-4e-65-4d-60 WARNING  --     at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:156)
2011-03-01T14:02:29+02:00 e0-cb-4e-65-4d-60 WARNING  --     at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:549)
{noformat}
The last exception while connecting was:
{noformat}
2011-03-01T14:02:33+02:00 e0-cb-4e-65-4d-60 ERR [zookeeper] --  [org.apache.zookeeper.server.quorum.Follower] Unexpected exception
2011-03-01T14:02:33+02:00 e0-cb-4e-65-4d-60 ERR java.net.ConnectException: --  Connection refused
2011-03-01T14:02:33+02:00 e0-cb-4e-65-4d-60 ERR  --     at java.net.PlainSocketImpl.socketConnect(Native Method)
2011-03-01T14:02:33+02:00 e0-cb-4e-65-4d-60 ERR  --     at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:310)
2011-03-01T14:02:33+02:00 e0-cb-4e-65-4d-60 ERR  --     at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:176)
2011-03-01T14:02:33+02:00 e0-cb-4e-65-4d-60 ERR  --     at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:163)
2011-03-01T14:02:33+02:00 e0-cb-4e-65-4d-60 ERR  --     at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:384)
2011-03-01T14:02:33+02:00 e0-cb-4e-65-4d-60 ERR  --     at java.net.Socket.connect(Socket.java:546)
2011-03-01T14:02:33+02:00 e0-cb-4e-65-4d-60 ERR  --     at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:156)
2011-03-01T14:02:33+02:00 e0-cb-4e-65-4d-60 ERR  --     at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:549)
2011-03-01T14:02:33+02:00 e0-cb-4e-65-4d-60 WARNING [zookeeper] --  [org.apache.zookeeper.server.quorum.Follower] Exception when following the leader
{noformat}

The leader started leading a bit later 
{noformat}
2011-03-01T14:02:29+02:00 e0-cb-4e-65-4d-7d INFO [zookeeper] --  [org.apache.zookeeper.server.quorum.FastLeaderElection] Notification: 0, 94489312534, 25, 2, LOOKING, LOOKING, 0
2011-03-01T14:02:29+02:00 e0-cb-4e-65-4d-7d INFO [zookeeper] --  [org.apache.zookeeper.server.quorum.FastLeaderElection] Adding vote
2011-03-01T14:02:32+02:00 e0-cb-4e-65-4d-7d WARNING [zookeeper] --  [org.apache.zookeeper.server.quorum.QuorumCnxManager] Cannot open channel to 1 at election address zookeeper2/192.168.132.10:3888
2011-03-01T14:02:32+02:00 e0-cb-4e-65-4d-7d WARNING  --     at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:323)
2011-03-01T14:02:50+02:00 e0-cb-4e-65-4d-7d INFO [zookeeper] --  [org.apache.zookeeper.server.quorum.QuorumPeer] LEADING
{noformat}

But at that time the follower had already terminated and started a new election, so the leader failed:
{noformat}
2011-03-01T14:02:50+02:00 e0-cb-4e-65-4d-7d INFO [zookeeper] --  [org.apache.zookeeper.server.ZooKeeperServer] Created server
2011-03-01T14:02:50+02:00 e0-cb-4e-65-4d-7d INFO [zookeeper] --  [org.apache.zookeeper.server.persistence.FileSnap] Reading snapshot /var/lib/zookeeper/version-2/snapshot.1600007d16
2011-03-01T14:02:50+02:00 e0-cb-4e-65-4d-7d INFO [zookeeper] --  [org.apache.zookeeper.server.persistence.FileTxnSnapLog] Snapshotting: 1600007d16
2011-03-01T14:02:53+02:00 e0-cb-4e-65-4d-7d WARNING [zookeeper] --  [org.apache.zookeeper.server.quorum.QuorumCnxManager] Cannot open channel to 1 at election address zookeeper2/192.168.132.10:3888
2011-03-01T14:02:53+02:00 e0-cb-4e-65-4d-7d WARNING  --     at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:323)
2011-03-01T14:02:53+02:00 e0-cb-4e-65-4d-7d WARNING  --     at org.apache.zookeeper.server.quorum.QuorumCnxManager.toSend(QuorumCnxManager.java:302)
2011-03-01T14:02:53+02:00 e0-cb-4e-65-4d-7d WARNING  --     at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.process(FastLeaderElection.java:323)
2011-03-01T14:02:53+02:00 e0-cb-4e-65-4d-7d WARNING  --     at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.run(FastLeaderElection.java:296)
2011-03-01T14:02:53+02:00 e0-cb-4e-65-4d-7d INFO [zookeeper] --  [org.apache.zookeeper.server.quorum.FastLeaderElection] Sending new notification.
2011-03-01T14:03:11+02:00 e0-cb-4e-65-4d-7d INFO [zookeeper] --  [org.apache.zookeeper.server.quorum.FastLeaderElection] Sending new notification.
2011-03-01T14:03:14+02:00 e0-cb-4e-65-4d-7d WARNING [zookeeper] --  [org.apache.zookeeper.server.quorum.QuorumCnxManager] Cannot open channel to 1 at election address zookeeper2/192.168.132.10:3888
2011-03-01T14:03:14+02:00 e0-cb-4e-65-4d-7d WARNING  --     at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:323)
2011-03-01T14:03:14+02:00 e0-cb-4e-65-4d-7d WARNING  --     at org.apache.zookeeper.server.quorum.QuorumCnxManager.toSend(QuorumCnxManager.java:302)
2011-03-01T14:03:14+02:00 e0-cb-4e-65-4d-7d WARNING  --     at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.process(FastLeaderElection.java:323)
2011-03-01T14:03:14+02:00 e0-cb-4e-65-4d-7d WARNING  --     at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.run(FastLeaderElection.java:296)
2011-03-01T14:03:14+02:00 e0-cb-4e-65-4d-7d INFO [zookeeper] --  [org.apache.zookeeper.server.quorum.FastLeaderElection] Sending new notification.
2011-03-01T14:03:32+02:00 e0-cb-4e-65-4d-7d INFO [zookeeper] --  [org.apache.zookeeper.server.quorum.FastLeaderElection] Sending new notification.
2011-03-01T14:03:34+02:00 e0-cb-4e-65-4d-7d INFO [zookeeper] --  [org.apache.zookeeper.server.quorum.Leader] Shutdown called
2011-03-01T14:03:34+02:00 e0-cb-4e-65-4d-7d INFO  --     at org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:371)
2011-03-01T14:03:34+02:00 e0-cb-4e-65-4d-7d INFO  --     at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:297)
2011-03-01T14:03:34+02:00 e0-cb-4e-65-4d-7d INFO  --     at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:562)
{noformat}

From http://zookeeper.apache.org/doc/r3.2.2/zookeeperStarted.html:
{quote}
The new entry, initLimit is timeouts ZooKeeper uses to limit the length of time the ZooKeeper servers in quorum have to connect to a leader
{quote}

Since we have initLimit=10 and tickTime=4000, we should have 40 seconds for a zookeeper server to contact the leader.

However, in the source code src/java/main/org/apache/zookeeper/server/quorum/Follower.java:

{noformat}
152                 for (int tries = 0; tries < 5; tries++) {
153                     try {
154                         //sock = new Socket();
155                         //sock.setSoTimeout(self.tickTime * self.initLimit);
156                         sock.connect(addr, self.tickTime * self.syncLimit);
157                         sock.setTcpNoDelay(nodelay);
158                         break;
159                     } catch (IOException e) {
160                         if (tries == 4) {
161                             LOG.error(""Unexpected exception"",e);
162                             throw e;
163                         } else {
164                             LOG.warn(""Unexpected exception, tries=""+tries,e);
165                             sock = new Socket();
166                             sock.setSoTimeout(self.tickTime * self.initLimit);
167                         }
168                     }
169                     Thread.sleep(1000);
170                 }
{noformat}

It appears as if we only have 4 seconds to contact the leader. The timeouts are applied to the socket, but do not take into account that the zookeeper leader may not have started its zookeeper service yet. 

Is this the expected behaviour? Or is the expected behaviour that followers should always be able to connect to the leader?",[],Bug,ZOOKEEPER-1005,Major,Alexandre Hardy,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper servers fail to elect a leader succesfully.,2022-02-03T08:50:28.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>]",4.0
,[],2011-03-01T00:14:22.000+0000,Eugene Joseph Koontz,"Jenkins (Hudson) shows an error when running test-cppunit. I am not able to replicate this error on my own build machine, so I am unable to diagnose. Perhaps someone with access to the Apache Jenkins. Please see attached output from https://hudson.apache.org/hudson/job/PreCommit-ZOOKEEPER-Build/163//console (click on ""full"" to see the attached output if your browser can handle that much text).

",[],Bug,ZOOKEEPER-1004,Major,Eugene Joseph Koontz,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,TestClient.cc:363: Assertion: equality assertion failed,2011-12-05T22:57:20.000+0000,[],0.0
Ching-Shen Chen,"[<JIRA Component: name='documentation', id='12312422'>]",2011-02-23T02:02:03.000+0000,Ching-Shen Chen,"Please see the Barrier sample code from ZooKeeper Tutorial(http://zookeeper.apache.org/doc/r3.3.1/zookeeperTutorial.html#sc_barriers), that should enable a group of processes to synchronize the beginning and the end of a computation.","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-1002,Minor,Ching-Shen Chen,Invalid,2014-04-23T22:26:45.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,The Barrier sample code should create a EPHEMERAL znode instead of EPHEMERAL_SEQUENTIAL znode,2014-04-23T22:26:45.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",4.0
,"[<JIRA Component: name='java client', id='12312381'>]",2011-02-21T19:02:58.000+0000,Alex,"stat on non-existing node causes NPE. client quit

stat /aa
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.zookeeper.ZooKeeperMain.printStat(ZooKeeperMain.java:130)
        at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:722)
        at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:581)
        at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:353)
        at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:311)
        at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:270)",[],Bug,ZOOKEEPER-996,Trivial,Alex,Duplicate,2011-05-27T16:08:20.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZkClient: stat on non-existing node causes NPE,2011-05-27T16:08:20.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",0.0
,"[<JIRA Component: name='c client', id='12312380'>]",2011-02-21T13:22:47.000+0000,Andrei Savu,When creating a new node while using a chrooted connection the client function returns the full path (no chroot prefix). I've encountered this while using zkpython and that's why I suppose it's a problem related to the C bindings. It seems like the java client it's not affected by the same issue (only tested using the command line interface). I will also attach a patch with failing test. ,[],Bug,ZOOKEEPER-995,Major,Andrei Savu,Duplicate,2014-04-25T00:33:31.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,C Client exposing chroot information,2014-04-25T00:33:31.000+0000,[],1.0
MIS,"[<JIRA Component: name='build', id='12312383'>]",2011-02-17T19:38:49.000+0000,MIS,"The ""eclipse"" target in the zoo-keeper build script doesn't include the accessive.jar present in the folder /src/java/libtest in the .classpath file. But the accessive.jar is being referenced from a couple of test classes.
However, the build is successful :)
","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-994,Minor,MIS,Fixed,2011-02-27T07:11:39.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"""eclipse"" target in the build script doesnot include libraray required for test classes in the classpath",2011-11-23T19:22:23.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",1.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2011-02-14T06:45:54.000+0000,Sandeep Maheshwari,"I don't understand why do we even need this code at first place.

if (remoteSid == QuorumPeer.OBSERVER_ID) {
               /*
                * Choose identifier at random. We need a value to identify
                * the connection.
                */

               remoteSid = observerCounter--;
               initializeMessageQueue(remoteSid);
               LOG.info(""Setting arbitrary identifier to observer: "" + remoteSid);
           }
Even if remove above code from  public Long readRemoteServerID(Socket sock) {} function the FLE will work correctly. Because when any other peer(PARTICIPANT) receive a notification from the observer, that peer won't consider his(observer) vote because of this check

 if(!self.getVotingView().containsKey(response.sid))

Hence there is no need of that code. Also bcoz to above code there is a possibility of creating redundant threads (SendWorker-ReceiveWorker) bcoz when same participant try to initiate connection with same peer we are doing (sid = observerCounter--;). So the same observer getting different sid and hence corresponding thread would be crated which will be of no use.

Please let me know if i am correct.",[],Bug,ZOOKEEPER-991,Major,Sandeep Maheshwari,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,QuoromPeer.OBSERVER_ID,2022-02-03T08:50:28.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",0.0
,"[<JIRA Component: name='server', id='12312382'>]",2011-02-14T00:20:09.000+0000,Xiaowei Jiang,"When there is large number of sessions, random session timeout starts after a few hours. It happens even though the load on the server is small (less than 1 out of 8 process busy and plenty of memory). Increase the timeout to 300 seconds only delays this but the session timeout eventually happens.",[],Bug,ZOOKEEPER-990,Major,Xiaowei Jiang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,random session timeout when there is a large number of sessions,2011-02-14T17:04:33.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",1.0
,"[<JIRA Component: name='c client', id='12312380'>]",2011-02-14T00:16:36.000+0000,Xiaowei Jiang,"In a 5-machine ZK cluster, when there is a large number of sessions, the 1st server seems to get more sessions.

1st server gets 25% sessions, while the remaining gets 18.75% sessions",[],Bug,ZOOKEEPER-989,Minor,Xiaowei Jiang,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZK servers not balanced in number of sessions,2011-03-19T20:15:41.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",1.0
,"[<JIRA Component: name='leaderElection', id='12312378'>]",2011-02-14T00:13:15.000+0000,Xiaowei Jiang,"org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run thread exited unexpected, so the server hang on leader election.

QuorumPeer:/0.0.0.0:2181:
 [1] sun.misc.Unsafe.park (native method)
 [2] java.util.concurrent.locks.LockSupport.parkNanos (LockSupport.java:198)
 [3] java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos (AbstractQueuedSynchronizer.java:1,963)
 [4] java.util.concurrent.LinkedBlockingQueue.poll (LinkedBlockingQueue.java:395)
 [5] org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader (FastLeaderElection.java:677)
 [6] org.apache.zookeeper.server.quorum.QuorumPeer.run (QuorumPeer.java:621)",[],Bug,ZOOKEEPER-988,Major,Xiaowei Jiang,Incomplete,2013-10-14T23:52:41.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZK server hang on leader election,2013-10-14T23:52:41.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2011-02-14T00:10:41.000+0000,Xiaowei Jiang,"ZK server hit fatal error after leader re-election:

2011-01-17 14:38:29,709 - DEBUG [WorkerSender Thread:QuorumCnxManager@384] - There is a connection already for server 4
2011-01-17 14:38:30,111 - DEBUG [WorkerReceiver Thread:FastLeaderElection$Messenger$WorkerReceiver@214] - Receive new notification message. My id = 1
2011-01-17 14:38:30,111 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 4 (n.leader), 8589936845 (n.zxid), 6 (n.round), LOOKING (n.state), 4 (n.sid), FOLLOWING (my state)
2011-01-17 14:38:30,111 - DEBUG [WorkerReceiver Thread:FastLeaderElection$Messenger$WorkerReceiver@288] - Sending new notification. My id =  1, Recipient = 4
2011-01-17 14:38:30,112 - DEBUG [WorkerSender Thread:QuorumCnxManager@384] - There is a connection already for server 4
2011-01-17 14:38:34,115 - INFO  [QuorumPeer:/0.0.0.0:2181:Learner@315] - Setting leader epoch 3
2011-01-17 14:38:34,117 - WARN  [QuorumPeer:/0.0.0.0:2181:Follower@116] - Got zxid 0x2000008ce expected 0x1
2011-01-17 14:38:34,117 - INFO  [QuorumPeer:/0.0.0.0:2181:FileTxnSnapLog@208] - Snapshotting: 300000000
2011-01-17 14:38:37,346 - WARN  [QuorumPeer:/0.0.0.0:2181:Follower@116] - Got zxid 0x300000001 expected 0x2000008cf
2011-01-17 14:38:37,988 - FATAL [QuorumPeer:/0.0.0.0:2181:FollowerZooKeeperServer@112] - Committing zxid 0x300000001 but next pending txn 0x2000008ce",[],Bug,ZOOKEEPER-987,Major,Xiaowei Jiang,Not A Problem,2011-02-14T06:14:01.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Fatal error after reelection,2011-02-14T06:14:01.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",0.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2011-02-11T12:04:23.000+0000,Sandeep Maheshwari,"Function for sending out the notification message to corresponding peer for leader election

private void processMessages() throws Exception {
             try {
                 ByteBuffer b = getLastMessageSent(sid);
                 if (b != null) {
                     send(b);
                 }
             } catch (IOException e) {
                 LOG.error(""Failed to send last message to "" + sid, e);
                 throw e;
             }
             try {
                 ArrayBlockingQueue<ByteBuffer> bq = queueSendMap.get(sid);
                 if (bq == null) {
                     dumpQueueSendMap();
                     throw new Exception(""No queue for incoming messages for "" +
                             ""sid="" + sid);
                 }
                 while (running && !shutdown && sock != null) {
                     ByteBuffer b = null;
                     try {
                        b = bq.poll(1000, TimeUnit.MILLISECONDS);
                         if(b != null){
                             recordLastMessageSent(sid, b);
                             send(b);
                         }
                     } catch (InterruptedException e) {
                         LOG.warn(""Interrupted while waiting for message on "" +
                                  ""queue"", e);
                     }
                 }
             } catch (Exception e) {
                 LOG.warn(""Exception when using channel: for id "" + sid
                          + "" my id = "" + self.getId() + "" error = "", e);
                 throw e;
             }
        }

This is the code taken from zookeeper patch 932.
Here we are adding the message to be sent in current round to lastMessageSent. But in next round that message will still be there. So when we try to send a new message to server it will again  do                  

ByteBuffer b = getLastMessageSent(sid);
                 if (b != null) {
                     send(b);
                 }
and it will again send back that old message to that server. So in this way it will send back every message twice. Though it will not affect the correctness of FLE but sending message twice it create an extra overhead and slow down the election process.
 ","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-986,Minor,Sandeep Maheshwari,Not A Problem,2014-05-19T21:42:43.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"In QuoromCnxManager we are adding sent messgae to lastMessageSent, but we are never removing that message from it after sending it, so this will lead to sending the same message again in next round",2014-05-19T21:42:43.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2011-02-09T19:24:51.000+0000,Mahadev Konar,The unit test fails on trunk on my mac. I think this might be the same on other platforms as well. Ill attach the error logs.,"[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-985,Major,Mahadev Konar,Fixed,2011-02-18T17:55:02.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Test BookieRecoveryTest fails on trunk.,2011-11-23T19:22:26.000+0000,[],1.0
,[],2011-02-07T18:34:32.000+0000,Patrick D. Hunt,"Got the following NPE on my internal jenkins setup running against released 3.3.2 (see attached log)

{noformat}
    [junit] 2011-02-06 10:39:56,988 - WARN  [QuorumPeer:/0.0.0.0:11365:Follower@116] - Got zxid 0x100000001 expected 0x1
    [junit] 2011-02-06 10:39:56,988 - INFO  [SyncThread:3:FileTxnLog@197] - Creating new log file: log.100000001
    [junit] 2011-02-06 10:39:56,989 - WARN  [QuorumPeer:/0.0.0.0:11364:Follower@116] - Got zxid 0x100000001 expected 0x1
    [junit] 2011-02-06 10:39:56,989 - INFO  [SyncThread:2:FileTxnLog@197] - Creating new log file: log.100000001
    [junit] 2011-02-06 10:39:56,990 - WARN  [QuorumPeer:/0.0.0.0:11363:Follower@116] - Got zxid 0x100000001 expected 0x1
    [junit] 2011-02-06 10:39:56,990 - INFO  [SyncThread:5:FileTxnLog@197] - Creating new log file: log.100000001
    [junit] 2011-02-06 10:39:56,990 - WARN  [QuorumPeer:/0.0.0.0:11366:Follower@116] - Got zxid 0x100000001 expected 0x1
    [junit] 2011-02-06 10:39:56,990 - INFO  [SyncThread:1:FileTxnLog@197] - Creating new log file: log.100000001
    [junit] 2011-02-06 10:39:56,991 - INFO  [SyncThread:4:FileTxnLog@197] - Creating new log file: log.100000001
    [junit] 2011-02-06 10:39:56,995 - INFO  [main-SendThread(localhost.localdomain:11363):ClientCnxn$SendThread@738] - Session establishment complete on server localhost.localdomain/127.0.0.1:11363, sessionid = 0x12dfc45e6dd0000, negotiated timeout = 30000
    [junit] 2011-02-06 10:39:56,996 - INFO  [CommitProcessor:1:NIOServerCnxn@1580] - Established session 0x12dfc45e6dd0000 with negotiated timeout 30000 for client /127.0.0.1:37810
    [junit] 2011-02-06 10:39:56,999 - INFO  [main:ZooKeeper@436] - Initiating client connection, connectString=127.0.0.1:11364 sessionTimeout=30000 watcher=org.apache.zookeeper.test.QuorumTest$5@248523a0 sessionId=85001345146093568 sessionPasswd=<hidden>
    [junit] 2011-02-06 10:39:57,000 - INFO  [main-SendThread():ClientCnxn$SendThread@1041] - Opening socket connection to server /127.0.0.1:11364
    [junit] 2011-02-06 10:39:57,000 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11364:NIOServerCnxn$Factory@251] - Accepted socket connection from /127.0.0.1:36682
    [junit] 2011-02-06 10:39:57,001 - INFO  [main-SendThread(localhost.localdomain:11364):ClientCnxn$SendThread@949] - Socket connection established to localhost.localdomain/127.0.0.1:11364, initiating session
    [junit] 2011-02-06 10:39:57,002 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11364:NIOServerCnxn@770] - Client attempting to renew session 0x12dfc45e6dd0000 at /127.0.0.1:36682
    [junit] 2011-02-06 10:39:57,002 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11364:Learner@95] - Revalidating client: 85001345146093568
    [junit] 2011-02-06 10:39:57,003 - INFO  [QuorumPeer:/0.0.0.0:11364:NIOServerCnxn@1580] - Established session 0x12dfc45e6dd0000 with negotiated timeout 30000 for client /127.0.0.1:36682
    [junit] 2011-02-06 10:39:57,004 - INFO  [main-SendThread(localhost.localdomain:11364):ClientCnxn$SendThread@738] - Session establishment complete on server localhost.localdomain/127.0.0.1:11364, sessionid = 0x12dfc45e6dd0000, negotiated timeout = 30000
    [junit] 2011-02-06 10:39:57,005 - WARN  [CommitProcessor:2:NIOServerCnxn@1524] - Unexpected exception. Destruction averted.
    [junit] java.lang.NullPointerException
    [junit] 	at org.apache.jute.BinaryOutputArchive.writeRecord(BinaryOutputArchive.java:123)
    [junit] 	at org.apache.zookeeper.proto.SetDataResponse.serialize(SetDataResponse.java:40)
    [junit] 	at org.apache.jute.BinaryOutputArchive.writeRecord(BinaryOutputArchive.java:123)
    [junit] 	at org.apache.zookeeper.server.NIOServerCnxn.sendResponse(NIOServerCnxn.java:1500)
    [junit] 	at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:367)
    [junit] 	at org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:73)
    [junit] Running org.apache.zookeeper.test.QuorumTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.zookeeper.test.QuorumTest FAILED (timeout)
    [junit] 2011-02-06 10:53:26,189 - INFO  [main:PortAssignment@31] - assigning port 11221
    [junit] 2011-02-06 10:53:26,192 - INFO  [main:PortAssignment@31] - assigning port 11222
{noformat}
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-984,Blocker,Patrick D. Hunt,Cannot Reproduce,2013-12-24T10:39:36.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,jenkins failure in testSessionMoved - NPE in quorum,2019-02-28T19:47:44.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",6.0
Patrick D. Hunt,"[<JIRA Component: name='scripts', id='12312384'>]",2011-02-03T05:21:01.000+0000,Patrick D. Hunt,"If zkServer.sh is run remotely using ssh as follows ssh will ""hang"" - i.e. not complete/return once the server is started. This is even though zkServer.sh starts the java vm in the background.

$ ssh <host> ""zkServer.sh start""

this is due to the following issue:

http://www.slac.stanford.edu/comp/unix/ssh_faq.html#logoff_hangs

","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-983,Minor,Patrick D. Hunt,Fixed,2011-02-27T06:57:03.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,running zkServer.sh start remotely using ssh hangs,2011-11-23T19:22:41.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",2.0
Thomas Koch,"[<JIRA Component: name='scripts', id='12312384'>]",2011-02-02T09:52:58.000+0000,Bjørn Remseth,"When running ""zkServer.sh start"" I get these error messages:

====
$sudo sh  bin/zkServer.sh start
MX enabled by default
bin/zkServer.sh: 69: cygpath: not found
Using config: 
grep: : No such file or directory
Starting zookeeper ... 
STARTED
$ Invalid config, exiting abnormally
====

The ""Invalid config..."" text is output from the server which terminates immediately after this message has been printed.

The fix is easy:   Inside zkServer.sh change the line
====
if $cygwin
====

into

====
if [ -n ""$cygwin"" ]
====

This fixes the problem and makes the server run


","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-982,Minor,Bjørn Remseth,Invalid,2011-12-12T17:47:55.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zkServer.sh won't start zookeeper on an ubuntu 10.10 system due to a bug in the startup script.,2011-12-12T17:47:55.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",2.0
Jeremy Stribling,"[<JIRA Component: name='c client', id='12312380'>]",2011-02-01T20:23:01.000+0000,Jeremy Stribling,"I saw a hang once when my C++ application called the zookeeper_close() method of the multi-threaded Zookeeper client library.  The stack trace of the hung thread was the following:

{quote}
Thread 8 (Thread 5644):
#0  0x00007f5d7bb5bbe4 in __lll_lock_wait () from /lib/libpthread.so.0
#1  0x00007f5d7bb59ad0 in pthread_cond_broadcast@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#2  0x00007f5d793628f6 in unlock_completion_list (l=0x32b4d68) at .../zookeeper/src/c/src/mt_adaptor.c:66
#3  0x00007f5d79354d4b in free_completions (zh=0x32b4c80, callCompletion=1, reason=-116) at .../zookeeper/src/c/src/zookeeper.c:1069
#4  0x00007f5d79355008 in cleanup_bufs (zh=0x32b4c80, callCompletion=1, rc=-116) at .../thirdparty/zookeeper/src/c/src/zookeeper.c:1125
#5  0x00007f5d79353200 in destroy (zh=0x32b4c80) at .../thirdparty/zookeeper/src/c/src/zookeeper.c:366
#6  0x00007f5d79358e0e in zookeeper_close (zh=0x32b4c80) at .../zookeeper/src/c/src/zookeeper.c:2326
#7  0x00007f5d79356d18 in api_epilog (zh=0x32b4c80, rc=0) at .../zookeeper/src/c/src/zookeeper.c:1661
#8  0x00007f5d79362f2f in adaptor_finish (zh=0x32b4c80) at .../zookeeper/src/c/src/mt_adaptor.c:205
#9  0x00007f5d79358c8c in zookeeper_close (zh=0x32b4c80) at .../zookeeper/src/c/src/zookeeper.c:2297 
...
{quote}

The omitted part of the stack trace is entirely within my application, and contains no other calls to/from the Zookeeper client.  In particular, I am not calling zookeeper_close() from within a completion handler or any of the library's threads.

I haven't been able to reproduce this, and when I encountered this I wasn't capturing logging from the client library, so unfortunately I don't have any more information at this time.  But I will update this JIRA if I see it again.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-981,Critical,Jeremy Stribling,Fixed,2011-09-14T04:10:37.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Hang in zookeeper_close() in the multi-threaded C client,2011-11-23T19:22:00.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",7.0
,"[<JIRA Component: name='server', id='12312382'>]",2011-01-27T16:44:00.000+0000,Hugh Warrington,"I'm using zk 3.3.2 and I'm seeing this in my logs around startup:

2011-01-27 10:16:21,513 [WorkerSender Thread] WARN  org.apache.zookeeper.server.quorum.QuorumCnxManager - Cannot open channel to 0 at election address xxx.yyy.com/10.2.131.19:3888
java.net.UnknownHostException
	at sun.nio.ch.Net.translateException(Net.java:100)
	at sun.nio.ch.SocketAdaptor.connect(SocketAdaptor.java:140)
	at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:366)
	at org.apache.zookeeper.server.quorum.QuorumCnxManager.toSend(QuorumCnxManager.java:335)
	at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.process(FastLeaderElection.java:360)
	at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.run(FastLeaderElection.java:333)
	at java.lang.Thread.run(Thread.java:636)

And all subsequent zk ops give {{ConnectionLossException}}.

I've just explained this to breed_zk on IRC, and he asked me to file a ticket, mentioning that UnknownHostException may sometimes be thrown for reasons other than host resolution. While I'm reasonably certain that the hostname is correct and should be contactable, I need to put some more time into checking our network setup to be absolutely sure. However, two observations arose while looking into this:

* At the top of QuorumCnxManager.connectOne(), we set electionAddr (or fail and return). But then a few lines later we don't actually use this local variable in the call to connect(). This seems like a minor programming mistake (although AFAICT it doesn't change the behaviour).
* In the subsequent catch block, the UnknownHostException that's thrown doesn't contain the address that we were trying to connect to (though if you capture WARN log messages, you can see what it was).",[],Bug,ZOOKEEPER-979,Minor,Hugh Warrington,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,UnknownHostException in QuorumCnxManager,2011-01-28T14:10:16.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",3.0
Thomas Koch,"[<JIRA Component: name='server', id='12312382'>]",2011-01-20T17:04:40.000+0000,Sergei Bobovich,"ZookeeperServer does not close zk database on shutdown leaving log files open. Not sure if this is an intention, but looks like a possible bug to me. Database is getting closed only from QuorumPeer class. 
Hit it when executing regression tests on windows: failed to delete log files from cleanup.
 ","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-978,Major,Sergei Bobovich,Duplicate,2014-05-18T02:33:00.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZookeeperServer does not close zk database on shutdwon,2014-05-18T02:33:00.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",2.0
Patrick D. Hunt,[],2011-01-18T01:36:16.000+0000,Patrick D. Hunt,"From bug filed on CDH: https://issues.cloudera.org/browse/DISTRO-47 - moving it to this jira to address:

------------------------------------------------------
Bug filed by ""grep.alex"" at http://getsatisfaction.com/cloudera/topics/cdh3b3_zookeeper_startup_script_doesnt_use_java_home

On RedHat 5 (using the RPM installer) I was able to install and run all the Hadoop components. The Zookeeper install was fine, but it wouldn't start:

{noformat}
[root@aholmes-desktop init.d]# ./hadoop-zookeeper start 
JMX enabled by default 
Using config: /etc/zookeeper/zoo.cfg 
Starting zookeeper ... 
STARTED 
[root@aholmes-desktop init.d]# Exception in thread ""main"" java.lang.NoSuchMethodError: method java.lang.management.ManagementFactory.getPlatformMBeanServer with signature ()Ljavax.management.MBeanServer; was not found. 
at org.apache.zookeeper.jmx.ManagedUtil.registerLog4jMBeans(ManagedUtil.java:48 
...
{noformat} 

After some digging around I found the cause - the Zookeeper startup script (/usr/lib/zookeeper/bin/zkServer.sh ) uses the java found in the path, whereas the other startup scripts use JAVA_HOME. In my case I had the default RHEL5 1.4 JDK in the path, and the 1.6 JDK RPM's installed under /usr/java, hence the above error, which I'm guessing is a fairly common setup.

In my opinion all the startup scripts should all use the same mechanism to determine where to pick java.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-976,Minor,Patrick D. Hunt,Fixed,2011-02-27T07:02:15.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeper startup script doesn't use JAVA_HOME,2011-11-23T19:22:31.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",2.0
Vishal Kher,[],2011-01-14T12:29:33.000+0000,Vishal Kher,"Scenario:
1. 2 of the 3 ZK nodes are online
2. Third node is attempting to join
3. Third node unnecessarily goes in ""LEADING"" state
4. Then third goes back to LOOKING (no majority of followers) and finally goes to FOLLOWING state.


While going through the logs I noticed that a peer C that is trying to
join an already formed cluster goes in LEADING state. This is because
QuorumCnxManager of A and B sends the entire history of notification
messages to C. C receives the notification messages that were
exchanged between A and B when they were forming the cluster.

In FastLeaderElection.lookForLeader(), due to the following piece of
code, C quits lookForLeader assuming that it is supposed to lead.

740                             //If have received from all nodes, then terminate
741                             if ((self.getVotingView().size() == recvset.size()) &&
742                                     (self.getQuorumVerifier().getWeight(proposedLeader) != 0)){
743                                 self.setPeerState((proposedLeader == self.getId()) ?
744                                         ServerState.LEADING: learningState());
745                                 leaveInstance();
746                                 return new Vote(proposedLeader, proposedZxid);
747
748                             } else if (termPredicate(recvset,


This can cause:
1.  C to unnecessarily go in LEADING state and wait for tickTime * initLimit and then restart the FLE.

2. C waits for 200 ms (finalizeWait) and then considers whatever
notifications it has received to make a decision. C could potentially
decide to follow an old leader, fail to connect to the leader, and
then restart FLE. See code below.

752                             if (termPredicate(recvset,
753                                     new Vote(proposedLeader, proposedZxid,
754                                             logicalclock))) {
755 
756                                 // Verify if there is any change in the proposed leader
757                                 while((n = recvqueue.poll(finalizeWait,
758                                         TimeUnit.MILLISECONDS)) != null){
759                                     if(totalOrderPredicate(n.leader, n.zxid,
760                                             proposedLeader, proposedZxid)){
761                                         recvqueue.put(n);
762                                         break;
763                                     }
764                                 }



In general, this does not affect correctness of FLE since C will
eventually go back to FOLLOWING state (A and B won't vote for
C). However, this delays C from joining the cluster. This can in turn
affect recovery time of an application.


Proposal: A and B should send only the latest notification (most
recent) instead of the entire history. Does this sound reasonable?



","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-975,Major,Vishal Kher,Fixed,2011-04-29T16:13:46.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,new peer goes in LEADING state even if ensemble is online,2011-11-23T19:22:03.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",4.0
Harsh J,"[<JIRA Component: name='server', id='12312382'>]",2011-01-05T06:47:52.000+0000,Vishal Kher,"setReuseAddress(true) should be used below.

    Leader(QuorumPeer self,LeaderZooKeeperServer zk) throws IOException {
        this.self = self;
        try {
            ss = new ServerSocket(self.getQuorumAddress().getPort());
        } catch (BindException e) {
            LOG.error(""Couldn't bind to port ""
                    + self.getQuorumAddress().getPort(), e);
            throw e;
        }
        this.zk=zk;
    }

","[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.3.5', id='12319081'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-973,Trivial,Vishal Kher,Fixed,2012-01-23T20:35:07.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,bind() could fail on Leader because it does not setReuseAddress on its ServerSocket ,2012-01-24T10:59:50.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",3.0
,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2011-01-03T19:06:51.000+0000,Robert Powers,"The issue I'm seeing seems strikingly similar to this: https://issues.apache.org/jira/browse/ZOOKEEPER-772

I have one writer process which adds sequenced children nodes to /queue and a separate reader process which sets a children watcher on /queue, waiting for children to be added or deleted. Long story short, every time a child node is added or deleted by the writer, the reader's watcher is supposed to trigger so the reader can check if it's time to get to work or go back to bed. Bad things seem to happen while the reader is waiting on the watcher and the writer adds or deletes a node.

In versions prior to 3.3.2, my code that sets a watcher on the children of a node using the perl binding would either lock up when trying to retrieve the children or would segfault when a child node was added while waiting on the watch. In 3.3.2, it seems to just do the locking up.

I'm seeing this: assertion botched (free()ed/realloc()ed-away memory was overwritten?): !(MallocCfg[MallocCfg_filldead] && MallocCfg[Mall
ocCfg_fillcheck]) || !cmp_pat_4bytes((unsigned char*)(p + 1), (((1 << ((bucket) >> 0)) + ((bucket >= 15 * 1) ? 4096 : 0)) - (siz
eof(union overhead) + sizeof (unsigned int))) + sizeof (unsigned int), fill_deadbeef) (malloc.c:1536)

I managed to get a stack trace

Program received signal SIGABRT, Aborted.
0xffffe410 in __kernel_vsyscall ()
(gdb) where
#0  0xffffe410 in __kernel_vsyscall ()
#1  0xf7b8ed80 in raise () from /lib/libc.so.6
#2  0xf7b90691 in abort () from /lib/libc.so.6
#3  0xf7d6d53f in botch (diag=0xa <Address 0xa out of bounds>, 
    s=0xf7ef42e8 ""!(MallocCfg[MallocCfg_filldead] && MallocCfg[MallocCfg_fillcheck]) || !cmp_pat_4bytes((unsigned char*)(p + 1),
 (((1 << ((bucket) >> 0)) + ((bucket >= 15 * 1) ? 4096 : 0)) - (sizeof(union overhead) + s""..., file=0xf7ef4119 ""malloc.c"", line
=1536) at malloc.c:1327
#4  0xf7d6d97a in Perl_malloc (nbytes=15530) at malloc.c:1535
#5  0xf7d6f974 in Perl_calloc (elements=1, size=0) at malloc.c:2314
#6  0xf7929eca in _zk_create_watch (my_perl=0x0) at ZooKeeper.xs:204
#7  0xf7929f8f in _zk_acquire_watch (my_perl=0x0) at ZooKeeper.xs:240
#8  0xf793450b in XS_Net__ZooKeeper_watch (my_perl=0x889c008, cv=0x89db8b4) at ZooKeeper.xs:2035
#9  0xf7e1dd67 in Perl_pp_entersub (my_perl=0x889c008) at pp_hot.c:2847
#10 0xf7de47ce in Perl_runops_debug (my_perl=0x889c008) at dump.c:1931
#11 0xf7e0d856 in perl_run (my_perl=0x889c008) at perl.c:2384
#12 0x08048ace in main (argc=2, argv=0xffe11814, env=0xffe11820) at perlmain.c:113

The code to reproduce:
sub bide_time
{
  my $root = '/queue';
  my $timeout = 20*1000;
  my $zkc = Net::ZooKeeper->new('localhost:2181');

  while (1) {
    print ""Retrieving $root\n"";
    my $child_watch = $zkc->watch('timeout' => $timeout);

    my @children = $zkc->get_children($root, watch=>$child_watch);
    if (scalar(@children)) {
      return @children if (rand(1) > 0.75);
    } else {
      print "" - No Children.\n"";
    }
    print ""Time to wait for the Children.\n"";
    if ($child_watch->wait()) {
      print ""watch triggered on node $root:\n"";
      print ""  event: $child_watch->{event}\n"";
      print ""  state: $child_watch->{state}\n"";
    } else {
      print ""watch timed out\n"";
    }
  }
}",[],Bug,ZOOKEEPER-972,Major,Robert Powers,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,perl Net::ZooKeeper segfaults when setting a watcher on get_children,2022-02-03T08:50:24.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",0.0
Carl Steinbach,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='documentation', id='12312422'>]",2010-12-23T08:34:39.000+0000,Carl Steinbach,"It's possible to make Forrest work with JDK6 by disabling sitemap validation
in the forrest.properties file. See FOR-984 and PIG-1508 for more details.","[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-963,Major,Carl Steinbach,Fixed,2010-12-29T01:08:57.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Make Forrest work with JDK6,2011-11-23T19:22:28.000+0000,[],1.0
Chia-Hung Lin,"[<JIRA Component: name='server', id='12312382'>]",2010-12-21T18:42:59.000+0000,Camille Fournier,"From mailing list:
It seems like we rely on the LearnerHandler thread startup to capture all of the missing committed
transactions in the SNAP or DIFF, but I don't see anything (especially in the DIFF case) that
is preventing us for committing more transactions before we actually start forwarding updates
to the new follower.

Let me explain using my example from ZOOKEEPER-919. Assume we have quorum already, so the
leader can be processing transactions while my follower is starting up.

I'm a follower at zxid N-5, the leader is at N. I send my FOLLOWERINFO packet to the leader
with that information. The leader gets the proposals from its committed log (time T1), then
syncs on the proposal list (LearnerHandler line 267. Why? It's a copy of the underlying proposal
list... this might be part of our problem). I check to see if the peerLastZxid is within my
max and min committed log and it is, so I'm going to send a diff. I set the zxidToSend to
be the maxCommittedLog at time T3 (we already know this is sketchy), and forward the proposals
from my copied proposal list starting at the peerLastZxid+1 up to the last proposal transaction
(as seen at time T1).

After I have queued up all those diffs to send, I tell the leader to startFowarding updates
to this follower (line 308). 

So, let's say that at time T2 I actually swap out the leader to the thread that is handling
the various request processors, and see that I got enough votes to commit zxid N+1. I commit
N+1 and so my maxCommittedLog at T3 is N+1, but this proposal is not in the list of proposals
that I got back at time T1, so I don't forward this diff to the client. Additionally, I processed
the commit and removed it from my leader's toBeApplied list. So when I call startForwarding
for this new follower, I don't see this transaction as a transaction to be forwarded. 

There's one problem. Let's also imagine, however, that I commit N+1 at time T4. The maxCommittedLog
value is consistent with the max of the diff packets I am going to send the follower. But,
I still committed N+1 and removed it from the toBeApplied list before calling startFowarding
with this follower. How does the follower get this transaction? Does it?

To put it another way, here is the thread interaction, hopefully formatted so you can read
it...

		LearnerHandlerThread					RequestProcessorThread
T1(LH):	get list of proposals (COPY)
T2(RPT):								commit N+1, remove from toBeApplied
T3(LH):	get maxCommittedLog
T4(LH):	send diffs from view at T1
T5(LH):	startForwarding


Or
T1(LH):	get list of proposals (COPY)
T2(LH):	get maxCommittedLog
T3(RPT):								commit N+1, remove from toBeApplied
T4(LH):	send diffs from view at T1
T5(LH):	startFowarding


I'm trying to figure out what, if anything, keeps the requests from being committed, removed,
and never seen by the follower before it fully starts up. 

","[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-962,Critical,Camille Fournier,Fixed,2011-01-23T05:31:02.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,leader/follower coherence issue when follower is receiving a DIFF,2011-11-23T19:22:24.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",3.0
Matthias Spycher,"[<JIRA Component: name='java client', id='12312381'>]",2010-12-21T16:07:16.000+0000,pmpm47,"Let's say you're using connection string ""127.0.0.1:2182/foo"".
1) put a childrenchanged watch on relative / (that is, on absolute path /foo)
2) stop the zk server
3) start the zk server
4) at this point, the client recovers the connection, and should have put back a watch on relative path /, but instead the client puts a watch on the *absolute* path /
- if some other client adds or removes a node under /foo, nothing will happen
- if some other client adds or removes a node under /, then you will get an error from the zk client library (string operation error)","[<JIRA Version: name='3.3.4', id='12316276'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-961,Critical,pmpm47,Fixed,2011-09-14T05:51:47.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Watch recovery after disconnection when connection string contains a prefix,2011-11-23T19:22:02.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",3.0
Ivan Kelly,"[<JIRA Component: name='contrib-hedwig', id='12313735'>]",2010-12-15T09:18:22.000+0000,Ivan Kelly,"Currently the hedwig cpp client will automatically send a consume message to the server when the calling client indicated that it has received the message. If the client wants to queue the messages and not acknowledge them to the server immediately, they need to block, which means interfering with any other running callbacks. ","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-958,Major,Ivan Kelly,Fixed,2010-12-21T19:34:07.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Flag to turn off autoconsume in hedwig c++ client,2011-11-23T19:22:42.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
Ted Dunning,[],2010-12-13T17:09:05.000+0000,Ted Dunning,"Somebody left some echo statements in the zkCleanup.sh which prevents the java commands from actually running.

Patch coming forthwith.","[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-957,Major,Ted Dunning,Fixed,2010-12-15T03:17:40.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkCleanup.sh doesn't do anything,2011-11-23T19:21:58.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",1.0
Hiroshi Ikeda,"[<JIRA Component: name='java client', id='12312381'>]",2010-11-29T09:21:35.000+0000,Thomas Koch,"JLM 	Synchronization performed on java.util.concurrent.LinkedBlockingQueue in org.apache.zookeeper.ClientCnxn$EventThread.queuePacket(ClientCnxn$Packet)
	
Bug type JLM_JSR166_UTILCONCURRENT_MONITORENTER (click for details)
In class org.apache.zookeeper.ClientCnxn$EventThread
In method org.apache.zookeeper.ClientCnxn$EventThread.queuePacket(ClientCnxn$Packet)
Type java.util.concurrent.LinkedBlockingQueue
Value loaded from field org.apache.zookeeper.ClientCnxn$EventThread.waitingEvents
At ClientCnxn.java:[line 411]
JLM 	Synchronization performed on java.util.concurrent.LinkedBlockingQueue in org.apache.zookeeper.ClientCnxn$EventThread.run()
	
Bug type JLM_JSR166_UTILCONCURRENT_MONITORENTER (click for details)
In class org.apache.zookeeper.ClientCnxn$EventThread
In method org.apache.zookeeper.ClientCnxn$EventThread.run()
Type java.util.concurrent.LinkedBlockingQueue
Value loaded from field org.apache.zookeeper.ClientCnxn$EventThread.waitingEvents
At ClientCnxn.java:[line 436]

The respective code:

409	       public void queuePacket(Packet packet) {
410	          if (wasKilled) {
411	             synchronized (waitingEvents) {
412	                if (isRunning) waitingEvents.add(packet);
413	                else processEvent(packet);
414	             }
415	          } else {
416	             waitingEvents.add(packet);
417	          }
418	       }
419	
420	        public void queueEventOfDeath() {
421	            waitingEvents.add(eventOfDeath);
422	        }
423	
424	        @Override
425	        public void run() {
426	           try {
427	              isRunning = true;
428	              while (true) {
429	                 Object event = waitingEvents.take();
430	                 if (event == eventOfDeath) {
431	                    wasKilled = true;
432	                 } else {
433	                    processEvent(event);
434	                 }
435	                 if (wasKilled)
436	                    synchronized (waitingEvents) {
437	                       if (waitingEvents.isEmpty()) {
438	                          isRunning = false;
439	                          break;
440	                       }
441	                    }
442	              }
",[],Bug,ZOOKEEPER-954,Minor,Thomas Koch,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Findbugs/ClientCnxn: Bug type JLM_JSR166_UTILCONCURRENT_MONITORENTER,2016-03-03T01:47:11.000+0000,[],2.0
,"[<JIRA Component: name='server', id='12312382'>]",2010-11-24T07:53:06.000+0000,Qian Ye,"I have a group of zookeeper servers, there are three servers in this group.
server.0=10.81.4.11:2888:3888
server.1=10.23.240.93:2888:3888
server.2=10.23.244.224:2888:3888

At first, the cluster ran well.  About several days ago, I shut down the zookeeper process on one of servers(server.2)., and today, I find that the other two servers run in wired status(the network is fine). The zookeeper process take pretty much resource on the two servers:

on server.1 (it's the leader)
  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                                                                                                                               
26836 work      18   0 12.8g 803m 8724 S  3.7 10.1 195:56.56 java 

$ ll /proc/26836/fd/ | wc -l
3586

[work@tc-test-aos03.tc.baidu.com conf]$ ll /proc/26836/task/ | wc -l
10510

some warning log:
2010-11-24 15:37:48,705 - WARN  [Thread-37409:QuorumCnxManager$SendWorker@589] - Send worker leaving thread
2010-11-24 15:39:48,626 - WARN  [Thread-37414:QuorumCnxManager$RecvWorker@658] - Connection broken:
java.nio.channels.AsynchronousCloseException
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:185)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:263)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:629)
2010-11-24 15:39:48,656 - WARN  [Thread-37413:QuorumCnxManager$SendWorker@581] - Interrupted while waiting for message on queue
java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1899)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1976)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:342)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:570)
2010-11-24 15:39:48,657 - WARN  [Thread-37413:QuorumCnxManager$SendWorker@589] - Send worker leaving thread
2010-11-24 15:41:48,614 - WARN  [Thread-37417:QuorumCnxManager$SendWorker@581] - Interrupted while waiting for message on queue
java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1899)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1976)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:342)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:570)
2010-11-24 15:41:48,643 - WARN  [Thread-37418:QuorumCnxManager$RecvWorker@658] - Connection broken:
java.nio.channels.AsynchronousCloseException
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:185)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:263)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:629)
2010-11-24 15:41:48,662 - WARN  [Thread-37417:QuorumCnxManager$SendWorker@589] - Send worker leaving thread
2010-11-24 15:43:48,627 - WARN  [Thread-37421:QuorumCnxManager$SendWorker@581] - Interrupted while waiting for message on queue
java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1899)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1976)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:342)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:570)
2010-11-24 15:43:48,627 - WARN  [Thread-37422:QuorumCnxManager$RecvWorker@658] - Connection broken:
java.nio.channels.AsynchronousCloseException
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:185)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:263)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:629)
2010-11-24 15:43:48,654 - WARN  [Thread-37421:QuorumCnxManager$SendWorker@589] - Send worker leaving thread
2010-11-24 15:44:48,622 - WARN  [Thread-37424:QuorumCnxManager$RecvWorker@658] - Connection broken:
java.nio.channels.AsynchronousCloseException
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:185)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:263)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:629)
2010-11-24 15:44:48,652 - WARN  [Thread-37423:QuorumCnxManager$SendWorker@581] - Interrupted while waiting for message on queue
java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1899)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1976)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:342)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:570)
2010-11-24 15:44:48,653 - WARN  [Thread-37423:QuorumCnxManager$SendWorker@589] - Send worker leaving thread
2010-11-24 15:45:48,668 - WARN  [Thread-37426:QuorumCnxManager$RecvWorker@658] - Connection broken:
java.io.IOException: Channel eof
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:630)
2010-11-24 15:46:48,647 - WARN  [Thread-37427:QuorumCnxManager$SendWorker@581] - Interrupted while waiting for message on queue
java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1899)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1976)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:342)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:570)
2010-11-24 15:46:48,722 - WARN  [Thread-37428:QuorumCnxManager$RecvWorker@658] - Connection broken:
java.nio.channels.AsynchronousCloseException
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:185)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:263)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:629)
2010-11-24 15:46:48,736 - WARN  [Thread-37427:QuorumCnxManager$SendWorker@589] - Send worker leaving thread
2010-11-24 15:47:48,687 - WARN  [Thread-37430:QuorumCnxManager$RecvWorker@658] - Connection broken:
java.io.IOException: Channel eof
        at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:630)


on server.0
  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                                                                                                                               
27322 work      19   0 15.2g 943m 9140 S 38.6 11.8   1396:51 java

$ ll /proc/27322/fd/ | wc -l
3587

$ ll /proc/27322/task/ | wc -l
12938

2010-11-24 15:37:49,269 - WARN  [Thread-37407:QuorumCnxManager$SendWorker@589] - Send worker leaving thread
2010-11-24 15:39:49,235 - WARN  [Thread-37412:QuorumCnxManager$RecvWorker@658] - Connection broken: 
java.io.IOException: Channel eof
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:630)
2010-11-24 15:39:49,410 - WARN  [Thread-37411:QuorumCnxManager$SendWorker@581] - Interrupted while waiting for message on queue
java.lang.InterruptedException
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1899)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1976)
    at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:342)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:570)
2010-11-24 15:39:49,411 - WARN  [Thread-37411:QuorumCnxManager$SendWorker@589] - Send worker leaving thread
2010-11-24 15:41:49,314 - WARN  [Thread-37416:QuorumCnxManager$RecvWorker@658] - Connection broken: 
java.io.IOException: Channel eof
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:630)
2010-11-24 15:41:49,383 - WARN  [Thread-37415:QuorumCnxManager$SendWorker@581] - Interrupted while waiting for message on queue
java.lang.InterruptedException
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1899)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1976)
    at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:342)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:570)
2010-11-24 15:41:49,405 - WARN  [Thread-37415:QuorumCnxManager$SendWorker@589] - Send worker leaving thread
2010-11-24 15:43:49,372 - WARN  [Thread-37420:QuorumCnxManager$RecvWorker@658] - Connection broken: 
java.io.IOException: Channel eof
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:630)
2010-11-24 15:43:49,512 - WARN  [Thread-37419:QuorumCnxManager$SendWorker@581] - Interrupted while waiting for message on queue
java.lang.InterruptedException
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1899)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1976)
    at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:342)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:570)
2010-11-24 15:43:49,513 - WARN  [Thread-37419:QuorumCnxManager$SendWorker@589] - Send worker leaving thread
2010-11-24 15:44:49,407 - WARN  [Thread-37422:QuorumCnxManager$RecvWorker@658] - Connection broken: 
java.io.IOException: Channel eof
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:630)
2010-11-24 15:45:49,645 - WARN  [Thread-37424:QuorumCnxManager$RecvWorker@658] - Connection broken: 
java.nio.channels.AsynchronousCloseException
    at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:185)
    at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:263)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:629)
2010-11-24 15:45:49,781 - WARN  [Thread-37423:QuorumCnxManager$SendWorker@581] - Interrupted while waiting for message on queue
java.lang.InterruptedException
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1899)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1976)
    at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:342)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:570)
2010-11-24 15:45:49,799 - WARN  [Thread-37423:QuorumCnxManager$SendWorker@589] - Send worker leaving thread
2010-11-24 15:46:49,495 - WARN  [Thread-37427:QuorumCnxManager$RecvWorker@658] - Connection broken: 
java.io.IOException: Channel eof
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:630)
2010-11-24 15:47:49,541 - WARN  [Thread-37429:QuorumCnxManager$RecvWorker@658] - Connection broken: 
java.nio.channels.AsynchronousCloseException
    at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:185)
    at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:263)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:629)
2010-11-24 15:47:49,622 - WARN  [Thread-37428:QuorumCnxManager$SendWorker@581] - Interrupted while waiting for message on queue
java.lang.InterruptedException
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1899)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1976)
    at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:342)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:570)
2010-11-24 15:47:49,622 - WARN  [Thread-37428:QuorumCnxManager$SendWorker@589] - Send worker leaving thread
2010-11-24 15:48:48,827 - WARN  [Thread-37431:QuorumCnxManager$RecvWorker@658] - Connection broken: 
java.io.IOException: Channel eof
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$RecvWorker.run(QuorumCnxManager.java:630)


What's more, the number of threads under the zookeeper process is still increasing time by time. It seems that , something is wrong in communication of the two servers. Have anyone met such problem before?",[],Bug,ZOOKEEPER-939,Major,Qian Ye,Duplicate,2011-09-06T03:10:59.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,the threads number of a zookeeper is increased all the time,2011-09-06T03:11:01.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",0.0
Erik Hetzner,"[<JIRA Component: name='scripts', id='12312384'>]",2010-11-19T22:12:26.000+0000,Erik Hetzner,test -e FILENAME is not support on /bin/sh in solaris. This is used in bin/zkEnv.sh. We can substitute test -f FILENAME. Attaching a patch.,"[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-937,Major,Erik Hetzner,Fixed,2010-12-07T19:02:17.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,test -e not available on solaris /bin/sh,2011-11-23T19:21:59.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>, <JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.3.2', id='12315108'>]",1.0
,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2010-11-18T16:02:06.000+0000,Gustavo Niemeyer,"It looks like there are no calls to deallocate_ACL_vector() within zookeeper.c in the zkpython binding, which means that (at least) the result of zoo_get_acl() must be leaking.",[],Bug,ZOOKEEPER-936,Major,Gustavo Niemeyer,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zkpython is leaking ACL_vector,2022-02-03T08:50:24.000+0000,[],3.0
Ivan Kelly,"[<JIRA Component: name='contrib-hedwig', id='12313735'>]",2010-11-15T09:55:32.000+0000,Ivan Kelly,,[],Bug,ZOOKEEPER-930,Major,Ivan Kelly,Fixed,2010-11-16T18:28:01.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Hedwig c++ client uses a non thread safe logging library,2010-11-17T10:55:02.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='build', id='12312383'>]",2010-11-11T17:03:15.000+0000,Patrick D. Hunt,"Hi Nigel can you take a look at this?

Following you'll see the email I got, notice that the patch is patch 908, however if you look at the hudson page it's linked to the change is documented as 909 patch file applied
https://hudson.apache.org/hudson/job/PreCommit-ZOOKEEPER-Build/25/changes

I looked at both jiras ZOOKEEPER-908 and ZOOKEEPER-909 both of these look good (the right names on patches) and qabot actually updated 908 with the comment (failure). However the ""change"" is listed as 909 which is wrong.


    [exec] -1 overall.  Here are the results of testing the latest attachment
    [exec]   http://issues.apache.org/jira/secure/attachment/12459361/ZOOKEEPER-908.patch
    [exec]   against trunk revision 1033770.
    [exec]
    [exec]     +1 @author.  The patch does not contain any @author tags.
    [exec]
    [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
    [exec]                         Please justify why no new tests are needed for this patch.
    [exec]                         Also please list what manual steps were performed to verify this patch.
    [exec]
    [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
    [exec]
    [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
    [exec]
    [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
    [exec]
    [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
    [exec]
    [exec]     +1 core tests.  The patch passed core unit tests.
    [exec]
    [exec]     +1 contrib tests.  The patch passed contrib unit tests.
    [exec]
    [exec] Test results: https://hudson.apache.org/hudson/job/PreCommit-ZOOKEEPER-Build/25//testReport/
    [exec] Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-ZOOKEEPER-Build/25//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
    [exec] Console output: https://hudson.apache.org/hudson/job/PreCommit-ZOOKEEPER-Build/25//console
    [exec]
    [exec] This message is automatically generated.
    [exec]",[],Bug,ZOOKEEPER-929,Major,Patrick D. Hunt,Cannot Reproduce,2013-10-08T21:56:03.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,hudson qabot incorrectly reporting issues as number 909 when the patch from 908 is the one being tested,2013-10-08T21:56:03.000+0000,[],0.0
,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2010-11-10T20:06:16.000+0000,Vishal Kher,"In Follower.followLeader() after syncing with the leader, the follower does:
                while (self.isRunning()) {
                    readPacket(qp);
                    processPacket(qp);
                }

It looks like it relies on socket timeout expiry to figure out if the connection with the leader has gone down.  So a follower *with no cilents* may never notice a faulty leader if a Leader has a software hang, but the TCP connections with the peers are still valid. Since it has no cilents, it won't hearbeat with the Leader. If majority of followers are not connected to any clients, then FLE will fail even if other followers attempt to elect a new leader.

We should keep track of pings received from the leader and see if we havent seen
a ping packet from the leader for (syncLimit * tickTime) time and give up following the
leader.",[],Bug,ZOOKEEPER-928,Critical,Vishal Kher,Won't Fix,2010-11-10T21:40:18.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Follower should stop following and start FLE if it does not receive pings from the leader,2010-11-11T17:07:34.000+0000,"[<JIRA Version: name='3.3.2', id='12315108'>]",2.0
Nicholas Knight,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2010-11-08T08:51:59.000+0000,Nicholas Knight,"Calling {{zookeeper.create()}} seems, under certain circumstances, to be corrupting a subsequent call to Python's {{logging}} module.

Specifically, if the node does not exist (but its parent does), I end up with a traceback like this when I try to make the logging call:

{noformat}
Traceback (most recent call last):
  File ""zktest.py"", line 21, in <module>
    logger.error(""Boom?"")
  File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/logging/__init__.py"", line 1046, in error
    if self.isEnabledFor(ERROR):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/logging/__init__.py"", line 1206, in isEnabledFor
    return level >= self.getEffectiveLevel()
  File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/logging/__init__.py"", line 1194, in getEffectiveLevel
    while logger:
TypeError: an integer is required
{noformat}

But if the node already exists, or the parent does not exist, I get the appropriate NodeExists or NoNode exceptions.

I'll be attaching a test script that can be used to reproduce this behavior.","[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-921,Major,Nicholas Knight,Fixed,2010-12-29T00:46:01.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkPython incorrectly checks for existence of required ACL elements,2011-11-23T19:22:03.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.4.0', id='12314469'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2010-11-04T13:43:21.000+0000,Chang Song,"I was testing stability of Zookeeper ensemble for production deployment. Three node ensemble cluster configuration.
In a loop, I kill/restart three Zookeeper clients that created one ephemeral node each, and at the same time,
I killed Java process on one of ensemble (dont' know if it was a leader or not). Then I restarted Zookeeper on the server,

It turns out that on two zookeeper ensemble servers, all the ephemeral nodes are gone (it should), but on the newly started
Zookeeper server, the two old ephemeral nodes stayed.  The zookeeper didn't restart in standalone mode since new ephemeral
nodes gets created on all ensemble servers. 
I captured the log.


2010-11-04 17:48:50,201 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17288:NIOServerCnxn$Factory@250] - Accepted socket connection from /10.25.131.21:11191
2010-11-04 17:48:50,202 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17288:NIOServerCnxn@776] - Client attempting to establish new session at /10.25.131.21:11191
2010-11-04 17:48:50,203 - INFO  [CommitProcessor:1:NIOServerCnxn@1579] - Established session 0x12c160c31fc000b with negotiated timeout 30000 for client /10.25.131.21:11191
2010-11-04 17:48:50,206 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17288:NIOServerCnxn@633] - EndOfStreamException: Unable to read additional data from client sessionid 0x12c160c31fc000b, likely client has closed socket
2010-11-04 17:48:50,207 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17288:NIOServerCnxn@1434] - Closed socket connection for client /10.25.131.21:11191 which had sessionid 0x12c160c31fc000b
2010-11-04 17:48:50,207 - ERROR [CommitProcessor:1:NIOServerCnxn@444] - Unexpected Exception:
java.nio.channels.CancelledKeyException
        at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:55)
        at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:59)
        at org.apache.zookeeper.server.NIOServerCnxn.sendBuffer(NIOServerCnxn.java:417)
        at org.apache.zookeeper.server.NIOServerCnxn.sendResponse(NIOServerCnxn.java:1508)
        at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:367)
        at org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:73)
","[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-919,Blocker,Chang Song,Duplicate,2011-11-19T01:11:55.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Ephemeral nodes remains in one of ensemble after deliberate SIGKILL,2011-11-23T19:22:22.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",2.0
,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='server', id='12312382'>]",2010-11-03T12:33:32.000+0000,Alexandre Hardy,"We had three nodes running zookeeper:
  * 192.168.130.10
  * 192.168.130.11
  * 192.168.130.14

192.168.130.11 failed, and was replaced by a new node 192.168.130.13 (automated startup). The new node had not participated in any zookeeper quorum previously. The node 192.148.130.11 was permanently removed from service and could not contribute to the quorum any further (powered off).

DNS entries were updated for the new node to allow all the zookeeper servers to find the new node.

The new node 192.168.130.13 was selected as the LEADER, despite the fact that it had not seen the latest zxid.

This particular problem has not been verified with later versions of zookeeper, and no attempt has been made to reproduce this problem as yet.",[],Bug,ZOOKEEPER-917,Critical,Alexandre Hardy,Not A Problem,2010-11-04T12:40:36.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Leader election selected incorrect leader,2011-11-19T01:01:09.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>]",1.0
Ivan Kelly,"[<JIRA Component: name='contrib-hedwig', id='12313735'>]",2010-11-03T09:10:22.000+0000,Ivan Kelly,"We see this bug with receiving messages from a subscribed channel.  This problem seems to happen with larger messages.  The flow is to first read at least 4 bytes from the socket channel. Extract the first 4 bytes to get the message size.  If we've read enough data into the buffer already, we're done so invoke the messageReadCallbackHandler passing the channel and message size.  If not, then do an async read for at least the remaining amount of bytes in the message from the socket channel.  When done, invoke the messageReadCallbackHandler.

The problem seems that when the second async read is done, the same sizeReadCallbackHandler is invoked instead of the messageReadCallbackHandler.  The result is that we then try to read the first 4 bytes again from the buffer.  This will get a random message size and screw things up.  I'm not sure if it's an incorrect use of the boost asio async_read function or we're doing the boost bind to the callback function incorrectly.


101015 15:30:40.108 DEBUG hedwig.channel.cpp - DuplexChannel::sizeReadCallbackHandler system:0,512 channel(0x80b7a18)
101015 15:30:40.108 DEBUG hedwig.channel.cpp - DuplexChannel::sizeReadCallbackHandler: size of buffer before reading message size: 512 channel(0x80b7a18)
101015 15:30:40.108 DEBUG hedwig.channel.cpp - DuplexChannel::sizeReadCallbackHandler: size of incoming message 599, currently in buffer 508 channel(0x80b7a18)
101015 15:30:40.108 DEBUG hedwig.channel.cpp - DuplexChannel::sizeReadCallbackHandler: Still have more data to read, 91 from channel(0x80b7a18)
101015 15:30:40.108 DEBUG hedwig.channel.cpp - DuplexChannel::sizeReadCallbackHandler system:0, 91 channel(0x80b7a18)
101015 15:30:40.108 DEBUG hedwig.channel.cpp - DuplexChannel::sizeReadCallbackHandler: size of buffer before reading message size: 599 channel(0x80b7a18)
101015 15:30:40.108 DEBUG hedwig.channel.cpp - DuplexChannel::sizeReadCallbackHandler: size of incoming message 134287360, currently in buffer 595 channel(0x80b7a18)
101015 15:30:40.108 DEBUG hedwig.channel.cpp - DuplexChannel::sizeReadCallbackHandler: Still have more data to read, 134286765 from channel(0x80b7a18)
",[],Bug,ZOOKEEPER-916,Major,Ivan Kelly,Fixed,2010-11-05T06:45:07.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Problem receiving messages from subscribed channels in c++ client ,2010-11-05T10:52:47.000+0000,[],1.0
gaoshu,[],2010-10-28T22:43:32.000+0000,Benjamin Reed,"If an error in sync() processing happens at the leader (SESSION_MOVED for example), they are not propagated back to the client.","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-915,Major,Benjamin Reed,,,This issue is being actively worked on at the moment by the assignee.,In Progress,0.0,Errors that happen during sync() processing at the leader do not get propagated back to the client.,2022-02-03T08:36:24.000+0000,[],3.0
Vishal Kher,"[<JIRA Component: name='leaderElection', id='12312378'>]",2010-10-27T19:54:48.000+0000,Vishal Kher,"This was a disaster. While testing our application we ran into a scenario where a rebooted follower could not join the cluster. Further debugging showed that the follower could not join because the QuorumCnxManager on the leader was blocked for indefinite amount of time in receiveConnect()

""Thread-3"" prio=10 tid=0x00007fa920005800 nid=0x11bb runnable [0x00007fa9275ed000]
   java.lang.Thread.State: RUNNABLE
    at sun.nio.ch.FileDispatcher.read0(Native Method)
    at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:21)
    at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:233)
    at sun.nio.ch.IOUtil.read(IOUtil.java:206)
    at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:236)
    - locked <0x00007fa93315f988> (a java.lang.Object)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager.receiveConnection(QuorumCnxManager.java:210)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager$Listener.run(QuorumCnxManager.java:501)

I had pointed out this bug along with several other problems in QuorumCnxManager earlier in 
https://issues.apache.org/jira/browse/ZOOKEEPER-900 and https://issues.apache.org/jira/browse/ZOOKEEPER-822.

I forgot to patch this one as a part of ZOOKEEPER-822. I am working on a fix and a patch will be out soon. 

The problem is that QuorumCnxManager is using SocketChannel in blocking mode. It does a read() in receiveConnection() and a write() in initiateConnection().

Sorry, but this is really bad programming. Also, points out to lack of failure tests for QuorumCnxManager.",[],Bug,ZOOKEEPER-914,Blocker,Vishal Kher,Duplicate,2010-11-12T22:47:19.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,QuorumCnxManager blocks forever ,2010-11-12T22:47:19.000+0000,[],1.0
Patrick D. Hunt,"[<JIRA Component: name='build', id='12312383'>]",2010-10-26T06:50:16.000+0000,Anthony Urso,"Cannot build 3.3.1 from release tarball do to VerGen parser inability to parse ""3.3.2-dev"".

version-info:
     [java] All version-related parameters must be valid integers!
     [java] Exception in thread ""main"" java.lang.NumberFormatException: For input string: ""2-dev""
     [java] 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
     [java] 	at java.lang.Integer.parseInt(Integer.java:481)
     [java] 	at java.lang.Integer.parseInt(Integer.java:514)
     [java] 	at org.apache.zookeeper.version.util.VerGen.main(VerGen.java:131)
     [java] Java Result: 1
","[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-913,Critical,Anthony Urso,Fixed,2011-01-27T07:45:24.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Version parser fails to parse ""3.3.2-dev"" from build.xml.",2011-11-23T19:22:17.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",2.0
Vishal Kher,[],2010-10-20T18:27:19.000+0000,Vishal Kher,"The sync request does not set the session owner in Request.

As a result, the leader keeps printing:
2010-07-01 10:55:36,733 - INFO  [ProcessThread:-1:PrepRequestProcessor@405] - Got user-level KeeperException when processing sessionid:0x298d3b1fa90000 type:sync: cxid:0x6 zxid:0xfffffffffffffffe txntype:unknown reqpath:/ Error Path:null Error:KeeperErrorCode = Session moved
","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-907,Blocker,Vishal Kher,Fixed,2010-11-04T16:29:28.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Spurious ""KeeperErrorCode = Session moved"" messages",2011-11-23T19:22:29.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",3.0
Camille Fournier,"[<JIRA Component: name='server', id='12312382'>]",2010-10-19T20:44:01.000+0000,Camille Fournier,"The documentation states:
New in 3.2:  Enables a ZooKeeper ensemble administrator to access the znode hierarchy as a ""super"" user. In particular no ACL checking occurs for a user authenticated as super.

However, if a super user does something like:
zk.setACL(""/"", Ids.READ_ACL_UNSAFE, -1);

the super user is now bound by read-only ACL. This is not what I would expect to see given the documentation. It can be fixed by moving the chec for the ""super"" authId in PrepRequestProcessor.checkACL to before the for(ACL a : acl) loop.","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-904,Major,Camille Fournier,Fixed,2010-10-26T22:31:11.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,super digest is not actually acting as a full superuser,2011-11-23T19:22:24.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",2.0
Flavio Paiva Junqueira,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2010-10-18T17:41:49.000+0000,Patrick D. Hunt,"https://hudson.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/970/artifact/trunk/findbugs/zookeeper-findbugs-report.html#Warnings_MALICIOUS_CODE

Malicious code vulnerability Warnings

Code	Warning
MS	org.apache.zookeeper.server.quorum.LeaderElection.epochGen isn't final but should be","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-902,Minor,Patrick D. Hunt,Fixed,2011-02-07T19:27:45.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Fix findbug issue in trunk ""Malicious code vulnerability""",2011-11-23T19:21:58.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",2.0
Jared Cantwell,"[<JIRA Component: name='c client', id='12312380'>]",2010-10-14T19:35:57.000+0000,Jared Cantwell,"I was looking through the c-client code and noticed a situation where a counter can be incorrectly incremented and a small memory leak can occur.

In zookeeper.c : add_completion(), if close_requested is true, then the completion will not be queued.  But at the end, outstanding_sync is still incremented and free() never called on the newly allocated completion_list_t.  

I will submit for review a diff that I believe corrects this issue.","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-898,Trivial,Jared Cantwell,Fixed,2010-10-28T18:51:35.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,C Client might not cleanup correctly during close,2011-11-23T19:22:20.000+0000,[],1.0
Jared Cantwell,"[<JIRA Component: name='c client', id='12312380'>]",2010-10-14T19:26:06.000+0000,Jared Cantwell,"We observed a crash while closing our c client.  It was in the do_io() thread that was processing as during the close() call.

#0  queue_buffer (list=0x6bd4f8, b=0x0, add_to_front=0) at src/zookeeper.c:969
#1  0x000000000046234e in check_events (zh=0x6bd480, events=<value optimized out>) at src/zookeeper.c:1687
#2  0x0000000000462d74 in zookeeper_process (zh=0x6bd480, events=2) at src/zookeeper.c:1971
#3  0x0000000000469c34 in do_io (v=0x6bd480) at src/mt_adaptor.c:311
#4  0x00007ffff7bc59ca in start_thread () from /lib/libpthread.so.0
#5  0x00007ffff6f706fd in clone () from /lib/libc.so.6
#6  0x0000000000000000 in ?? ()

We tracked down the sequence of events, and the cause is that input_buffer is being freed from a thread other than the do_io thread that relies on it:

1. do_io() call check_events()
2. if(events&ZOOKEEPER_READ) branch executes
3. if (rc > 0) branch executes
4. if (zh->input_buffer != &zh->primer_buffer) branch executes
.....in the meantime......
     5. zookeeper_close() called
     6. if (inc_ref_counter(zh,0)!=0) branch executes
     7. cleanup_bufs() is called
     8. input_buffer is freed at the end
..... back to check_events().........
9. queue_events() is called on a NULL buffer.

I believe the patch is to only call free_completions() in zookeeper_close() and not cleanup_bufs().  The original reason cleanup_bufs() was added was to call any outstanding synhcronous completions, so only free_completions (which is guarded) is needed.  I will submit a patch for review with this change.","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-897,Major,Jared Cantwell,Fixed,2010-10-28T16:25:53.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,C Client seg faults during close,2011-11-23T19:22:21.000+0000,[],1.0
,[],2010-10-14T07:25:53.000+0000,Thomas Koch,"authInfo can be accessed concurrently by different Threads, as exercised in 
org.apache.zookeeper.test.ACLTest

The two concurrent access points in this case were (presumably):
org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:805) and
org.apache.zookeeper.ClientCnxn.addAuthInfo(ClientCnxn.java:1121)

The line numbers refer to the latest patch in ZOOKEEPER-823.

The exception that pointed to this issue:
    [junit] 2010-10-13 09:35:55,113 [myid:] - WARN  [main-SendThread(localhost:11221):ClientCnxn$SendThread@713] - Session 0x0 for server localhost/127.0.0.1:11221, unexpected error, closing socket connection and attempting reconnect
    [junit] java.util.ConcurrentModificationException
    [junit] 	at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
    [junit] 	at java.util.AbstractList$Itr.next(AbstractList.java:343)
    [junit] 	at org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:805)
    [junit] 	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:247)
    [junit] 	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:694)

Proposed solution: Use a thread save list for authInfo",[],Bug,ZOOKEEPER-895,Major,Thomas Koch,Fixed,2010-11-19T17:40:29.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ClientCnxn.authInfo must be thread safe,2010-11-19T17:40:29.000+0000,[],1.0
Thijs Terlouw,"[<JIRA Component: name='server', id='12312382'>]",2010-10-11T07:15:55.000+0000,Thijs Terlouw,"When ZooKeeper receives certain illegally formed messages on the internal communication port (:4181 by default), it's possible for ZooKeeper to enter an infinite loop which causes 100% cpu usage. It's related to ZOOKEEPER-427, but that patch does not resolve all issues.

from: src/java/main/org/apache/zookeeper/server/quorum/QuorumCnxManager.java 

the two affected parts:
===========
int length = msgLength.getInt();                                                        
if(length <= 0) {                                                                       
    throw new IOException(""Invalid packet length:"" + length);                           
} 
===========


===========
while (message.hasRemaining()) {                                                    
    temp_numbytes = channel.read(message);                                          
    if(temp_numbytes < 0) {                                                         
        throw new IOException(""Channel eof before end"");                            
    }                                                                               
    numbytes += temp_numbytes;                                                      
} 
===========

how to replicate this bug:

perform an nmap portscan against your zookeeper server: ""nmap -sV -n your.ip.here -p4181""
wait for a while untill you see some messages in the logfile and then you will see 100% cpu usage. It does not recover from this situation. With my patch, it does not occur anymore","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-893,Critical,Thijs Terlouw,Fixed,2010-10-19T22:38:09.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeper high cpu usage when invalid requests,2011-11-23T19:22:29.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",3.0
,"[<JIRA Component: name='c client', id='12312380'>]",2010-10-07T07:53:45.000+0000,Austin Bennett,"Code using the C client assumes that watcher callbacks are called exactly once. If the watcher is called more than once, the process will likely overwrite freed memory and/or crash.

collect_session_watchers (zk_hashtable.c) gathers watchers from active_node_watchers, active_exist_watchers, and active_child_watchers without removing them. This results in watchers being invoked more than once.

Test code is attached that reproduces the bug, along with a proposed patch.",[],Bug,ZOOKEEPER-890,Critical,Austin Bennett,Not A Problem,2010-10-13T17:11:33.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,C client invokes watcher callbacks multiple times,2010-10-13T17:11:33.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",0.0
,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2010-10-07T04:16:01.000+0000,Austin Bennett,"The pyzoo_aget_children function passes the completion callback (""pyw"") in place of the watcher callback (""get_pyw""). Since it is a one-shot callback, it is deallocated after the completion callback fires, causing a crash when the watcher callback should be invoked.
",[],Bug,ZOOKEEPER-889,Critical,Austin Bennett,Fixed,2010-10-07T04:19:49.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,pyzoo_aget_children crashes due to incorrect watcher context,2010-10-07T04:19:49.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",1.0
Lukas,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='contrib-bindings', id='12312860'>]",2010-10-06T13:26:52.000+0000,Lukas,"the c-client / zkpython wrapper invokes already freed watcher callback

steps to reproduce:
  0. start a zookeper server on your machine
  1. run the attached python script
  2. suspend the zookeeper server process (e.g. using `pkill -STOP -f org.apache.zookeeper.server.quorum.QuorumPeerMain` )
  3. wait until the connection and the node observer fired with a session event
  4. resume the zookeeper server process  (e.g. using `pkill -CONT -f org.apache.zookeeper.server.quorum.QuorumPeerMain` )

-> the client tries to dispatch the node observer function again, but it was already freed -> double free corruption","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-888,Critical,Lukas,Fixed,2010-10-19T19:02:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,c-client / zkpython: Double free corruption on node watcher,2011-11-23T19:22:44.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",3.0
,"[<JIRA Component: name='java client', id='12312381'>]",2010-10-06T04:26:35.000+0000,sanjivsingh,"I tried to test  Producer-Consumer Example published at ...
http://hadoop.apache.org/zookeeper/docs/r3.0.0/zookeeperTutorial.html

Queue.produce( int p)   working correctly,,,

there is problem in Queue.consume( )  method.

 int consume() throws KeeperException, InterruptedException{
            int retvalue = -1;
            Stat stat = null;

            // Get the first element available
            while (true) {
                synchronized (mutex) {
                    List<String> list = zk.getChildren(root, true);
                    if (list.size() == 0) {
                        System.out.println(""Going to wait"");
                        mutex.wait();
                    } else {
                        Integer min = new
Integer(list.get(0).substring(7));
                        for(String s : list){
                            Integer tempValue = new
Integer(s.substring(7));
                            //System.out.println(""Temporary value: "" +
tempValue);
                            if(tempValue < min) min = tempValue;
                        }
                        System.out.println(""Temporary value: "" + root
+ ""/element"" + min);
                        byte[] b = zk.getData(root + ""/element"" + min,
                                    false, stat);
                        zk.delete(root + ""/element"" + min, 0);
                        ByteBuffer buffer = ByteBuffer.wrap(b);
                        retvalue = buffer.getInt();

                        return retvalue;
                    }
                }
            }
        }

    wat exactly produce( )  doing   is that add child under root  like
element000000001,
   element000000002 ,element000000003 etc....

   but
  In consume( ) method ,
          1.  Integer min = new Integer(list.get(0).substring(7));
          2.             for(String s : list){
          3.                 Integer tempValue = new
Integer(s.substring(7));
          4.                  if(tempValue < min) min = tempValue;
          5.               }
          6.       byte[] b = zk.getData(root + ""/element"" + min,
false, stat);
          7.        zk.delete(root + ""/element"" + min, 0);

   bcuz of..
  line 1 & 3 .. converting  like  String  000000001   --------->
Interger  1
  and bcuz of this , in line 6 & 7

  It is tring to access znode like   at  root + ""/element1"" rather
than  root + ""/element000000001""
  that is definelty no-existing one..........

 I m putting forward  a solution....

    int consume() throws KeeperException, InterruptedException{
            int retvalue = -1;
            Stat stat = null;

            // Get the first element available
            while (true) {
                synchronized (mutex) {

                    List<String> list = zk.getChildren(root, true);
                    if (list.size() == 0) {
                        System.out.println(""Going to wait"");
                        mutex.wait();
                    } else {
                        Integer min = new
Integer(list.get(0).substring(7));

                        int i=0 ,p=0;
                        for(String s : list){
                            Integer tempValue = new
Integer(s.substring(7));
                            if(tempValue < min)
                                    p=i;
                            i++;
                        }

                        byte[] b = zk.getData(root + ""/element"" +
list.get(p).substring(7), false, stat);
                        zk.delete(root + ""/element"" +
list.get(p).substring(7), 0);
                        ByteBuffer buffer = ByteBuffer.wrap(b);
                        retvalue = buffer.getInt();

                        return retvalue;
                    }
                }
            }
        }
    }

 ",[],Bug,ZOOKEEPER-887,Minor,sanjivsingh,,,The issue is open and ready for the assignee to start work on it.,Open,0.0, Bug at - Producer-Consumer Example,2016-09-08T06:07:20.000+0000,[],2.0
Erwin Tam,"[<JIRA Component: name='contrib-hedwig', id='12313735'>]",2010-10-05T20:18:00.000+0000,Erwin Tam,"The Hedwig Server is connected to ZooKeeper.  In the ZkTopicManager, it registers a watcher so that if it ever gets disconnected from ZK, it will temporarily fail all incoming requests since the Hedwig server does not know for sure if it is still the master for the topics.  When the ZK client gets reconnected, the logic currently is wrong and it does not unset the suspended flag.  Thus once it gets disconnected, it will stay in the suspended state forever, thereby making the Hedwig server hub dead.",[],Bug,ZOOKEEPER-886,Major,Erwin Tam,Fixed,2010-10-11T20:55:27.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"Hedwig Server stays in ""disconnected"" state when connection to ZK dies but gets reconnected",2010-10-12T10:52:04.000+0000,[],1.0
,"[<JIRA Component: name='server', id='12312382'>]",2010-10-01T14:43:27.000+0000,Alexandre Hardy,"A zookeeper server under minimum load, with a number of clients watching exactly one node will fail to maintain the connection when the machine is subjected to moderate IO load.

In a specific test example we had three zookeeper servers running on dedicated machines with 45 clients connected, watching exactly one node. The clients would disconnect after moderate load was added to each of the zookeeper servers with the command:
{noformat}
dd if=/dev/urandom of=/dev/mapper/nimbula-test
{noformat}

The {{dd}} command transferred data at a rate of about 4Mb/s.

The same thing happens with
{noformat}
dd if=/dev/zero of=/dev/mapper/nimbula-test
{noformat}

It seems strange that such a moderate load should cause instability in the connection.

Very few other processes were running, the machines were setup to test the connection instability we have experienced. Clients performed no other read or mutation operations.

Although the documents state that minimal competing IO load should present on the zookeeper server, it seems reasonable that moderate IO should not cause problems in this case.",[],Bug,ZOOKEEPER-885,Major,Alexandre Hardy,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Zookeeper drops connections under moderate IO load,2022-02-03T08:50:24.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>, <JIRA Version: name='3.3.1', id='12314846'>]",19.0
Flavio Paiva Junqueira,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2010-10-01T09:11:25.000+0000,Flavio Paiva Junqueira,"We no longer use LedgerSequence, so we need to remove references in documentation and comments sprinkled throughout the code.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-884,Major,Flavio Paiva Junqueira,Fixed,2010-11-05T05:18:45.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Remove LedgerSequence references from BookKeeper documentation and comments in tests ,2011-11-23T19:22:02.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",1.0
,"[<JIRA Component: name='server', id='12312382'>]",2010-09-30T08:38:29.000+0000,Lars George,"Monitoring the ZooKeeper nodes by polling the various ports using Nagios' open port checks seems to cause a substantial raise of CPU being used by the ZooKeeper daemons. Over the course of a week an idle cluster grew from a baseline 2% to >10% CPU usage. Attached is a stack dump and logs showing the occupied threads. At the end the daemon starts failing on ""too many open files"" errors as all handles are used up.",[],Bug,ZOOKEEPER-883,Major,Lars George,Implemented,2013-10-10T00:33:16.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Idle cluster increasingly consumes CPU resources,2013-10-10T00:33:16.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",1.0
Jared Cantwell,"[<JIRA Component: name='server', id='12312382'>]",2010-09-28T23:46:36.000+0000,Jared Cantwell,"On startup, the server first loads the latest snapshot, and then loads from the log starting at the last transaction in the snapshot.  It should begin from one past that last transaction in the log.  I will attach a possible patch.","[<JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-882,Minor,Jared Cantwell,Fixed,2010-12-23T12:43:17.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Startup loads last transaction from snapshot,2011-11-23T19:22:29.000+0000,[],1.0
Jared Cantwell,"[<JIRA Component: name='server', id='12312382'>]",2010-09-28T23:41:03.000+0000,Jared Cantwell,"zkDb.loadDataBase() is called twice at the beginning of loadData().  It shouldn't have any negative affects, but is unnecessary.   A patch should be trivial.","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-881,Trivial,Jared Cantwell,Fixed,2010-10-18T18:30:02.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeperServer.loadData loads database twice,2011-11-23T19:21:59.000+0000,[],1.0
Vishal Kher,[],2010-09-27T23:40:46.000+0000,Jean-Daniel Cryans,"We're seeing an issue where one server in the ensemble has a steady growing number of QuorumCnxManager$SendWorker threads up to a point where the OS runs out of native threads, and at the same time we see a lot of exceptions in the logs.  This is on 3.2.2 and our config looks like:

{noformat}
tickTime=3000
dataDir=/somewhere_thats_not_tmp
clientPort=2181
initLimit=10
syncLimit=5
server.0=sv4borg9:2888:3888
server.1=sv4borg10:2888:3888
server.2=sv4borg11:2888:3888
server.3=sv4borg12:2888:3888
server.4=sv4borg13:2888:3888
{noformat}

The issue is on the first server. I'm going to attach threads dumps and logs in moment.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-880,Blocker,Jean-Daniel Cryans,Fixed,2011-03-16T18:49:33.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,QuorumCnxManager$SendWorker grows without bounds,2011-11-23T19:22:01.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",4.0
Daniel Enman,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2010-09-22T10:44:09.000+0000,TuxRacer,"as written in the contrib/zkpython/README file:


""Python >= 2.6 is required. We have tested against 2.6. We have not tested against 3.x.""

this is probably more a 'new feature' request than a bug; anyway compiling the pythn module and calling it returns an error at load time:


python3.1
Python 3.1.2 (r312:79147, May  8 2010, 16:36:46) 
[GCC 4.4.4] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import zookeeper
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: /usr/local/lib/python3.1/dist-packages/zookeeper.so: undefined symbol: PyString_AsString



are there any plan to support Python3.X?

I also tried to write a 3.1 ctypes wrapper but the C API seems in fact to be written in C++, so python ctypes cannot be used.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-877,Major,TuxRacer,Fixed,2013-10-08T06:46:53.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkpython does not work with python3.1,2014-03-13T18:16:55.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",5.0
Diogo,[],2010-09-21T13:37:41.000+0000,Diogo,"When starting a new leadership, unnecessary snapshot transfers happen between new leader and followers. This is so because of multiple small bugs. 

1) the comparison of zxids is done based on a new proposal, instead of the last logged zxid. (LearnerHandler.java ~ 297)
2) if follower is one zxid behind, the check of the interval of committed logs excludes the follower. (LearnerHandler.java ~ 277)
3) the bug reported in ZOOKEEPER-874 (commitLogs are empty after recover).","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-876,Minor,Diogo,Fixed,2013-07-01T17:28:50.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Unnecessary snapshot transfers between new leader and followers,2013-07-01T17:28:50.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",3.0
Diogo,"[<JIRA Component: name='leaderElection', id='12312378'>]",2010-09-17T16:01:03.000+0000,Diogo,"FileTxnSnapLog.restore() does not call listener passed as parameter. The result is that the commitLogs list is empty. When a follower connects to the leader, the leader is forced to send a snapshot to the follower instead of a couple of requests and commits.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-874,Trivial,Diogo,Fixed,2011-04-13T16:10:24.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,FileTxnSnapLog.restore does not call listener,2013-05-02T02:29:33.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",2.0
Vishal Kher,[],2010-09-15T01:51:33.000+0000,Vishal Kher,"PurgeTxnLog forces us to have at least 2 backups (by having count >= 3. Also, it prints to stdout instead of using Logger.",[],Bug,ZOOKEEPER-872,Minor,Vishal Kher,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Small fixes to PurgeTxnLog ,2022-02-03T08:50:23.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",1.0
,[],2010-09-14T22:39:54.000+0000,Mahadev Konar,The fd counts has increased. The tests are repeatedly failing on hudson machines. I probably think this is related to netty server changes. We have to fix this before we release 3.4,[],Bug,ZOOKEEPER-871,Blocker,Mahadev Konar,Cannot Reproduce,2013-10-08T22:55:16.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ClientTest testClientCleanup is failing due to high fd count.,2013-10-08T22:55:30.000+0000,[],0.0
Mahadev Konar,[],2010-09-14T22:16:29.000+0000,Mahadev Konar,the zookeeper current trunk build is broken mostly due to some netty changes. This is causing a huge backlog of PA's and other impediments to the review process. For now I plan to disable the test and fix them as part of 3.4 later.,"[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-870,Major,Mahadev Konar,Fixed,2010-09-15T05:57:29.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Zookeeper trunk build broken.,2011-11-23T19:22:32.000+0000,[],1.0
Patrick D. Hunt,"[<JIRA Component: name='tests', id='12312427'>]",2010-09-07T07:29:12.000+0000,Patrick D. Hunt,client cleanup test is failing on hudson. fd count is off.,"[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-867,Blocker,Patrick D. Hunt,Fixed,2010-09-14T23:22:17.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ClientTest is failing on hudson - fd cleanup,2011-11-23T19:22:01.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
,[],2010-09-03T15:07:05.000+0000,Stephen McCants,"I'm starting a standalone Zookeeper server (v3.3.1).  That starts normally and does not have a runaway thread.

Next, I start an based Eclipse application that is using ZK 3.3.0 to register itself with the ZooKeeper server (3.3.1).  The Eclipse application using the following arguments to Eclipse:

-Dzoodiscovery.autoStart=true
-Dzoodiscovery.flavor=zoodiscovery.flavor.centralized=smccants.austin.ibm.com

When the Eclipse application starts, the ZK server prints out:

2010-09-03 09:59:46,006 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn$Factory@250] - Accepted socket connection from /9.53.189.11:42271
2010-09-03 09:59:46,039 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@776] - Client attempting to establish new session at /9.53.189.11:42271
2010-09-03 09:59:46,045 - INFO  [SyncThread:0:NIOServerCnxn@1579] - Established session 0x12ad81b90000002 with negotiated timeout 4000 for client /9.53.189.11:42271
2010-09-03 09:59:46,046 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn$Factory@250] - Accepted socket connection from /9.53.189.11:42272
2010-09-03 09:59:46,078 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@776] - Client attempting to establish new session at /9.53.189.11:42272
2010-09-03 09:59:46,080 - INFO  [SyncThread:0:NIOServerCnxn@1579] - Established session 0x12ad81b90000003 with negotiated timeout 4000 for client /9.53.189.11:42272

Then both the Eclipse application and the ZK server go into runaway states and consume 100% of the CPU.

Here is a view from top:

  PID USER        PR  NI  VIRT    RES  SHR S %CPU %MEM    TIME+  COMMAND
4949 smccants  15   0  597m  78m 5964 S    66.2      1.0      1:03.14 autosubmitter
4876 smccants  17   0  554m  27m 6688 S    30.9       0.3     0:34.74 java

PID 4949 (autosubmitter) is the Eclipse application and is using more than twice the CPU of PID 4876 (java) which is the ZK server.  They will continue in this state indefinitely.

I can attach a debugger to the Eclipse application and if I stop the thread named ""pool-1-thread-2-SendThread(smccants.austin.ibm.com:2181)"" and the runaway condition stops on both the application and ZK server.  However the ZK server reports:

2010-09-03 10:03:38,001 - INFO  [SessionTracker:ZooKeeperServer@315] - Expiring session 0x12ad81b90000003, timeout of 4000ms exceeded
2010-09-03 10:03:38,002 - INFO  [ProcessThread:-1:PrepRequestProcessor@208] - Processed session termination for sessionid: 0x12ad81b90000003
2010-09-03 10:03:38,005 - INFO  [SyncThread:0:NIOServerCnxn@1434] - Closed socket connection for client /9.53.189.11:42272 which had sessionid 0x12ad81b90000003

Here is the stack trace from the suspended thread:

EPollArrayWrapper.epollWait(long, int, long, int) line: not available [native method]	
EPollArrayWrapper.poll(long) line: 215	
EPollSelectorImpl.doSelect(long) line: 77	
EPollSelectorImpl(SelectorImpl).lockAndDoSelect(long) line: 69	
EPollSelectorImpl(SelectorImpl).select(long) line: 80	
ClientCnxn$SendThread.run() line: 1066	

Any ideas what might be going wrong?

Thanks.",[],Bug,ZOOKEEPER-865,Critical,Stephen McCants,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Runaway thread,2010-09-03T15:09:30.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>, <JIRA Version: name='3.3.1', id='12314846'>]",2.0
,[],2010-09-03T14:33:42.000+0000,Stephen McCants,"I'm running Zookeeper inside an Eclipse application.  When I launch the application from inside Eclipse I use the following arguments:

-Dzoodiscovery.autoStart=true
-Dzoodiscovery.flavor=zoodiscovery.flavor.centralized=localhost

This causes the application to start its own ZooKeeper server inside the JVM/application.  It immediately goes into a runaway state.  The name of the runaway thread is ""NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181"".  When I suspend this thread, the CPU usage returns to 0.  Here is a stack trace from that thread when it is suspended:

EPollArrayWrapper.epollWait(long, int, long, int) line: not available [native method]	
EPollArrayWrapper.poll(long) line: 215	
EPollSelectorImpl.doSelect(long) line: 77	
EPollSelectorImpl(SelectorImpl).lockAndDoSelect(long) line: 69	
EPollSelectorImpl(SelectorImpl).select(long) line: 80	
NIOServerCnxn$Factory.run() line: 232	

Any ideas what might be going wrong?

Thanks.",[],Bug,ZOOKEEPER-863,Critical,Stephen McCants,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Runaway thread - Zookeeper inside Eclipse,2010-09-03T18:52:11.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",2.0
Erwin Tam,"[<JIRA Component: name='contrib-hedwig', id='12313735'>]",2010-09-02T17:40:49.000+0000,Erwin Tam,The Hedwig code checked into Apache is missing a test SSL certificate file used for running the server junit tests.  We need this file otherwise the tests that use this (e.g. TestHedwigHub) will fail.,"[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-861,Minor,Erwin Tam,Fixed,2010-09-07T18:29:25.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Missing the test SSL certificate used for running junit tests.,2011-11-23T19:22:37.000+0000,[],2.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2010-08-31T02:14:32.000+0000,qing yan,"Per mailing list discussion:

<quote>

the client only finds out about session expiration events when the client reconnects to the cluster. if zk tells a client that its session is expired, the ephemerals that correspond to that session will already be cleaned up.

- deletion of an ephemeral file due to loss of client connection will occur
after the client gets a connection loss

- deletion of an ephemeral file will precede delivery of a session
expiration event to the owner
</quote>

So session expirations means two things here : server view(ephemeral clean up) & client view(event delivery) , there are
no guarantee how long it will take in between, correct?

I guess the confusion rises from the documention which doesn't distinguish these two concepts, e.g. in the javadoc http://hadoop.apache.org/zookeeper/docs/r3.3.1/api/index.html

An ephemeral node will be removed by the ZooKeeper automatically when the session associated with the creation of the node expires.

It is actually refering to the server view not the client view.",[],Bug,ZOOKEEPER-857,Major,qing yan,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,clarify client vs. server view of session expiration event,2022-02-03T08:50:21.000+0000,[],0.0
Mahadev Konar,[],2010-08-26T19:10:01.000+0000,Travis Crawford,"We've experienced a number of issues lately where ""ruok"" requests would take upwards of 10 seconds to return, and ZooKeeper instances were extremely sluggish. The sluggish instance requires a restart to make it responsive again.

I believe the issue is connections are very imbalanced, leading to certain instances having many thousands of connections, while other instances are largely idle.

A potential solution is periodically disconnecting/reconnecting to balance connections over time; this seems fine because sessions should not be affected, and therefore ephemaral nodes and watches should not be affected.",[],Bug,ZOOKEEPER-856,Major,Travis Crawford,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Connection imbalance leads to overloaded ZK instances,2022-02-03T08:50:13.000+0000,[],7.0
Jared Cantwell,"[<JIRA Component: name='documentation', id='12312422'>]",2010-08-26T14:49:18.000+0000,Jared Cantwell,The server documentation states that the configuration parameter for binding to a specific ip address is clientPortBindAddress.  The code believes the parameter is clientPortAddress.  The documentation for 3.3.X versions needs changed to reflect the correct parameter .  This parameter was added in ZOOKEEPER-635.,"[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-855,Trivial,Jared Cantwell,Fixed,2010-10-18T21:56:47.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,clientPortBindAddress should be clientPortAddress,2011-11-23T19:22:48.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>, <JIRA Version: name='3.3.1', id='12314846'>]",1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2010-08-19T20:04:45.000+0000,Flavio Paiva Junqueira,BookKeeper does not compile due to changes in the NIOServerCnxn class of ZooKeeper.,"[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-854,Major,Flavio Paiva Junqueira,Fixed,2010-08-28T16:11:43.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,BookKeeper does not compile due to changes in the ZooKeeper code,2011-11-23T19:22:03.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",1.0
,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2010-08-16T14:12:47.000+0000,Vishal Kher,"I had a 3 node cluster running. The zoo.cfg on each contained 3 entries as show below:

tickTime=2000
dataDir=/var/zookeeper
clientPort=2181
initLimit=5
syncLimit=2
server.0=10.150.27.61:2888:3888
server.1=10.150.27.62:2888:3888
server.2=10.150.27.63:2888:3888

I wanted to add another node to the cluster. In fourth node's zoo.cfg, I created another entry for that node and started zk server. The zoo.cfg on the first 3 nodes was left unchanged. The fourth node was able to join the cluster even though the 3 nodes had no idea about the fourth node.

zoo.cfg on fourth node:
tickTime=2000
dataDir=/var/zookeeper
clientPort=2181
initLimit=5
syncLimit=2
server.0=10.150.27.61:2888:3888
server.1=10.150.27.62:2888:3888
server.2=10.150.27.63:2888:3888
server.3=10.17.117.71:2888:3888

It looks like 10.17.117.71 is becoming an observer in this case. I was expecting that the leader will reject 10.17.117.71.

# telnet 10.17.117.71 2181
Trying 10.17.117.71...
Connected to 10.17.117.71.
Escape character is '^]'.
stat
Zookeeper version: 3.3.0--1, built on 04/02/2010 22:40 GMT
Clients:
 /10.17.117.71:37297[1](queued=0,recved=1,sent=0)

Latency min/avg/max: 0/0/0
Received: 3
Sent: 2
Outstanding: 0
Zxid: 0x200000065
Mode: follower
Node count: 288",[],Bug,ZOOKEEPER-851,Critical,Vishal Kher,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,ZK lets any node to become an observer,2022-02-03T08:50:12.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",6.0
,"[<JIRA Component: name='java client', id='12312381'>]",2010-08-13T13:01:30.000+0000,Patrick Datko,"I watched the source of the zookeeper class and I missed an acl check in the asynchronous version of the create operation. Is there any reason, that in the asynch version is no
check whether the acl is valid, or did someone forget to implement it. It's interesting because we worked on a refactoring of the zookeeper client and don't want to implement a bug.

The following code is missing:
        if (acl != null && acl.size() == 0) {
            throw new KeeperException.InvalidACLException();
        }
",[],Bug,ZOOKEEPER-847,Major,Patrick Datko,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Missing acl check in zookeeper create,2022-02-03T08:50:17.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.3.3', id='12315482'>]",3.0
Patrick D. Hunt,"[<JIRA Component: name='java client', id='12312381'>]",2010-08-12T17:19:53.000+0000,Ted Yu,"Using HBase 0.20.6 (with HBASE-2473) we encountered a situation where Regionserver
process was shutting down and seemed to hang.

Here is the bottom of region server log:
http://pastebin.com/YYawJ4jA

zookeeper-3.2.2 is used.

Here is relevant portion from jstack - I attempted to attach jstack twice in my email to dev@hbase.apache.org but failed:

""DestroyJavaVM"" prio=10 tid=0x00002aabb849c800 nid=0x6c60 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""regionserver/10.32.42.245:60020"" prio=10 tid=0x00002aabb84ce000 nid=0x6c81 in Object.wait() [0x0000000043755000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00002aaab76633c0> (a org.apache.zookeeper.ClientCnxn$Packet)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1099)
        - locked <0x00002aaab76633c0> (a org.apache.zookeeper.ClientCnxn$Packet)
        at org.apache.zookeeper.ClientCnxn.close(ClientCnxn.java:1077)
        at org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:505)
        - locked <0x00002aaabf5e0c30> (a org.apache.zookeeper.ZooKeeper)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.close(ZooKeeperWrapper.java:681)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:654)
        at java.lang.Thread.run(Thread.java:619)

""main-EventThread"" daemon prio=10 tid=0x0000000043474000 nid=0x6c80 waiting on condition [0x00000000413f3000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaabf6e9150> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:414)
","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-846,Blocker,Ted Yu,Fixed,2010-09-22T06:39:00.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper client doesn't shut down cleanly on the close call,2011-11-23T19:22:15.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>]",3.0
Camille Fournier,"[<JIRA Component: name='java client', id='12312381'>]",2010-08-12T16:05:02.000+0000,Camille Fournier,"ClientCnxn.java currently has the following code:
  if (replyHdr.getXid() == -4) {
                // -2 is the xid for AuthPacket
                // TODO: process AuthPacket here
                if (LOG.isDebugEnabled()) {
                    LOG.debug(""Got auth sessionid:0x""
                            + Long.toHexString(sessionId));
                }
                return;
            }

Auth failures appear to cause the server to disconnect but the client never gets a proper state change or notification that auth has failed, which makes handling this scenario very difficult as it causes the client to go into a loop of sending bad auth, getting disconnected, trying to reconnect, sending bad auth again, over and over. 
","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-844,Major,Camille Fournier,Fixed,2010-10-06T16:19:27.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,handle auth failure in java client,2011-11-23T19:22:13.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",1.0
Thomas Koch,"[<JIRA Component: name='documentation', id='12312422'>]",2010-08-06T19:17:25.000+0000,Bruce Mitchener,"This is apparently a known issue:

http://mail-archives.apache.org/mod_mbox/hadoop-zookeeper-user/201005.mbox/%3C562709E0-0516-481F-87AD-2039A564E5BD@yahoo-inc.com%3E

None of the attachments on the Presentations page in the wiki work (nor does the link to the screenshot on the performance page).
",[],Bug,ZOOKEEPER-833,Major,Bruce Mitchener,Fixed,2011-09-05T19:55:49.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Attachments in the wiki do not work (so no presentations),2011-09-05T19:55:49.000+0000,[],1.0
Mohammad Arshad,"[<JIRA Component: name='server', id='12312382'>]",2010-08-05T19:16:36.000+0000,Ryan Holmes,"Steps to reproduce:

1.) Connect to a standalone server using the Java client.
2.) Stop the server.
3.) Delete the contents of the data directory (i.e. the persisted session data).
4.) Start the server.

The client now automatically tries to reconnect but the server refuses the connection because the session id is invalid. The client and server are now in an infinite loop of attempted and rejected connections. While this situation represents a catastrophic failure and the current behavior is not incorrect, it appears that there is no way to detect this situation on the client and therefore no way to recover.

The suggested improvement is to send an event to the default watcher indicating that the current state is ""session invalid"", similar to how the ""session expired"" state is handled.

Server log output (repeats indefinitely):
2010-08-05 11:48:08,283 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn$Factory@250] - Accepted socket connection from /127.0.0.1:63292
2010-08-05 11:48:08,284 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@751] - Refusing session request for client /127.0.0.1:63292 as it has seen zxid 0x44 our last zxid is 0x0 client must try another server
2010-08-05 11:48:08,284 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1434] - Closed socket connection for client /127.0.0.1:63292 (no session established for client)


Client log output (repeats indefinitely):
11:47:17 org.apache.zookeeper.ClientCnxn startConnect INFO line 1000 - Opening socket connection to server localhost/127.0.0.1:2181
11:47:17 org.apache.zookeeper.ClientCnxn run WARN line 1120 - Session 0x12a3ae4e893000a for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1078)
11:47:17 org.apache.zookeeper.ClientCnxn cleanup DEBUG line 1167 - Ignoring exception during shutdown input
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.shutdownInput(SocketChannelImpl.java:638)
	at sun.nio.ch.SocketAdaptor.shutdownInput(SocketAdaptor.java:360)
	at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:1164)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1129)
11:47:17 org.apache.zookeeper.ClientCnxn cleanup DEBUG line 1174 - Ignoring exception during shutdown output
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.shutdownOutput(SocketChannelImpl.java:649)
	at sun.nio.ch.SocketAdaptor.shutdownOutput(SocketAdaptor.java:368)
	at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:1171)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1129)
",[],Bug,ZOOKEEPER-832,Critical,Ryan Holmes,Invalid,2021-01-07T10:25:16.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Invalid session id causes infinite loop during automatic reconnect,2022-03-06T14:14:13.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.5.0', id='12316644'>, <JIRA Version: name='3.4.11', id='12339207'>]",41.0
Flavio Paiva Junqueira,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2010-08-04T21:25:16.000+0000,Flavio Paiva Junqueira,"Reads and writes in BookKeeper are asymmetric: a write request writes one entry, whereas a read request may read multiple requests. The current implementation of throttling only counts the number of read requests instead of counting the number of entries being read. Consequently, a few read requests reading a large number of entries each will spawn a large number of read-entry requests. ","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-831,Major,Flavio Paiva Junqueira,Fixed,2010-09-17T16:59:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,BookKeeper: Throttling improved for reads,2013-05-02T02:29:29.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",2.0
,"[<JIRA Component: name='c client', id='12312380'>]",2010-07-21T07:13:00.000+0000,Michi Mutsuzaki,"In cli.c, zoo_add_auth() gets called right after zookeeper_init(). Instead, zoo_add_auth() should be called in the callback after the connection is established. 

--Michi",[],Bug,ZOOKEEPER-826,Minor,Michi Mutsuzaki,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,cli.c should not call zoo_add_auth immediately after zookeeper_init() ,2010-07-21T09:06:55.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",0.0
Vishal Kher,"[<JIRA Component: name='quorum', id='12312379'>]",2010-07-19T15:51:01.000+0000,Vishal Kher,"Created a 3 node cluster.

1 Fail the ZK leader
2. Let leader election finish. Restart the leader and let it join the 
3. Repeat 

After a few rounds leader election takes anywhere 25- 60 seconds to finish. Note- we didn't have any ZK clients and no new znodes were created.

zoo.cfg is shown below:

#Mon Jul 19 12:15:10 UTC 2010
server.1=192.168.4.12\:2888\:3888
server.0=192.168.4.11\:2888\:3888
clientPort=2181
dataDir=/var/zookeeper
syncLimit=2
server.2=192.168.4.13\:2888\:3888
initLimit=5
tickTime=2000

I have attached logs from two nodes that took a long time to form the cluster after failing the leader. The leader was down anyways so logs from that node shouldn't matter.
Look for ""START HERE"". Logs after that point should be of our interest.","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-822,Blocker,Vishal Kher,Fixed,2010-10-06T17:03:01.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Leader election taking a long time  to complete,2011-11-23T19:22:27.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",4.0
Michi Mutsuzaki,[],2010-07-16T16:00:32.000+0000,Patrick D. Hunt,"When the c unit tests are run sometimes the server doesn't shutdown at the end of the test, this causes subsequent tests (hudson esp) to fail.

1) we should try harder to make the server shut down at the end of the test, I suspect this is related to test failing/cleanup
2) before the tests are run we should see if the old server is still running and try to shut it down
","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-820,Critical,Patrick D. Hunt,Fixed,2010-10-20T18:46:25.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"update c unit tests to ensure ""zombie"" java server processes don't cause failure",2011-11-23T19:22:45.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",1.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2010-07-15T20:14:22.000+0000,Patrick D. Hunt,"Funny: ""Ephemeral nodes are useful when you want to implement [tbd]."" there are a few others in that doc that are should really be fixed.
",[],Bug,ZOOKEEPER-815,Minor,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"fill in ""TBD""s in overview doc",2022-02-03T08:50:23.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",2.0
Andrei Savu,"[<JIRA Component: name='contrib', id='12312700'>]",2010-07-14T06:59:31.000+0000,Patrick D. Hunt,"Andrei, I just realized that src/contrib/monitoring files are missing apache license headers.  Please add them (in particular any script files like python, see similar files in svn for examples - in some cases like README it's not strictly necessary.) 

You can run the RAT tool to verify (see build.xml or http://incubator.apache.org/rat/)
","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-814,Blocker,Patrick D. Hunt,Fixed,2010-07-26T22:01:02.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,monitoring scripts are missing apache license headers,2011-11-23T19:22:13.000+0000,[],1.0
Jeff Hodges,"[<JIRA Component: name='build', id='12312383'>]",2010-07-12T07:14:10.000+0000,Jeff Hodges,"SBT doesn't like the pom file for zookeeper because while it's under the ""org.apache.hadoop"" directory, it's organisation is actually ""org.apache.zookeeper"". A simple fix for this is to just change ""org.apache.zookeeper"" to ""org.apache.hadoop"".","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-813,Critical,Jeff Hodges,Duplicate,2010-07-12T22:49:14.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,maven install is broken due to incorrect organisation,2011-11-23T19:22:49.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",0.0
,"[<JIRA Component: name='documentation', id='12312422'>, <JIRA Component: name='server', id='12312382'>]",2010-07-06T23:48:42.000+0000,Patrick D. Hunt,"In both 3.3 branch and trunk ""echo stat|nc localhost 2181"" fails against the ZK server on Ubuntu Lucid Lynx.

I noticed this after upgrading to lucid lynx - which is now shipping openbsd nc as the default:

OpenBSD netcat (Debian patchlevel 1.89-3ubuntu2)

vs nc traditional

[v1.10-38]

which works fine. Not sure if this is a bug in us or nc.openbsd, but it's currently not working for me. Ugh.
","[<JIRA Version: name='3.4.6', id='12323310'>]",Bug,ZOOKEEPER-805,Critical,Patrick D. Hunt,Fixed,2014-04-30T20:19:56.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,four letter words fail with latest ubuntu nc.openbsd,2014-04-30T20:19:56.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.4.0', id='12314469'>]",3.0
Michi Mutsuzaki,"[<JIRA Component: name='c client', id='12312380'>]",2010-07-05T20:35:49.000+0000,Patrick D. Hunt,"I'm seeing this frequently:

     [exec] Zookeeper_simpleSystem::testPing : elapsed 18006 : OK
     [exec] Zookeeper_simpleSystem::testAcl : elapsed 1022 : OK
     [exec] Zookeeper_simpleSystem::testChroot : elapsed 3145 : OK
     [exec] Zookeeper_simpleSystem::testAuth ZooKeeper server started : elapsed 25687 : OK
     [exec] zktest-mt: /home/phunt/dev/workspace/gitzk/src/c/src/zookeeper.c:1952: zookeeper_process: Assertion `cptr' failed.
     [exec] make: *** [run-check] Aborted
     [exec] Zookeeper_simpleSystem::testHangingClient

Mahadev can you take a look?
","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-804,Critical,Patrick D. Hunt,Fixed,2010-10-20T16:27:44.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"c unit tests failing due to ""assertion cptr failed""",2011-11-23T19:22:40.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",1.0
,[],2010-07-02T20:52:47.000+0000,Travis Crawford,"This issue is in response to ZOOKEEPER-801. Short version is a small number of buggy clients opened thousands of connections and caused Zookeeper to fail.

The misbehaving client did not correctly handle expired sessions, creating a new connection each time. The huge number of connections exacerbated the issue.",[],Bug,ZOOKEEPER-803,Major,Travis Crawford,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Improve defenses against misbehaving clients,2020-06-25T01:13:00.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",2.0
Michi Mutsuzaki,"[<JIRA Component: name='c client', id='12312380'>]",2010-06-29T23:26:03.000+0000,Michi Mutsuzaki,"This happened when I called zoo_add_auth() immediately after zookeeper_init(). It took me a while to figure out that authentication actually failed since zoo_add_auth() returned ZOK. It should return ZINVALIDSTATE instead. 

--Michi","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-800,Minor,Michi Mutsuzaki,Fixed,2010-10-21T22:52:01.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zoo_add_auth returns ZOK if zookeeper handle is in ZOO_CLOSED_STATE,2011-11-23T19:22:43.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",4.0
Alex Newman,"[<JIRA Component: name='scripts', id='12312384'>]",2010-06-28T22:02:04.000+0000,Alex Newman,So currently the pid file has to be tied to the datadirectory when starting zkServer.sh. It would be good to be able to break them up.,"[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-796,Major,Alex Newman,Fixed,2010-07-06T21:51:48.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkServer.sh should support an external PIDFILE variable,2011-11-23T19:22:31.000+0000,[],1.0
Sergey Doroshenko,"[<JIRA Component: name='java client', id='12312381'>]",2010-06-28T10:12:26.000+0000,mathieu barcikowski,"Hi,

I notice a problem with the eventThread located in ClientCnxn.java file.
The eventThread isn't shutdown after a connection ""session expired"" event coming (i.e. never receive EventOfDeath).

When a session timeout occurs and the session is marked as expired, the connexion is fully closed (socket, SendThread...) expect for the eventThread.
As a result, if i create a new zookeeper object and connect through it, I got a zombi thread which will never be kill (as for the previous zookeeper object, the state is already close, calling close again don't do anything).

So everytime I will create a new zookeeper connection after a expired session, I will have a one more zombi EventThread.

How to reproduce :
- Start a zookeeper client connection in debug mode
- Pause the jvm enough time to the expired event occur
- Watch for example with jvisualvm the list of threads, the sendThread is succesfully killed, but the EventThread go to wait state for a infinity of time
- if you reopen a new zookeeper connection, and do again the previous steps, another EventThread will be present in infinite wait state




","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-795,Blocker,mathieu barcikowski,Fixed,2010-08-17T20:05:18.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"eventThread isn't shutdown after a connection ""session expired"" event coming",2013-06-13T18:28:19.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",4.0
Alexis Midon,"[<JIRA Component: name='java client', id='12312381'>]",2010-06-26T02:47:15.000+0000,Alexis Midon,"I noticed that ZooKeeper has different behaviors when calling synchronous or asynchronous actions on a closed ZooKeeper client.
Actually a synchronous call will throw a ""session expired"" exception while an asynchronous call will do nothing. No exception, no callback invocation.

Actually, even if the EventThread receives the Packet with the session expired err code, the packet is never processed since the thread has been killed by the ventOfDeath. So the call back is not invoked.

","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-794,Blocker,Alexis Midon,Fixed,2010-10-21T00:47:39.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Callbacks are not invoked when the client is closed,2011-11-23T19:22:42.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",4.0
Lei Zhang,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2010-06-24T21:36:54.000+0000,Lei Zhang,"We recently upgraded zookeeper from 3.2.1 to 3.3.1, now we are seeing less client deadlock on session expiration, which is a definite plus!

Unfortunately we are seeing memory leak that requires our zk clients to be restarted every half-day. Valgrind result:

==8804== 25 (12 direct, 13 indirect) bytes in 1 blocks are definitely lost in loss record 255 of 670
==8804==    at 0x4021C42: calloc (vg_replace_malloc.c:418)
==8804==    by 0x5047B42: parse_acls (zookeeper.c:369)
==8804==    by 0x5047EF6: pyzoo_create (zookeeper.c:1009)
==8804==    by 0x40786CC: PyCFunction_Call (in /usr/lib/libpython2.4.so.1.0)
==8804==    by 0x40B31DC: PyEval_EvalFrame (in /usr/lib/libpython2.4.so.1.0)
==8804==    by 0x40B4485: PyEval_EvalCodeEx (in /usr/lib/libpython2.4.so.1.0)
","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-792,Major,Lei Zhang,Fixed,2010-08-23T02:59:54.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkpython memory leak,2011-11-23T19:22:41.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",1.0
,[],2010-06-24T12:20:18.000+0000,Sergey Doroshenko,"I start 2 of 3 servers of an ensemble, connect to it with zkCli.sh, do ""ls / 1"" which registers a watch.
Then I kill one of 2 servers which makes alive one to lose a quorum and forces client to reconnect. 

And when the client connects to this alive server (but gets quickly dropped by the server afterwards), watch is triggered:
WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/ 

I can reproduce it only with command-line client, and quite rarely. I tried to write unit test, but id didn't catch this.
Has anybody seen this before?",[],Bug,ZOOKEEPER-791,Minor,Sergey Doroshenko,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Watches get triggered during client's reconnection,2010-06-24T12:22:18.000+0000,[],1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='quorum', id='12312379'>]",2010-06-22T16:47:56.000+0000,Flavio Paiva Junqueira,"The leader code is setting the last processed zxid to the first of the new epoch even before connecting to a quorum of followers. Because the leader code sets this value before connecting to a quorum of followers (Leader.java:281) and the follower code throws an IOException (Follower.java:73) if the leader epoch is smaller, we have that when the false leader drops leadership and becomes a follower, it finds a smaller epoch and kills itself.","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-790,Blocker,Flavio Paiva Junqueira,Fixed,2010-07-29T21:11:57.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Last processed zxid set prematurely while establishing leadership,2011-11-23T19:22:17.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",3.0
,[],2010-06-10T16:46:55.000+0000,Chris Conrad,"The pom deployed to repo1.maven.org has the project declared like this:

<groupId>org.apache.zookeeper</groupId>
<artifactId>zookeeper</artifactId>
<packaging>jar</packaging>
<version>3.3.1</version>

But it is deployed here: http://repo2.maven.org/maven2/org/apache/hadoop/zookeeper/3.3.1

So either the groupId needs to change or the location it is deployed to needs to be changed because having them different results in bad behavior.  If you specify the correct groupId in your own pom/ivy files you can't even download zookeeper because it's not where your pom says it is and if you use the ""incorrect"" groupId then you can download zookeeper but then ivy complains about:

[error] :: problems summary ::
[error] :::: ERRORS
[error] 		public: bad organisation found in http://repo1.maven.org/maven2/org/apache/hadoop/zookeeper/3.3.1/zookeeper-3.3.1.pom: expected='org.apache.hadoop' found='org.apache.zookeeper'
","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-787,Blocker,Chris Conrad,Fixed,2010-09-15T15:39:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,groupId in deployed pom is wrong,2011-11-23T19:22:17.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",2.0
Thomas Koch,"[<JIRA Component: name='java client', id='12312381'>]",2010-06-04T21:25:32.000+0000,Stephen Green,"When trying to call ZooKeeper.toString during client disconnections, an exception can be generated:


[04/06/10 15:39:57.744] ERROR Error while calling watcher 
java.lang.Error: java.net.SocketException: Socket operation on non-socket
	at sun.nio.ch.Net.localAddress(Net.java:128)
	at sun.nio.ch.SocketChannelImpl.localAddress(SocketChannelImpl.java:430)
	at sun.nio.ch.SocketAdaptor.getLocalAddress(SocketAdaptor.java:147)
	at java.net.Socket.getLocalSocketAddress(Socket.java:717)
	at org.apache.zookeeper.ClientCnxn.getLocalSocketAddress(ClientCnxn.java:227)
	at org.apache.zookeeper.ClientCnxn.toString(ClientCnxn.java:183)
	at java.lang.String.valueOf(String.java:2826)
	at java.lang.StringBuilder.append(StringBuilder.java:115)
	at org.apache.zookeeper.ZooKeeper.toString(ZooKeeper.java:1486)
	at java.util.Formatter$FormatSpecifier.printString(Formatter.java:2794)
	at java.util.Formatter$FormatSpecifier.print(Formatter.java:2677)
	at java.util.Formatter.format(Formatter.java:2433)
	at java.util.Formatter.format(Formatter.java:2367)
	at java.lang.String.format(String.java:2769)
	at com.echonest.cluster.ZooContainer.process(ZooContainer.java:544)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:488)
Caused by: java.net.SocketException: Socket operation on non-socket
	at sun.nio.ch.Net.localInetAddress(Native Method)
	at sun.nio.ch.Net.localAddress(Net.java:125)
	... 15 more
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-786,Minor,Stephen Green,Fixed,2011-10-17T01:00:39.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Exception in ZooKeeper.toString,2011-10-17T01:00:39.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2010-06-02T22:51:50.000+0000,Alex Newman,"The following config causes an infinite loop

[zoo.cfg]
tickTime=2000
dataDir=/var/zookeeper/
clientPort=2181
initLimit=10
syncLimit=5
server.0=localhost:2888:3888

Output:

2010-06-01 16:20:32,471 - INFO [main:QuorumPeerMain@119] - Starting quorum peer
2010-06-01 16:20:32,489 - INFO [main:NIOServerCnxn$Factory@143] - binding to port 0.0.0.0/0.0.0.0:2181
2010-06-01 16:20:32,504 - INFO [main:QuorumPeer@818] - tickTime set to 2000
2010-06-01 16:20:32,504 - INFO [main:QuorumPeer@829] - minSessionTimeout set to -1
2010-06-01 16:20:32,505 - INFO [main:QuorumPeer@840] - maxSessionTimeout set to -1
2010-06-01 16:20:32,505 - INFO [main:QuorumPeer@855] - initLimit set to 10
2010-06-01 16:20:32,526 - INFO [main:FileSnap@82] - Reading snapshot /var/zookeeper/version-2/snapshot.c
2010-06-01 16:20:32,547 - INFO [Thread-1:QuorumCnxManager$Listener@436] - My election bind port: 3888
2010-06-01 16:20:32,554 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@620] - LOOKING
2010-06-01 16:20:32,556 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@649] - New election. My id = 0, Proposed zxid = 12
2010-06-01 16:20:32,558 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@689] - Notification: 0, 12, 1, 0, LOOKING, LOOKING, 0
2010-06-01 16:20:32,560 - WARN [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@623] - Unexpected exception
java.lang.NullPointerException
at org.apache.zookeeper.server.quorum.FastLeaderElection.totalOrderPredicate(FastLeaderElection.java:496)
at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:709)
at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:621)
2010-06-01 16:20:32,560 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@620] - LOOKING
2010-06-01 16:20:32,560 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@649] - New election. My id = 0, Proposed zxid = 12
2010-06-01 16:20:32,561 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@689] - Notification: 0, 12, 2, 0, LOOKING, LOOKING, 0
2010-06-01 16:20:32,561 - WARN [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@623] - Unexpected exception
java.lang.NullPointerException
at org.apache.zookeeper.server.quorum.FastLeaderElection.totalOrderPredicate(FastLeaderElection.java:496)
at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:709)
at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:621)
2010-06-01 16:20:32,561 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@620] - LOOKING
2010-06-01 16:20:32,562 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@649] - New election. My id = 0, Proposed zxid = 12
2010-06-01 16:20:32,562 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@689] - Notification: 0, 12, 3, 0, LOOKING, LOOKING, 0
2010-06-01 16:20:32,562 - WARN [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@623] - Unexpected exception
java.lang.NullPointerException


Things like HBase require that the zookeeper servers be listed in the zoo.cfg. This is a bug on their part, but zookeeper shouldn't null pointer in a loop though.
","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-785,Major,Alex Newman,Fixed,2010-09-14T21:09:31.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0, Zookeeper 3.3.1 shouldn't infinite loop if someone creates a server.0 line,2011-11-23T19:22:38.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",1.0
Henry Robinson,"[<JIRA Component: name='server', id='12312382'>]",2010-06-01T19:22:43.000+0000,Henry Robinson,"ZKDatabase.getCommittedLog() returns a reference to the LinkedList<Proposal> committedLog in ZKDatabase. This is then iterated over by at least one caller. 

I have seen a bug that causes a NPE in LinkedList.clear on committedLog, which I am pretty sure is due to the lack of synchronization. This bug has not been apparent in normal ZK operation, but in code that I have that starts and stops a ZK server in process repeatedly (clear() is called from ZooKeeperServerMain.shutdown()). 

It's better style to defensively copy the list in getCommittedLog, and to synchronize on the list in ZKDatabase.clear.

","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-783,Critical,Henry Robinson,Fixed,2010-07-26T22:45:07.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,committedLog in ZKDatabase is not properly synchronized,2011-11-23T19:22:08.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",1.0
Mahadev Konar,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='documentation', id='12312422'>]",2010-05-31T20:47:30.000+0000,Dave Wright," The C API Doxygen documentation states:

"" .... If the client is ever disconnected from the service, even if the
  disconnection is temporary, the watches of the client will be removed from
  the service, so a client must treat a disconnect notification as an implicit
  trigger of all outstanding watches.""

This is incorrect as of v.3. Watches are only lost and need to be re-registered when a session times out. When a normal disconnection occurs watches are reset automatically on reconnection.

The documentation in zookeeper.h needs to be updated to correct this explanation.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-782,Trivial,Dave Wright,Fixed,2011-07-14T17:54:01.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Incorrect C API documentation for Watches,2011-11-23T19:22:40.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",2.0
,"[<JIRA Component: name='scripts', id='12312384'>]",2010-05-25T10:22:11.000+0000,Miguel Correia,"I'm starting to play with Zookeeper so I'm still running it in standalone mode. This is not a big issue, but here it goes for the records. 

I've run zkCli.sh to run some commands in the server. I created a znode /groups. When I tried to create a znode client_1 inside /groups, I forgot to include the data: an exception was generated and zkCli-sh crashed, instead of just showing an error. I tried a few variations and it seems like the problem is not including the data.

A copy of the screen:

[zk: localhost:2181(CONNECTED) 3] create /groups firstgroup
Created /groups
[zk: localhost:2181(CONNECTED) 4] create -e /groups/client_1
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 3
	at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:678)
	at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:581)
	at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:353)
	at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:311)
	at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:270)
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-780,Minor,Miguel Correia,Invalid,2014-04-24T23:52:03.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zkCli.sh  generates a ArrayIndexOutOfBoundsException ,2014-04-24T23:52:03.000+0000,"[<JIRA Version: name='3.3.1', id='12314846'>]",3.0
,"[<JIRA Component: name='server', id='12312382'>]",2010-05-21T15:52:54.000+0000,Kapil Thangavelu,"currently it just returns successfully, but the acl can't be retrieved, and if any value is being stored, its overwritten when the node is created.",[],Bug,ZOOKEEPER-777,Major,Kapil Thangavelu,Invalid,2011-11-19T01:07:44.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,setting acl on a non existant node should return no node error,2011-11-19T01:07:44.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>, <JIRA Version: name='3.3.1', id='12314846'>]",0.0
Sergey Doroshenko,"[<JIRA Component: name='recipes', id='12313246'>]",2010-05-12T21:31:11.000+0000,Sergey Doroshenko,As title,"[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-774,Minor,Sergey Doroshenko,Fixed,2010-05-14T23:32:45.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Recipes tests are slightly outdated: they do not compile against JUnit 4.8,2011-11-23T19:22:43.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",1.0
Henry Robinson,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2010-05-10T14:42:21.000+0000,Kapil Thangavelu,"When utilizing the zkpython async get children api with a watch, i consistently get segfaults when the watcher is invoked to process events. ","[<JIRA Version: name='3.3.2', id='12315108'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-772,Major,Kapil Thangavelu,Fixed,2010-08-11T18:31:53.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkpython segfaults when watcher from async get children is invoked.,2011-11-23T19:22:11.000+0000,[],1.0
,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2010-05-07T19:51:48.000+0000,Kapil Thangavelu,"If you attempt to utilize an invalid auth scheme when adding authentication, you'll end up with an error return value in your callback. But the handle itself will be hosed,  attempting to utilize it with any part of the api will return

SystemError: error return without exception set




",[],Bug,ZOOKEEPER-771,Minor,Kapil Thangavelu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zkpython return without exception set on invalid auth scheme,2010-05-07T20:07:49.000+0000,[],1.0
Craig Calef,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='contrib-bindings', id='12312860'>]",2010-05-06T19:50:27.000+0000,Kapil Thangavelu,"Calls to add_auth are a bit slow from the c client library. The auth callback typically takes multiple seconds to fire. I instrumented the java, c binding, and python binding with a few log statements to find out where the slowness was occuring ( http://bazaar.launchpad.net/~hazmat/zookeeper/fast-auth-instrumented/revision/647). It looks like when the io thread polls, it doesn't register interest in the incoming packet, so the auth success message from the server and the auth callback are only processed when the poll timeouts. I tried modifying mt_adapter.c so the poll registers interest in both events, this causes a considerably more wakeups but it does address the issue of making add_auth fast.  I think the ideal solution would be some sort of additional auth handshake state on the handle, that zookeeper_interest could utilize to suggest both POLLIN|POLLOUT are wanted for subsequent calls to poll during the auth handshake handle state.

i'm attaching a script that takes 13s or 1.6s for the auth callback depending on the session time out value (which in turn figures into the calculation of the poll timeout).
","[<JIRA Version: name='3.9.0', id='12351304'>]",Bug,ZOOKEEPER-770,Major,Kapil Thangavelu,,,A patch for this issue has been uploaded to JIRA by a contributor.,Patch Available,0.0,Slow add_auth calls with multi-threaded client,2022-02-03T08:36:25.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>, <JIRA Version: name='3.3.3', id='12315482'>, <JIRA Version: name='3.4.0', id='12314469'>]",8.0
Sergey Doroshenko,[],2010-05-06T18:01:46.000+0000,Sergey Doroshenko,"In short: it seems leader can treat observers as quorum members.

Steps to repro:

1. Server configuration: 3 voters, 2 observers (attached).
2. Bring up 2 voters and one observer. It's enough for quorum.
3. Shut down the one from the quorum who is the follower.

As I understand, expected result is that leader will start a new election round so that to regain quorum.
But the real situation is that it just says goodbye to that follower, and is still operable. (When I'm shutting down 3rd one -- observer -- leader starts trying to regain a quorum).

(Expectedly, if on step 3 we shut down the leader, not the follower, remaining follower starta new leader election, as it should be).","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-769,Major,Sergey Doroshenko,Fixed,2010-05-21T16:23:11.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Leader can treat observers as quorum members,2011-11-23T19:22:27.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",4.0
,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2010-05-06T16:05:55.000+0000,Kapil Thangavelu,"While trying to create a test case showing slow average add_auth, i stumbled upon a test case that reliably segfaults for me, albeit with variable amount of iterations (anwhere from 0 to 20 typically). fwiw, I've got about 220 processes in my test environment (ubuntu lucid 10.04). The test case opens a connection, adds authentication to it, and closes the connection, in a loop. I'm including the sample program and the gdb stack traces from the core file. I can upload the core file if thats helpful.",[],Bug,ZOOKEEPER-768,Major,Kapil Thangavelu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zkpython segfault on close (assertion error in io thread),2010-05-06T22:13:59.000+0000,"[<JIRA Version: name='3.4.0', id='12314469'>]",0.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>, <JIRA Component: name='recipes', id='12313246'>]",2010-05-05T19:04:27.000+0000,Patrick D. Hunt,Update the forrest recipes docs to point to the recipe implementations (where available).,"[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-766,Minor,Patrick D. Hunt,Fixed,2010-05-05T22:51:25.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,forrest recipes docs don't mention the lock/queue recipe implementations available in the release,2011-11-23T19:22:07.000+0000,[],1.0
Henry Robinson,"[<JIRA Component: name='quorum', id='12312379'>]",2010-05-04T21:41:07.000+0000,Flavio Paiva Junqueira,"In ZOOKEEPER-690, we noticed that an observer was being elected, and Henry proposed a patch to fix the issue. However, it seems that the patch does not solve the issue one user (Alan Cabrera) has observed. Given that we would like to fix this issue, and to work separately with Alan to determine the problem with his setup, I'm creating this jira and re-posting Henry's patch.","[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-764,Major,Flavio Paiva Junqueira,Fixed,2010-05-05T22:27:31.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Observer elected leader due to inconsistent voting view,2011-11-23T19:22:22.000+0000,[],1.0
Henry Robinson,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2010-05-04T13:09:43.000+0000,Kapil Thangavelu,"deadlocks occur if we attempt to close a handle while there are any outstanding async requests (aget, acreate, etc). Normally on close both the io thread terminates and the completion thread are terminated and joined, however w\ith outstanding async requests, the completion thread won't be in a joinable state, and we effectively hang when the main thread does the join.

afaics ideal behavior would be on close of a handle, to effectively clear out any remaining callbacks and let the completion thread terminate.

i've tried adding some bookkeeping to within a python client to guard against closing while there is an outstanding async completion request, but its an imperfect solution since even after the python callback is executed there is still a window for deadlock before the completion thread finishes the callback.

a simple example to reproduce the deadlock is attached.","[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-763,Major,Kapil Thangavelu,Fixed,2010-05-05T22:02:10.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Deadlock on close w/ zkpython / c client,2011-11-23T19:22:44.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",1.0
Kapil Thangavelu,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2010-04-29T01:28:10.000+0000,Kapil Thangavelu,"Currently when setting an acl, there is a minimal parse to ensure that its a list of dicts, however if one of the dicts is missing a required key, the subsequent usage doesn't check for it, and will segfault.. for example using an acl of [{""schema"":id, ""id"":world, permissions:PERM_ALL}] will segfault if used, because the scheme key is missing (its been purposefully typo'd to schema in example). 

I've expanded the check_acl macro to include verifying that all keys are present and added some unit tests against trunk in the attachments.","[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-758,Major,Kapil Thangavelu,Fixed,2010-05-01T01:14:12.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkpython segfaults on invalid acl with missing key,2011-11-23T19:22:46.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>, <JIRA Version: name='3.4.0', id='12314469'>]",1.0
,"[<JIRA Component: name='contrib-bindings', id='12312860'>, <JIRA Component: name='documentation', id='12312422'>]",2010-04-29T00:13:25.000+0000,Kapil Thangavelu,"
The zookeeper   digest authentication and acl scheme needs a bit more documentation. Currently its documented in the programmer guide.

""""""
digest uses a username:password string to generate MD5 hash which is then used as an ACL ID identity. Authentication is done by sending the username:password in clear text. When used in the ACL the expression will be the username:base64 encoded SHA1 password digest.
""""""

however its actually the digest of the entire credential that needs to be used.

I've attached a python unit test that sets and verifies an acl on a node.





		
		
",[],Bug,ZOOKEEPER-757,Major,Kapil Thangavelu,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zkpython acl/auth usage needs documentation + unit test,2010-04-29T00:36:22.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>, <JIRA Version: name='3.4.0', id='12314469'>]",1.0
Sean Busbey,[],2010-04-26T06:31:39.000+0000,Karthik K,"http://repo2.maven.org/maven2/org/apache/hadoop/zookeeper/3.3.0/zookeeper-3.3.0.pom 

The pom contains log4j dependency as itself. 

  <dependency> 
      <groupId>log4j</groupId> 
      <artifactId>log4j</artifactId> 
      <version>1.2.15</version> 
      <scope>compile</scope> 
    </dependency> 

This is broken without an exclusion list, since the pending dependencies of javax.mail. etc. are not necessary for the most part. 

Please fix this along with 3.3.1 and republish new dependencies , since at its current state , it is usable by some projects (to host in central , say). 

Correct dependency for log4j: 


<dependency> 
      <groupId>log4j</groupId> 
      <artifactId>log4j</artifactId> 
      <version>1.2.15</version> 
      <scope>compile</scope> 
      <exclusions> 
        <exclusion> 
          <groupId>javax.mail</groupId> 
          <artifactId>mail</artifactId> 
        </exclusion> 
        <exclusion> 
          <groupId>javax.jms</groupId> 
          <artifactId>jms</artifactId> 
        </exclusion> 
        <exclusion> 
          <groupId>com.sun.jdmk</groupId> 
          <artifactId>jmxtools</artifactId> 
        </exclusion> 
        <exclusion> 
          <groupId>com.sun.jmx</groupId> 
          <artifactId>jmxri</artifactId> 
        </exclusion> 
      </exclusions> 
    </dependency> 
","[<JIRA Version: name='3.4.6', id='12323310'>]",Bug,ZOOKEEPER-753,Major,Karthik K,Fixed,2012-12-12T07:41:49.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,update log4j dependency from 1.2.15 to 1.2.16 in branch 3.4,2014-03-13T18:16:59.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>]",4.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2010-04-22T20:36:01.000+0000,Patrick D. Hunt,"http://hadoop.apache.org/zookeeper/docs/r3.3.0/recipes.html#sc_recoverableSharedLocks
uses the heading ""recoverable"" locks, but the text refers to ""revocable"".","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-752,Major,Patrick D. Hunt,Duplicate,2014-05-04T12:18:25.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"address use of ""recoverable"" vs ""revocable"" in lock recipes documentation",2014-05-04T12:18:25.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='build', id='12312383'>]",2010-04-22T16:45:27.000+0000,Patrick D. Hunt,"The maven artifacts are currently (3.3.0) put into the toplevel of the release. This causes confusion
amonst new users (ie ""which jar do I use?""). Also the naming of the bin jar is wrong for maven (to put
onto the maven repo it must be named without the -bin) which adds extra burden for the release
manager. Putting into a subdir fixes this and makes it explicit what's being deployed to maven repo.
","[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-750,Major,Patrick D. Hunt,Fixed,2010-04-29T02:26:35.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"move maven artifacts into ""dist-maven"" subdir of the release (package target)",2011-11-23T19:22:44.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='build', id='12312383'>]",2010-04-22T16:21:07.000+0000,Patrick D. Hunt,"See this JIRA/comment for background:
https://issues.apache.org/jira/browse/ZOOKEEPER-425?focusedCommentId=12859697&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12859697

basically the issue is that OSGi metadata is included in the legacy jar (zookeeper-<version>.jar) but not in the binary only
jar (zookeeper-<version>-bin.jar) which is eventually deployed to the maven repo.
","[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-749,Critical,Patrick D. Hunt,Fixed,2010-04-29T02:02:37.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,OSGi metadata not included in binary only jar,2011-11-23T19:22:23.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2010-04-21T22:10:46.000+0000,Patrick D. Hunt,"usability issue, should be in hex:

2010-04-21 11:31:13,827 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11354:Learner@95] - Revalidating client: 83353578391797760
","[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-746,Minor,Patrick D. Hunt,Fixed,2010-04-26T04:18:44.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,learner outputs session id to log in dec (should be hex),2011-11-23T19:22:08.000+0000,[],1.0
,[],2010-04-16T14:20:02.000+0000,Ivan Kelly,"http://hadoop.apache.org/zookeeper/docs/r3.1.2/zookeeperInternals.html

In the active messaging diagram, one of the commit arrows is going the wrong way.",[],Bug,ZOOKEEPER-743,Trivial,Ivan Kelly,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Diagram error on Zookeeper internals page,2010-04-16T14:20:02.000+0000,[],0.0
Henry Robinson,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='contrib', id='12312700'>, <JIRA Component: name='contrib-bindings', id='12312860'>]",2010-04-15T23:21:08.000+0000,Josh Fraser,"On write operations, getting:

Fatal Python error: deallocating None
Aborted

This error happens on write operations only.  Here's the backtrace:

Fatal Python error: deallocating None

Program received signal SIGABRT, Aborted.
0x000000383fc30215 in raise () from /lib64/libc.so.6
(gdb) bt
#0  0x000000383fc30215 in raise () from /lib64/libc.so.6
#1  0x000000383fc31cc0 in abort () from /lib64/libc.so.6
#2  0x00002adbd0be8189 in Py_FatalError () from /usr/lib64/libpython2.4.so.1.0
#3  0x00002adbd0bc7493 in PyEval_EvalFrame () from /usr/lib64/libpython2.4.so.1.0
#4  0x00002adbd0bcab66 in PyEval_EvalFrame () from /usr/lib64/libpython2.4.so.1.0
#5  0x00002adbd0bcbfe5 in PyEval_EvalCodeEx () from /usr/lib64/libpython2.4.so.1.0
#6  0x00002adbd0bcc032 in PyEval_EvalCode () from /usr/lib64/libpython2.4.so.1.0
#7  0x00002adbd0be8729 in ?? () from /usr/lib64/libpython2.4.so.1.0
#8  0x00002adbd0be9bd8 in PyRun_SimpleFileExFlags () from /usr/lib64/libpython2.4.so.1.0
#9  0x00002adbd0bf000d in Py_Main () from /usr/lib64/libpython2.4.so.1.0
#10 0x000000383fc1d974 in __libc_start_main () from /lib64/libc.so.6
#11 0x0000000000400629 in _start ()
","[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-742,Major,Josh Fraser,Fixed,2010-04-22T06:44:05.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Deallocatng None on writes,2011-11-23T19:22:47.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>, <JIRA Version: name='3.3.0', id='12313976'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='contrib', id='12312700'>]",2010-04-14T17:12:17.000+0000,Patrick D. Hunt,"Create /foo using the REST proxy fails.

Also upgrade to the latest Jersey/Grizzly while we are at it (fixes for func/security)","[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-741,Critical,Patrick D. Hunt,Fixed,2010-04-22T05:43:49.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,root level create on REST proxy fails,2011-11-23T19:22:23.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",1.0
Henry Robinson,[],2010-04-13T08:08:17.000+0000,Federico,"The program that we are implementing uses the python binding for zookeeper but sometimes it crash with segfault; here is the bt from gdb:

Program received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0xad244b70 (LWP 28216)]
0x080611d5 in PyObject_Call (func=0x862fab0, arg=0x8837194, kw=0x0)
    at ../Objects/abstract.c:2488
2488    ../Objects/abstract.c: No such file or directory.
        in ../Objects/abstract.c
(gdb) bt
#0  0x080611d5 in PyObject_Call (func=0x862fab0, arg=0x8837194, kw=0x0)
    at ../Objects/abstract.c:2488
#1  0x080d6ef2 in PyEval_CallObjectWithKeywords (func=0x862fab0,
    arg=0x8837194, kw=0x0) at ../Python/ceval.c:3575
#2  0x080612a0 in PyObject_CallObject (o=0x862fab0, a=0x8837194)
    at ../Objects/abstract.c:2480
#3  0x0047af42 in watcher_dispatch (zzh=0x86174e0, type=-1, state=1,
    path=0x86337c8 """", context=0x8588660) at src/c/zookeeper.c:314
#4  0x00496559 in do_foreach_watcher (zh=0x86174e0, type=-1, state=1,
    path=0x86337c8 """", list=0xa5354140) at src/zk_hashtable.c:275
#5  deliverWatchers (zh=0x86174e0, type=-1, state=1, path=0x86337c8 """",
    list=0xa5354140) at src/zk_hashtable.c:317
#6  0x0048ae3c in process_completions (zh=0x86174e0) at src/zookeeper.c:1766
#7  0x0049706b in do_completion (v=0x86174e0) at src/mt_adaptor.c:333
#8  0x0013380e in start_thread () from /lib/tls/i686/cmov/libpthread.so.0
#9  0x002578de in clone () from /lib/tls/i686/cmov/libc.so.6",[],Bug,ZOOKEEPER-740,Major,Federico,Fixed,2014-04-25T01:45:26.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zkpython leading to segfault on zookeeper,2014-04-25T01:45:26.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",5.0
Jozef Hatala,"[<JIRA Component: name='c client', id='12312380'>]",2010-04-10T21:17:56.000+0000,Patrick D. Hunt,"/home/y/include/zookeeper/zookeeper.jute.h:96: error: extra semicolon
/home/y/include/zookeeper/zookeeper.jute.h:158: error: extra semicolon
/home/y/include/zookeeper/zookeeper.jute.h:288: error: extra semicolon

the code generator needs to be updated to not output a naked semi
","[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-738,Major,Patrick D. Hunt,Fixed,2010-04-26T18:58:09.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper.jute.h fails to compile with -pedantic ,2011-11-23T19:22:42.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",1.0
Mahadev Konar,"[<JIRA Component: name='server', id='12312382'>]",2010-04-10T21:13:23.000+0000,Patrick D. Hunt,"nc closes the write channel as soon as it's sent it's information, for example ""echo stat|nc localhost 2181""
in general this is fine, however the server code will close the socket as soon as it receives notice that nc has
closed it's write channel. if not all the 4 letter word result has been written back to the client yet, this will cause
some or all of the result to be lost - ie the client will not see the full result. this was introduced in 3.3.0 as part
of a change to reduce blocking of the selector by long running 4letter words.

here's an example of the logs from the server during this

echo -n stat | nc localhost 2181
2010-04-09 21:55:36,124 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn$Factory@251] - Accepted socket connection from /127.0.0.1:42179
2010-04-09 21:55:36,124 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@968] - Processing stat command from /127.0.0.1:42179
2010-04-09 21:55:36,125 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@606] - EndOfStreamException: Unable to read additional data from client sessionid 0x0, likely client has closed socket
2010-04-09 21:55:36,125 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1286] - Closed socket connection for client /127.0.0.1:42179 (no session established for client)
[phunt@gsbl90850 zookeeper-3.3.0]$ 2010-04-09 21:55:36,126 - ERROR [Thread-15:NIOServerCnxn@422] - Unexpected Exception: 
java.nio.channels.CancelledKeyException
	at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:55)
	at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:59)
	at org.apache.zookeeper.server.NIOServerCnxn.sendBuffer(NIOServerCnxn.java:395)
	at org.apache.zookeeper.server.NIOServerCnxn$SendBufferWriter.checkFlush(NIOServerCnxn.java:907)
	at org.apache.zookeeper.server.NIOServerCnxn$SendBufferWriter.flush(NIOServerCnxn.java:945)
	at java.io.BufferedWriter.flush(BufferedWriter.java:236)
	at java.io.PrintWriter.flush(PrintWriter.java:276)
	at org.apache.zookeeper.server.NIOServerCnxn$2.run(NIOServerCnxn.java:1089)
2010-04-09 21:55:36,126 - ERROR [Thread-15:NIOServerCnxn$Factory$1@82] - Thread Thread[Thread-15,5,main] died
java.nio.channels.CancelledKeyException
	at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:55)
	at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:64)
	at org.apache.zookeeper.server.NIOServerCnxn$SendBufferWriter.wakeup(NIOServerCnxn.java:927)
	at org.apache.zookeeper.server.NIOServerCnxn$SendBufferWriter.checkFlush(NIOServerCnxn.java:909)
	at org.apache.zookeeper.server.NIOServerCnxn$SendBufferWriter.flush(NIOServerCnxn.java:945)
	at java.io.BufferedWriter.flush(BufferedWriter.java:236)
	at java.io.PrintWriter.flush(PrintWriter.java:276)
	at org.apache.zookeeper.server.NIOServerCnxn$2.run(NIOServerCnxn.java:1089)
","[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-737,Blocker,Patrick D. Hunt,Fixed,2010-05-04T21:51:42.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,some 4 letter words may fail with netcat (nc),2011-12-29T23:08:07.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",3.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2010-04-08T23:17:11.000+0000,Patrick D. Hunt,"the docs (admin) should do a better job specifying which config parameters are required and the defaults if any. initLimit/syncLimit are 
both examples where we don't do this",[],Bug,ZOOKEEPER-736,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,docs for server config options should specify which are required and which have defaults,2022-02-03T08:50:20.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",0.0
Mahadev Konar,"[<JIRA Component: name='tests', id='12312427'>]",2010-04-07T21:26:46.000+0000,Mahadev Konar,The test should be fixed so that it runs only if ipv6 is enabled and does not run if ipv6 is not enabled.,"[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-735,Major,Mahadev Konar,Fixed,2010-04-08T18:32:57.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,cppunit test testipv6 assumes that the machine is ipv6 enabled.,2011-11-23T19:22:14.000+0000,[],1.0
Vishal Kher,"[<JIRA Component: name='tests', id='12312427'>]",2010-04-06T22:43:57.000+0000,Vishal Kher,"While runniing ""ant test-core-java"" QuorumPeerTestBase.java and ZooKeeperServerMainTest.java fail. The problem seems to be in ZookeeperserverMainTest.java:MainThread():66 and in QuorumPeerBaseTest.java:MainThread:76.

FileWriter.write() writes windows path to the conf file. Java does not like windows path. Therefore, the test complains that it cannot find myid and fails. 

Solution - convert windows path to UNIX path. This worked for me on windows.  Diffs are attached below. Solution not tested on Linux since for some reason build is failing (due to problems not related to this change).


vmc-floorb-dhcp116-114:/opt/zksrc/zookeeper-3.3.0/src/java/test/org/apache/zookeeper/server # svn diff
Index: ZooKeeperServerMainTest.java
===================================================================
--- ZooKeeperServerMainTest.java	(revision 931240)
+++ ZooKeeperServerMainTest.java	(working copy)
@@ -61,7 +61,8 @@
             if (!dataDir.mkdir()) {
                 throw new IOException(""unable to mkdir "" + dataDir);
             }
-            fwriter.write(""dataDir="" + dataDir.toString() + ""\n"");
+            String data = dataDir.toString().replace('\\', '/');
+            fwriter.write(""dataDir="" + data + ""\n"");
 
             fwriter.write(""clientPort="" + clientPort + ""\n"");
             fwriter.flush();
Index: quorum/QuorumPeerTestBase.java
===================================================================
--- quorum/QuorumPeerTestBase.java	(revision 931240)
+++ quorum/QuorumPeerTestBase.java	(working copy)
@@ -73,7 +73,8 @@
             if (!dataDir.mkdir()) {
                 throw new IOException(""Unable to mkdir "" + dataDir);
             }
-            fwriter.write(""dataDir="" + dataDir.toString() + ""\n"");
+            String data = dataDir.toString().replace('\\', '/');
+            fwriter.write(""dataDir="" + data + ""\n"");
 
             fwriter.write(""clientPort="" + clientPort + ""\n"");
             fwriter.write(quorumCfgSection + ""\n"");","[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-734,Major,Vishal Kher,Fixed,2010-04-26T20:02:17.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,QuorumPeerTestBase.java and ZooKeeperServerMainTest.java do not handle windows path correctly,2011-11-23T19:22:33.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",1.0
Lei Zhang,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2010-03-29T22:48:45.000+0000,Gustavo Niemeyer,"Apparently errors returned by the C library are not being correctly converted into a Python exception in some cases: 

>>> zookeeper.get_children(0, ""/"", None)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
SystemError: error return without exception set
","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-732,Minor,Gustavo Niemeyer,Fixed,2013-10-03T21:51:14.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Improper translation of error into Python exception,2014-03-13T18:17:05.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",6.0
Thomas Koch,"[<JIRA Component: name='documentation', id='12312422'>]",2010-03-26T23:12:47.000+0000,Karthik K,"    /**
     * The Asynchronous version of delete. ""The request doesn't  *missing* actually until
     * the asynchronous callback is called.""
     */
    public void delete(final String path, int version, VoidCallback cb, Object ctx) .. 


Also some information in the javadoc about how to instantiate the callback objects / context would be useful . 
","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-731,Minor,Karthik K,Fixed,2011-09-06T14:12:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Zookeeper#delete  , #create - async versions miss a verb in the javadoc ",2011-11-23T19:22:41.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",2.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2010-03-25T22:54:31.000+0000,Patrick D. Hunt,"The roadmap page is out of date wrt our current b/w compatibility rules. We need to update this.
http://wiki.apache.org/hadoop/ZooKeeper/Roadmap

additionally we need to include some of this detail in the release notes.
",[],Bug,ZOOKEEPER-727,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,roadmap needs to be updated re our backward compatibility guarantees,2022-02-03T08:50:23.000+0000,[],1.0
Ivan Kelly,"[<JIRA Component: name='scripts', id='12312384'>]",2010-03-22T14:49:42.000+0000,Ivan Kelly,"zkServer.sh output the PID of the zookeeper process with:
echo -n $! > ""$ZOOPIDFILE""

This uses -n which sh's builtin echo does not support. From echo's manpage.
<snip>
     Some shells may provide a builtin echo command which is similar or identical to this utility.  Most notably, the builtin echo in sh(1) does not accept
     the -n option.  Consult the builtin(1) manual page.
</snip>

This means that echo -n PID > ZOOPIDFILE will mean the contents of ZOOPIDFILE will be ""-n PID"". This stops zkServer.sh stop from working correctly.","[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-722,Minor,Ivan Kelly,Fixed,2010-04-12T06:33:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"zkServer.sh uses sh's builtin echo on BSD, behaves incorrectly.",2011-11-23T19:22:33.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",1.0
Paolo Castagna,"[<JIRA Component: name='build', id='12312383'>]",2010-03-21T08:04:47.000+0000,Paolo Castagna,"The artifact with the sources to be published in the Maven repository should be named ${artifactId}-${version}-sources.jar not ${artifactId}-${version}-src.jar.

See also: http://maven.apache.org/guides/mini/guide-central-repository-upload.html and ZOOKEEPER-224","[<JIRA Version: name='3.3.1', id='12314846'>, <JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-720,Trivial,Paolo Castagna,Fixed,2010-04-12T06:19:52.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Use zookeeper-{version}-sources.jar instead of zookeeper-{version}-src.jar to publish sources in the Maven repository,2011-11-23T19:22:39.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2010-03-19T17:35:11.000+0000,Flavio Paiva Junqueira,Add throttling to client to control the rate of operations to bookies. ,"[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-719,Major,Flavio Paiva Junqueira,Fixed,2010-07-09T20:54:36.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Add throttling to BookKeeper client,2011-11-23T19:22:40.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",1.0
Benjamin Reed,[],2010-03-19T16:09:05.000+0000,Benjamin Reed,"when we moved to ivy, we didn't update the fatjar build.xml to grab libraries out of the new location that ivy uses for downloaded libraries.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-718,Major,Benjamin Reed,Fixed,2010-03-19T17:28:07.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,the fatjar is missing libraries,2010-03-26T17:25:12.000+0000,[],2.0
,"[<JIRA Component: name='server', id='12312382'>]",2010-03-18T17:38:12.000+0000,Patrick D. Hunt,"If an out of memory occurs during snapshotting we recover from the error, however we should be more explict about the problem. 

Currently we log a warning, really we should log a FATAL and exit. This would force the server to restart (supervisor) and make it very clear to the user there is a problem. In some cases users don't notice the OOM exception (we have trouble noticing it sometimes ourselves) as it doesn't stand out on the logs. Exiting would be more explicit. However since we'd like to remove sysexits due to container issues it's not clear what we should do here in addition to making this FATAL.",[],Bug,ZOOKEEPER-714,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,snapshotting doesn't handle runtime exceptions (like out of memory) well,2022-02-03T08:50:27.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>]",2.0
,[],2010-03-18T15:53:19.000+0000,Lukasz Osipiuk,"Hi guys,

The following is not a bug report but rather a question - but as I am attaching large files I am posting it here rather than on mailinglist.

Today we had major failure in our production environment. Machines in zookeeper cluster gone wild and all clients got disconnected.
We tried to restart whole zookeeper cluster but cluster got stuck in leader election phase.

Calling stat command on any machine in the cluster resulted in 'ZooKeeperServer not running' message
In one of logs I noticed 'Invalid snapshot'  message which disturbed me a bit.

We did not manage to make cluster work again with data. We deleted all version-2 directories on all nodes and then cluster started up without problems.
Is it possible that snapshot/log data got corrupted in a way which made cluster unable to start?
Fortunately we could rebuild data we store in zookeeper as we use it only for locks and most of nodes is ephemeral.

I am attaching contents of version-2 directory from all nodes and server logs.
Source problem occurred some time before 15. First cluster restart happened at 15:03.
At some point later we experimented with deleting version-2 directory so I would not look at following restart because they can be misleading due to our actions.

I am also attaching zoo.cfg. Maybe something is wrong at this place. 
As I know look into logs i see read timeout during initialization phase after 20secs (initLimit=10, tickTime=2000).
Maybe all I have to do is increase one or other. which one? Are there any downsides of increasing tickTime.

Best regards, Łukasz Osipiuk

PS. due to attachment size limit I used split. to untar use 
cat nodeX-version-2.tgz-* |tar -xz
",[],Bug,ZOOKEEPER-713,Major,Lukasz Osipiuk,Invalid,2010-03-18T17:41:28.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper fails to start - broken snapshot?,2010-03-26T17:20:46.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>]",0.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2010-03-18T11:39:31.000+0000,Lukasz Osipiuk,"Originally problem was described on Users mailing list starting with this [post|http://mail-archives.apache.org/mod_mbox/hadoop-zookeeper-user/201003.mbox/<3b910d891003160743k38e2e7c9y830b182d88396d55@mail.gmail.com>].
Below I restate it in more organized form.

We occasionally (few times a day) observe that our client application disconnects from Zookeeper cluster.
Application is written in C++ and we are using libzookeeper_mt library. In version 3.2.2.

The disconnects we are observing are probably related to some problems with our network infrastructure - we are observing periods with great packet loss between machines in our DC. 

Sometimes after client application (i.e. zookeeper library) reconnects to zookeeper cluster we are observing that all subsequent requests return ZSESSIONMOVED error. Restarting client app helps - we always pass 0 as clientid to zookeeper_init function so old session is not reused.

On 16-03-2010 we observed few occurences of problem. Example ones:
- 22:08; client IP 10.1.112.60 (app1); sessionID 0x22767e1c9630000
- 14:21; client IP 10.1.112.61 (app2); sessionID 0x324dcc1ba580085

I attach logs of cluster and application nodes (only stuff concerining zookeeper):
- [^zookeeper-node1.log.2010-03-16.gz] - logs of zookeepr cluster node 1 10.1.112.62
- [^zookeeper-node2.log.2010-03-16.gz] - logs of zookeepr cluster node 2 10.1.112.63
- [^zookeeper-node3.log.2010-03-16.gz] - logs of zookeepr cluster node 3 10.1.112.64
- [^app1.log.2010-03-16.gz] - application logs of app1 10.1.112.60
- [^app2.log.2010-03-16.gz] - application logs of app2 10.1.112.61

I also made some analysis of case at 22:08:
- Network glitch which resulted in problem occurred at about 22:08.
- From what I see since 17:48 node2 was the leader and it did not
change later yesterday.
- Client was connected to node2 since 17:50
- At around 22:09 client tried to connect to every node (1,2,3).
Connections to node1 and node3 were closed
 with exception ""Exception causing close of session 0x22767e1c9630000
due to java.io.IOException: Read error"".
 Connection to node2 stood alive.
- All subsequent operations were refused with ZSESSIONMOVED error.
Error visible both on client and on server side.

","[<JIRA Version: name='3.2.3', id='12314847'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-710,Blocker,Lukasz Osipiuk,Fixed,2010-03-19T17:19:38.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,permanent ZSESSIONMOVED error after client app reconnects to zookeeper cluster,2010-03-26T17:25:12.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>]",3.0
Patrick D. Hunt,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2010-03-18T05:57:45.000+0000,Patrick D. Hunt,"ant test in bookkeeper results in

compile-test:
    [javac] Compiling 10 source files to /home/phunt/dev/workspace/gitzk/build/contrib/bookkeeper/test
    [javac] /home/phunt/dev/workspace/gitzk/src/contrib/bookkeeper/test/org/apache/bookkeeper/test/BaseTestCase.java:91: cannot find symbol
    [javac] symbol  : constructor Factory(java.lang.Integer)
    [javac] location: class org.apache.zookeeper.server.NIOServerCnxn.Factory
    [javac]         serverFactory = new NIOServerCnxn.Factory(ZooKeeperDefaultPort);
    [javac]                         ^
    [javac] 1 error

Flavio can you take a look at this one? (patch)
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-709,Blocker,Patrick D. Hunt,Fixed,2010-03-18T06:23:51.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,bookkeeper build failing with missing factory,2010-03-26T17:25:11.000+0000,[],2.0
Mahadev Konar,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='contrib-bindings', id='12312860'>]",2010-03-18T05:34:53.000+0000,Patrick D. Hunt,"ant test in zkpython is failing. I think this is due to mahadev's changes to remove unnecessary exports from the client lib.

     [exec] ImportError: /home/phunt/dev/workspace/gitzk/build/contrib/zkpython/lib.linux-x86_64-2.6/zookeeper.so: undefined symbol: deallocate_String_vector

Mahadev can you take a look?
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-708,Blocker,Patrick D. Hunt,Fixed,2010-03-18T06:48:51.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkpython failing due to undefined symbol deallocate_String_vector,2010-03-26T17:25:11.000+0000,[],2.0
Mahadev Konar,"[<JIRA Component: name='c client', id='12312380'>]",2010-03-18T00:21:09.000+0000,Patrick D. Hunt,"saw this in the zktest_mt at the end of 3.3.0, seems unlikely to happen though as it only failed after running the test 10-15 times.

Zookeeper_simpleSystem::testAuth ZooKeeper server started : elapsed 26011 : OK
Zookeeper_simpleSystem::testHangingClientzktest-mt: src/zookeeper.c:1950: zookeeper_process: Assertion `cptr' failed.
Aborted

",[],Bug,ZOOKEEPER-707,Critical,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,c client close can crash with cptr null,2022-02-03T08:50:12.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",0.0
Chris Thunes,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>]",2010-03-17T18:08:37.000+0000,Patrick D. Hunt,"If a client sets a large number of watches the ""set watches"" operation during session re-establishment can fail.

for example:
 WARN  [NIOServerCxn.Factory:22801:NIOServerCnxn@417] - Exception causing close of session 0xe727001201a4ee7c due to java.io.IOException: Len error 4348380

in this case the client was a web monitoring app and had set both data and child watches on > 32k znodes.

there are two issues I see here we need to fix:

1) handle this case properly (split up the set watches into multiple calls I guess...)
2) the session should have expired after the ""timeout"". however we seem to consider any message from the client as re-setting the expiration on the server side. Probably we should only consider messages from the client that are sent during an established session, otherwise we can see this situation where the session is not established however the session is not expired either. Perhaps we should create another JIRA for this particular issue.


","[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.2', id='12331981'>, <JIRA Version: name='3.6.0', id='12326518'>]",Bug,ZOOKEEPER-706,Critical,Patrick D. Hunt,Fixed,2015-06-13T00:50:17.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,large numbers of watches can cause session re-establishment to fail,2021-09-26T07:23:39.000+0000,"[<JIRA Version: name='3.1.2', id='12314394'>, <JIRA Version: name='3.2.2', id='12314335'>, <JIRA Version: name='3.3.0', id='12313976'>]",33.0
Thomas Koch,"[<JIRA Component: name='c client', id='12312380'>]",2010-03-14T20:44:28.000+0000,Thomas Koch,"The Debian package of Zookeeper[1] fails to build on some architectures, see below. The issue is an unknown assembler opcode. The Bugs is reported in the debian bugtracker as #568618[2]. A patch is available. I've uploaded it here too for your convenience.

[1] http://packages.qa.debian.org/z/zookeeper.html
[2] http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=568618


Buildd status overview:
https://buildd.debian.org/status/package.php?p=zookeeper

Some Failed Buildd Logs:
Sparc: https://buildd.debian.org/fetch.cgi?pkg=zookeeper;ver=3.2.2%2Bdfsg3-2;arch=sparc;stamp=1265466795
S390: https://buildd.debian.org/fetch.cgi?pkg=zookeeper;ver=3.2.2%2Bdfsg3-2;arch=s390;stamp=1265415637
powerpc: https://buildd.debian.org/fetch.cgi?pkg=zookeeper;ver=3.2.2%2Bdfsg3-2;arch=powerpc;stamp=1266677031
mipsel: https://buildd.debian.org/fetch.cgi?pkg=zookeeper&arch=mipsel&ver=3.2.2%2Bdfsg3-2&stamp=1268124320&file=log&as=raw
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-705,Minor,Thomas Koch,Duplicate,2013-12-10T21:25:10.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Fails to Build due to unknown opcode 'lock' in mt_adaptor.c,2018-06-05T20:14:13.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>]",3.0
Patrick D. Hunt,"[<JIRA Component: name='tests', id='12312427'>]",2010-03-12T22:22:01.000+0000,Patrick D. Hunt,"in some cases the tests are failing with JMX errors. From the logs I can see that QP was shutdown, however it did not exit it's thread until some time much later. This is causing interference with subsequent tests, causing the test to fail.

I have a patch that attempts to verify that the QP was shutdown (by joining the thread). It turns out that tests based on QuorumBase do this check (join) however some of the other tests do not. I believe this will address the issue.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-698,Critical,Patrick D. Hunt,Fixed,2010-03-13T00:07:28.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,intermittent JMX test failures due to not verifying QuorumPeer shutdown ,2010-03-26T17:25:11.000+0000,[],2.0
Mahadev Konar,[],2010-03-10T16:20:35.000+0000,Mahadev Konar,"The hudson test build failed 

http://hudson.zones.apache.org/hudson/job/ZooKeeper-trunk/729/testReport/junit/org.apache.zookeeper.test/QuorumQuotaTest/testQuotaWithQuorum/

","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-697,Critical,Mahadev Konar,Cannot Reproduce,2013-10-09T06:23:16.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,TestQuotaQuorum is failing on Hudson,2013-10-09T06:23:16.000+0000,[],0.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2010-03-10T05:41:35.000+0000,Patrick D. Hunt,"seeing the following on the console for http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/729/

looks like the cnxn is closed twice? (the second time 'sock' is null). perhaps it's due to client closing and sending session term, then closing socket, server sees the read return -1, so closes cnxn, then sees the session close request (which was queued)?

    [junit] 2010-03-10 03:15:53,205 - INFO  [main:NIOServerCnxn@1232] - Closed socket connection for client /127.0.0.1:41285 which had sessionid 0x127461233fc0000
    [junit] 2010-03-10 03:15:53,206 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11221:NIOServerCnxn$Factory@269] - Ignoring unexpected runtime exception
    [junit] java.lang.NullPointerException
    [junit] 	at org.apache.zookeeper.server.NIOServerCnxn.close(NIOServerCnxn.java:1232)
    [junit] 	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:594)
    [junit] 	at org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:259)
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-696,Blocker,Patrick D. Hunt,Fixed,2010-03-11T03:17:29.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"NPE in the hudson logs, seems nioservercnxn closed twice",2010-03-26T17:25:11.000+0000,[],2.0
,[],2010-03-10T01:25:28.000+0000,Benjamin Reed,To do the LENonTerminateTest we need to hook into the lookForLeader code. In the last release we did this by duplicating the code. This needs to be cleaned up.,[],Bug,ZOOKEEPER-695,Major,Benjamin Reed,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Need to remove the lookForLeader duplicated code in LENonTerminateTest,2010-03-10T18:55:32.000+0000,[],0.0
Flavio Paiva Junqueira,[],2010-03-09T21:40:06.000+0000,Henry Robinson,"See http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/Zookeeper-Patch-h7.grid.sp2.yahoo.net/77/testReport/junit/org.apache.zookeeper.test/ObserverTest/testObserver/

     [exec]     [junit] 2010-03-04 00:23:37,803 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11229:FastLeaderElection@683] - Notification time out: 3200
     [exec]     [junit] 2010-03-04 00:23:37,804 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11229:FastLeaderElection@689] - Notification: 2, 0, 2, 3, LOOKING, LOOKING, 1

ad infinitum. ","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-693,Critical,Henry Robinson,Fixed,2010-03-10T18:52:46.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,TestObserver stuck in tight notification loop in FLE,2010-03-26T17:25:11.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",4.0
Benjamin Reed,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2010-03-09T20:40:59.000+0000,Benjamin Reed,"BookKeeper starts a ZooKeeper server and needs to create an NIOServer.Factory, but the constructor changed.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-691,Major,Benjamin Reed,Fixed,2010-03-09T20:50:51.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Interface changed for NIOServer.Factory,2010-03-26T17:25:11.000+0000,[],2.0
Henry Robinson,[],2010-03-09T01:42:31.000+0000,Mahadev Konar,the hudson test failed on http://hudson.zones.apache.org/hudson/job/Zookeeper-Patch-h1.grid.sp2.yahoo.net/2/testReport/. There are huge set of cancelledkeyexceptions in the logs. Still going through the logs to find out the reason for failure.,"[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-690,Major,Mahadev Konar,Cannot Reproduce,2011-07-16T17:49:55.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,AsyncTestHammer test fails on hudson.,2011-11-23T19:22:30.000+0000,[],1.0
Patrick D. Hunt,"[<JIRA Component: name='build', id='12312383'>]",2010-03-09T01:23:12.000+0000,Patrick D. Hunt,"ivysettings.xml was added in 3.3.0 but it is not copied into the release artifact via ant ""package"" target of build.xml","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-689,Blocker,Patrick D. Hunt,Fixed,2010-03-09T01:50:53.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"release build broken - ivysettings.xml not copied during ""package""",2010-03-26T17:25:11.000+0000,[],2.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2010-03-05T17:10:39.000+0000,Patrick D. Hunt,"We are not clear enough (and the diagram we do have seems misleading) on _when_ session expirations are generated. In particular the fact that you only get expirations when the client is connected to the cluster, not when disconnected.

we need to detail:

1) when do you get expiration
2) what is the sequence of events that the watcher sees, from disco state, to getting the expiration (say the expiration happens when the client is disco, what do you see in the watcher while you are getting reconnected)
3) we need to give some examples of how to test this. We should be explicit that ""pulling the network cable"" on the client will not show expiration since the cliient will not be reconnected.
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-688,Critical,Patrick D. Hunt,Fixed,2010-03-05T22:03:17.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,explain session expiration better in the docs & faq,2010-03-26T17:25:11.000+0000,[],2.0
Mahadev Konar,[],2010-03-04T22:12:23.000+0000,Mahadev Konar,"LENonterminateTest fails with the following error:

{noformat}
2010-03-04 20:26:32,347 - INFO  [Thread-0:LeaderElection@155] - Server address: 0.0.0.0/0.0.0.0:11223
2010-03-04 20:26:32,348 - WARN  [Thread-0:LeaderElection@195] - Ignoring exception while looking for leader
java.io.IOException: Network is unreachable
	at java.net.PlainDatagramSocketImpl.send(Native Method)
	at java.net.DatagramSocket.send(DatagramSocket.java:612)
	at org.apache.zookeeper.server.quorum.LeaderElection.lookForLeader(LeaderElection.java:169)
	at org.apache.zookeeper.test.LENonTerminateTest$LEThread.run(LENonTerminateTest.java:83)
{noformat}
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-687,Major,Mahadev Konar,Fixed,2010-03-04T22:56:09.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,LENonterminatetest fails on some machines.,2010-03-26T17:25:11.000+0000,[],2.0
Henry Robinson,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='server', id='12312382'>]",2010-03-03T10:18:39.000+0000,Flavio Paiva Junqueira,"testNonTermination failed during a Hudson run for ZOOKEEPER-59. After inspecting the output, it looks like server is electing 2 as a leader and leaving. Given that 2 is just a mock server, server 0 remains alone in leader election.
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-684,Critical,Flavio Paiva Junqueira,Fixed,2010-03-10T01:28:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Race in LENonTerminateTest,2010-03-26T17:25:11.000+0000,[],2.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2010-03-01T07:04:20.000+0000,Patrick D. Hunt,"LogFormatter fails to parse txn log files - seems the tool was never updated to handle FileHeader.

It would be good to update the docs on txn log file to include detail on the file format.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-683,Blocker,Patrick D. Hunt,Fixed,2010-03-03T23:07:51.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,LogFormatter fails to parse transactional log files,2010-03-26T17:25:11.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>]",2.0
Scott Wang,"[<JIRA Component: name='java client', id='12312381'>]",2010-02-25T12:01:43.000+0000,Scott Wang,"After the event notification response from server is received, the client will convert the server path to the client path if chrooted by:

event.setPath(serverPath.substring(chrootPath.length());

If chrootPath and serverPath are the same, then the event's path will be set to a null string.

But the key of the watcher's map is ""/"", not a null string, so the watcher will not get notified at all.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-682,Blocker,Scott Wang,Fixed,2010-03-04T01:29:04.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Event is not processed when the watcher is set to watch ""/"" if chrooted",2010-03-26T17:25:11.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>]",3.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2010-02-25T11:25:07.000+0000,Vegard B. Havdal,"Just a small issue, the doc says that ""Setting this to 0 or omitting it entirely removes the limit on concurrent connections."", but we ran without this setting, and saw: WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn$Factory@226] - Too many connections from /10.76.251.190 - max is 10

Bug in doc possibly?

","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-681,Blocker,Vegard B. Havdal,Fixed,2010-03-05T02:27:39.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Minor doc issue re unset maxClientCnxns,2010-03-26T17:25:11.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2010-02-24T21:51:57.000+0000,Vegard B. Havdal,"Include server.#=... and/or a myid file when running standalone, zk server will crash with
java.lang.NullPointerException
   at org.apache.zookeeper.server.quorum.FastLeaderElection.totalOrderPredicate(FastLeaderElection.java:466)
   at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:635)
   at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:515)

Seen when running zk embedded in other server, using
String[] args = new String[]{zookeeperCfgFile};
org.apache.zookeeper.server.quorum.QuorumPeerMain.main(args);

The workaround is of course to fix the config, but 3.1.1 managed to not crash on this.",[],Bug,ZOOKEEPER-680,Minor,Vegard B. Havdal,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Including quorum config when standalone leads to crash,2022-02-03T08:50:13.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>]",0.0
Mahadev Konar,"[<JIRA Component: name='c client', id='12312380'>]",2010-02-22T18:20:07.000+0000,Patrick D. Hunt,The c client doesn't handle ipv6 numeric addresses as they are colon : delmited. After splitting the host/port on : we look for the port as the second entry in the array rather than the last entry in the array.,"[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-677,Critical,Patrick D. Hunt,Fixed,2010-03-10T16:34:04.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,c client doesn't allow ipv6 numeric connect string,2010-03-26T17:25:10.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>]",2.0
,"[<JIRA Component: name='contrib', id='12312700'>, <JIRA Component: name='contrib-bindings', id='12312860'>]",2010-02-22T01:03:15.000+0000,Josh Fraser,"I get about 50-75% connection loss exceptions and about 10% Bus Error when using the contrib/zkpython zookeeper.so.  Below is the exception:

2010-02-21 16:57:56,138:18481(0xb0081000):ZOO_ERROR@handle_socket_error_msg@1359: Socket [fe80::1002:885:7f00:1:2181] zk retcode=-4, errno=47(Address family not supported by protocol family): connect() call failed
Traceback (most recent call last):
  File ""./zksh.py"", line 63, in <module>
2010-02-21 16:57:56,138:18481(0xb0081000):ZOO_INFO@check_events@1439: initiated connection to server [127.0.0.1:2181]
    zkcli.dispatch(cmd,*args)
  File ""./zksh.py"", line 56, in dispatch
    returned = run(*args)
  File ""./zksh.py"", line 48, in ls
    print ""\n"".join(self.cmd.listNode(node))
  File ""/Users/josh/git/zktools/commands.py"", line 22, in listNode
    for path in zookeeper.get_children(self.zk, node):
zookeeper.ConnectionLossException: connection loss

I've run this in gdb and have this backtrace:

#0  free_pywatcher (pw=0x0) at src/c/zookeeper.c:199
#1  0x0025ae09 in pyzoo_exists (self=0x0, args=0x0) at src/c/zookeeper.c:765
#2  0x0018f51e in PyEval_EvalFrameEx ()
#3  0x00191173 in PyEval_EvalCodeEx ()
#4  0x0013b488 in PyFunction_SetClosure ()
#5  0x00121505 in PyObject_Call ()
#6  0x0018fcd0 in PyEval_EvalFrameEx ()
#7  0x00191173 in PyEval_EvalCodeEx ()
#8  0x0013b488 in PyFunction_SetClosure ()
#9  0x00121505 in PyObject_Call ()
#10 0x0018fcd0 in PyEval_EvalFrameEx ()
#11 0x00191173 in PyEval_EvalCodeEx ()
#12 0x0018f79d in PyEval_EvalFrameEx ()
#13 0x00191173 in PyEval_EvalCodeEx ()
#14 0x00191260 in PyEval_EvalCode ()
#15 0x001a883c in PyErr_Display ()
#16 0x001aa4ab in PyRun_InteractiveOneFlags ()
#17 0x001aa5f9 in PyRun_InteractiveLoopFlags ()
#18 0x001aaa2b in PyRun_AnyFileExFlags ()
#19 0x001b5a57 in Py_Main ()
#20 0x00001fca in ?? ()

zookeeper.c @ line 199:

 void free_pywatcher( pywatcher_t *pw)
{
  Py_DECREF(pw->callback);
  free(pw);
}

That's as far as I've dug so far -- I ended up just writing a retry decorator to get around it for now.  On the same machine, the zkCli.sh test client works flawlessly.  Also, here's the Mac OS X Bus Error trace:

Process:         Python [18556]
Path:            /System/Library/Frameworks/Python.framework/Versions/2.5/Resources/Python.app/Contents/MacOS/Python
Identifier:      Python
Version:         ??? (???)
Code Type:       X86 (Native)
Parent Process:  bash [18436]

Interval Since Last Report:          3323078 sec
Crashes Since Last Report:           50
Per-App Interval Since Last Report:  0 sec
Per-App Crashes Since Last Report:   38

Date/Time:       2010-02-21 17:07:27.399 -0800
OS Version:      Mac OS X 10.5.8 (9L31a)
Report Version:  6
Anonymous UUID:  FA533BDA-50B2-47A9-931C-6F2614C741F0

Exception Type:  EXC_BAD_ACCESS (SIGBUS)
Exception Codes: KERN_PROTECTION_FAILURE at 0x0000000000000004
Crashed Thread:  0

Thread 0 Crashed:
0   zookeeper.so                  	0x002332bd free_pywatcher + 10 (zookeeper.c:199)
1   zookeeper.so                  	0x00239e09 pyzoo_exists + 984 (zookeeper.c:765)
2   org.python.python             	0x0018f51e PyEval_EvalFrameEx + 17116
3   org.python.python             	0x0018f700 PyEval_EvalFrameEx + 17598
4   org.python.python             	0x00191173 PyEval_EvalCodeEx + 1638
5   org.python.python             	0x0013b488 PyFunction_SetClosure + 2667
6   org.python.python             	0x00121505 PyObject_Call + 50
7   org.python.python             	0x0018fcd0 PyEval_EvalFrameEx + 19086
8   org.python.python             	0x00191173 PyEval_EvalCodeEx + 1638
9   org.python.python             	0x0013b488 PyFunction_SetClosure + 2667
10  org.python.python             	0x00121505 PyObject_Call + 50
11  org.python.python             	0x0018fcd0 PyEval_EvalFrameEx + 19086
12  org.python.python             	0x00191173 PyEval_EvalCodeEx + 1638
13  org.python.python             	0x00191260 PyEval_EvalCode + 87
14  org.python.python             	0x001a883c PyErr_Display + 1896
15  org.python.python             	0x001a8e66 PyRun_FileExFlags + 135
16  org.python.python             	0x001aa7d2 PyRun_SimpleFileExFlags + 421
17  org.python.python             	0x001b5a57 Py_Main + 3095
18  org.python.pythonapp          	0x00001fca 0x1000 + 4042

Thread 1:
0   libSystem.B.dylib             	0x9265fe0e poll$UNIX2003 + 10
1   libSystem.B.dylib             	0x9262a155 _pthread_start + 321
2   libSystem.B.dylib             	0x9262a012 thread_start + 34

Thread 2:
0   libSystem.B.dylib             	0x9260046e __semwait_signal + 10
1   libSystem.B.dylib             	0x9262adcd pthread_cond_wait$UNIX2003 + 73
2   libzookeeper_mt.2.dylib       	0x00247e9f do_completion + 223
3   libSystem.B.dylib             	0x9262a155 _pthread_start + 321
4   libSystem.B.dylib             	0x9262a012 thread_start + 34

Thread 0 crashed with X86 Thread State (32-bit):
  eax: 0x00000000  ebx: 0x00239a3c  ecx: 0x00000000  edx: 0x00000000
  edi: 0x001efaa0  esi: 0x00000000  ebp: 0xbfffe508  esp: 0xbfffe4f0
   ss: 0x0000001f  efl: 0x00010286  eip: 0x002332bd   cs: 0x00000017
   ds: 0x0000001f   es: 0x0000001f   fs: 0x00000000   gs: 0x00000037
  cr2: 0x00000004

Binary Images:
    0x1000 -     0x1ffe  org.python.pythonapp 2.5.0 (2.5.0a0) <5aa9f0cc36fda395f965e08c96613cf5> /System/Library/Frameworks/Python.framework/Versions/2.5/Resources/Python.app/Contents/MacOS/Python
  0x119000 -   0x1e5feb  org.python.python 2.5 (2.5) <523ba54c654eeed6bc670db2f58a73ab> /System/Library/Frameworks/Python.framework/Versions/2.5/Python
  0x232000 -   0x23ffff +zookeeper.so ??? (???) <77134e53d6dbc7bbcf783b0fc2b16d6e> /Library/Python/2.5/site-packages/zookeeper.so
  0x246000 -   0x255fef +libzookeeper_mt.2.dylib ??? (???) <839ce6d5a904236d0f0112e75656ecfd> /usr/local/lib/libzookeeper_mt.2.dylib
0x8fe00000 - 0x8fe2db43  dyld 97.1 (???) <458eed38a009e5658a79579e7bc26603> /usr/lib/dyld
0x925f8000 - 0x9275fff3  libSystem.B.dylib ??? (???) <ae47ca9b1686b065f8ac4d2de09cc432> /usr/lib/libSystem.B.dylib
0x96aea000 - 0x96af1fe9  libgcc_s.1.dylib ??? (???) <e280ddf3f5fb3049e674edcb109f389a> /usr/lib/libgcc_s.1.dylib
0x96fc9000 - 0x96fcdfff  libmathCommon.A.dylib ??? (???) /usr/lib/system/libmathCommon.A.dylib
0xffff0000 - 0xffff1780  libSystem.B.dylib ??? (???) /usr/lib/libSystem.B.dylib

",[],Bug,ZOOKEEPER-676,Major,Josh Fraser,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,50%-75% connection loss exceptions using zkpython,2010-02-22T18:47:28.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>]",2.0
Henry Robinson,"[<JIRA Component: name='leaderElection', id='12312378'>]",2010-02-20T14:32:41.000+0000,Flavio Paiva Junqueira,"After applying the patch of ZOOKEEPER-569, I observed a failure of LETest. From a cursory inspection of the log, I can tell that a leader is being elected, but some thread is not joining. At this point I'm not sure if this is a problem with the leader election implementation or the test itself. 

Just to be clear, the patch of ZOOKEEPER-569 solved a real issue, but it seems that there is yet another problem with LETest.


","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-675,Critical,Flavio Paiva Junqueira,Won't Fix,2011-11-10T12:30:44.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,LETest thread fails to join,2011-11-10T12:30:44.000+0000,[],0.0
,"[<JIRA Component: name='c client', id='12312380'>]",2010-02-19T23:48:56.000+0000,Patrick D. Hunt,"The c client compiles on cygwin 1.5 after ZOOKEEPER-586 is applied, however not all the tests pass.",[],Bug,ZOOKEEPER-674,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,c client tests fail on cygwin,2022-02-03T08:50:13.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>]",0.0
Flavio Paiva Junqueira,[],2010-02-16T19:03:13.000+0000,Flavio Paiva Junqueira,We just need to remove the first two paragraphs of Section 2.,"[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-673,Major,Flavio Paiva Junqueira,Fixed,2010-02-19T02:08:25.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Fix observer documentation regarding leader election,2010-03-26T17:25:10.000+0000,[],2.0
Henry Robinson,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2010-02-12T23:21:13.000+0000,Lei Zhang,"Zookeeper client using zkpython segfaults on zookeeper server restart. It is reliably reproducible using the attached script zk.py.
I'm able to stop segfault using the attached patch voyager.patch, but zkpython seems to have deeper issue on its use of watcher_dispatch - on zookeeper server restart, I see up to 6 invocation of watcher_dispatch while my script is simply sleeping in the main thread. This can't be right.",[],Bug,ZOOKEEPER-670,Critical,Lei Zhang,Fixed,2014-04-25T01:44:48.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,zkpython leading to segfault on zookeeper server restart,2014-04-25T01:44:48.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.2.2', id='12314335'>]",7.0
Patrick D. Hunt,[],2010-02-12T20:46:04.000+0000,Patrick D. Hunt,the current tostring method is broken,"[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-669,Critical,Patrick D. Hunt,Fixed,2010-02-23T18:25:59.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,watchedevent tostring should clearly output the state/type/path,2010-03-26T17:25:10.000+0000,"[<JIRA Version: name='3.1.2', id='12314394'>, <JIRA Version: name='3.2.2', id='12314335'>]",2.0
Flavio Paiva Junqueira,[],2010-02-12T12:12:01.000+0000,Flavio Paiva Junqueira,I think we should remove the close call in LedgerInputStream. ,"[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-668,Major,Flavio Paiva Junqueira,Fixed,2010-02-19T02:43:07.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Close method in LedgerInputStream doesn't do anything,2010-03-26T17:25:10.000+0000,[],2.0
Patrick D. Hunt,"[<JIRA Component: name='java client', id='12312381'>]",2010-02-11T19:26:56.000+0000,Patrick D. Hunt,"The java client doesn't handle ipv6 numeric addresses as they are colon (:) delmited. After splitting the host/port on : we look for the port as the second entry in the array rather than the last entry in the array.
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-667,Critical,Patrick D. Hunt,Fixed,2010-03-12T18:55:30.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,java client doesn't allow ipv6 numeric connect string,2012-06-20T10:39:39.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>]",3.0
,"[<JIRA Component: name='java client', id='12312381'>]",2010-02-10T23:53:18.000+0000,Martin Traverso,"The following code may result in a data race due to unsafe publication of a reference to ""this"". The call to cnxn.start() spawns threads that have access to the partially-constructed reference to the ZooKeeper object. 

See http://www.ibm.com/developerworks/java/library/j-jtp0618.html for some background info.

{noformat}
public ZooKeeper(String connectString, int sessionTimeout, Watcher watcher)
    throws IOException
{
        .....
        cnxn = new ClientCnxn(connectString, sessionTimeout, this, watchManager);
        cnxn.start();
}
{noformat}

The obvious fix is to move the call to cnxn.start() into a separate start() method.
",[],Bug,ZOOKEEPER-666,Major,Martin Traverso,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Unsafe publication in client API,2022-02-03T08:50:21.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>]",2.0
Mahadev Konar,"[<JIRA Component: name='server', id='12312382'>]",2010-02-02T17:30:51.000+0000,Patrick D. Hunt,"http://hudson.zones.apache.org/hudson/job/ZooKeeper-trunk/686/

java.lang.RuntimeException: Unable to run quorum server 
	at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:380)
	at org.apache.zookeeper.test.ZkDatabaseCorruptionTest.testCorruption(ZkDatabaseCorruptionTest.java:99)
Caused by: java.io.IOException: Invalid magic number 0 != 1514884167
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.inStreamCreated(FileTxnLog.java:455)
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.createInputArchive(FileTxnLog.java:471)
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.goToNextLog(FileTxnLog.java:438)
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:519)
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:145)
	at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:193)
	at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:377)","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-663,Critical,Patrick D. Hunt,Fixed,2010-03-09T01:25:03.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,hudson failure in ZKDatabaseCorruptionTest,2010-03-26T17:25:10.000+0000,[],2.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2010-02-02T02:57:18.000+0000,Qian Ye,"I have a zookeeper cluster with 5 servers, zookeeper version 3.2.1, here is the content in the configure file, zoo.cfg

======
# The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial 
# synchronization phase can take
initLimit=5
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
syncLimit=2
# the directory where the snapshot is stored.
dataDir=./data/
# the port at which the clients will connect
clientPort=8181

# zookeeper cluster list
server.100=10.23.253.43:8887:8888
server.101=10.23.150.29:8887:8888
server.102=10.23.247.141:8887:8888
server.200=10.65.20.68:8887:8888
server.201=10.65.27.21:8887:8888
=====

Before the problem happened, the server.200 was the leader. Yesterday morning, I found the there were many sockets with the state of CLOSE_WAIT on the clientPort (8181),  the total was over about 120. Because of these CLOSE_WAIT, the server.200 could not accept more connections from the clients. The only thing I can do under this situation is restart the server.200, at about 2010-02-01 06:06:35. The related log is attached to the issue.","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-662,Major,Qian Ye,Duplicate,2011-07-16T17:48:49.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Too many CLOSE_WAIT socket state on a server,2011-11-23T19:22:30.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",2.0
Karthik K,"[<JIRA Component: name='tests', id='12312427'>]",2010-01-24T01:27:49.000+0000,Karthik K,Thread.run() used instead of Thread.start() . ,"[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-656,Major,Karthik K,Fixed,2010-01-25T03:35:57.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,SledgeHammer test - thread.run() deprecated ,2010-03-26T17:25:10.000+0000,[],2.0
,[],2010-01-22T23:03:30.000+0000,Henry Robinson,"Tests have been failing for us in an environment where we removed the CONSOLE appender from log4j. This breaks a couple of tests in QuorumPeerMainTest at least.

I have fixed in our builds by replacing CONSOLE with ROLLINGFILE (which we are using) for the time being, but messing with the log config shouldn't break tests. ",[],Bug,ZOOKEEPER-654,Minor,Henry Robinson,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Tests should not rely on CONSOLE appender being present,2010-01-23T00:17:32.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",0.0
Patrick D. Hunt,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2010-01-22T18:07:26.000+0000,Patrick D. Hunt,"http://hudson.zones.apache.org/hudson/job/ZooKeeper-trunk/675/testReport/org.apache.zookeeper.test/LETest/testLE/

junit.framework.AssertionFailedError: Threads didn't join
	at org.apache.zookeeper.test.LETest.testLE(LETest.java:116)
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-653,Major,Patrick D. Hunt,Fixed,2011-09-06T15:57:00.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,hudson failure in LETest,2012-03-29T11:40:30.000+0000,[],2.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2010-01-19T10:59:43.000+0000,Qian Ye,Server fails to join ensemble.,[],Bug,ZOOKEEPER-650,Major,Qian Ye,Cannot Reproduce,2011-09-06T15:57:32.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Servers cannot join in quorum,2011-09-06T15:57:32.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",2.0
Henry Robinson,"[<JIRA Component: name='quorum', id='12312379'>]",2010-01-17T21:37:33.000+0000,Henry Robinson,"See http://hudson.zones.apache.org/hudson/job/Zookeeper-Patch-h8.grid.sp2.yahoo.net/105/testReport/org.apache.zookeeper.test/ObserverTest/testObserver/ - test has only failed once and is hard to reproduce, so am waiting for more data. ","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-649,Minor,Henry Robinson,Cannot Reproduce,2014-04-23T22:13:46.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,testObserver timed out once on Hudson,2014-04-23T22:13:46.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='server', id='12312382'>]",2010-01-15T22:33:58.000+0000,Patrick D. Hunt,"http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/666/testReport/org.apache.zookeeper.test/QuorumTest/testLeaderShutdown/

junit.framework.AssertionFailedError: QP failed to shutdown in 30 seconds
	at org.apache.zookeeper.test.QuorumBase.shutdown(QuorumBase.java:293)
	at org.apache.zookeeper.test.QuorumBase.shutdownServers(QuorumBase.java:281)
	at org.apache.zookeeper.test.QuorumBase.tearDown(QuorumBase.java:266)
	at org.apache.zookeeper.test.QuorumTest.tearDown(QuorumTest.java:55)

Flavio, can you triage this one?
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-647,Critical,Patrick D. Hunt,Fixed,2010-01-21T03:08:33.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,hudson failure in testLeaderShutdown,2010-03-26T17:25:09.000+0000,[],2.0
Mahadev Konar,"[<JIRA Component: name='recipes', id='12313246'>]",2010-01-15T05:19:32.000+0000,Jaakko Laine,"Not sure, but there seem to be two issues in the example WriteLock:

(1) ZNodeName is sorted according to session ID first, and then according to znode sequence number. This might cause starvation as lower session IDs always get priority. WriteLock is not thread-safe in the first place, so having session ID involved in compare operation does not seem to make sense.

(2) if findPrefixInChildren finds previous ID, it should add dir in front of the ID",[],Bug,ZOOKEEPER-645,Minor,Jaakko Laine,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Bug in WriteLock recipe implementation?,2022-02-03T08:50:25.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>]",13.0
Patrick D. Hunt,"[<JIRA Component: name='build', id='12312383'>]",2010-01-14T18:43:44.000+0000,Mahadev Konar,"the nighthly build has been failing. http://hudson.zones.apache.org/hudson/job/ZooKeeper-trunk/664/. The problem seems to be 
{code}
BUILD FAILED
java.lang.NoClassDefFoundError: org/apache/ivy/ant/IvyMakePom$Mapping

Total time: 15 minutes 14 seconds
{code}","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-644,Major,Mahadev Konar,Fixed,2010-01-15T22:36:06.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Nightly build failed on hudson.,2010-03-26T17:25:09.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",2.0
Marc Celani,"[<JIRA Component: name='c client', id='12312380'>]",2010-01-13T01:52:42.000+0000,Dale Johnson,"More important zookeeper warnings are drown out by the following several times per minute:

2010-01-12 17:39:57,227:22317(0x4147eb90):ZOO_WARN@zookeeper_interest@1335: Exceeded deadline by 13ms

Perhaps this is an issue with the way virtualized systems manage gettimeofday results?

Maybe the current 10ms threshold could be pushed up a bit.  I notice that 95% of the messages are below 50ms.

Is there an obvious configuration change that I can make to fix this?

config file below:

# The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
dataDir=/mnt/zookeeper
# the port at which the clients will connect
clientPort=2181

server.1=hbase.1:2888:3888
server.2=hbase.2:2888:3888
server.3=hbase.3:2888:3888
server.4=hbase.4:2888:3888
server.5=hbase.5:2888:3888
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-642,Major,Dale Johnson,Fixed,2012-05-11T05:55:15.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"""exceeded deadline by N ms"" floods logs",2012-05-11T11:00:44.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",6.0
Flavio Paiva Junqueira,"[<JIRA Component: name='build', id='12312383'>]",2010-01-06T16:26:42.000+0000,Flavio Paiva Junqueira,"The trunk build is failing when Hudson runs it. The problem seems to be that ivy-init is executed only once, but its definitions (in particular ivy:settings) do not persist, and the failure occurs when we run ivy-retrieve a second time, which requires the definition of ivy:settings.

It seems that the problem occur with ant 1.7.0, but not with 1.7.1, so it could be an ant issue. ","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-637,Major,Flavio Paiva Junqueira,Fixed,2010-01-07T08:42:59.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Trunk build is failing,2010-03-26T17:33:22.000+0000,[],2.0
,"[<JIRA Component: name='c client', id='12312380'>]",2009-12-19T22:13:17.000+0000,Andrew Reynhout,"I'm working on Ruby bindings for ZK (3.2.2) via the C API.  This involves some painful kludgery due to MRI Ruby's threading model, but I believe this bug report is valid:

Some of the elements of Stat structs, when returned via the C client, have values inconsistent with the synchronous call, zkCli output, and expectations.  E.g.:

{noformat:title=zkCli command line}
$ zk/bin/zkCli.sh -server localhost:2182 set /lala lala234
Connecting to localhost:2182

WATCHER::

WatchedEvent: Server state change. New state: SyncConnected
cZxid = 3
ctime = Sat Dec 19 14:32:03 EST 2009
mZxid = 69
mtime = Sat Dec 19 15:31:46 EST 2009
pZxid = 3
cversion = 0
dataVersion = 4
aclVersion = 0
ephemeralOwner = 0
dataLength = 7
numChildren = 0

{noformat}

{noformat:title=Ruby script + verbose ZK log output}
2009-12-19 15:31:48,952:45753(0x751000):ZOO_DEBUG@process_completions@1902: Switching on cptr->completion_type (async)
2009-12-19 15:31:48,952:45753(0x751000):ZOO_DEBUG@process_completions@1916: Calling COMPLETION_STAT for xid=4b2d3837 rc=0
2009-12-19 15:31:48,952:45753(0x751000):ZOO_DEBUG@process_completions@1949: --- PRE ruby callback.
   -----------------------
   &res.stat    0x00750e30
        .czxid        3
        .mzxid        69
        .ctime        1261251123149
        .mtime        1261254706180
        .version      4
        .cversion     0
        .aversion     0
        .ephemeralOwner 0
        .dataLength   7
        .numChildren  0
        .pzxid        3
   =======================
2009-12-19 15:31:48,952:45753(0x751000):ZOO_DEBUG@ruby_stat_completion_wrapper@1702: --- creating ruby thread.
2009-12-19 15:31:48,952:45753(0x7ee000):ZOO_DEBUG@ruby_stat_completion_wrapper_2@1685: --- invoking ruby callback.
   -----------------------
   cb->dc         0x002fe000
   cb->rc         0
   cb->ctx        0x01a03870
   cb->stat       0x00750e30
     ->czxid        3
     ->mzxid        69
     ->ctime        1261251123149
     ->mtime        140735387442616
     ->version      0
     ->cversion     0
     ->aversion     27264392
     ->ephemeralOwner 4302638816
     ->dataLength   -2100901903
     ->numChildren  32767
     ->pzxid        3
   =======================
2009-12-19 15:31:48,952:45753(0x7ee000):ZOO_DEBUG@ruby_stat_completion_wrapper_2@1687: --- callback returned, freeing struct.
2009-12-19 15:31:48,952:45753(0x7ee000):ZOO_DEBUG@ruby_stat_completion_wrapper_2@1689: --- struct freed.
sync returned #<ZooKeeperFFI::Stat:0x000001013783d0>
async callback.return_code 0
           attr  ??                   sync                  async
           ----  --                   ----                  -----
          czxid  OK                      3                      3
          mzxid  OK                     69                     69
          ctime  OK          1261251123149          1261251123149
          mtime  --          1261254706180        140735387442616
        version  --                      4                      0
       cversion  OK                      0                      0
       aversion  --                      0               27264392
 ephemeralOwner  --                      0             4302638816
     dataLength  --                      7            -2100901903
    numChildren  --                      0                  32767
          pzxid  OK                      3                      3
2009-12-19 15:31:49,052:45753(0x6ce000):ZOO_DEBUG@do_io@316: IO thread terminated
2009-12-19 15:31:49,052:45753(0x751000):ZOO_DEBUG@do_completion@335: completion thread terminated
2009-12-19 15:31:49,052:45753(0x705f3be0):ZOO_INFO@zookeeper_close@2407: Closing zookeeper session 125a86bdc640020 to [127.0.0.1:2182]
{noformat}

The last set of lines, above, are output from my Ruby code, comparing the differences between sync and async calls.  I added the LOG_DEBUG in zookeeper.c to dump the contents of res.stat before it gets shipped off to Ruby...as you can see, the LOG_DEBUG values match the values in the async Ruby call, but they're wrong.

I've currently only tested on OSX 10.5 (32bit) and 10.6 (64bit), but sometimes different elements are wrong on each.  I can get a comprehensive list if it's useful, but as examples:

{noformat:title=data variation examples}
## aexists
# ctime (int64_t):
#  32b OK
#  64b OK
# dataLength (int32_t):
#  32b OK
#  64b NG (== -2100901903 instead of 10)

## aget
# ctime (int64_t):
#  32b OK
#  64b NG (== 0 instead of something unixtimey e.g. 1261251123149)
# dataLength (int32_t):
#  32b NG (== -1856148083 instead of 10)
#  64b OK

## aset
# ctime (int64_t):
#  32b OK
#  64b OK
# dataLength (int32_t):
#  32b OK
#  64b NG (== -2100901903 instead of 10)

{noformat}


Lastly, a snippet of my changes to zookeeper.c, hopefully showing that I haven't done any harm to the data.  Note that I'm also seeing similar data variations for COMPLETION_DATA, which also returns a Stat struct.  I haven't finished the Ruby code for COMPLETION_ACLLIST (again, returns a Stat), so the same issue might arise there as well.

I realize that hacking the ZK C API is not a great way to build a foreign binding.  :(

{code:title=src/c/src/zookeeper.c}

            case COMPLETION_STAT:
                LOG_DEBUG((""Calling COMPLETION_STAT for xid=%x rc=%d"",cptr->xid,rc));
                if (rc) {
                    //cptr->c.stat_result(rc, 0, cptr->data);
                    ruby_stat_completion_wrapper(cptr->c.stat_result, rc, 0, (void *)cptr->data);
                } else {
                    struct SetDataResponse res;
                    deserialize_SetDataResponse(ia, ""reply"", &res);
  LOG_DEBUG((""--- PRE ruby callback.\n\
   -----------------------\n\
   &res.stat    0x%08x\n\
        .czxid        %ld\n\
        .mzxid        %ld\n\
        .ctime        %ld\n\
        .mtime        %ld\n\
        .version      %d\n\
        .cversion     %d\n\
        .aversion     %d\n\
        .ephemeralOwner %ld\n\
        .dataLength   %d\n\
        .numChildren  %d\n\
        .pzxid        %ld\n\
   ======================="",
    &res.stat,
    res.stat.czxid, res.stat.mzxid, res.stat.ctime, res.stat.mtime,
    res.stat.version, res.stat.cversion, res.stat.aversion,
    res.stat.ephemeralOwner, res.stat.dataLength, res.stat.numChildren,
    res.stat.pzxid ));
                    //cptr->c.stat_result(rc, &res.stat, cptr->data);
                    ruby_stat_completion_wrapper(cptr->c.stat_result, rc, &res.stat, (void *)cptr->data);
                    deallocate_SetDataResponse(&res);
                }
                break;

{code}


",[],Bug,ZOOKEEPER-634,Minor,Andrew Reynhout,Invalid,2009-12-19T22:20:46.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,some elements of struct Stat have incorrect values when returned asynchronously,2010-01-05T20:48:06.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",0.0
Henry Robinson,"[<JIRA Component: name='tests', id='12312427'>]",2009-12-16T07:39:59.000+0000,Henry Robinson,There are two identical ObserverTest.java files in trunk. ,"[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-630,Major,Henry Robinson,Fixed,2009-12-16T18:30:27.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Trunk has duplicate ObserverTest.java files,2010-03-26T17:25:09.000+0000,[],2.0
Flavio Paiva Junqueira,[],2009-12-16T05:00:56.000+0000,Mahadev Konar,FLELostMessageTest assumes that the first zxid exchange will be -1 zxid. WIth ZOOKEEPER-596 the zxid would be 0 and not -1. So the corresponding change needs to be made to this test.,"[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-629,Major,Mahadev Konar,Fixed,2009-12-18T02:20:53.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,FLELostMessageTest assumes that the first zxid on a startup of quorum is -1.,2013-05-02T02:29:23.000+0000,[],2.0
,"[<JIRA Component: name='server', id='12312382'>]",2009-12-16T01:58:31.000+0000,Qian Ye,"I find a very strange scenario today, I'm not sure how it happen, I just found it like this. Maybe you can give me some information about it, my Zookeeper Server is version 3.2.1.

My Zookeeper cluster contains three servers, with ip: 10.81.12.144,10.81.12.145,10.81.12.141. I wrote a client to create ephemeral node under znode: se/diserver_tc. The client runs on the server with ip 10.81.13.173. The client can create a ephemeral node on zookeeper server and write the host ip (10.81.13.173) in to the node as its data. There is only one client process can be running at a time, because the client will listen to a certain port.

It is strange that I found there were two ephemeral node with the ip 10.81.13.173 under znode se/diserver_tc.
se/diserver_tc/diserver_tc0000000067
STAT:
        czxid: 124554079820
        mzxid: 124554079820
        ctime: 1260609598547
        mtime: 1260609598547
        version: 0
        cversion: 0
        aversion: 0
        ephemeralOwner: 226627854640480810
        dataLength: 92
        numChildren: 0
        pzxid: 124554079820

se/diserver_tc/diserver_tc0000000095
STAT:
        czxid: 128849019107
        mzxid: 128849019107
        ctime: 1260772197356
        mtime: 1260772197356
        version: 0
        cversion: 0
        aversion: 0
        ephemeralOwner: 154673159808876591
        dataLength: 92
        numChildren: 0
        pzxid: 128849019107

There are TWO with different session id! And after I kill the client process on the server 10.81.13.173, the se/diserver_tc/diserver_tc0000000095 node disappear, but the se/diserver_tc/diserver_tc0000000067 stay the same. That means it is not my coding mistake to create the node twice. I checked several times and I'm sure that there is no another client instance running. And I use the 'stat' command to check the three zookeeper servers, and there is no client from 10.81.13.173,

$echo stat | nc 10.81.12.144 2181   
Zookeeper version: 3.2.1-808558, built on 08/27/2009 18:48 GMT
Clients:
 /10.81.13.173:35676[1](queued=0,recved=0,sent=0) # it is caused by the nc process

Latency min/avg/max: 0/3/254
Received: 11081
Sent: 0
Outstanding: 0
Zxid: 0x1e000001f5
Mode: follower
Node count: 32

$ echo stat | nc 10.81.12.141 2181
Zookeeper version: 3.2.1-808558, built on 08/27/2009 18:48 GMT
Clients:
 /10.81.12.152:58110[1](queued=0,recved=10374,sent=0)
 /10.81.13.173:35677[1](queued=0,recved=0,sent=0) # it is caused by the nc process

Latency min/avg/max: 0/0/37
Received: 37128
Sent: 0
Outstanding: 0
Zxid: 0x1e000001f5
Mode: follower
Node count: 26

$ echo stat | nc 10.81.12.145 2181
Zookeeper version: 3.2.1-808558, built on 08/27/2009 18:48 GMT
Clients:
 /10.81.12.153:19130[1](queued=0,recved=10624,sent=0)
 /10.81.13.173:35678[1](queued=0,recved=0,sent=0) # it is caused by the nc process

Latency min/avg/max: 0/2/213
Received: 26700
Sent: 0
Outstanding: 0
Zxid: 0x1e000001f5
Mode: leader
Node count: 26

The three 'stat' commands show different Node count! ",[],Bug,ZOOKEEPER-628,Major,Qian Ye,Cannot Reproduce,2011-09-06T15:58:02.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,the ephemeral node wouldn't disapper due to session close error,2011-09-06T15:58:02.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",1.0
Henry Robinson,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2009-12-16T00:45:07.000+0000,Henry Robinson,"Reported on the list:

""
I'm working on using ZooKeeper for an internal application at Digg.  I've been using the zkpython package and I just noticed that the data I was receiving from a zookeeper.get() call was being truncated.  After some quick digging I found that zookeeper.c limits the data returned to 512 characters (see http://svn.apache.org/viewvc/hadoop/zookeeper/tags/release-3.2.2/src/contrib/zkpython/src/c/zookeeper.c?view=markup line 855).

Is there a reason for this?  The only information regarding node size that I've read is that it should not exceed 1MB so this limit seems a bit arbitrary and restrictive.

Thanks for the great work!

Rich""","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-627,Major,Henry Robinson,Fixed,2009-12-16T23:28:40.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkpython arbitrarily restricts the size of a 'get' to 512 bytes,2010-03-26T17:25:09.000+0000,[],3.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>]",2009-12-15T17:54:33.000+0000,Patrick D. Hunt,"Java/c clients should output xid/sessionids (incl ephemeralowner) in hex format

    private static void printStat(Stat stat) {
        System.err.println(""cZxid = "" + stat.getCzxid());
        System.err.println(""ctime = "" + new Date(stat.getCtime()).toString());
        System.err.println(""mZxid = "" + stat.getMzxid());
        System.err.println(""mtime = "" + new Date(stat.getMtime()).toString());
        System.err.println(""pZxid = "" + stat.getPzxid());
        System.err.println(""cversion = "" + stat.getCversion());
        System.err.println(""dataVersion = "" + stat.getVersion());
        System.err.println(""aclVersion = "" + stat.getAversion());
        System.err.println(""ephemeralOwner = "" + stat.getEphemeralOwner());
        System.err.println(""dataLength = "" + stat.getDataLength());
        System.err.println(""numChildren = "" + stat.getNumChildren());
    }
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-626,Major,Patrick D. Hunt,Fixed,2010-01-22T00:49:36.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ensure the c/java cli's print xid/sessionid/etc... in hex,2010-03-26T17:25:09.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",2.0
Mahadev Konar,"[<JIRA Component: name='c client', id='12312380'>]",2009-12-15T03:17:08.000+0000,Qian Ye,"I encountered a problem today that the Zookeeper C Client (version 3.2.0) core dump when reconnected and did some operations on the zookeeper server which just restarted. The gdb infomation is like:

(gdb) bt
#0  0x000000302af71900 in memcpy () from /lib64/tls/libc.so.6
#1  0x000000000047bfe4 in ia_deserialize_string (ia=Variable ""ia"" is not available.) at src/recordio.c:270
#2  0x000000000047ed20 in deserialize_CreateResponse (in=0x9cd870, tag=0x50a74e ""reply"", v=0x409ffe70) at generated/zookeeper.jute.c:679
#3  0x000000000047a1d0 in zookeeper_process (zh=0x9c8c70, events=Variable ""events"" is not available.) at src/zookeeper.c:1895
#4  0x00000000004815e6 in do_io (v=Variable ""v"" is not available.) at src/mt_adaptor.c:310
#5  0x000000302b80610a in start_thread () from /lib64/tls/libpthread.so.0
#6  0x000000302afc6003 in clone () from /lib64/tls/libc.so.6
#7  0x0000000000000000 in ?? ()
(gdb) f 1
#1  0x000000000047bfe4 in ia_deserialize_string (ia=Variable ""ia"" is not available.) at src/recordio.c:270
270     in src/recordio.c
(gdb) info locals
priv = (struct buff_struct *) 0x9cd8d0
len = -1
rc = Variable ""rc"" is not available.

According to the source code,
int ia_deserialize_string(struct iarchive *ia, const char *name, char **s)
{
    struct buff_struct *priv = ia->priv;
    int32_t len;
    int rc = ia_deserialize_int(ia, ""len"", &len);
    if (rc < 0)
        return rc;
    if ((priv->len - priv->off) < len) {
        return -E2BIG;
    }
    *s = malloc(len+1);
    if (!*s) {
        return -ENOMEM;
    }
    memcpy(*s, priv->buffer+priv->off, len);
    (*s)[len] = '\0';
    priv->off += len;
    return 0;
}

the variable len is set by ia_deserialize_int, and the returned len doesn't been checked, so the client segment fault when trying to memcpy -1 byte data.
In the source file recordio.c, there are many functions which don't check the returned len. They all might cause segment fault in some kind of  situations.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-624,Major,Qian Ye,Fixed,2010-03-17T00:05:38.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,The C Client cause core dump when receive error data from Zookeeper Server,2010-03-26T17:25:08.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",3.0
Flavio Paiva Junqueira,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2009-12-14T08:49:23.000+0000,Flavio Paiva Junqueira,"Class org.apache.bookkeeper.util.ClientBase requires junit, and when I tried to just compile bookkeeper, no test, with the patch of ZOOKEEPER-534, compilation failed. ","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-623,Major,Flavio Paiva Junqueira,Fixed,2010-01-12T20:12:51.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ClientBase in bookkeeper.util requires junit,2013-05-02T02:29:22.000+0000,[],2.0
Benjamin Reed,"[<JIRA Component: name='c client', id='12312380'>]",2009-12-12T22:22:59.000+0000,Steven Cheng,"Valgrind found:

{quote}
==2357== Conditional jump or move depends on uninitialised value(s)
==2357==    at 0x807FDCA: check_events (zookeeper.c:1180)
==2357==    by 0x808043A: zookeeper_process (zookeeper.c:1775)
==2357==    by 0x806A21B: Zookeeper_close::testCloseConnected1() (TestZookeeperClose.cc:161)
==2357==    by 0x806C6BF: CppUnit::TestCaller<Zookeeper_close>::runTest() (TestCaller.h:166)
{quote}

zookeeper.c:1180 was the first if in send_set_watches.

","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-622,Major,Steven Cheng,Fixed,2010-03-06T05:35:08.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Test for pending watches in send_set_watches should be moved,2010-03-26T17:25:08.000+0000,[],2.0
Giridharan Kesavan,"[<JIRA Component: name='build', id='12312383'>]",2009-12-12T21:36:22.000+0000,Patrick D. Hunt,"Giri can you take a look? Any recent changes on hudson that could have caused this?

http://hudson.zones.apache.org/hudson/job/ZooKeeper-trunk/621/

jute:
    [javac] Compiling 38 source files to /grid/0/hudson/hudson-slave/workspace/ZooKeeper-trunk/trunk/build/classes
   [clover] Clover Version 2.4.3, built on March 09 2009 (build-756)
   [clover] Loaded from: /homes/hudson/tools/clover/latest/lib/clover.jar
   [clover] Clover: Open Source License registered to Apache.
   [clover] Failed to create temp directory
   [clover] ** Error(s) occurred and the instrumentation process can't continue.

BUILD FAILED
/grid/0/hudson/hudson-slave/workspace/ZooKeeper-trunk/trunk/build.xml:879: The following error occurred while executing this line:
/grid/0/hudson/hudson-slave/workspace/ZooKeeper-trunk/trunk/build.xml:199: com.cenqua.clover.CloverException: Failed to create temp directory
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-621,Critical,Patrick D. Hunt,Fixed,2009-12-14T10:42:05.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,hudson failure ZooKeeper-trunk/621 - clover issue,2010-03-26T17:25:08.000+0000,[],2.0
Giridharan Kesavan,"[<JIRA Component: name='tests', id='12312427'>]",2009-12-10T21:34:02.000+0000,Patrick D. Hunt,"http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/590/warningsResult/HIGH/

If you click on any of these links you will see that these are not compiler warnings:
http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/590/warningsResult/HIGH/file.-1602148846/

Giri can you take a look at these and resolve?","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-620,Major,Patrick D. Hunt,Fixed,2010-01-19T10:33:58.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,hudson is not reporting compiler warning correctly,2010-03-26T17:25:08.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",2.0
Patrick D. Hunt,[],2009-12-10T21:31:25.000+0000,Patrick D. Hunt,"Hudson is not reporting this correctly but:
http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/612/

    [junit] 2009-12-03 08:46:27,297 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11301:LeaderElection@109] - 5	-> 2
    [junit] Running org.apache.zookeeper.test.QuorumTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.zookeeper.test.QuorumTest FAILED (timeout)
    [junit] 2009-12-03 08:46:28,390 - INFO  [main:PortAssignment@31] - assigning port 11221
    [junit] 2009-12-03 08:46:28,393 - INFO  [main:PortAssignment@31] - assigning port 11222
    [junit] Running org.apache.zookeeper.test.QuorumZxidSyncTest

but this makes no sense - how is this a timeout?

One concern is - perhaps this is a deadlock?","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-619,Critical,Patrick D. Hunt,Invalid,2009-12-11T19:09:27.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,hudson test failure in QuorumTest -- timeout error,2010-03-26T17:25:08.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",1.0
Giridharan Kesavan,"[<JIRA Component: name='tests', id='12312427'>]",2009-12-10T21:29:07.000+0000,Patrick D. Hunt,"See
http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/612/
this test failed however there is no indication in the hudson page why it failed.

Looking at the raw console output I see

    [junit] Running org.apache.zookeeper.test.QuorumTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.zookeeper.test.QuorumTest FAILED (timeout)

So it seems there is an error in hudson reporting.

Giri can you look into this?
",[],Bug,ZOOKEEPER-618,Critical,Patrick D. Hunt,Invalid,2009-12-15T16:06:28.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,hudson not reporting failures correctly,2009-12-15T16:06:28.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",0.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2009-12-10T19:11:11.000+0000,Patrick D. Hunt,"http://hadoop.apache.org/zookeeper/docs/current/zookeeperAdmin.html#sc_zkMulitServerSetup

1) the config file is missing line returns
2) call out setting up the myid file as it's own bullet, otw it's too easy to miss
3) we should make sure the values we use in examples are consistent, and resonable defaults","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-617,Major,Patrick D. Hunt,Fixed,2010-03-12T18:27:47.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,improve cluster setup documentation in forrest,2010-03-26T17:25:08.000+0000,[],2.0
Mahadev Konar,[],2009-12-09T22:17:48.000+0000,Mahadev Konar,"The javadoc for create with a sequence flag mentions a suffix of ""\_i"" but the true suffix is just ""i"" and no ""\_"".
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-615,Major,Mahadev Konar,Fixed,2009-12-11T19:39:02.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,wrong javadoc for create with a sequence flag,2010-03-26T17:25:08.000+0000,[],2.0
Henry Robinson,"[<JIRA Component: name='server', id='12312382'>]",2009-12-07T23:31:18.000+0000,Henry Robinson,getClientCnxnCount reads from Factory.ipMap without synchronizing. ,"[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-614,Major,Henry Robinson,Fixed,2009-12-11T23:04:14.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Improper synchronisation in getClientCnxnCount,2010-03-26T17:25:08.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.2.2', id='12314335'>]",3.0
Mahadev Konar,[],2009-12-07T01:47:46.000+0000,Mahadev Konar,"The hudson build failure failed again on 

http://hudson.zones.apache.org/hudson/job/ZooKeeper-trunk/582/
","[<JIRA Version: name='3.1.2', id='12314394'>, <JIRA Version: name='3.2.2', id='12314335'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-611,Major,Mahadev Konar,Fixed,2009-12-08T20:21:07.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,hudson build failiure,2010-03-26T17:25:08.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>, <JIRA Version: name='3.2.1', id='12314068'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2009-12-04T06:50:58.000+0000,Patrick D. Hunt,There are a number of places where we have non-final fields that could (should) be declared as final.,"[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-610,Minor,Patrick D. Hunt,Fixed,2009-12-11T22:57:06.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"cleanup final fields, esp those used for locking",2010-03-26T17:25:08.000+0000,[],2.0
Henry Robinson,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2009-12-03T19:32:32.000+0000,Patrick D. Hunt,"ObserverTest failed running on 8core

I ran the test as:

ant -Dtest.junit.output.format=xml -Dtest.output -Dtestcase=AsyncHammerTest clean test-core-java &> test.out","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-609,Blocker,Patrick D. Hunt,Fixed,2009-12-16T18:29:54.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"ObserverTest failure ""zk should not be connected expected not same""",2010-03-26T17:25:08.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='scripts', id='12312384'>]",2009-12-02T01:10:23.000+0000,Patrick D. Hunt,the scripts in bin fail under cygwin due to spaces not handled properly,"[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-606,Major,Patrick D. Hunt,Fixed,2009-12-17T22:09:57.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,bin scripts don't work in cygwin (spaces in paths),2010-03-26T17:25:07.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",2.0
Mahadev Konar,"[<JIRA Component: name='c client', id='12312380'>]",2009-12-01T21:01:58.000+0000,Alex Newman,"Currently the zookeeper seems to be exporting symbols not in the api. An example of this seems to be the symbol hash, which interferes with me using memcached and zookeeper in the same program.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-604,Critical,Alex Newman,Fixed,2010-03-12T21:44:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zk needs to prevent export of any symbol not listed in their api,2010-09-03T17:21:25.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>, <JIRA Version: name='3.1.1', id='12313649'>, <JIRA Version: name='3.1.2', id='12314394'>, <JIRA Version: name='3.2.0', id='12313491'>, <JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.2.2', id='12314335'>, <JIRA Version: name='3.3.0', id='12313976'>, <JIRA Version: name='4.0.0', id='12313382'>]",4.0
Lei Zhang,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2009-12-01T19:27:49.000+0000,Henry Robinson,"The general pattern is that the construction of a collection might fail, but the module is not freeing the memory that it has already allocated. Exceptions that are raised during this process aren't always propagated back to the Python side either. 

","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-603,Major,Henry Robinson,Duplicate,2010-08-12T22:29:15.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkpython should do a better job of freeing memory under error conditions,2011-11-23T19:22:16.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",1.0
Rakesh Radhakrishnan,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2009-12-01T19:25:06.000+0000,Patrick D. Hunt,the java code should add a ThreadGroup exception handler that logs at ERROR level any uncaught exceptions thrown by Thread run methods.,"[<JIRA Version: name='3.4.7', id='12325149'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-602,Blocker,Patrick D. Hunt,Fixed,2015-06-18T19:00:12.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,log all exceptions not caught by ZK threads,2015-06-22T14:11:29.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",17.0
Gustavo Niemeyer,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2009-11-30T22:22:37.000+0000,Gustavo Niemeyer,"I suppose the TODO below is referring to the ""path"" variable which is passed in as an output variable to PyArg_ParseTuple right below.  The TODO may be removed, since the code is right.  Code using PyArg_ParseTuple will borrow the reference from the calling code, since there's a stack behind the call to the enclosing function (pyzoo_get_children in this case) which won't go away until the function returns.


Index: src/contrib/zkpython/src/c/zookeeper.c
===================================================================
--- src/contrib/zkpython/src/c/zookeeper.c	(revision 885582)
+++ src/contrib/zkpython/src/c/zookeeper.c	(working copy)
@@ -774,8 +774,6 @@
 
 static PyObject *pyzoo_get_children(PyObject *self, PyObject *args)
 {
-  // TO DO: Does Python copy the string or the reference? If it's the former
-  // we should free the String_vector
   int zkhid;
   char *path;
   PyObject *watcherfn = Py_None;
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-600,Trivial,Gustavo Niemeyer,Fixed,2009-12-17T23:39:01.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,TODO pondering about allocation behavior in zkpython may be removed,2010-03-26T17:25:07.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",3.0
Henry Robinson,"[<JIRA Component: name='quorum', id='12312379'>]",2009-11-25T19:53:54.000+0000,Henry Robinson,Typo in thread constructor. Oops. ,"[<JIRA Version: name='3.2.2', id='12314335'>]",Bug,ZOOKEEPER-598,Trivial,Henry Robinson,Fixed,2009-11-30T12:04:55.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,LearnerHandler is misspelt in the thread's constructor,2010-03-26T17:28:56.000+0000,"[<JIRA Version: name='3.2.2', id='12314335'>]",3.0
Benjamin Reed,"[<JIRA Component: name='tests', id='12312427'>]",2009-11-25T18:18:59.000+0000,Patrick D. Hunt,"ASyncHammerTest is failing intermittently on hudson trunk. There is no clear reason why this is happening, but
it seems from the logs that a session connection to a follower is failing during session establishment - the
failure seems to be a problem either on the follower or leader. The server gets the session create request, but
it stalls in the request processor pipeline. (we see it go in, but we do not see it com eout)

unfortunately all efforts to reproduce this on non-hudson trunk have failed. Even trying to reproduce by
running on hudson host itself (manually) has failed.

We need to instrument the client session creation code in the test to dump the thread stack if the
session creation fails.
","[<JIRA Version: name='3.1.2', id='12314394'>, <JIRA Version: name='3.2.2', id='12314335'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-597,Critical,Patrick D. Hunt,Fixed,2009-12-08T19:15:29.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ASyncHammerTest is failing intermittently on hudson trunk,2010-03-26T17:25:07.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>, <JIRA Version: name='3.2.1', id='12314068'>]",2.0
Mahadev Konar,[],2009-11-24T22:51:29.000+0000,Mahadev Konar,"It is possible that the last loggged zxid as reported by all the servers during leader election is not the last zxid that the server can upload data to. It is very much possible that some transaction or snapshot gets corrupted and the servers actually do not have valid data till last logged zxid. We need to make sure that what the servers report as there last logged zxid, they are able to load data till that zxid.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-596,Major,Mahadev Konar,Fixed,2009-12-18T02:20:28.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,The last logged zxid calculated by zookeeper servers could cause problems in leader election if data gets corrupted.,2013-05-02T02:29:23.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='java client', id='12312381'>]",2009-11-24T16:54:45.000+0000,Patrick D. Hunt,"The java client api does not allow the client to access the negotiated session timeout (c does allow this).

In some cases the client may not get the requested timeout (server applies a min/max bound) in which case
the client user code may want to examine the timeout it did receive.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-593,Major,Patrick D. Hunt,Fixed,2010-01-26T02:19:37.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,java client api does not allow client to access negotiated session timeout,2010-03-26T17:25:04.000+0000,[],2.0
,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2009-11-24T09:48:39.000+0000,Flavio Paiva Junqueira,"When trying to compile bookkeeper, the compiler complains that it can't find junit. I suspect that this is related to the fact that zookeeper now fetches junit using ivy.",[],Bug,ZOOKEEPER-592,Major,Flavio Paiva Junqueira,Fixed,2010-02-02T14:47:42.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,BookKeeper cannot find junit,2010-03-26T17:31:34.000+0000,[],1.0
Mahadev Konar,"[<JIRA Component: name='c client', id='12312380'>]",2009-11-23T09:35:37.000+0000,Qian Ye,"The following code produce a situation, where the C Client can not exit properly,

#include ""include/zookeeper.h""

void default_zoo_watcher(zhandle_t *zzh, int type, int state, const char *path, void* context){
    int zrc = 0;
    struct String_vector str_vec = {0, NULL};
    printf(""in the default_zoo_watcher\n"");
    zrc = zoo_wget_children(zzh, ""/mytest"", default_zoo_watcher, NULL, &str_vec);
    printf(""zoo_wget_children, error: %d\n"", zrc);

    return;
}

int main()
{
    int zrc = 0;
    int buff_len = 10; 
    char buff[10] = ""hello"";
    char path[512];
    struct Stat stat;
    struct String_vector str_vec = {0, NULL};

    zhandle_t *zh = zookeeper_init(""10.81.20.62:2181"", NULL, 30000, 0, 0, 0); 
    zrc = zoo_create(zh, ""/mytest"", buff, 10, &ZOO_OPEN_ACL_UNSAFE, 0, path, 512);
    printf(""zoo_create, error: %d\n"", zrc);

    zrc = zoo_wget_children(zh, ""/mytest"", default_zoo_watcher, NULL, &str_vec);
    printf(""zoo_wget_children, error: %d\n"", zrc);

    zrc = zoo_create(zh, ""/mytest/test1"", buff, 10, &ZOO_OPEN_ACL_UNSAFE, 0, path, 512);
    printf(""zoo_create, error: %d\n"", zrc);

    zrc = zoo_wget_children(zh, ""/mytest"", default_zoo_watcher, NULL, &str_vec);
    printf(""zoo_wget_children, error: %d\n"", zrc);

    zrc = zoo_delete(zh, ""/mytest/test1"", -1);

    printf(""zoo_delete, error: %d\n"", zrc);
    zookeeper_close(zh);
    return 0;
}


running this code can cause the program hang at zookeeper_close(zh);(line 38). using gdb to attach the process, I found that the main thread is waiting for do_completion thread to finish,
(gdb) bt
#0  0x000000302b806ffb in pthread_join () from /lib64/tls/libpthread.so.0
#1  0x000000000040de3b in adaptor_finish (zh=0x515b60) at src/mt_adaptor.c:219
#2  0x00000000004060ba in zookeeper_close (zh=0x515b60) at src/zookeeper.c:2100
#3  0x000000000040220b in main ()

and the thread which handle the zoo_wget_children(in the default_zoo_watcher) is waiting for sc->cond. 
(gdb) thread 2
[Switching to thread 2 (Thread 1094719840 (LWP 25093))]#0  0x000000302b8089aa in pthread_cond_wait@@GLIBC_2.3.2 ()
   from /lib64/tls/libpthread.so.0
(gdb) bt
#0  0x000000302b8089aa in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/tls/libpthread.so.0
#1  0x000000000040d88b in wait_sync_completion (sc=0x5167f0) at src/mt_adaptor.c:82
#2  0x00000000004082c9 in zoo_wget_children (zh=0x515b60, path=0x40ebc0 ""/mytest"", watcher=0x401fd8 <default_zoo_watcher>, watcherCtx=Variable ""watcherCtx"" is not available.)
    at src/zookeeper.c:2884
#3  0x0000000000402037 in default_zoo_watcher ()
#4  0x000000000040d664 in deliverWatchers (zh=0x515b60, type=4, state=3, path=0x515100 ""/mytest"", list=0x5177d8) at src/zk_hashtable.c:274
#5  0x0000000000403861 in process_completions (zh=0x515b60) at src/zookeeper.c:1631
#6  0x000000000040e1b5 in do_completion (v=Variable ""v"" is not available.) at src/mt_adaptor.c:333
#7  0x000000302b80610a in start_thread () from /lib64/tls/libpthread.so.0
#8  0x000000302afc6003 in clone () from /lib64/tls/libc.so.6
#9  0x0000000000000000 in ?? ()

here, a deadlock presents.

","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-591,Blocker,Qian Ye,Fixed,2010-03-18T01:45:01.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,The C Client cannot exit properly in some situation,2010-03-26T17:25:04.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2009-11-23T07:01:43.000+0000,Patrick D. Hunt,"when the server is logging session related log messages it must include the session id in hex form

this greatly simplifies debugging - being able to relate a session message back to a particular session. otw there's
too much going on and there is no way to determine what messages are related to a particular session",[],Bug,ZOOKEEPER-590,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,review logging to ensure that session related messages include session id,2022-02-03T08:50:20.000+0000,[],1.0
Benjamin Reed,"[<JIRA Component: name='c client', id='12312380'>]",2009-11-23T02:47:54.000+0000,Qian Ye,"In the comments of client C API which associated with creating znode, eg. zoo_acreate, it is said that the initial ACL of the node ""if null, the ACL of the parent will be used"". However, the it doesn't work. When execute this kind of request at the server side, it raises InvalidACLException. The source code show that, the function fixupACL return false when it get a null ACL. ","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-589,Major,Qian Ye,Fixed,2010-02-08T22:38:36.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"When create a znode, a NULL ACL parameter cannot be accepted",2010-03-26T17:25:04.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2009-11-21T23:23:59.000+0000,Patrick D. Hunt,"Why are we logging this? It's unnecessary and just annoying afaict. We should remove it entirely.

2009-11-18 05:37:29,312 WARN org.apache.zookeeper.server.Request: Ignoring exception during toString
java.nio.BufferUnderflowException
	at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:127)
	at java.nio.ByteBuffer.get(ByteBuffer.java:675)
	at org.apache.zookeeper.server.Request.toString(Request.java:199)
	at java.lang.String.valueOf(String.java:2827)
	at java.lang.StringBuilder.append(StringBuilder.java:115)
	at org.apache.zookeeper.server.quorum.CommitProcessor.processRequest(CommitProcessor.java:167)
	at org.apache.zookeeper.server.quorum.FollowerRequestProcessor.run(FollowerRequestProcessor.java:68)
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-588,Minor,Patrick D. Hunt,Fixed,2009-12-11T20:24:51.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,remove unnecessary/annoying log of tostring error in Request.toString(),2010-05-07T17:39:56.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>]",2009-11-21T23:21:29.000+0000,Patrick D. Hunt,The ZK client should log the timeout negotiated with the server if the time is different than the timeout parameter specified by the client.,"[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-587,Major,Patrick D. Hunt,Fixed,2009-12-11T21:46:49.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,client should log timeout negotiated with server,2010-05-07T17:40:07.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>]",2009-11-21T01:18:13.000+0000,Patrick D. Hunt,the c client fails to compile under cygwin,"[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-586,Major,Patrick D. Hunt,Fixed,2010-03-16T23:44:10.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,c client does not compile under cygwin,2010-03-26T17:25:04.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",2.0
,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>]",2009-11-17T22:47:32.000+0000,Patrick D. Hunt,"Both the c and java clients attempt to connect to a server in the cluster by iterating through
a randomized list of servers as listed in the connect string passed to the zookeeper_init (c)
or ZooKeeper constructor (java). The clients do this indefinitely, until successfully connecting
to a server or until the client is close()ed. Additionally if a client is disconnected from a server
it will attempt to reconnect to another server in the cluster, in this case it will only connect
to a server that has the same, or higher, zxid as seen by the client on the previous server that
it was connected to (this ensures that the client never sees old data).

In some weird cases (in particular where operators reset the server database, clearing out the
existing snapshots and txnlogs) existing clients will now see a much lower zxid (due to the
epoch number being reset) regardless of the server that the client attempts to connect to. In this
case the current client will iterate essentially forever.

Instead the client should throw session expired in this case (notify any watchers). After iterating
through all of the servers in the list, if none of the servers have an acceptable zxid the client
should expire the session and shut down the handle. This will ensure that the client will eventually
shutdown in this unusual, but possible (esp with server operators who don't also control the
clients) situation.
",[],Bug,ZOOKEEPER-583,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,on resync client should generate session expired exception if there is no server in cluster with acceptable zxid,2022-02-03T08:50:27.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>, <JIRA Version: name='3.2.1', id='12314068'>]",2.0
Mahadev Konar,"[<JIRA Component: name='server', id='12312382'>]",2009-11-17T20:56:40.000+0000,Benjamin Reed,"when zookeeper starts up it will restore the most recent state (latest zxid) it finds in the data directory. unfortunately, in the quorum version of zookeeper updates are logged using an epoch based on the latest log file in a directory. if there is a snapshot with a higher epoch than the log files, the zookeeper server will start logging using an epoch one higher than the highest log file.

so if a data directory has a snapshot with an epoch of 27 and there are no log files, zookeeper will start logging changes using epoch 1. if the cluster restarts the state will be restored from the snapshot with the epoch of 27, which in effect, restores old data.

normal operation of zookeeper will never result in this situation.

this does not effect standalone zookeeper.

a fix should make sure to use an epoch one higher than the current state, whether it comes from the snapshot or log, and should include a sanity check to make sure that a follower never connects to a leader that has a lower epoch than its own.","[<JIRA Version: name='3.1.2', id='12314394'>, <JIRA Version: name='3.2.2', id='12314335'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-582,Blocker,Benjamin Reed,Fixed,2009-11-20T22:30:51.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeper can revert to old data when a snapshot is created outside of normal processing,2010-03-26T17:25:03.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>, <JIRA Version: name='3.2.1', id='12314068'>]",3.0
Henry Robinson,"[<JIRA Component: name='server', id='12312382'>]",2009-11-11T18:30:49.000+0000,Henry Robinson,"See e.g. lookForLeader in LeaderElection.java and termPredicate in AuthFastLeaderElection.java

Should use containsQuorum. ",[],Bug,ZOOKEEPER-577,Major,Henry Robinson,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,LeaderElection code hardcodes majority quorums in at least two places,2013-05-02T02:29:21.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",0.0
Benjamin Reed,[],2009-11-11T18:08:30.000+0000,Mahadev Konar,the handling and implications of session moved exception should be documented.,"[<JIRA Version: name='3.2.2', id='12314335'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-576,Major,Mahadev Konar,Fixed,2009-11-20T19:47:49.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,docs need to be updated for session moved exception and how to handle it,2010-03-26T17:25:03.000+0000,[],2.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2009-11-11T00:43:34.000+0000,Patrick D. Hunt,"I believe it's 100k, not 10k

-----------------------
snapCount
(Java system property: zookeeper.snapCount)

Clients can submit requests faster than ZooKeeper can process them, especially if there are a lot of clients. To prevent ZooKeeper from running out of memory due to queued requests, ZooKeeper will throttle clients so that there is no more than globalOutstandingLimit outstanding requests in the system. The default limit is 1,000.ZooKeeper logs transactions to a transaction log. After snapCount transactions are written to a log file a snapshot is started and a new transaction log file is started. The default snapCount is 10,000.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-574,Minor,Patrick D. Hunt,Fixed,2010-01-23T00:31:18.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,the documentation on snapcount in the admin guide has the wrong default,2010-03-26T17:25:03.000+0000,[],2.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2009-11-10T01:05:59.000+0000,Patrick D. Hunt,,"[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-573,Major,Patrick D. Hunt,Fixed,2010-01-20T20:17:04.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,the dump 4letterword is not formatting sessionids in hex,2010-03-26T17:25:03.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='tests', id='12312427'>]",2009-11-08T06:37:46.000+0000,Patrick D. Hunt,"the asynchammertest is not validating the rc in the callback, more serious is that it is using path in the create callback
to delete the node, rather than name (which is important in the case of a sequential node creation as in this case)","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-570,Critical,Patrick D. Hunt,Fixed,2009-11-08T22:46:18.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"AsyncHammerTest is broken, callbacks need to validate rc parameter",2010-03-26T17:25:03.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",2.0
Henry Robinson,[],2009-11-06T19:47:54.000+0000,Henry Robinson,"It is possible for basic LeaderElection to enter a situation where it never terminates. 

As an example, consider a three node cluster A, B and C.

1. In the first round, A votes for A, B votes for B and C votes for C
2. Since C > B > A, all nodes resolve to vote for C in the second round as there is no first round winner
3. A, B vote for C, but C fails.
4. C is not elected because neither A nor B hear from it, and so votes for it are discarded
5. A and B never reset their votes, despite not hearing from C, so continue to vote for it ad infinitum. 

Step 5 is the bug. If A and B reset their votes to themselves in the case where the heard-from vote set is empty, leader election will continue.

I do not know if this affects running ZK clusters, as it is possible that the out-of-band failure detection protocols may cause leader election to be restarted anyhow, but I've certainly seen this in tests. 

I have a trivial patch which fixes it, but it needs a test (and tests for race conditions are hard to write!)","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-569,Major,Henry Robinson,Fixed,2010-02-20T14:27:43.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Failure of elected leader can lead to never-ending leader election,2010-03-26T17:25:03.000+0000,[],3.0
Patrick D. Hunt,[],2009-11-03T01:38:35.000+0000,Patrick D. Hunt,"Noticed the following issues in SyncRequestProcessor

1) logCount is incremented even for non-log events (say getData)
txnlog should return indication if request was logged or not (if hdr ==null it returns)

also:

2) move r.nextInt below logCount++ (ie if an actual log event)
3) fix indentation after txnlog.append (for some reason has unnecessary 4 char indent)
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-568,Major,Patrick D. Hunt,Fixed,2009-11-07T08:08:51.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,SyncRequestProcessor snapping too frequently - counts non-log events as log events,2019-03-15T21:40:47.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>]",2009-11-01T04:41:05.000+0000,Patrick D. Hunt,"the javadoc/cdoc for getchildren2 needs to mention that the methods are ""new in 3.3.0""
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-567,Major,Patrick D. Hunt,Fixed,2009-11-12T20:03:05.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"javadoc for getchildren2 needs to mention ""new in 3.3.0""",2010-03-26T17:25:03.000+0000,[],2.0
Patrick D. Hunt,[],2009-10-29T05:27:26.000+0000,Patrick D. Hunt,"the four letter word ""reqs"" doesn't do anything - it always returns empty data. Seems that ""outstanding"" field is always empty and never set.

we should remove outstanding and also update the reqs code to correctly output the outstanding requests (if not possible then remove the cmd and update docs - although this is very useful command, hate to see us lose it)","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-566,Critical,Patrick D. Hunt,Fixed,2009-11-12T19:54:26.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"""reqs"" four letter word (command port) returns no information",2010-03-26T17:25:03.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",2.0
Mahadev Konar,"[<JIRA Component: name='build', id='12312383'>]",2009-10-28T17:09:27.000+0000,Mahadev Konar,"With ZOOKEEPER-529 checked in, ant test for recipes broke. Its a minor change to the build for including librariries from the new location where jars are downloaded by ivy.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-563,Major,Mahadev Konar,Fixed,2009-10-29T04:02:41.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ant test for recipes is broken.,2010-03-26T17:25:03.000+0000,[],2.0
Benjamin Reed,"[<JIRA Component: name='c client', id='12312380'>]",2009-10-26T20:45:28.000+0000,Patrick D. Hunt,"The c client can flood the server with pings if the tcp queue is filled.

Say the cluster is overloaded and shuts down the recv processing

a c client can send a ping, but since last_send is only updated on successful pushing of data into the 
socket, if flush_send_queue fails to send any data (send_buffer returns 0) then last_send is not updated
and zookeeper_interest will again send a ping the next time it is woken - which could be 0 if recv_to is close
to 0, easily could happen if server is not sending data to the client.
","[<JIRA Version: name='3.1.2', id='12314394'>, <JIRA Version: name='3.2.2', id='12314335'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-562,Blocker,Patrick D. Hunt,Fixed,2009-10-29T21:35:25.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,c client can flood server with pings if tcp send queue filled,2010-03-26T17:25:03.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",2.0
Lei Zhang,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2009-10-26T17:11:27.000+0000,Patrick D. Hunt,"I'm seeing some weird behavior running zk-latencies.py
http://github.com/phunt/zk-smoketest

don't know if it's related to zkbindings itself, but I ran valgrind to see if it noticed any issues. see attached.

afaict these issues are related to zkpython binding, however I'm not sure. I did run valgrind against the
zookeeper c library tests and these issues were not highlighted. So I'm thinking this is zkpython errors, however
I'm not 100% sure. 

Henry can you take a look?
 ",[],Bug,ZOOKEEPER-559,Major,Patrick D. Hunt,Duplicate,2013-10-10T17:32:11.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,valgrind warnings running zkpython bindings,2013-10-10T17:32:18.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2009-10-25T00:08:18.000+0000,Patrick D. Hunt,"the server and connection ""sent"" stat is not being updated. if you run ""stat"" on the client port the sent packets is much lower than it should be

seems that sendbuffer is not updating the stats when it shortcircuits the send.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-558,Critical,Patrick D. Hunt,Fixed,2009-11-07T00:48:26.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"server ""sent"" stats not being updated",2010-03-26T17:25:02.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",2.0
,[],2009-10-24T21:03:25.000+0000,Cyril Lakech,"There are many cool release of hadoop zookeeper and this project is an apache project, as the maven project.
But the released jars must be download manually and then deploy to a private repository before they can be used by developer using maven2.

Please could you upload the zookeeper  jars on the public maven2 repository ?

Of course, we can help to deploy those artifact if necessary.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-557,Major,Cyril Lakech,Duplicate,2009-11-06T21:24:26.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Upload Zookeeper jars to a public maven repository,2010-03-26T17:25:02.000+0000,[],3.0
Henry Robinson,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2009-10-14T22:55:06.000+0000,Henry Robinson,C client returns NULL for stat object for deleted nodes. zookeeper.c blindly dereferences it. Segfault. ,"[<JIRA Version: name='3.2.2', id='12314335'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-554,Major,Henry Robinson,Fixed,2009-10-19T19:43:10.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkpython can segfault when statting a deleted node,2010-03-26T17:25:02.000+0000,[],2.0
,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='contrib-bindings', id='12312860'>]",2009-10-14T00:47:18.000+0000,Steven K. Wong,"Using zkpython with ZK 3.2.1 release:

  import zookeeper as z
  zh = z.init(...)
  z.state(zh)  # returns 3 == z.CONNECTED_STATE
  # kill standalone ZK server
  z.state(zh)  # returns 0 == ???

The problem is that 0 is not a state defined by the C client's zookeeper.[ch]. Perhaps 0 should've been defined as something like DISCONNECTED_STATE? (Java's KeeperState.Disconnected is 0, if that matters.)

If the fix is to define 0 as a new state, changes will be needed to both the C client and zkpython. Not sure about other bindings.
",[],Bug,ZOOKEEPER-553,Minor,Steven K. Wong,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,C client's zoo_state function returns unknown state 0,2022-02-03T08:50:24.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",0.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>]",2009-10-13T04:10:33.000+0000,Patrick D. Hunt,"The java client is sending a SetWatches message even on a new session (always empty). Additionally SetWatches is called
even in the case of re-establishing session, however no watches are set. The code should check for watches before
sending this (ie don't send empty setwatches). I see this on java, investigate c as well.
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-551,Minor,Patrick D. Hunt,Fixed,2009-11-08T21:42:07.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,unnecessary SetWatches message on new session,2010-03-26T17:25:02.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2009-10-08T22:59:10.000+0000,Patrick D. Hunt,"ZooKeeperException is not being added to the zookeeper module in zookeeper.c (zkpython). The other exceptions
are added but not ZooKeeperException. Sorry, I missed this in my previous change, I got all the subclasses but not zkex itself.
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-548,Critical,Patrick D. Hunt,Invalid,2009-10-09T04:35:00.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper.ZooKeeperException not added to the module in zkpython,2010-03-26T17:25:02.000+0000,[],1.0
Mahadev Konar,"[<JIRA Component: name='leaderElection', id='12312378'>, <JIRA Component: name='server', id='12312382'>]",2009-10-08T22:01:41.000+0000,Mahadev Konar,We need to put some sanity checks in QuorumCnxnManager and the other quorum port for rogue clients. Sometimes a clients might get misconfigured and they might send random characters on such ports. We need to make sure that such rogue clients do not bring down the clients and need to put in some sanity checks with respect to packet lengths and deserialization.,"[<JIRA Version: name='3.2.2', id='12314335'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-547,Major,Mahadev Konar,Fixed,2009-11-17T23:39:20.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Sanity check in QuorumCnxn Manager and quorum communication port.,2010-03-26T17:25:02.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>, <JIRA Version: name='3.2.1', id='12314068'>]",2.0
Christian Wiedmann,"[<JIRA Component: name='c client', id='12312380'>]",2009-10-06T01:54:02.000+0000,Christian Wiedmann,"Due to a mismatch between zookeeper_interest() and zookeeper_process(), when the zookeeper server is unresponsive the client can spin when reconnecting to the server.

In particular, zookeeper_interest() adds ZOOKEEPER_WRITE whenever there is data to be sent, but flush_send_queue() only writes the data if the state is ZOO_CONNECTED_STATE.  When in ZOO_ASSOCIATING_STATE, this results in spinning.

This probably doesn't affect production, but I had a runaway process in a development deployment that caused performance issues on the node.  This is easy to reproduce in a single node environment by doing a kill -STOP on the server and waiting for the session timeout.

Patch to be added.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-542,Major,Christian Wiedmann,Fixed,2009-10-07T18:11:37.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,c-client can spin when server unresponsive,2010-03-26T17:25:02.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>, <JIRA Version: name='3.2.1', id='12314068'>]",2.0
Henry Robinson,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2009-10-06T00:04:47.000+0000,Patrick D. Hunt,"zkpython is currently limited to a max of 256 total handles - not 256 open handles, but rather 256 total handles created
over the lifetime of the python application.

In general this isn't a real issue, however in the case of a long lived application which polls the cluster periodically (closing
the session btw calls) this is an issue.

it would be great if the slots could be reused? or perhaps a more complex structure, such as a linked list, which would allow
dynamic growth/shrinkage of the handle list.

Also see ZOOKEEPER-540
","[<JIRA Version: name='3.2.2', id='12314335'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-541,Major,Patrick D. Hunt,Fixed,2009-10-13T23:14:58.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkpython limited to 256 handles,2010-03-26T17:25:01.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",2.0
Henry Robinson,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2009-10-01T22:02:41.000+0000,Patrick D. Hunt,"I was getting a python segfault in one of my scripts. Turns out I was closing a session handle and then reusing it (async call). This was causing python to segfault.

zkpython should track handle state and complain, rather than crash, if the handle is invalid (closed).
","[<JIRA Version: name='3.2.2', id='12314335'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-540,Major,Patrick D. Hunt,Fixed,2009-10-09T00:43:13.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkpython needs better tracking of handle validity,2010-03-26T17:25:01.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",2.0
Henry Robinson,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2009-09-30T06:00:21.000+0000,Patrick D. Hunt,"Henry, can you take a look at this, am I doing it right?

calling 
        zookeeper.async(self.handle, path)
causes python to segfault.

see: http://github.com/phunt/zk-smoketest/blob/master/zk-smoketest.py

","[<JIRA Version: name='3.2.2', id='12314335'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-538,Critical,Patrick D. Hunt,Fixed,2009-10-02T00:26:42.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper.async causes python to segfault,2010-03-26T17:25:01.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",3.0
Thomas Dudziak,"[<JIRA Component: name='build', id='12312383'>]",2009-09-25T22:25:43.000+0000,Thomas Dudziak,"This is a problem if you use zookeeper as a dependency in maven because for whatever reason the maven compiler plugin will pick up the java files in the jar and compile them to the output directory. From there they will land in the generated jar file for whatever project happens to depend on zookeeper thus introducing duplicate classes (once in zookeeper.jar, once in the project's artifact).","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-537,Major,Thomas Dudziak,Fixed,2009-11-06T21:20:40.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,The zookeeper jar includes the java source files,2010-03-26T17:25:01.000+0000,"[<JIRA Version: name='3.3.0', id='12313976'>]",3.0
Patrick D. Hunt,"[<JIRA Component: name='build', id='12312383'>]",2009-09-24T17:56:42.000+0000,Patrick D. Hunt,"Ant resolves the same dependencies multiple times if multiple targets are run on the command line:

""ant b c"", where b and c both depend on a, results in a being executed twice. However if you have a
target d which depends on both b and c, ""ant d"" will only result in a being executed once.

say ""ant jar compile-test"" is run, this will currently fail as ivy-init is run twice, resulting in the taskdef failing.

Rather we need a guard on the ivy-init target itself to ensure it isn't run twice.

ie: put an unless attrib on ivy-init, then set that property in the body of the target
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-535,Major,Patrick D. Hunt,Fixed,2009-09-24T18:52:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ivy task does not enjoy being defined twice (build error),2010-03-26T17:25:01.000+0000,[],2.0
Patrick D. Hunt,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2009-09-23T18:46:40.000+0000,Mahadev Konar,The test target in contib/bookkeeper does not depend on jar target. So the ant test target gives compilation errors if the main is not compiled which can be prevented if it depends on jar. It can then check if main has been compiled or not and throw out a reasonable error.,"[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-534,Major,Mahadev Konar,Fixed,2009-12-16T23:46:55.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,The test target in contib/bookkeeper does not depend on jar target.,2013-05-02T02:29:22.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>, <JIRA Version: name='3.2.1', id='12314068'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='build', id='12312383'>]",2009-09-23T17:41:41.000+0000,Patrick D. Hunt,"if clean is run twice in a row (ie already clean, then run clean) an error is generated.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-533,Major,Patrick D. Hunt,Fixed,2009-09-23T21:54:20.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ant error running clean twice,2010-03-26T17:25:01.000+0000,[],2.0
Hiram R. Chirino,[],2009-09-22T20:09:46.000+0000,Hiram R. Chirino,"The jars released in 3.2.1 will not run on Java 1.5.  With a small build change, it is possible to generate jars that will run on Java 1.5.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-532,Major,Hiram R. Chirino,Fixed,2009-11-18T17:30:38.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,java compiler should be target Java 1.5,2010-03-26T17:25:01.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",2.0
Flavio Paiva Junqueira,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2009-09-21T17:09:13.000+0000,Patrick D. Hunt,"Flavio, can you take a look, this is very unusual. This test seems to be failing due to interrupt being received and not handled properly:

http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/470/testReport/org.apache.zookeeper.test/HierarchicalQuorumTest/testHierarchicalQuorum/

I don't know why the interrupt would be received though...

here's an example:
2009-09-21 10:46:47,681 - WARN  [Thread-8:QuorumCnxManager$SendWorker@539] - Interrupted while waiting for message on queue
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1899)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1934)
	at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:317)
	at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:533)
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-531,Blocker,Patrick D. Hunt,Cannot Reproduce,2010-01-12T20:12:06.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Hudson trunk failure in heirarchical quorum test (interrupt problem),2010-03-26T17:25:00.000+0000,[],1.0
Isabel Drost-Fromm,"[<JIRA Component: name='c client', id='12312380'>]",2009-09-21T09:17:08.000+0000,Isabel Drost-Fromm,"I tried to run zookeeper c-client on a machine with IPv6 enabled. When connecting to the IPv6 address a connect(...) gave a ""Address family not supported by protocol"" error. The reason was, that a few lines earlier, the socket was opened with PF_INET instead of PF_INET6. Changing that the following way:

{code}
           if (zh->addrs[zh->connect_index].sa_family == AF_INET) {
            	zh->fd = socket(PF_INET, SOCK_STREAM, 0);
            } else {
            	zh->fd = socket(PF_INET6, SOCK_STREAM, 0);
            }
{code}

turned the error message into ""Invalid argument"". 

When printing out sizeof(struct sockaddr), sizeof(struct sockaddr_in) and sizeof(struct sockaddr_in6) I got sockaddr: 16, sockaddr_in: 16 and sockaddr_in6: 28. 

So in the code calling 

{code}
           connect(zh->fd, &zh->addrs[zh->connect_index], sizeof(struct sockaddr_in));
{code}

the parameter address_len is too small.

Same applies to how IPv6 addresses are handled in the function getaddrs(zhandle_t *zh).

(Big Thanks+kiss to Thilo Fromm for helping me debug this.)","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-530,Major,Isabel Drost-Fromm,Fixed,2009-10-21T21:01:11.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Memory corruption: Zookeeper c client IPv6 implementation does not honor struct sockaddr_in6 size,2010-03-26T17:25:00.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>, <JIRA Version: name='3.2.1', id='12314068'>]",3.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>]",2009-09-16T18:46:54.000+0000,Patrick D. Hunt,"If I create 100k nodes on /misc then

      CPPUNIT_ASSERT_EQUAL(0, zoo_get_children(zh2, ""/misc"", 0, &children));
      for (int i = 0; i < children.count; i++) {
        sprintf(path, ""/misc/%s"", children.data[i]);
        CPPUNIT_ASSERT_EQUAL(0, zoo_exists(zh2, path, 1, &stat));
        CPPUNIT_ASSERT_EQUAL(0, zoo_wexists(zh3, path, watcher, &ctx3, &stat));
      }

around 47k or so through the loop the client fails with -4 (connection loss), the client timeout is 30 seconds. The server command port shows the following, so it looks like it's not the server but some issue with watcher reg on the c client?

phunt@valhalla:~$ echo stat | nc localhost 22181
Zookeeper version: 3.3.0--1, built on 07/22/2009 23:55 GMT
Clients:
 /127.0.0.1:45729[1](queued=0,recved=100024,sent=0)
 /127.0.0.1:50229[1](queued=0,recved=0,sent=0)
 /127.0.0.1:45731[1](queued=0,recved=47116,sent=0)
 /127.0.0.1:45730[1](queued=0,recved=47117,sent=1)

Latency min/avg/max: 0/196/1026
Received: 194257
Sent: 1
Outstanding: 0
Zxid: 0x186a4
Mode: standalone
Node count: 100005


729 is a separate client - the one that created the nodes originally.

731 and 730 are zh2/zh3 in the code.
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-528,Critical,Patrick D. Hunt,Invalid,2009-10-14T17:27:20.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,c client exists() call with watch on large number of nodes (>100k) causes connection loss,2010-03-26T17:25:00.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",1.0
Mahadev Konar,"[<JIRA Component: name='server', id='12312382'>]",2009-09-16T17:34:23.000+0000,Patrick D. Hunt,"the following test failed:

http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/462/testReport/org.apache.zookeeper.test/QuorumQuotaTest/testQuotaWithQuorum/

here's the interesting log:

2009-09-16 10:35:52,728 - WARN  [CommitProcessor:1:DataTree@409] - Quota exceeded: /a bytes=1808 limit=1000
2009-09-16 10:36:34,000 - INFO  [SessionTracker:SessionTrackerImpl@133] - Expiring session 0x423c26c1d220000
2009-09-16 10:36:12,725 - WARN  [main-SendThread(localhost:11225):ClientCnxn$SendThread@969] - Exception closing session 0x423c26c1d220000 to sun.nio.ch.SelectionKeyImpl@a7dd39
java.io.IOException: TIMED OUT
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:908)

looks like a create call hung and the session eventually expired

perhaps updating the quota had some issue? deadlock or ...
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-527,Critical,Patrick D. Hunt,Cannot Reproduce,2009-12-07T21:13:00.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,hudson trunk failure in  quota test,2010-03-26T17:25:00.000+0000,"[<JIRA Version: name='3.2.1', id='12314068'>]",1.0
Benjamin Reed,"[<JIRA Component: name='server', id='12312382'>, <JIRA Component: name='tests', id='12312427'>]",2009-09-10T23:36:16.000+0000,Patrick D. Hunt,"DBSizeTest looks like it should be testing latency, but it doesn't seem to do it (assert is commented out).

We need to decide if this test should be fixed, or just dropped.

Also note: this test takes 40seconds on my system. Way too long. Perhaps async create operations should be used
to populate the database. I also noticed that data size has a big impact on overall test time (1k vs 5 bytes is something
like a 2x time diff for time to run the test).
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-524,Minor,Patrick D. Hunt,Fixed,2010-02-20T01:21:54.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,DBSizeTest is not really testing anything,2010-03-26T17:25:00.000+0000,[],2.0
,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2009-09-08T17:37:23.000+0000,Mahadev Konar,"In one of the scenarios, one of our users cleaned up the server database, upgraded the zookeeper servers to 3.* from 2.* and did not shut down there clients. The clients kept spinning since they couldnt find a server that was up to date. Though this was a mistake on the users side, but the spinning of clients caused more problems (like zookeeper server running out of file handles since the clients kept spinning throguh servers). In such a case we should shut down the clients since its this should never happen.
",[],Bug,ZOOKEEPER-523,Major,Mahadev Konar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zookeeper c client should shutdown if it sees its zxid is too high from all the server its connecting to.,2022-02-03T08:50:18.000+0000,[],0.0
Mahadev Konar,[],2009-08-25T23:49:16.000+0000,Mahadev Konar,"We noticed this in our tests -

{code}

java.net.SocketException: Broken pipe
        at java.net.SocketOutputStream.socketWrite0(Native Method)
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)
        at java.io.DataOutputStream.write(DataOutputStream.java:90)
        at java.io.FilterOutputStream.write(FilterOutputStream.java:80)
        at org.apache.jute.BinaryOutputArchive.writeBuffer(BinaryOutputArchive.java:122)
        at org.apache.zookeeper.server.DataNode.serialize(DataNode.java:126)
        at org.apache.jute.BinaryOutputArchive.writeRecord(BinaryOutputArchive.java:126)
        at org.apache.zookeeper.server.DataTree.serializeNode(DataTree.java:878)
        at org.apache.zookeeper.server.DataTree.serializeNode(DataTree.java:890)
        at org.apache.zookeeper.server.DataTree.serializeNode(DataTree.java:890)
        at org.apache.zookeeper.server.DataTree.serializeNode(DataTree.java:890)
        at org.apache.zookeeper.server.DataTree.serializeNode(DataTree.java:890)
        at org.apache.zookeeper.server.DataTree.serializeNode(DataTree.java:890)
        at org.apache.zookeeper.server.DataTree.serialize(DataTree.java:940)
        at org.apache.zookeeper.server.util.SerializeUtils.serializeSnapshot(SerializeUtils.java:102)
        at org.apache.zookeeper.server.ZooKeeperServer.serializeSnapshot(ZooKeeperServer.java:269)
        at org.apache.zookeeper.server.quorum.FollowerHandler.run(FollowerHandler.java:263)
{code}

So the followerhandler got an exception while writing to the socket but the follower was still waiting on the socket for a read and got a read timeout after 60 seconds or so. To just make sure we handle this rightly, we should close the socket at the followerhandler when we get an excpetion, so that the follower immediately recognizes that its disconnected from the leader.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-519,Major,Mahadev Konar,Fixed,2009-11-18T17:45:10.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Followerhandler should close the socket if it gets an exception on a write.,2010-03-26T17:25:00.000+0000,[],2.0
Jay Shrauner,"[<JIRA Component: name='server', id='12312382'>]",2009-08-25T23:32:42.000+0000,Mahadev Konar,The code in NIO factory is such that if we fail to accept a connection due to some reasons (too many file handles maybe one of them) we do not close the connections that are in CLOSE_WAIT. We need to call an explicit close on these sockets and then close them. One of the solutions might be to move doIO before accpet so that we can still close connection even if we cannot accept connections.,[],Bug,ZOOKEEPER-517,Critical,Mahadev Konar,Duplicate,2012-07-27T05:18:57.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,NIO factory fails to close connections when the number of file handles run out.,2012-07-27T05:18:57.000+0000,"[<JIRA Version: name='3.4.3', id='12319288'>, <JIRA Version: name='3.5.0', id='12316644'>]",2.0
,"[<JIRA Component: name='server', id='12312382'>]",2009-08-25T13:10:58.000+0000,Qian Ye,"The Zookeeper quorum, containing 5 servers, didn't provide service when restart after an ""Out of memory"" crash. 

It happened as following:
1. we built  a Zookeeper quorum which contained  5 servers, say 1, 3, 4, 5, 6 (have no 2), and 6 was the leader.
2. we created 18 threads on 6 different servers to set and get data from a znode in the Zookeeper at the same time.  The size of the data is 1MB. The test threads did their job as fast as possible, no pause between two operation, and they repeated the setting and getting 4000 times. 
3. the Zookeeper leader crashed about 10 mins  after the test threads started. The leader printed out the log:

2009-08-25 12:00:12,301 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x523
4223c2dc00b5 due to java.io.IOException: Read error
2009-08-25 12:00:12,318 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x523
4223c2dc00b6 due to java.io.IOException: Read error
2009-08-25 12:03:44,086 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x523
4223c2dc00b8 due to java.io.IOException: Read error
2009-08-25 12:04:53,757 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x523
4223c2dc00b7 due to java.io.IOException: Read error
2009-08-25 12:15:45,151 - FATAL [SyncThread:0:SyncRequestProcessor@131] - Severe unrecoverable error, exiting
java.lang.OutOfMemoryError: Java heap space
    at java.util.Arrays.copyOf(Arrays.java:2786)
    at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:71)
    at java.io.DataOutputStream.writeInt(DataOutputStream.java:180)
    at org.apache.jute.BinaryOutputArchive.writeInt(BinaryOutputArchive.java:55)
    at org.apache.zookeeper.txn.SetDataTxn.serialize(SetDataTxn.java:42)
    at org.apache.zookeeper.server.persistence.Util.marshallTxnEntry(Util.java:262)
    at org.apache.zookeeper.server.persistence.FileTxnLog.append(FileTxnLog.java:154)
    at org.apache.zookeeper.server.persistence.FileTxnSnapLog.append(FileTxnSnapLog.java:268)
    at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:100)

It is clear that the leader ran out of memory. then the server 4 was down almost at the same time, and printed out the log:
2009-08-25 12:15:45,995 - ERROR [FollowerRequestProcessor:3:FollowerRequestProcessor@91] - Unexpected exception causing
exit
java.net.SocketException: Connection reset
    at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:96)
    at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
    at java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)
    at java.io.DataOutputStream.write(DataOutputStream.java:90)
    at java.io.FilterOutputStream.write(FilterOutputStream.java:80)
    at org.apache.jute.BinaryOutputArchive.writeBuffer(BinaryOutputArchive.java:119)
    at org.apache.zookeeper.server.quorum.QuorumPacket.serialize(QuorumPacket.java:51)
    at org.apache.jute.BinaryOutputArchive.writeRecord(BinaryOutputArchive.java:123)
    at org.apache.zookeeper.server.quorum.Follower.writePacket(Follower.java:97)
    at org.apache.zookeeper.server.quorum.Follower.request(Follower.java:399)
    at org.apache.zookeeper.server.quorum.FollowerRequestProcessor.run(FollowerRequestProcessor.java:86)
2009-08-25 12:15:45,996 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x423
4ab894330075 due to java.net.SocketException: Broken pipe
2009-08-25 12:15:45,996 - FATAL [SyncThread:3:SyncRequestProcessor@131] - Severe unrecoverable error, exiting
java.net.SocketException: Broken pipe
    at java.net.SocketOutputStream.socketWrite0(Native Method)
    at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
    at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
    at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
    at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
    at org.apache.zookeeper.server.quorum.Follower.writePacket(Follower.java:100)
    at org.apache.zookeeper.server.quorum.SendAckRequestProcessor.flush(SendAckRequestProcessor.java:52)
    at org.apache.zookeeper.server.SyncRequestProcessor.flush(SyncRequestProcessor.java:147)
    at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:92)
2009-08-25 12:15:45,995 - WARN  [QuorumPeer:/0.0.0.0:2181:Follower@309] - Exception when following the leader
java.net.SocketException: Broken pipe
    at java.net.SocketOutputStream.socketWrite0(Native Method)
    at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
    at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
    at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
    at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
    at org.apache.zookeeper.server.quorum.Follower.writePacket(Follower.java:100)
    at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:256)
    at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:498)
2009-08-25 12:15:46,022 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x0 d
ue to java.io.IOException: ZooKeeperServer not running
2009-08-25 12:15:46,022 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x0 d
ue to java.io.IOException: ZooKeeperServer not running
2009-08-25 12:15:46,023 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x0 d
ue to java.io.IOException: ZooKeeperServer not running

It was really strange that when the 2 server down, the other three servers cannot provide service any more, the 'stat' command all leaded to ""ZooKeeperServer not running"".


4. I restarted the server 6(the former leader) and the server 4. But the service didn't come back. All the five servers printed ""ZookeeperServer not running"". The server 6 printed the logs:

2009-08-25 14:02:15,395 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x0 d
ue to java.io.IOException: ZooKeeperServer not running 
2009-08-25 14:02:27,703 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x0 d
ue to java.io.IOException: Responded to info probe
2009-08-25 14:02:28,733 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x0 d
ue to java.io.IOException: ZooKeeperServer not running 
2009-08-25 14:02:42,070 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x0 d
ue to java.io.IOException: ZooKeeperServer not running 
2009-08-25 14:02:55,407 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x0 d
ue to java.io.IOException: ZooKeeperServer not running 
2009-08-25 14:03:08,744 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x0 d
ue to java.io.IOException: ZooKeeperServer not running 
2009-08-25 14:03:22,080 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x0 d
ue to java.io.IOException: ZooKeeperServer not running 
2009-08-25 14:03:29,396 - ERROR [main:Util@238] - Last transaction was partial.
2009-08-25 14:03:35,417 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x0 d
ue to java.io.IOException: ZooKeeperServer not running 
2009-08-25 14:03:48,761 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x0 d
ue to java.io.IOException: ZooKeeperServer not running 

The server 4 printed logs like:

2009-08-25 14:03:48,747 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x0 d
ue to java.io.IOException: ZooKeeperServer not running
2009-08-25 14:04:02,091 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x0 d
ue to java.io.IOException: ZooKeeperServer not running
2009-08-25 14:04:15,427 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x0 d
ue to java.io.IOException: ZooKeeperServer not running
2009-08-25 14:04:17,816 - WARN  [QuorumPeer:/0.0.0.0:2181:Follower@164] - Unexpected exception, tries=0
java.net.ConnectException: Connection refused
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
    at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
    at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
    at java.net.Socket.connect(Socket.java:525)
    at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:156)
    at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:498)
2009-08-25 14:04:18,820 - WARN  [QuorumPeer:/0.0.0.0:2181:Follower@164] - Unexpected exception, tries=1
java.net.ConnectException: Connection refused
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
    at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
    at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
    at java.net.Socket.connect(Socket.java:525)
    at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:156)
    at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:498)
2009-08-25 14:04:19,823 - WARN  [QuorumPeer:/0.0.0.0:2181:Follower@164] - Unexpected exception, tries=2
java.net.ConnectException: Connection refused
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
    at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
    at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
    at java.net.Socket.connect(Socket.java:525)
    at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:156)
    at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:498)
2009-08-25 14:04:28,764 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x0 d
ue to java.io.IOException: ZooKeeperServer not running
2009-08-25 14:04:42,101 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x0 d
ue to java.io.IOException: ZooKeeperServer not running

the server 1, 3, 5 printed out the logs like:

2009-08-25 14:01:35,396 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x0 d
ue to java.io.IOException: ZooKeeperServer not running
2009-08-25 14:01:36,554 - WARN  [QuorumPeer:/0.0.0.0:2181:LeaderElection@194] - Ignoring exception while looking for lea
der
java.net.SocketTimeoutException: Receive timed out 
    at java.net.PlainDatagramSocketImpl.receive0(Native Method)
    at java.net.PlainDatagramSocketImpl.receive(PlainDatagramSocketImpl.java:136)
    at java.net.DatagramSocket.receive(DatagramSocket.java:712)
    at org.apache.zookeeper.server.quorum.LeaderElection.lookForLeader(LeaderElection.java:170)
    at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:488)
2009-08-25 14:01:37,758 - WARN  [QuorumPeer:/0.0.0.0:2181:LeaderElection@194] - Ignoring exception while looking for lea
der
java.net.SocketTimeoutException: Receive timed out 
    at java.net.PlainDatagramSocketImpl.receive0(Native Method)
    at java.net.PlainDatagramSocketImpl.receive(PlainDatagramSocketImpl.java:136)
    at java.net.DatagramSocket.receive(DatagramSocket.java:712)
    at org.apache.zookeeper.server.quorum.LeaderElection.lookForLeader(LeaderElection.java:170)
    at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:488)
2009-08-25 14:01:37,865 - WARN  [QuorumPeer:/0.0.0.0:2181:Follower@164] - Unexpected exception, tries=0
java.net.ConnectException: Connection refused
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
    at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
    at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
    at java.net.Socket.connect(Socket.java:525)
    at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:156)
    at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:498)
2009-08-25 14:01:38,289 - WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn@497] - Exception causing close of session 0x0 d
ue to java.io.IOException: Responded to info probe


my zoo.cfg is like:
tickTime=2000
dataDir=./status/
clientPort=2181
initLimit=10
syncLimit=2
server.1=10.81.11.107:2888:3888
server.2=10.81.11.106:2888:3888
server.3=10.81.11.89:2888:3888
server.4=10.81.11.99:2888:3888
server.5=10.81.11.79:2888:3888

Several questions:
1. Why the leader selection failed after the restart?
2. Is the size of data too big to be processed properly?
3. How can I recover from this situation? Can I just remove the version-2 directory on server 6(the former leader) and restart the server?

","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-515,Major,Qian Ye,Invalid,2011-09-06T16:01:35.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,"Zookeeper quorum didn't provide service when restart after an ""Out of memory"" crash",2011-09-06T16:01:35.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",0.0
,[],2009-08-24T17:44:28.000+0000,Patrick D. Hunt,"http://hudson.zones.apache.org/hudson/job/ZooKeeper-trunk/424/testReport/org.apache.zookeeper.test/CnxManagerTest/testCnxManager/

Flavio can you take a look?
","[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-514,Major,Patrick D. Hunt,Won't Fix,2009-08-24T19:04:11.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,test failure on trunk in testCnxManager - NPE,2009-09-05T22:36:24.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",1.0
,"[<JIRA Component: name='c client', id='12312380'>]",2009-08-22T12:12:03.000+0000,Qian Ye,"The client which created an ephemeral node at the zookeeper server, printed the following log

WARNING: 08-20 03:09:20:  auto * 182894118176 [logid:][reqip:][auto_exchanger_zk_basic.cpp:605]get children fail.[/forum/elect_nodes][-7][operation timeout]

and the Zookeeper client printed the following log (the log level is INFO)

2009-08-19 21:36:18,067:3813(0x9556c520):ZOO_INFO@log_env@545: Client environment:zookeeper.version=zookeeper C client 3.2.0
    606 2009-08-19 21:36:18,067:3813(0x9556c520):ZOO_INFO@log_env@549: Client environment:host.name=jx-ziyuan-test00.jx.baidu.com
    607 2009-08-19 21:36:18,068:3813(0x9556c520):ZOO_INFO@log_env@557: Client environments.name=Linux
    608 2009-08-19 21:36:18,068:3813(0x9556c520):ZOO_INFO@log_env@558: Client environments.arch=2.6.9-52bs
    609 2009-08-19 21:36:18,068:3813(0x9556c520):ZOO_INFO@log_env@559: Client environments.version=#2 SMP Fri Jan 26 13:34:38 CST 2007
    610 2009-08-19 21:36:18,068:3813(0x9556c520):ZOO_INFO@log_env@567: Client environment:user.name=club
    611 2009-08-19 21:36:18,068:3813(0x9556c520):ZOO_INFO@log_env@577: Client environment:user.home=/home/club
    612 2009-08-19 21:36:18,068:3813(0x9556c520):ZOO_INFO@log_env@589: Client environment:user.dir=/home/club/user/luhongbo/auto-exchanger
    613 2009-08-19 21:36:18,068:3813(0x9556c520):ZOO_INFO@zookeeper_init@613: Initiating client connection, host=127.0.0.1:2181,127.0.0.1:2182 sessionTimeout=2000 wa        tcher=0x408c56 sessionId=0x0 sessionPasswd=<null> context=(nil) flags=0
    614 2009-08-19 21:36:18,069:3813(0x41401960):ZOO_INFO@check_events@1439: initiated connection to server [127.0.0.1:2181]
    615 2009-08-19 21:36:18,070:3813(0x41401960):ZOO_INFO@check_events@1484: connected to server [127.0.0.1:2181] with session id=1232c1688a20093
    616 2009-08-20 02:48:01,780:3813(0x41401960):ZOO_WARN@zookeeper_interest@1335: Exceeded deadline by 520ms
    617 2009-08-20 03:08:52,332:3813(0x41401960):ZOO_WARN@zookeeper_interest@1335: Exceeded deadline by 14ms
    618 2009-08-20 03:09:04,666:3813(0x41401960):ZOO_WARN@zookeeper_interest@1335: Exceeded deadline by 48ms
    619 2009-08-20 03:09:09,733:3813(0x41401960):ZOO_WARN@zookeeper_interest@1335: Exceeded deadline by 24ms
    620 2009-08-20 03:09:20,289:3813(0x41401960):ZOO_WARN@zookeeper_interest@1335: Exceeded deadline by 264ms
    621 2009-08-20 03:09:20,295:3813(0x41401960):ZOO_ERROR@handle_socket_error_msg@1388: Socket [127.0.0.1:2181] zk retcode=-7, errno=110(Connection timed out): conn        ection timed out (exceeded timeout by 264ms)
    622 2009-08-20 03:09:20,309:3813(0x41401960):ZOO_WARN@zookeeper_interest@1335: Exceeded deadline by 284ms
    623 2009-08-20 03:09:20,309:3813(0x41401960):ZOO_ERROR@handle_socket_error_msg@1433: Socket [127.0.0.1:2182] zk retcode=-4, errno=111(Connection refused): server         refused to accept the client
    624 2009-08-20 03:09:20,353:3813(0x41401960):ZOO_INFO@check_events@1439: initiated connection to server [127.0.0.1:2181]
    625 2009-08-20 03:09:20,552:3813(0x41401960):ZOO_INFO@check_events@1484: connected to server [127.0.0.1:2181] with session id=1232c1688a20093

The problem happened at 03:09:20, it seems that the zookeeper refused to accept the client, and I don't know why.

the zoo.cfg is like:
# The number of milliseconds of each tick
tickTime=500
# The number of ticks that the initial 
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
dataDir=./data/
# the port at which the clients will connect
clientPort=2181

the C client used multi-thread library, and the session timeout is set to 2000 when the zookeeper handler was initialized.
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-513,Major,Qian Ye,Cannot Reproduce,2013-10-10T17:33:16.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,C client disconnect with stand-alone server abnormally,2013-10-10T17:33:16.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",0.0
Flavio Paiva Junqueira,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2009-08-20T00:11:07.000+0000,Patrick D. Hunt,"I was doing some fault injection testing of 3.2.1 with ZOOKEEPER-508 patch applied and noticed that after some time the ensemble failed to re-elect a leader.

See the attached log files - 5 member ensemble. typically 5 is the leader

Notice that after 16:23:50,525 no quorum is formed, even after 20 minutes elapses w/no quorum

environment:

I was doing fault injection testing using aspectj. The faults are injected into socketchannel read/write, I throw exceptions randomly at a 1/200 ratio (rand.nextFloat() <= .005 => throw IOException

You can see when a fault is injected in the log via:
2009-08-19 16:57:09,568 - INFO  [Thread-74:ReadRequestFailsIntermittently@38] - READPACKET FORCED FAIL

vs a read/write that didn't force fail:
2009-08-19 16:57:09,568 - INFO  [Thread-74:ReadRequestFailsIntermittently@41] - READPACKET OK

otw standard code/config (straight fle quorum with 5 members)

also see the attached jstack trace. this is for one of the servers. Notice in particular that the number of sendworkers != the number of recv workers.

","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-512,Blocker,Patrick D. Hunt,Fixed,2009-10-27T21:14:23.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,FLE election fails to elect leader,2010-03-26T17:25:00.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",2.0
Mahadev Konar,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2009-08-19T22:35:23.000+0000,Patrick D. Hunt,"in FollowerHandler if sendPackets gets an ioexception on writeRecord the send thread will exit, however the 
socket isn't necessarily closed.

2009-08-19 15:28:46,869 - WARN  [Sender-/127.0.0.1:58179:FollowerHandler@131] - Unexpected exception
	at org.apache.zookeeper.server.quorum.FollowerHandler.sendPackets(FollowerHandler.java:128)
	at org.apache.zookeeper.server.quorum.FollowerHandler.access$0(FollowerHandler.java:107)
	at org.apache.zookeeper.server.quorum.FollowerHandler$1.run(FollowerHandler.java:325)

This results in the follower taking a very long time to recover and rejoin the quorum.
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-511,Major,Patrick D. Hunt,Fixed,2010-03-11T14:06:33.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,bad error handling in FollowerHandler.sendPackets,2010-03-26T17:25:00.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",2.0
Henry Robinson,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2009-08-19T21:13:42.000+0000,Patrick D. Hunt,"The current zkpython bindings always throw ""IOError(""text"")"" exceptions, even for ZK specific exceptions such as NODEEXISTS. This makes it difficult (error prone) to handle exceptions in python code. You can't easily pickup a connection loss vs a node exists for example. Of course you could match the error string, but this seems like a bad idea imo.

We need to add specific exception types to the python binding that map directly to KeeperException/java types. It would also be useful to include the information provided by the KeeperException (like path in some cases), etc... as part of the error thrown to the python code. Would probably be a good idea to stay as close to java api as possible wrt mapping the errors.","[<JIRA Version: name='3.2.2', id='12314335'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-510,Major,Patrick D. Hunt,Fixed,2009-10-14T00:14:27.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"zkpython lumps all exceptions as IOError, needs specialized exceptions for KeeperException types",2010-03-26T17:25:00.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",2.0
Benjamin Reed,[],2009-08-19T17:41:14.000+0000,Mahadev Konar,The truncating of logs does not work right because we use BufferedInputStream and the truncation happens at the buffered read which is greater than the actual truncate position.,"[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-509,Blocker,Mahadev Konar,Fixed,2009-08-25T05:57:32.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Truncating of logs does not work right at the followers.,2009-09-05T22:36:20.000+0000,[],2.0
Mahadev Konar,"[<JIRA Component: name='quorum', id='12312379'>]",2009-08-17T18:39:08.000+0000,Mahadev Konar,The proposals and commits sent by the leader after it asks the followers to truncate there logs or starts sending a diff has missing messages which causes out of order commits messages and causes the followers to shutdown because of these out of order commits.,"[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-508,Blocker,Mahadev Konar,Fixed,2009-08-25T05:56:53.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,proposals and commits for DIFF and Truncate messages from the leader to followers is buggy.,2009-09-05T22:36:20.000+0000,[],2.0
Utkarsh Srivastava,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2009-08-08T20:57:35.000+0000,Utkarsh Srivastava,"The test case testAsyncCreateClose is badly broken. I was wondering why all the unit tests are passing inspite of having found so many different problems with LedgerManagementProcessor. 

There is a big try-catch block sitting in the test case that catches all exception, prints their stack trace, and exits, thereby allowing the test to pass. In general, unit tests shouldnt catch exceptions unless it is something you are expecting that will happen.

Another problem is that the same ControlObject is used for synchronization throughout. Since we already have the problem of callbacks being called multiple times (ZOOKEEPER-502), notify() on the control object is called too many times, resulting in the unit test not waiting for certain callbacks.

Thus the test never waits for the asyncOpenLedger() to finish, and hence still succeeds. I believe asyncOpenLedger() has never worked right. ","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-505,Critical,Utkarsh Srivastava,Fixed,2010-01-26T23:17:08.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,testAsyncCreateClose is badly broken,2010-03-26T17:25:00.000+0000,[],2.0
Utkarsh Srivastava,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2009-08-07T17:35:50.000+0000,Utkarsh Srivastava,"
java.lang.ClassCastException: org.apache.bookkeeper.client.LedgerManagementProcessor$OpenLedgerOp cannot be cast to org.apache.bookkeeper.client.LedgerManagementProcessor$CloseLedgerOp
	at org.apache.bookkeeper.client.LedgerManagementProcessor.processResult(LedgerManagementProcessor.java:1083)

This seems to be happening because its a nested switch case statement. And the OPEN: case, doesn't ever call a break. It only calls a break from the inner switch-case and hence falls through into the CLOSE: case.
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-504,Major,Utkarsh Srivastava,Fixed,2010-01-26T23:17:35.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ClassCastException in LedgerManagementProcessor,2010-03-26T17:24:59.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",2.0
Benjamin Reed,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2009-08-07T01:06:12.000+0000,Benjamin Reed,"there is a race condition between the zookeeper completion thread and the bookeeper processing queue during create. if the zookeeper completion thread falls behind due to scheduling, the action counter of the create operation may go backwards.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-503,Major,Benjamin Reed,Fixed,2010-01-26T23:18:04.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,race condition in asynchronous create,2013-05-02T02:29:21.000+0000,[],3.0
Flavio Paiva Junqueira,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2009-08-07T00:57:50.000+0000,Benjamin Reed,"when calling the asynchronous version of create, the completion routine is called more than once.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-502,Major,Benjamin Reed,Fixed,2010-01-26T23:19:07.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,bookkeeper create calls completion too many times,2010-03-26T17:24:56.000+0000,[],2.0
Flavio Paiva Junqueira,[],2009-08-06T22:11:42.000+0000,Flavio Paiva Junqueira,"It timed out according to the console output:

http://hudson.zones.apache.org/hudson/job/ZooKeeper-trunk/406/testReport/org.apache.zookeeper.test/CnxManagerTest/testCnxManager/","[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-501,Major,Flavio Paiva Junqueira,Fixed,2009-08-07T21:03:30.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,CnxManagerTest failed on hudson,2009-09-05T22:36:20.000+0000,[],2.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>, <JIRA Component: name='tests', id='12312427'>]",2009-08-05T20:32:20.000+0000,Patrick D. Hunt,"there's a regression in 3.2 - electionAlg is no longer defaulting to 3 (incorrectly defaults to 0)

also - need to have tests to validate this","[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-499,Blocker,Patrick D. Hunt,Fixed,2009-08-10T22:25:56.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,electionAlg should default to FLE (3) - regression,2009-09-05T22:36:20.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",2.0
Flavio Paiva Junqueira,"[<JIRA Component: name='leaderElection', id='12312378'>]",2009-08-04T21:18:51.000+0000,Todd Greenwood-Geer,"In a WAN configuration, ZooKeeper is endlessly electing, terminating, and re-electing a ZooKeeper leader. The WAN configuration involves two groups, a central DC group of ZK servers that have a voting weight = 1, and a group of servers in remote pods with a voting weight of 0.

What we expect to see is leaders elected only in the DC, and the pods to contain only followers. What we are seeing is a continuous cycling of leaders. We have seen this consistently with 3.2.0, 3.2.0 + recommended patches (473, 479, 481, 491), and now release 3.2.1.","[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-498,Critical,Todd Greenwood-Geer,Fixed,2009-08-11T21:27:36.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Unending Leader Elections : WAN configuration,2009-09-05T22:36:20.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2009-08-04T20:51:29.000+0000,Patrick D. Hunt,"the api (c/java clients) and the forrest docs should talk about thread safety - in particular we don't
mention that ZooKeeper class is thread safe (etc...) Docs should be updated.
",[],Bug,ZOOKEEPER-497,Minor,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,api and forrest docs should mention if classes are thread safe,2022-02-03T08:50:20.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",0.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>]",2009-08-03T18:20:00.000+0000,Michi Mutsuzaki,"The C client logs this error message when zookeeper_init is called with chroot. 

2009-08-03 18:14:29,130:6624(0x5e66e950):ZOO_ERROR@sub_string@730: server path  does not include chroot path /chroot

I'll attach a simple program to reproduce this.

Thanks!
--Michi","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-495,Minor,Michi Mutsuzaki,Fixed,2010-01-26T03:11:29.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,c client logs an invalid error when zookeeper_init is called with chroot,2010-05-07T17:40:33.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",2.0
steve bendiola,"[<JIRA Component: name='java client', id='12312381'>]",2009-08-01T09:40:58.000+0000,steve bendiola,"the command line ""setquota"" tries to use argument 3 as both a path and a value
","[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-493,Minor,steve bendiola,Fixed,2009-08-04T21:39:21.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,patch for command line setquota ,2009-09-05T22:36:20.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",2.0
,"[<JIRA Component: name='tests', id='12312427'>]",2009-07-31T04:24:39.000+0000,Patrick D. Hunt,"the tests currently rely (QuorumPeerMainTest in particular) on conf/log4j.properties. if the user changes this file
and then runs the tests the tests may fail. the tests should have their own log4j.properties maintained w/in the test
directory itself, separate from conf/log4j.properties
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-492,Minor,Patrick D. Hunt,Invalid,2014-04-23T20:40:46.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,the tests should have their own log4j.properties,2014-04-23T20:40:46.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>, <JIRA Version: name='3.2.0', id='12313491'>]",2.0
Flavio Paiva Junqueira,"[<JIRA Component: name='leaderElection', id='12312378'>]",2009-07-29T18:51:10.000+0000,Flavio Paiva Junqueira,This is a fix to prevent zero-weight servers from being elected leaders. This will allow in wide-area scenarios to restrict the set of servers that can lead the ensemble.,"[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-491,Major,Flavio Paiva Junqueira,Fixed,2009-08-04T18:17:48.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Prevent zero-weight servers from being elected,2013-05-02T02:29:26.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",2.0
Patrick D. Hunt,[],2009-07-29T18:27:29.000+0000,Patrick D. Hunt,"the javadoc for ZooKeeper constructor says:

     * The client object will pick an arbitrary server and try to connect to it.
     * If failed, it will try the next one in the list, until a connection is
     * established, or all the servers have been tried.

the ""or all server tried"" phrase is misleading, it should indicate that we retry until success, con closed, or session expired. 

we also need ot mention that connection is async, that constructor returns immed and you need to look for connection event in watcher","[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-490,Major,Patrick D. Hunt,Fixed,2009-08-07T18:17:54.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,the java docs for session creation are misleading/incomplete,2009-09-05T22:36:20.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>, <JIRA Version: name='3.2.0', id='12313491'>]",2.0
Giridharan Kesavan,"[<JIRA Component: name='build', id='12312383'>]",2009-07-28T06:40:27.000+0000,Patrick D. Hunt,"for some reason hudson.zones violations report has jumped up to 2000+ findbugs issues in just the last couple of weeks. It's
jumped up from 0 violations so something is weird...

http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/

Giri can you take a look?
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-489,Minor,Patrick D. Hunt,Cannot Reproduce,2009-09-29T09:59:38.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,findbugs on hudson.zones.a.o has jumped way up (2k+),2010-03-26T17:24:55.000+0000,[],1.0
Mahadev Konar,"[<JIRA Component: name='server', id='12312382'>]",2009-07-24T20:38:00.000+0000,Mahadev Konar,"setdata on root ""/"" crashes the servers with the followimg exception. Unfortunately we never had a setdata test on root. The following is the exception. This happens with 3.1.1 as well. We might want to consider releasing 3.1.2 just for this jira.

{code}
java.lang.IllegalArgumentException: Invalid path /
        at org.apache.zookeeper.common.PathTrie.findMaxPrefix(PathTrie.java:255)
        at org.apache.zookeeper.server.DataTree.setData(DataTree.java:543)
        at org.apache.zookeeper.server.DataTree.processTxn(DataTree.java:701)
        at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:94)
        at org.apache.zookeeper.server.SyncRequestProcessor.flush(SyncRequestProcessor.java:127)
        at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:75)
{code}
","[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-487,Blocker,Mahadev Konar,Fixed,2009-07-31T19:08:57.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,setdata on root (/) crashes the servers.,2009-09-05T22:36:20.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>, <JIRA Version: name='3.2.0', id='12313491'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>, <JIRA Component: name='server', id='12312382'>]",2009-07-23T21:47:03.000+0000,Patrick D. Hunt,"We need ops documentation detailing what to do if the ZK server VM fails - by fail I mean the jvm process
exits/dies/crashes/etc...

In general a supervisor process should be used to start/stop/restart/etc... the ZK server vm.

Something like daemontools http://cr.yp.to/daemontools.html could be used, or more simply a wrapper script
should monitor the status of the pid and restart if the jvm fails. It's up to the operator, if this is not done
automatically then it will have to be done manually, by operator restarting the ZK server jvm

The inherent behavior of ZK wrt to failures - ie that it automatically recovers as long as quorum is maintained - 
fits into this nicely.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-485,Major,Patrick D. Hunt,Fixed,2010-02-24T20:04:26.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,need ops documentation that details supervision of ZK server processes,2010-03-26T17:24:55.000+0000,[],3.0
Mahadev Konar,"[<JIRA Component: name='server', id='12312382'>]",2009-07-23T21:09:54.000+0000,Mahadev Konar,When a client is connected to follower and get disconnected and connects to a leader it gets SESSION MOVED excpetion. This is beacuse of a bug in the new feature of ZOOKEEPER-417 that we added in 3.2. All the releases before 3.2 DO NOT have this problem. The fix is to make sure the ownership of a connection gets changed when a session moves from follower to the leader. The workaround to it in 3.2.0 would be to swithc off connection from clients to the leader. take a look at *leaderServers* java property in http://hadoop.apache.org/zookeeper/docs/r3.2.0/zookeeperAdmin.html.,"[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-484,Blocker,Mahadev Konar,Fixed,2009-08-07T00:39:36.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Clients get SESSION MOVED exception when switching from follower to a leader.,2009-09-05T22:36:20.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",3.0
Benjamin Reed,[],2009-07-23T20:40:49.000+0000,ryan rawson,"here are the part of the log whereby my zookeeper instance crashed, taking 3 out of 5 down, and thus ruining the quorum for all clients:

2009-07-23 12:29:06,769 WARN org.apache.zookeeper.server.NIOServerCnxn: Exception causing close of session 0x52276d1d5161350 due to java.io.IOException: Read error
2009-07-23 12:29:00,756 WARN org.apache.zookeeper.server.quorum.Follower: Exception when following the leader
java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:375)
        at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
        at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:65)
        at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:108)
        at org.apache.zookeeper.server.quorum.Follower.readPacket(Follower.java:114)
        at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:243)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:494)
2009-07-23 12:29:06,770 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x52276d1d5161350 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.168:39489]
2009-07-23 12:29:06,770 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x12276d15dfb0578 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.159:46797]
2009-07-23 12:29:06,771 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x42276d1d3fa013e NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.153:33998]
2009-07-23 12:29:06,771 WARN org.apache.zookeeper.server.NIOServerCnxn: Exception causing close of session 0x52276d1d5160593 due to java.io.IOException: Read error
2009-07-23 12:29:06,808 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e02bb NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.158:53758]
2009-07-23 12:29:06,809 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x42276d1d3fa13e4 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.154:58681]
2009-07-23 12:29:06,809 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e691382 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.162:59967]
2009-07-23 12:29:06,809 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x12276d15dfb1354 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.163:49957]
2009-07-23 12:29:06,809 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x42276d1d3fa13cd NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.150:34212]
2009-07-23 12:29:06,809 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e691383 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.159:46813]
2009-07-23 12:29:06,809 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x12276d15dfb0350 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.162:59956]
2009-07-23 12:29:06,809 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e139b NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.156:55138]
2009-07-23 12:29:06,809 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e1398 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.167:41257]
2009-07-23 12:29:06,810 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x52276d1d5161355 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.153:34032]
2009-07-23 12:29:06,810 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x52276d1d516011c NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.155:56314]
2009-07-23 12:29:06,810 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e056b NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.155:56322]
2009-07-23 12:29:06,810 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x52276d1d516011f NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.157:49618]
2009-07-23 12:29:06,810 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e11ea NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.10.20.42:55483]
2009-07-23 12:29:06,810 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e02ba NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.157:49632]
2009-07-23 12:29:06,810 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x12276d15dfb1355 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.169:58824]
2009-07-23 12:29:06,810 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e691378 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.161:40973]
2009-07-23 12:29:06,811 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e691380 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.162:59944]
2009-07-23 12:29:06,811 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e0311 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.160:56167]
2009-07-23 12:29:06,811 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e690374 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.169:58815]
2009-07-23 12:29:06,811 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e139f NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.151:51396]
2009-07-23 12:29:06,811 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e139c NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.155:56315]
2009-07-23 12:29:06,811 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e69137b NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.162:59859]
2009-07-23 12:29:06,811 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x52276d1d5160594 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.151:51370]
2009-07-23 12:29:06,811 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e69137a NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.159:46682]
2009-07-23 12:29:06,812 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x52276d1d5160347 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.165:35722]
2009-07-23 12:29:06,812 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e69137f NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.159:46754]
2009-07-23 12:29:06,812 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x52276d1d5160121 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.155:56307]
2009-07-23 12:29:06,812 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x12276d15dfb0126 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.154:58688]
2009-07-23 12:29:06,812 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x42276d1d3fa05fc NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.152:45067]
2009-07-23 12:29:06,812 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e0316 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.169:58800]
2009-07-23 12:29:06,812 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e69137e NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.159:46737]
2009-07-23 12:29:06,813 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e69137d NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.159:46733]
2009-07-23 12:29:06,813 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x42276d1d3fa13df NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.156:55137]
2009-07-23 12:29:06,813 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x12276d15dfb134e NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.166:40443]
2009-07-23 12:29:06,813 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e691381 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.161:41086]
2009-07-23 12:29:06,813 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x52276d1d5161356 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.165:35719]
2009-07-23 12:29:06,813 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x12276d15dfb1349 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.
20.20.158:53770]
2009-07-23 12:29:06,813 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x12276d15dfb0352 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.165:35718]
2009-07-23 12:29:06,813 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e691379 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.162:59823]
2009-07-23 12:29:06,814 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x52276d1d516000e NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.150:34216]
2009-07-23 12:29:06,814 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e1397 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.169:58829]
2009-07-23 12:29:06,814 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e69137c NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.162:59862]
2009-07-23 12:29:06,814 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x42276d1d3fa0140 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.155:56271]
2009-07-23 12:29:06,814 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x42276d1d3fa13e1 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.157:49608]
2009-07-23 12:29:06,814 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e691377 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.162:59789]
2009-07-23 12:29:06,814 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x52276d1d5160593 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.165:35703]
2009-07-23 12:29:06,814 INFO org.apache.zookeeper.server.FinalRequestProcessor: shutdown of request processor complete
2009-07-23 12:29:06,814 INFO org.apache.zookeeper.server.quorum.FollowerRequestProcessor: FollowerRequestProcessor exited loop!
2009-07-23 12:29:06,814 INFO org.apache.zookeeper.server.quorum.CommitProcessor: CommitProcessor exited loop!
2009-07-23 12:29:06,815 INFO org.apache.zookeeper.server.quorum.Follower: shutdown called
java.lang.Exception: shutdown Follower
        at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:427)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:498)
2009-07-23 12:29:06,815 WARN org.apache.zookeeper.server.NIOServerCnxn: Ignoring exception
java.nio.channels.CancelledKeyException
        at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:55)
        at sun.nio.ch.SelectionKeyImpl.readyOps(SelectionKeyImpl.java:69)
        at org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:201)
2009-07-23 12:29:06,815 INFO org.apache.zookeeper.server.quorum.QuorumPeer: LOOKING
2009-07-23 12:29:06,817 WARN org.apache.zookeeper.server.NIOServerCnxn: Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
2009-07-23 12:29:06,817 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x0 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.156:55206]
2009-07-23 12:29:06,818 WARN org.apache.zookeeper.server.NIOServerCnxn: Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
2009-07-23 12:29:06,818 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x0 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.155:56331]
[elided lots of the same]
2009-07-23 12:29:33,008 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x0 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.152:5945
8]
2009-07-23 12:29:33,011 FATAL org.apache.zookeeper.server.SyncRequestProcessor: Severe unrecoverable error, exiting
java.net.SocketException: Socket closed
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:99)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at org.apache.zookeeper.server.quorum.Follower.writePacket(Follower.java:100)
        at org.apache.zookeeper.server.quorum.SendAckRequestProcessor.flush(SendAckRequestProcessor.java:52)
        at org.apache.zookeeper.server.SyncRequestProcessor.flush(SyncRequestProcessor.java:131)
        at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:76)

The good news is when I restarted the downed zookeepers, everything returned to normal.","[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-483,Major,ryan rawson,Fixed,2009-08-25T05:58:27.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"ZK fataled on me, and ugly",2009-09-05T22:36:20.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>]",4.0
Chris Darroch,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='tests', id='12312427'>]",2009-07-22T22:44:53.000+0000,Chris Darroch,"The testRetry test silently exits for me periodically, especially, it seems, on newer hardware.  It also spits out from log messages clutter the test output.

The silent exits turn out to be because SIGPIPE is sometimes delivered during the sleep(1) in createClient(), the second time createClient() is called.  Since SIGPIPE is not being ignored and there is no signal handler, the process exists immediately.  This leaves the test suite in a broken state, with the test ZooKeeper process still running because ""zkServer.sh stop"" is not run by tearDown().  You have to manually kill the ZK server and retry the tests; sometimes they succeed and sometimes they don't.

I described SIGPIPE handling a little in ZOOKEEPER-320.  The appropriate thing, I think, is for the client application to ignore or handle SIGPIPE.  In this case, that falls to the test processes.  The attached patch fixes the issue for me with testRetry.

The patch uses sigaction() to ignore SIGPIPE in TestClientRetry.cc and, for good measure (although I never saw it actually fail for me), TestClient.cc, since that file also uses sleep() extensively.

I also removed a couple of unused functions and a macro definition from TestClientRetry.cc, just to simply matters, and turned off log output, which makes the testRetry output much, much cleaner (otherwise you get a lot of log output spamming into the nice clean cppunit output :-).","[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-482,Minor,Chris Darroch,Fixed,2009-07-30T22:34:09.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ignore sigpipe in testRetry to avoid silent immediate failure,2009-09-05T22:36:20.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",2.0
Flavio Paiva Junqueira,"[<JIRA Component: name='leaderElection', id='12312378'>]",2009-07-20T13:43:29.000+0000,Flavio Paiva Junqueira,"Currently we rely on TCP for reliable delivery of FLE messages. However, as we concurrently drop and create new connections, it is possible that a message is sent but never received. With this patch, cnx manager keeps a list of last messages sent, and resends the last one sent. Receiving multiples copies is harmless. ","[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-481,Major,Flavio Paiva Junqueira,Fixed,2009-08-03T21:32:33.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Add lastMessageSent to QuorumCnxManager,2013-05-02T02:29:26.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>, <JIRA Version: name='3.2.0', id='12313491'>]",2.0
Flavio Paiva Junqueira,[],2009-07-20T13:33:29.000+0000,Flavio Paiva Junqueira,"As a server may join leader election while others have already elected a leader, it is necessary that a server handles some special cases of leader election when notifications are from servers that are either LEADING or FOLLOWING. In such special cases, we check if we have received a message from the leader to declare a leader elected. This check does not consider the case that the process performing the check might be a recently elected leader, and consequently the check fails.

This patch also adds a new case, which corresponds to adding a vote to recvset when the notification is from a process LEADING or FOLLOWING. This fixes the case raised in ZOOKEEPER-475.","[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-480,Major,Flavio Paiva Junqueira,Fixed,2009-08-04T18:07:54.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,FLE should perform leader check when node is not leading and add vote of follower,2013-05-02T02:29:26.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",2.0
Flavio Paiva Junqueira,"[<JIRA Component: name='quorum', id='12312379'>]",2009-07-20T13:00:17.000+0000,Flavio Paiva Junqueira,"QuorumHierarchical::containsQuorum should not verify if all groups represented in the input set have more than half of the total weight. Instead, it should check only for an overall majority of groups. ","[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-479,Major,Flavio Paiva Junqueira,Fixed,2009-08-03T22:30:03.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,QuorumHierarchical does not count groups correctly,2013-05-02T02:29:26.000+0000,[],2.0
Fernando Padilla,"[<JIRA Component: name='scripts', id='12312384'>]",2009-07-17T19:05:17.000+0000,Fernando Padilla,"the zkCleanup.sh script is buggy in two ways:

1) it doesn't actually pass through the snapshot count, so it doesn't work
2) it assumes that there is only dataDir, it doesn't support dataLogDir

And it can use cleanup, so that it doesn't blindly call eval from the config file..
","[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-477,Major,Fernando Padilla,Fixed,2009-08-11T00:30:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkCleanup.sh is flaky,2009-09-05T22:36:19.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",2.0
Flavio Paiva Junqueira,"[<JIRA Component: name='quorum', id='12312379'>]",2009-07-16T21:39:26.000+0000,Mahadev Konar,"THe flenewepochtest failed on one of the nightly builds -
http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/377.
","[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-475,Blocker,Mahadev Konar,Fixed,2009-08-04T18:18:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,FLENewEpochTest failed on nightly builds.,2013-05-02T02:29:26.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",2.0
Chris Darroch,"[<JIRA Component: name='c client', id='12312380'>]",2009-07-14T18:25:45.000+0000,Chris Darroch,Include unistd.h for sleep() calls in C tests to ensure successful compilation on some platforms.,"[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-470,Minor,Chris Darroch,Fixed,2009-07-20T22:55:48.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,include unistd.h for sleep() in c tests,2009-09-05T22:36:19.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",2.0
Chris Darroch,"[<JIRA Component: name='c client', id='12312380'>]",2009-07-14T18:10:28.000+0000,Chris Darroch,"When compiling with --with-cppunit-prefix, CPPUNIT_CFLAGS is set by the AM_PATH_CPPUNIT macro.  In configure.ac, it is then reset in order to set the -DZKSERVER_CMD command line argument.  Instead, that argument should be added to CPPUNIT_CFLAGS so that things like a custom -I include location set by AM_PATH_CPPUNIT are not lost.  Otherwise, a custom cppunit installation is not properly supported, despite the --with-cppunit-prefix option.","[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-469,Major,Chris Darroch,Fixed,2009-07-16T23:30:08.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,make sure CPPUNIT_CFLAGS isn't overwritten,2009-09-05T22:36:19.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",2.0
Chris Darroch,"[<JIRA Component: name='c client', id='12312380'>]",2009-07-14T17:44:34.000+0000,Chris Darroch,"Older compilers may complain that rc may be used without initialization in send_auth_info(), if -Wall is specified.  The fix is a simple initialization.","[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-468,Minor,Chris Darroch,Fixed,2009-07-16T17:42:28.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,avoid compile warning in send_auth_info(),2009-09-05T22:36:19.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",2.0
Flavio Paiva Junqueira,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2009-07-14T07:47:19.000+0000,Flavio Paiva Junqueira,"I have accidentally left a log.warn in BookieHandle. This is a pretty simple patch, so I would appreciate if we could review it quickly.","[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-467,Major,Flavio Paiva Junqueira,Fixed,2009-07-30T21:54:43.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Change log level in BookieHandle,2009-09-05T22:36:19.000+0000,[],2.0
Chris Darroch,"[<JIRA Component: name='c client', id='12312380'>]",2009-07-14T03:25:12.000+0000,Chris Darroch,"The free_auth_info() function calls deallocate_Buffer(&auth->auth) on every element in the auth list; that function frees any memory pointed to by auth->auth.buff if that field is non-NULL.

In zoo_add_auth(), when certLen is zero (or cert is NULL), auth.buff is set to 0, but then not assigned to authinfo->auth when auth.buff is NULL.  The result is uninitialized data in auth->auth.buff in free_auth_info(), and potential crashes.

The attached patch adds a test which attempts to duplicate this error; it works for me but may not always on all systems as it depends on the uninitialized data being non-zero; there's not really a simple way I can see to trigger this in the current test framework.  The patch also fixes the problem, I believe.","[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-466,Major,Chris Darroch,Fixed,2009-08-04T01:59:43.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,crash on zookeeper_close() when using auth with empty cert,2013-05-02T02:29:26.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",2.0
,"[<JIRA Component: name='tests', id='12312427'>]",2009-07-13T13:14:39.000+0000,Henry Robinson,--wrap is an unsupported command line flag for ld on Mac OS. The cppunit tests therefore won't build.,"[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-463,Minor,Henry Robinson,Duplicate,2014-04-23T20:26:08.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,C++ tests can't be built on Mac OS using XCode command line tools,2014-04-23T20:26:08.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",4.0
Mahadev Konar,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='tests', id='12312427'>]",2009-07-09T16:39:28.000+0000,Patrick D. Hunt,"the followng code failed on hudson
http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/371/

      watchctx_t ctx1, ctx2;
      zhandle_t *zk1 = createClient(&ctx1);
      CPPUNIT_ASSERT_EQUAL(true, ctx1.waitForConnected(zk1));
      zhandle_t *zk2 = createClient(&ctx2);
      zookeeper_close(zk1);
      CPPUNIT_ASSERT_EQUAL(true, ctx2.waitForConnected(zk2));

there's a problem with this test, it assumes that close(1) can be called before createclient(2) gets connected.

this is not correct: createclient is an async call an in some cases the connection can be established before
create client returns.

this shows a failure in this case because client1 was created, then client2 attempted to connect
but failed due to this on the server (max conn exceeded):
        sprintf(cmd, ""export ZKMAXCNXNS=1;%s startClean %s"", ZKSERVER_CMD, getHostPorts());

conn 2 failed and therefore the following assert eventually failed.

this code should not assume that close(1) will beat connect(2)


Henry can you take a look?

","[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-460,Major,Patrick D. Hunt,Fixed,2009-08-03T21:35:05.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,bad testRetry in cppunit tests (hudson failure),2009-09-05T22:36:19.000+0000,[],2.0
Flavio Paiva Junqueira,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2009-07-03T14:16:46.000+0000,Luca Telloli,"I've experienced by accident the following behavior: 
- created a ledger with a password
- opened a ledger with a different password 

No exception was raised, the openLedger(...) would return correctly but containing 0 entries ",[],Bug,ZOOKEEPER-459,Major,Luca Telloli,Not A Problem,2010-12-18T15:30:58.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Opening a ledger with wrong password doesn't raise an exception,2010-12-18T15:30:58.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>]",0.0
Mahadev Konar,"[<JIRA Component: name='c client', id='12312380'>]",2009-07-03T00:05:41.000+0000,Mahadev Konar,"connect_index in zookeeper handle might get out of bound. the zokoeeper_init method checks for index == count and sets it to zero. If the index becomes greater than count, then it will go out of bounds.",[],Bug,ZOOKEEPER-458,Major,Mahadev Konar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,connect_index in zookeeper handle might get out of bound.,2022-02-03T08:50:20.000+0000,[],1.0
ryan rawson,[],2009-07-01T18:58:04.000+0000,ryan rawson,"hi folks, we have made some changes to zookeeper to facilitate providing an embedded zk client in our own hbase client.  This will allow our users to use 1 shell to manipulate both hbase things and zookeeper things.  It requires making a few things public, and in the process rearranging how some static things are initialized.  

It's fairly trivial refactoring, hopefully you guys approve!

Thanks!","[<JIRA Version: name='3.2.1', id='12314068'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-457,Major,ryan rawson,Fixed,2009-07-31T20:32:59.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Make ZookeeperMain public, support for HBase (and other) embedded clients",2009-09-05T22:36:19.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>]",3.0
Mahadev Konar,[],2009-06-30T23:44:02.000+0000,Mahadev Konar,"the zookeeper c client crashes with chroot specified in the string. This does not fail on 2.6 but on ubuntu where the malloced memory is nto initialized to \0.
","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-455,Blocker,Mahadev Konar,Fixed,2009-07-01T16:39:51.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper c client crashes with chroot specified in the string.,2009-07-08T20:24:06.000+0000,[],1.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2009-06-30T20:53:41.000+0000,Patrick D. Hunt,"we no longer officially support jdk1.5 however it still compiles -- except for a recent @override that was added for an interface method. jdk1.6
allows this but jdk1.5 does not (must be superclass). small patch will address this. 3.3 we will drop 1.5 concerns entirely but for now...","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-454,Minor,Patrick D. Hunt,Fixed,2009-06-30T20:57:08.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,allow compilation with jdk1.5,2009-07-08T20:24:06.000+0000,[],1.0
Flavio Paiva Junqueira,[],2009-06-30T16:25:34.000+0000,Flavio Paiva Junqueira,This causes a problem when we crash and restart a replica R because other replicas believe that they still have a connection to R after restart.,"[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-453,Blocker,Flavio Paiva Junqueira,Fixed,2009-06-30T20:47:21.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Worker is not removed in QuorumCnxManager upon crash,2009-07-08T20:24:06.000+0000,[],1.0
Mahadev Konar,[],2009-06-30T01:17:21.000+0000,Mahadev Konar,THe image is wrong and should have percentage of reads on the x axis.. ,"[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-452,Major,Mahadev Konar,Fixed,2009-06-30T01:29:40.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper performance graph should have percentage of reads rather than percentage of writes -  zkperfRW-3.2.jpg,2009-07-08T20:24:06.000+0000,[],1.0
Benjamin Reed,[],2009-06-29T15:29:01.000+0000,Benjamin Reed,"The session move patch broke ephemeral cleanup during session expiration. tragically, we didn't have test coverage to detect the bug.","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-450,Blocker,Benjamin Reed,Fixed,2009-06-30T01:32:40.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,emphemeral cleanup not happening with session timeout,2009-07-08T20:24:06.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",1.0
Mahadev Konar,[],2009-06-26T20:36:25.000+0000,Mahadev Konar,sesssionmoved in java code and ZCLOSING in C have the same value. We need to assign a new value to ZSESSIONMOVED.,"[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-449,Major,Mahadev Konar,Fixed,2009-06-26T21:16:14.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,sesssionmoved in java code and ZCLOSING in C have the same value.,2009-07-08T20:24:06.000+0000,[],1.0
Mahadev Konar,[],2009-06-26T18:39:41.000+0000,Mahadev Konar,png images are not compatible with forrest generating pdf. We can them to jpg to get them into pdfs.,"[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-448,Major,Mahadev Konar,Fixed,2009-06-26T18:51:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,png files do nto work with forrest.,2009-07-08T20:24:06.000+0000,[],1.0
Benjamin Reed,[],2009-06-19T22:44:19.000+0000,Benjamin Reed,"the host auth scheme was removed because it used a blocking call in an async pipeline. however, tragically, the blocking call was not removed including a couple of other stray classes.","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-446,Major,Benjamin Reed,Fixed,2009-06-22T20:55:03.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,some traces of the host auth scheme left,2009-07-08T20:24:05.000+0000,[],1.0
,"[<JIRA Component: name='server', id='12312382'>]",2009-06-18T16:06:38.000+0000,Manos Kapritsos,"There is a suspicious line in server/quorum/Leader.java:226. It reads
if (stop) {
LOG.info(""exception while shutting down acceptor: "" + e);
stop = true;
}
","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-445,Minor,Manos Kapritsos,Invalid,2014-04-23T20:30:10.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Potential bug in leader code,2014-04-23T20:30:10.000+0000,[],2.0
Mahadev Konar,[],2009-06-15T18:48:24.000+0000,Mahadev Konar,the perms_all definition in Java is PERMS.ALL and does not include ADMIN perms but in c the PERMS_ALL def includes the ADMIN perms. We should make it consistent to include or not include the admin perms in both c and java.,"[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-444,Blocker,Mahadev Konar,Fixed,2009-06-17T17:37:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,perms definition for PERMS_ALL differ in C and java,2009-07-08T20:24:05.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>]",1.0
Henry Robinson,[],2009-06-09T11:32:58.000+0000,Henry Robinson,The latest version of trunk has a src/c/tests/TestClientRetry.cc file that has the actual file from ZK-336 appended to itself. This causes the compilation to fail due to lots of redeclaration errors. ,[],Bug,ZOOKEEPER-441,Blocker,Henry Robinson,Fixed,2009-06-09T16:46:47.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Zk-336 diff got applied twice to TestClientRetry.cc C test, causing compilation failure",2010-03-26T17:31:23.000+0000,[],2.0
Benjamin Reed,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>]",2009-06-08T18:04:40.000+0000,Patrick D. Hunt,"if addauth is called on a new client connection that's never connected to the server, when the client does connect
(syncconnected) the auth is not passed to the server. we should ensure we addauth when the client connects or reconnects","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-438,Blocker,Patrick D. Hunt,Fixed,2009-06-25T20:42:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,addauth fails to register auth on new client that's not yet connected,2009-07-08T20:24:05.000+0000,[],1.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2009-06-05T20:42:24.000+0000,Patrick D. Hunt,"the server has a ""super"" digest based auth user that enables administrative access (ie has access to znodes regardless
of acl settings) but the password is not configurable

1) make the default digest null, ie turn off ""super"" by default
2) if a command line option is specified when starting server then use the provided digest for super

eg. java -Dzookeeper.DigestAuthenticationProvider.superDigest=xkxkxkxkx ....

also this is not documented in the forrest docs - need to add that along with tests as part of the patch.
","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-435,Critical,Patrick D. Hunt,Fixed,2009-06-09T05:21:48.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"allow ""super"" admin digest based auth to be configurable",2009-07-08T20:24:05.000+0000,[],1.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2009-06-05T17:57:28.000+0000,Patrick D. Hunt,"running a ""getAcl"" on the root znode ""/"" fails with the following:

Fri Jun  5 10:21:17 2009: 2009-06-05 10:21:17,072 - ERROR [CommitProcessor:3:FinalRequestProcessor@243] - Failed to process sessionid:0x321b16868f40003 ty
pe:getACL cxid:0x3 zxid:0xfffffffffffffffe txntype:unknown n/a 
Fri Jun  5 10:21:17 2009: java.lang.NullPointerException
Fri Jun  5 10:21:17 2009:       at java.util.ArrayList.<init>(ArrayList.java:131)
Fri Jun  5 10:21:17 2009:       at org.apache.zookeeper.server.DataTree.getACL(DataTree.java:622)
Fri Jun  5 10:21:17 2009:       at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:216)
Fri Jun  5 10:21:17 2009:       at org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:74)
Fri Jun  5 10:21:17 2009: 2009-06-05 10:21:17,073 - ERROR [CommitProcessor:3:FinalRequestProcessor@250] - Dumping request buffer: 0x00012f


We need to support getting/setting the root acl in particular -- not being able to control acls on this node makes multi-tenancy a non-starter.
","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-433,Blocker,Patrick D. Hunt,Fixed,2009-06-18T01:09:34.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,getacl on root znode (/) fails,2009-07-08T20:24:05.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>]",1.0
,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2009-06-03T16:41:06.000+0000,Patrick D. Hunt,"currently when a node is set > 1mb (default) the server disconnects the client and logs information about node too large.
however the client has no indication why the connection was killed. we need to improve the error reporting here.
(might be a good time to review error reporting in general)",[],Bug,ZOOKEEPER-430,Minor,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,improve error reporting to client when setdata data length > max (default 1mb),2022-02-03T08:50:16.000+0000,[],0.0
,[],2009-06-03T07:14:52.000+0000,Flavio Paiva Junqueira,"I was checking the console output of:

http://hudson.zones.apache.org/hudson/job/Zookeeper-Patch-vesta.apache.org/100/

and I observed the following output for the contrib tests:
{noformat}
     [exec] ======================================================================
     [exec] ======================================================================
     [exec]     Running contrib tests.
     [exec] ======================================================================
     [exec] ======================================================================
     [exec] 
     [exec] 
     [exec] Warning: bad ps syntax, perhaps a bogus '-'? See http://procps.sf.net/faq.html
     [exec] /bin/kill -9 30147 
     [exec] kill: No such process
     [exec] /home/hudson/tools/ant/latest/bin/ant  -Declipse.home=/home/nigel/tools/eclipse/latest -Dpython.home=/home/nigel/tools/python/latest -DZooKeeperPatchProcess= -Dtest.junit.output.format=xml -Dtest.output=yes test-contrib
     [exec] Buildfile: build.xml
     [exec] 
     [exec] test-contrib:
     [exec] 
     [exec] BUILD SUCCESSFUL
     [exec] Total time: 0 seconds
{noformat}",[],Bug,ZOOKEEPER-429,Major,Flavio Paiva Junqueira,Invalid,2009-06-03T11:31:24.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Python contrib test failing on hudson,2009-06-03T11:31:24.000+0000,[],0.0
Flavio Paiva Junqueira,[],2009-06-02T21:01:41.000+0000,Satish Kotha,"I am running a 5 node ZooKeeper cluster and I noticed that one of them has very high CPU usage:

 PID   USER      PR  NI  VIRT  RES  SHR S   %CPU %MEM    TIME+   COMMAND 
 6883  infact       22   0   725m  41m  4188 S   95       0.5          5671:54  java

It is not ""doing anything"" application-wise at this point, so I was wondering why the heck it's using up so much CPU.","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-427,Blocker,Satish Kotha,Fixed,2009-06-18T20:33:45.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeper server unexpectedly high CPU utilisation,2010-10-11T07:34:54.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>]",1.0
Henry Robinson,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2009-05-28T18:39:39.000+0000,Patrick D. Hunt,"the scripts in the test directory of zkpython are missing #! headers

Probably:

#!/bin/sh

for shell scripts and 

#!/usr/bin/python

for .py scripts?

Also include a shell script that will svn chmod the *.py scripts so that they can be executed individually from the command line (shortcut
rather than (python foo.py).
","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-421,Minor,Patrick D. Hunt,Fixed,2009-06-01T21:00:43.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkpython run_tests.sh is missing #!,2009-07-08T20:24:05.000+0000,[],1.0
Henry Robinson,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2009-05-28T18:36:00.000+0000,Patrick D. Hunt,"Currently you cannot just build and test the zkpython contrib, you need to actually install the zookeeper client c library as well
as the zkpython lib itself.

There really needs to be 2 steps:

1) build/test zkpython ""encapsulated"" within the src repository, there should be no requirement to actually install anything
(this is esp the case for automated processes and for review by PMC during release time for example)
2) build an egg that can be distributed/installed by end user
","[<JIRA Version: name='3.2.2', id='12314335'>, <JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-420,Major,Patrick D. Hunt,Fixed,2009-10-01T21:38:07.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,build/test should not require install in zkpython,2010-03-26T17:24:54.000+0000,[],4.0
Henry Robinson,"[<JIRA Component: name='contrib-bindings', id='12312860'>]",2009-05-28T00:24:09.000+0000,Henry Robinson,"Due to reference counts being incremented incorrectly for stat-based calls, Python's GC occasionally aborts.","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-419,Critical,Henry Robinson,Fixed,2009-05-28T18:31:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Reference counting bug in Python bindings causes abort errors,2009-07-08T20:24:04.000+0000,[],3.0
Ted Dunning,[],2009-05-27T07:10:50.000+0000,Ted Dunning,"It would be very nice to have a browser that would allow the state of a Zoo to be examined.  Even nice would be such a utility that showed changes in real time.

","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-418,Major,Ted Dunning,Won't Fix,2010-12-13T01:03:09.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Need nifty zookeeper browser,2011-11-23T19:22:13.000+0000,[],4.0
Benjamin Reed,[],2009-05-26T18:10:27.000+0000,Benjamin Reed,"There is  a possibility for stray messages from a previous connection to violate ordering and generally cause problems. Here is a scenario: we have a client, C, two followers, F1 and F2, and a leader, L. The client is connected to F1, which is a slow follower. C sends setData(""/a"", ""1"") to F1 and then loses the connection, so C reconnects to F2 and sends setData(""/a"", ""2"").  it is possible, if F1 is slow enough and the setData(""/a"", ""1"") got onto the network before the connection break, for F1 to forward the setData(""/a"", ""1"") to L after F2 forwards setData(""/a"", ""2"").

to fix this, the leader should keep track of which follower last registered a session for a client and drop any requests from followers for clients for whom they do not have a registration. ","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-417,Blocker,Benjamin Reed,Fixed,2009-06-26T21:09:06.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,stray message problem when changing servers,2009-07-08T20:24:04.000+0000,[],1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2009-05-26T15:51:26.000+0000,Flavio Paiva Junqueira,"I was checking the bookkeper jar, and I found that it includes some unnecessary files related to junit, such as:

{noformat}
     0 Tue May 12 19:00:14 PDT 2009 tmp/
     0 Tue May 12 19:00:00 PDT 2009 tmp/test14667.junit.dir/
     0 Tue May 12 19:00:08 PDT 2009 tmp/test14667.junit.dir/version-2/
     0 Tue May 12 19:00:10 PDT 2009 tmp/test16109.junit.dir/
     0 Tue May 12 19:00:16 PDT 2009 tmp/test16109.junit.dir/version-2/
     0 Tue May 12 19:00:14 PDT 2009 tmp/test16113.junit.dir/
     0 Tue May 12 19:00:16 PDT 2009 tmp/test16113.junit.dir/version-2/
  2256 Tue May 12 18:59:08 PDT 2009 logs/TEST-org.apache.bookkeeper.test.BookieClientTest.txt
167046 Tue May 12 19:00:00 PDT 2009 logs/TEST-org.apache.bookkeeper.test.BookieReadWriteTest.txt
 13035 Tue May 12 19:00:08 PDT 2009 logs/TEST-org.apache.bookkeeper.test.CloseTest.txt
 25036 Tue May 12 19:00:16 PDT 2009 logs/TEST-org.apache.bookkeeper.test.LedgerRecoveryTest.txt
   896 Tue May 12 19:00:16 PDT 2009 logs/TEST-org.apache.bookkeeper.test.NIOServerFactoryTest.txt
{noformat}","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-416,Major,Flavio Paiva Junqueira,Fixed,2009-05-28T04:55:48.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,BookKeeper jar includes unnecessary files,2009-07-08T20:24:04.000+0000,[],1.0
Mahadev Konar,[],2009-05-21T20:29:44.000+0000,Mahadev Konar,the c tests hang sometimes. ,"[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-415,Major,Mahadev Konar,Fixed,2009-05-22T07:25:56.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper c tests hang.,2009-07-08T20:24:04.000+0000,[],2.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='tests', id='12312427'>]",2009-05-20T21:59:15.000+0000,Patrick D. Hunt,"1) createClient in testclient.cc (check all tests) is not correctly waiting for syncconnected to the server
2) there are some instances of while(xxx); in the test code, this could cause problems, really we need to
have some limit on the number of iterations (other than just the test, which may never return false), also the
loop should have some sort of sleep(100msec) (whatever time) in order to limit cpu use.
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-413,Minor,Patrick D. Hunt,Fixed,2010-01-26T02:57:04.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,two flaws need addressing in the c tests that can cause false positive failures,2010-03-26T17:24:54.000+0000,[],2.0
Akihiro Suda,[],2009-05-20T08:35:37.000+0000,Giridharan Kesavan,"BUILD FAILED
/home/hudson/hudson-slave/workspace/ZooKeeper-trunk/trunk/build.xml:865: Unable to create a Checker: cannot initialize module PackageHtml - Unable to instantiate PackageHtml
Tnx!

","[<JIRA Version: name='3.5.2', id='12331981'>]",Bug,ZOOKEEPER-412,Major,Giridharan Kesavan,Fixed,2015-12-15T19:17:57.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,checkstyle target fails trunk build,2016-07-21T20:18:36.000+0000,[],7.0
Mahadev Konar,[],2009-05-19T22:39:10.000+0000,Lee Tucker,"     [exec] Zookeeper_simpleSystem::testAsyncWatcherAutoReset : assertion
     [exec] 
     [exec] /grid/0/gs/gridre/hudson/workspace/zootestbuild/trunk/src/c/tests/TestClient.cc:499: Assertion: assertion failed [Expression: ctx.waitForDisconnected(zk)]
     [exec] Failures !!!
     [exec] Run: 32   Failure total: 1   Failures: 1   Errors: 0
     [exec] make: *** [run-check] Error 1
","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-411,Major,Lee Tucker,Fixed,2009-05-20T22:41:43.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Building zookeeper fails on RHEL 5 64 bit during test-cppunit,2009-07-08T20:24:04.000+0000,[],1.0
Mahadev Konar,[],2009-05-18T17:24:02.000+0000,Mahadev Konar,The java client shell does not handle null return data and throws out null pointer exception.,"[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-405,Major,Mahadev Konar,Fixed,2009-05-20T17:59:28.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,nullpointer exception in zookeeper java shell.,2009-07-08T20:24:04.000+0000,[],1.0
Henry Robinson,[],2009-05-18T16:50:10.000+0000,Mahadev Konar,"the nightly build failed with the following error

compile:
     [echo] contrib: zkpython

BUILD FAILED
/home/hudson/hudson-slave/workspace/ZooKeeper-trunk/trunk/build.xml:444: The following error occurred while executing this line:
/home/hudson/hudson-slave/workspace/ZooKeeper-trunk/trunk/src/contrib/build.xml:39: The following error occurred while executing this line:
/home/hudson/hudson-slave/workspace/ZooKeeper-trunk/trunk/src/contrib/build-contrib.xml:79: srcdir ""/home/hudson/hudson-slave/workspace/ZooKeeper-trunk/trunk/src/contrib/zkpython/src/java"" does not exist!

Total time: 32 seconds
Publishing Javadoc
Recording test results
Recording fingerprints
Publishing Clover coverage report...
Sending e-mails to: zookeeper-dev@hadoop.apache.org
finished: FAILURE
","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-404,Critical,Mahadev Konar,Fixed,2009-05-19T21:27:46.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,nightly build failed on hudson.,2009-07-08T20:24:04.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",1.0
Mahadev Konar,[],2009-05-13T22:36:40.000+0000,Mahadev Konar,the zookeeper c client library seg faults on data being null for a zoo node. ,"[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-402,Critical,Mahadev Konar,Fixed,2009-05-21T21:22:46.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper c library segfaults on data for a node in zookeeper being null.,2009-07-08T20:24:03.000+0000,"[<JIRA Version: name='3.1.0', id='12313381'>, <JIRA Version: name='3.1.1', id='12313649'>]",2.0
,[],2009-05-12T21:35:29.000+0000,Konstantin I Boudnik,"During the conversion of mainline tests from JUnit into TestNG it has been noticed (partially because TestNG doesn't intercept tests output) that some of log4j configuration parameters aren't very handy. E.g.
 - some of really non-important messages go at WARN level
 - by default all output goes into ConsoleAppender, which is a lot and makes it difficult to trace the execution from simply observing the console output
 - it'd be good to have a couple different log4j configs to be able to turn ConsoleAppender on and off by simply passing a parameter to ant build",[],Bug,ZOOKEEPER-401,Minor,Konstantin I Boudnik,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Cleaning of logging in the code and log4j configuration might be a good idea,2009-05-12T21:35:29.000+0000,[],0.0
Flavio Paiva Junqueira,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2009-05-12T21:26:25.000+0000,Flavio Paiva Junqueira,"When running a few experiments with BookKeeper, I found a couple of issues with closing a ledger:

* On ClientCBWorker::run(), we should call setAddConfirmed before invoking the callback method. Otherwise, it is possible that an application closes a ledger before ClientCBWorker modifies the last confirmed operation, and the value written to ZooKeeper won't be the last one written;
* LedgerHandle should write the last add confirmed instead of the last counter. The last attribute counts the operations issued, and we use it to determine the id of the next entry. If an application calls close upon receiving all callbacks, then with the previous modification, the last confirmed add must be equal to (last-1). However, if an application invokes close before receiving all callbacks, the ledger may be left in an inconsistent state because the last entry written to ZooKeeper may be an operation that hasn't completed yet. 

Although the modifications required are simple, they are important.","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-400,Critical,Flavio Paiva Junqueira,Fixed,2009-05-14T08:22:07.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Issues with procedure to close ledger,2013-05-02T02:29:24.000+0000,[],1.0
Mahadev Konar,"[<JIRA Component: name='c client', id='12312380'>]",2009-05-09T00:41:05.000+0000,Mahadev Konar,"There is a race condition in zoopkeeper client library wherein if the application calls zookeeper_close() and zoo_anysynchronouscall simultaneously, sometimes the zoo_sync api call gets hung waiting for a notification whcih will never come.
We might want to create another bugfix release for this.","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-396,Critical,Mahadev Konar,Fixed,2009-06-08T20:39:31.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,race condition in zookeeper client library between zookeeper_close and zoo_synchronous api,2009-07-08T20:24:03.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>]",1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2009-05-06T18:25:18.000+0000,Patrick D. Hunt,"Bookeeper mainline code is calling printStackTrace, it should be using logging instead.","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-391,Major,Patrick D. Hunt,Fixed,2009-05-12T21:00:00.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,bookeeper mainline code should not be calling printStackTrace,2009-07-08T20:24:03.000+0000,[],1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2009-04-29T08:32:49.000+0000,Flavio Paiva Junqueira,"We had initially assumed one ledger open at a time, and LedgerRecoverMonitor reads and closes a ledger without instantiating a QuorumEngine. Consequently, when closing a ledger, we have to shut down a QuorumEngine only if there is one. A poor way of implementing it is by having a for loop that iterates over all available QuorumEngines and shut them down. Given the assumptions of one ledger at a time and no QuorumEngine for LedgerRecoveryMonitor, this implementation works. However, as we move to a more flexible model in which multiple ledgers can be open and closed at arbitrary times, the for loop choice no longer works. 

The attached patch fixes this problem by just shutting down the QuorumEngine instance corresponding to the ledger being closed, and it checks if a QuorumEngine exists before submitting a STOP operation. The patch also includes a test that opens four ledger, and then sequentially writes to the each ledger and closes it.",[],Bug,ZOOKEEPER-387,Major,Flavio Paiva Junqueira,Duplicate,2009-05-04T15:02:34.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,closeLedger shouldn't shut down all QuorumEngines,2009-05-04T15:02:34.000+0000,[],0.0
Mahadev Konar,"[<JIRA Component: name='server', id='12312382'>, <JIRA Component: name='tests', id='12312427'>]",2009-04-27T22:38:07.000+0000,Patrick D. Hunt,"http://hudson.zones.apache.org/hudson/job/Zookeeper-Patch-vesta.apache.org/53/testReport/org.apache.zookeeper.server/CRCTest/testChecksums/

crctest failed with

Error Message

Unreasonable length = 518291091

Stacktrace

java.io.IOException: Unreasonable length = 518291091
	at org.apache.jute.BinaryInputArchive.readBuffer(BinaryInputArchive.java:101)
	at org.apache.zookeeper.server.DataNode.deserialize(DataNode.java:116)
	at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:109)
	at org.apache.zookeeper.server.DataTree.deserialize(DataTree.java:954)
	at org.apache.zookeeper.server.util.SerializeUtils.deserializeSnapshot(SerializeUtils.java:91)
	at org.apache.zookeeper.server.persistence.FileSnap.deserialize(FileSnap.java:125)
	at org.apache.zookeeper.server.CRCTest.testChecksums(CRCTest.java:146)
","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-385,Critical,Patrick D. Hunt,Fixed,2009-05-22T07:43:59.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,crctest failed on hudson patch test,2009-07-08T20:24:03.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='java client', id='12312381'>]",2009-04-27T20:55:46.000+0000,Patrick D. Hunt,"keeper exceptions thrown by the java client don't include path, having path helps in debugging.
","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-384,Minor,Patrick D. Hunt,Fixed,2009-05-05T04:55:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,keeper exceptions missing path,2009-07-08T20:24:03.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>, <JIRA Version: name='3.1.1', id='12313649'>]",1.0
Mahadev Konar,[],2009-04-27T18:30:04.000+0000,Mahadev Konar,"the zookeeper c tests fail on 64 bit machines with gcc 4.1.2 with the following error

 [exec] /workspace/zootestbuild/trunk/src/c/tests/TestClient.cc: In static member function 'static void Zookeeper_simpleSystem::statCompletion(int, const Stat*, const void*)':
      [exec] /workspace/zootestbuild/trunk/src/c/tests/TestClient.cc:273: error: cast from 'const void*' to 'int' loses precision
      [exec]/workspace/zootestbuild/trunk/src/c/tests/TestClient.cc: In static member function 'static void Zookeeper_simpleSystem::voidCompletion(int, const void*)':
      [exec] /workspace/zootestbuild/trunk/src/c/tests/TestClient.cc:291: error: cast from 'const void*' to 'int' loses precision
  ","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-382,Major,Mahadev Konar,Fixed,2009-04-29T19:12:04.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper cpp tests fails on 64 bit machines with gcc 4.1.2,2009-07-08T20:24:03.000+0000,[],1.0
Mahadev Konar,"[<JIRA Component: name='tests', id='12312427'>]",2009-04-21T06:30:02.000+0000,Giridharan Kesavan,"patch test build failed with "" equality assertion failed""
link: http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/Zookeeper-Patch-vesta.apache.org/44/console

Mahadev,
Could you take a look?
Tnx!","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-379,Major,Giridharan Kesavan,Invalid,2009-05-22T00:27:44.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,cppunit tests failed during hudson patch tests ,2009-07-08T20:24:03.000+0000,[],1.0
Giridharan Kesavan,[],2009-04-17T18:14:36.000+0000,Patrick D. Hunt,"Giri, can you take a look at this?

I ran ""ant test""

looks like this test failed:
     [exec] Zookeeper_operations::testOperationsAndDisconnectConcurrently1 : assertion
 
then later...

     [exec] 
     [exec] /home/phunt/dev/workspace/svnzk_apache/src/c/tests/TestOperations.cc:551: Assertion: equality assertion failed [Expected: -4, Actual  : 0, ZCONNECTIONLOSS != rc]
     [exec] Failures !!!
     [exec] Run: 38   Failure total: 1   Failures: 1   Errors: 0
     [exec] make: *** [run-check] Error 1
     [exec] Result: 2

test-core:

test-contrib:

BUILD SUCCESSFUL
Total time: 15 minutes 39 seconds
","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-377,Critical,Patrick D. Hunt,Fixed,2009-04-20T22:16:43.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"running ant cppunit tests, a failure still results in BUILD SUCCESSFUL",2009-07-08T20:24:03.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='build', id='12312383'>, <JIRA Component: name='c client', id='12312380'>]",2009-04-15T22:10:59.000+0000,Patrick D. Hunt,"Giri, can you take a look at this:

1) looks like autoreconf is always run, which means that a re-configure/re-make is run each time the tests are run using ant

2) tabs were introduced to the build when cppunit changes were made, please only use spaces (would be great if you could fix this too -- hard to read in my editor)

Thanks!
","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-376,Minor,Patrick D. Hunt,Fixed,2009-05-07T22:38:26.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ant test target re-compiles cppunit code every time,2009-07-08T20:24:03.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",1.0
Mahadev Konar,"[<JIRA Component: name='c client', id='12312380'>]",2009-04-15T16:03:31.000+0000,Patrick D. Hunt,"zoo_add_auth doesn't maintain a list of auths - it only stores the most recent auth send to the server. As a result on re-sync to the cluster it will lose (not reregister) any auths prior to the most recent.

This code should maintain a list of auths similar to the java code. Be sure to free the memory in close.

","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-375,Blocker,Patrick D. Hunt,Fixed,2009-06-09T19:01:20.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zoo_add_auth only retains most recent auth on re-sync,2009-07-08T20:24:02.000+0000,"[<JIRA Version: name='3.1.0', id='12313381'>, <JIRA Version: name='3.1.1', id='12313649'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>]",2009-04-10T09:28:43.000+0000,Nitay Joffe,"nitay-joffes-macbook-pro:c nitay$ pwd
/Users/nitay/code/zookeeper/src/c

nitay-joffes-macbook-pro:c nitay$ make
make  all-am
/bin/sh ./libtool --tag=CC   --mode=compile gcc -DHAVE_CONFIG_H -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O2 -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c -o zookeeper.lo `test -f 'src/zookeeper.c' || echo './'`src/zookeeper.c
libtool: compile:  gcc -DHAVE_CONFIG_H -I. -I./include -I./tests -I./generated -Wall -Werror -g -O2 -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c src/zookeeper.c  -fno-common -DPIC -o .libs/zookeeper.o
cc1: warnings being treated as errors
src/zookeeper.c: In function 'zoo_add_auth':
src/zookeeper.c:2378: warning: 'auth.buff' may be used uninitialized in this function
src/zookeeper.c:2378: warning: 'auth.len' may be used uninitialized in this function
make[1]: *** [zookeeper.lo] Error 1
make: *** [all] Error 2


Need to set auth.buff and auth.len to zero.","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-374,Trivial,Nitay Joffe,Fixed,2009-04-16T20:28:46.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Uninitialized struct variable in C causes warning which is treated as an error,2009-07-08T20:24:02.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>]",1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='leaderElection', id='12312378'>]",2009-04-07T21:26:40.000+0000,Flavio Paiva Junqueira,There are a couple of problems pointed out by findbugs and that we can easily fix.,"[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-370,Major,Flavio Paiva Junqueira,Fixed,2009-04-08T21:10:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Fix critical problems reported by findbugs,2009-07-08T20:24:02.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>]",1.0
Mahadev Konar,"[<JIRA Component: name='server', id='12312382'>]",2009-04-04T00:29:32.000+0000,Patrick D. Hunt,during local testing I received the attached recoverytest failure,"[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-367,Critical,Patrick D. Hunt,Fixed,2009-04-17T18:12:25.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"RecoveryTest failure - ""unreasonable length"" IOException",2009-07-08T20:24:02.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",1.0
Benjamin Reed,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2009-04-03T22:12:24.000+0000,Benjamin Reed,"the leader tracks session expirations by calculating when a session will timeout and then periodically checking to see what needs to be timed out based on the current time. this works great as long as the leaders clock progresses at a steady pace. the problem comes when there are big (session size) changes in clock, by ntp for example. if time gets adjusted forward, all the sessions could timeout immediately. if time goes backward sessions that should timeout may take a lot longer to actually expire.

this is really just a leader issue. the easiest way to deal with this is to have the leader relinquish leadership if it detects a big jump forward in time. when a new leader gets elected, it will recalculate timeouts of active sessions.","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-366,Major,Benjamin Reed,Duplicate,2014-04-23T20:46:35.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Session timeout detection can go wrong if the leader system time changes,2014-04-23T20:46:35.000+0000,[],3.0
Flavio Paiva Junqueira,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2009-04-03T21:56:01.000+0000,Patrick D. Hunt,"Note: the javadoc is wrong here:

    /**
     * Returns the last entry identifier submitted and increments it.
     * @return long
     */
    long setLast(long last){

also would be great to have javadoc for the legerrecoverymonitor getNextHint method. I was reviewing this code and it would have been helpful to know what to expect of this method. (possible return values, etc...)","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-365,Minor,Patrick D. Hunt,Fixed,2009-04-29T19:16:57.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,javadoc is wrong for setLast in LedgerHandle,2009-07-08T20:24:02.000+0000,"[<JIRA Version: name='3.2.0', id='12313491'>]",1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2009-04-03T11:08:05.000+0000,Luca Telloli,"When recovering a ledger, LedgerRecoveryMonitor currently start from the entry preceding the hint. if the hint is zero, then it causes an access out of the bounds of the bookie array in QuorumEngine, leading to the mentioned NPE.
","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-363,Major,Luca Telloli,Fixed,2009-04-07T02:43:59.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,NPE when recovering ledger with no hint ,2009-07-08T20:24:02.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>]",1.0
Flavio Paiva Junqueira,[],2009-04-03T09:04:29.000+0000,Flavio Paiva Junqueira,"I have been able to identify two reasons that cause FLENewEpochTest to fail:

1- There is a race condition that is triggered when two peers try to establish a connection to each other for leader election. Basically, if they start roughly at the same time, the server with highest id will try to open two connections. The two competing connections will lead to one notification message to be lost. This message happens to be critical for this two process scenario; 
2- The code to shut down a peer is not working well with the unit tests. For this particular unit test, we need to be able to shut down a peer completely to check the situation the test tries to reproduce. However, it seems that in some runs timing causes the other peers to believe it is still alive, and end up electing it. This peer, however, eventually shuts down and leader election fails.","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-362,Major,Flavio Paiva Junqueira,Fixed,2009-04-03T22:12:11.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Issues with FLENewEpochTest,2009-07-08T20:24:02.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>]",1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2009-04-02T16:32:07.000+0000,Flavio Paiva Junqueira,We need a strong reference to prevent a key in masterKeys on Bookie.java to be garbage collected.,"[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-360,Major,Flavio Paiva Junqueira,Fixed,2009-04-03T21:43:56.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,WeakHashMap in Bookie.java causes NPE,2009-07-08T20:24:02.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>]",1.0
,[],2009-04-01T05:02:00.000+0000,Benjamin Reed,"Last week I setup an unattended ZooKeeper cluster that I expected to be able to take care of things while I was on vacation. I used 7 dedicated servers with two disks. I had done extensive burn in testing of the servers, so I fully expected the system to work problem free for the entire week. Indeed, when I returned yesterday none of the 7 servers had any hardware or software problem the entire time nor had there been any network problems. On the other hand the Zoo was in complete chaos. Even though there were no errors in the ZooKeeper log most of the animals were near starvation, except for the lions who had gotten loose and eaten some of the pigs. I'll not even go into the stench from the dirty cages!

Either the documentation needs to more clearly explain how to configure the server properly in the Zoo environment, or it should clearly state that ZooKeeper cannot take care of the Zoo out of the box!",[],Bug,ZOOKEEPER-357,Critical,Benjamin Reed,Invalid,2009-04-02T16:20:09.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,ZooKeeper cannot actually take care of a Zoo,2009-04-02T16:20:09.000+0000,[],0.0
Patrick D. Hunt,[],2009-03-27T05:09:51.000+0000,Mahadev Konar,make  validatePath non public in Zookeeper client api.,"[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-355,Major,Mahadev Konar,Fixed,2009-04-09T21:03:07.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,make  validatePath non public in Zookeeper client api.,2009-07-08T20:24:01.000+0000,"[<JIRA Version: name='3.1.0', id='12313381'>, <JIRA Version: name='3.1.1', id='12313649'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>]",2009-03-26T19:11:45.000+0000,Jeff Terrace,"In 3.0.1, I could create a sequence node like this:
/nodes/0000001
like this:
string path = ""/nodes/"";
string value = ""data"";
int rc = zoo_acreate(zh, path.c_str(), value.c_str(), value.length(), &ZOO_OPEN_ACL_UNSAFE, ZOO_EPHEMERAL | ZOO_SEQUENCE, &czoo_created, &where);

In 3.1.1, this fails with error -8 (ZBADARGUMENTS).

Adding something after the ""/"" in the path makes the code work fine:
string path = ""/nodes/n"";

I assume something is checking if the path ends in ""/"" but not checking the sequence flag.","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-348,Minor,Jeff Terrace,Fixed,2009-04-09T21:04:17.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Creating node with path ending in ""/"" with sequence flag set",2009-07-08T20:24:01.000+0000,"[<JIRA Version: name='3.1.0', id='12313381'>, <JIRA Version: name='3.1.1', id='12313649'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='contrib-zkfuse', id='12312644'>]",2009-03-26T00:13:38.000+0000,Nitay Joffe,"I'm getting an error compiling zkfuse:

g++ -DHAVE_CONFIG_H -I. -I..    -I/home/nitay/code/zookeeper-git/src/contrib/zkfuse/../../c/include -I/home/nitay/code/zookeeper-git/src/contrib/zkfuse/../../c/generated -I../include -I/usr/include -D_FILE_OFFSET_BITS=64 -D_REENTRANT -g -O2 -MT zkfuse.o -MD -MP -MF .deps/zkfuse.Tpo -c -o zkfuse.o zkfuse.cc
zkfuse.cc: In function 'int main(int, char**)':
zkfuse.cc:4282: error: 'String' does not name a type
zkfuse.cc:4283: error: 'file' was not declared in this scope
make[2]: *** [zkfuse.o] Error 1
make[2]: Leaving directory `/home/nitay/code/zookeeper-git/src/contrib/zkfuse/src'
make[1]: *** [all-recursive] Error 1
make[1]: Leaving directory `/home/nitay/code/zookeeper-git/src/contrib/zkfuse'
make: *** [all] Error 2

4279     /**
4280      * Initialize log4cxx 
4281      */
4282     const String file(""log4cxx.properties"");
4283     PropertyConfigurator::configureAndWatch( file, 5000 );
4284     LOG_INFO(LOG, ""Starting zkfuse"");


String is not standard, we should change it to std::string.","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-347,Minor,Nitay Joffe,Fixed,2009-04-08T22:39:43.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zkfuse uses non-standard String,2009-07-08T20:24:01.000+0000,"[<JIRA Version: name='3.1.0', id='12313381'>]",1.0
Patrick D. Hunt,[],2009-03-23T23:34:45.000+0000,Mahadev Konar,we should just remove the kill command from the client port. Its a security risk (though we do not have much security right now) to be able to kill the server from a tcp port without any authentication... ,"[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-346,Major,Mahadev Konar,Fixed,2009-04-17T20:48:51.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,remove the kill command fro mthe client port.,2009-07-08T20:24:01.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2009-03-18T18:45:48.000+0000,bryan thompson,"I have been having a problem with zookeeper 3.0.1 and now with 3.1.0 where I see a lot of expired sessions.  I am using a 16 node cluster which is all on the same local network.  There is a single zookeeper instance (these are benchmarking runs).
The problem appears to be correlated with either run time or system load.\

Personally I think that it is system load because I have session session expired events under a Windows platform running zookeeper and the application (i.e., everthing is local) when the application load suddenly spikes.  To me this suggests that the client is not able to renew (ping) the zookeeper service in a timely manner and is expired.  But the log messages below with the ""read error"" suggest that maybe there is something else going on?

Zookeeper Configuration
#Wed Mar 18 12:41:05 GMT-05:00 2009
clientPort=2181
dataDir=/var/bigdata/benchmark/zookeeper/1
syncLimit=2
dataLogDir=/var/bigdata/benchmark/zookeeper/1
tickTime=2000

Some representative log messages are below.

Client side messages (from our app)
ERROR [main-EventThread] com.bigdata.zookeeper.ZLockImpl$ZLockWatcher.process(ZLockImpl.java:400) 2009-03-18 13:35:40,335 - Session expired: WatchedEvent: Server state change. New state: Expired : zpath=/benchmark/jobs/com.bigdata.service.jini.benchmark.ThroughputMaster/test_1/client1160/locknode
ERROR [main-EventThread] com.bigdata.zookeeper.ZLockImpl$ZLockWatcher.process(ZLockImpl.java:400) 2009-03-18 13:35:40,335 - Session expired: WatchedEvent: Server state change. New state: Expired : zpath=/benchmark/jobs/com.bigdata.service.jini.benchmark.ThroughputMaster/test_1/client1356/locknode

Server side messages:
 WARN [NIOServerCxn.Factory:2181] org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:417) 2009-03-18 13:06:57,252 - Exception causing close of session 0x1201aac14300022 due to java.io.IOException: Read error
 WARN [NIOServerCxn.Factory:2181] org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:417) 2009-03-18 13:06:58,198 - Exception causing close of session 0x1201aac1430000f due to java.io.IOException: Read error
","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-344,Major,bryan thompson,Invalid,2009-05-06T17:04:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"doIO in NioServerCnxn: Exception causing close of session : cause is ""read error""",2009-07-08T20:24:01.000+0000,"[<JIRA Version: name='3.1.0', id='12313381'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='quorum', id='12312379'>, <JIRA Component: name='server', id='12312382'>]",2009-03-18T16:11:12.000+0000,Patrick D. Hunt,ZOOKEEPER 330/336 caused a regression in QuorumPeerMain -- cannot reliably start a cluster due to missing tickTime.,"[<JIRA Version: name='3.1.1', id='12313649'>, <JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-341,Blocker,Patrick D. Hunt,Fixed,2009-03-18T16:38:38.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"regression in QuorumPeerMain, tickTime from config is lost, cannot start quorum",2009-07-08T20:24:00.000+0000,[],1.0
Mahadev Konar,"[<JIRA Component: name='jute', id='12312385'>, <JIRA Component: name='server', id='12312382'>]",2009-03-17T22:16:39.000+0000,Mahadev Konar,"binaryinputarchive throws out runtimeexceptions for unreasonable length datastructures. We should change that to be IOExceptions so that we can handle partial writes to logs,, machine powerdown better.
","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-340,Major,Mahadev Konar,Fixed,2009-04-17T23:19:49.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,binaryinputarchive throws out runtimeexceptions for unreasonable length datastructures.,2009-07-08T20:24:00.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>, <JIRA Version: name='3.1.1', id='12313649'>]",1.0
Benjamin Reed,"[<JIRA Component: name='server', id='12312382'>]",2009-03-04T18:55:48.000+0000,Mahadev Konar,"currently the zookeeper followers do not commit the new leader election. This will cause problems in a failure scenarios with a follower acking to the same leader txn id twice, which might be two different intermittent leaders and allowing them to propose two different txn's of the same zxid.
","[<JIRA Version: name='3.4.0', id='12314469'>]",Bug,ZOOKEEPER-335,Blocker,Mahadev Konar,Fixed,2011-06-14T05:14:31.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper servers should commit the new leader txn to their logs.,2019-11-17T10:05:08.000+0000,"[<JIRA Version: name='3.1.0', id='12313381'>]",9.0
Mahadev Konar,"[<JIRA Component: name='contrib-bookkeeper', id='12312643'>]",2009-02-27T23:55:44.000+0000,Mahadev Konar,"bookkeeper benchmark (testclient.java) has compiling errors.
","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-334,Major,Mahadev Konar,Fixed,2009-03-03T22:49:12.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,bookkeeper benchmark (testclient.java) has compiling errros.,2009-07-08T20:24:00.000+0000,"[<JIRA Version: name='3.1.0', id='12313381'>]",1.0
Mahadev Konar,"[<JIRA Component: name='c client', id='12312380'>]",2009-02-27T21:56:25.000+0000,Patrick D. Hunt,"helgrind generated a number of issues, I pulled a bunch of them. Most are related to the test, some are really issues with the mt zk client code though:

valgrind --tool=helgrind --log-file=helgrind_mt.out ./zktest-mt


==31294== Thread #2: pthread_cond_{timed}wait called with un-held mutex
==31294==    at 0x4027F8F: pthread_cond_wait@* (hg_intercepts.c:560)
==31294==    by 0x404D881: pthread_cond_wait@GLIBC_2.0 (in /lib/tls/i686/cmov/libpthread-2.8.90.so)
==31294==    by 0x4028037: pthread_cond_wait@* (hg_intercepts.c:574)
==31294==    by 0x809EBB7: pthread_cond_wait (PthreadMocks.cc:54)
==31294==    by 0x80ABCF6: notify_thread_ready (mt_adaptor.c:136)
==31294==    by 0x80ABE90: do_io (mt_adaptor.c:277)


==31294== Possible data race during write of size 4 at 0x42E9A58
==31294==    at 0x8050D83: terminateZookeeperThreads(_zhandle*) (ZKMocks.cc:518)
==31294==    by 0x805543B: DeliverWatchersWrapper::call(_zhandle*, int, int, char const*, watcher_object_list**) (ZKMocks.cc:261)
==31294==    by 0x80520F7: __wrap_deliverWatchers (ZKMocks.cc:220)
==31294==    by 0x80A287B: process_completions (zookeeper.c:1393)
==31294==    by 0x80ABDAA: do_completion (mt_adaptor.c:332)

==31294== Possible data race during write of size 4 at 0xBEFF5F30
==31294==    at 0x80589AF: Zookeeper_watchers::ConnectionWatcher::~ConnectionWatcher() (TestWatchers.cc:54)
==31294==    by 0x805D062: Zookeeper_watchers::testDefaultSessionWatcher1() (TestWatchers.cc:438)
==31294==    by 0x805608C: CppUnit::TestCaller<Zookeeper_watchers>::runTest() (TestCaller.h:166)


==31294== Possible data race during write of size 4 at 0x42EB104
==31294==    at 0x80A03EE: queue_completion (zookeeper.c:1776)
==31294==    by 0x80A3A44: zookeeper_process (zookeeper.c:1598)
==31294==    by 0x80AC00B: do_io (mt_adaptor.c:309)


==31294== Thread #29: pthread_cond_{timed}wait called with un-held mutex
==31294==    at 0x4027F8F: pthread_cond_wait@* (hg_intercepts.c:560)
==31294==    by 0x404D881: pthread_cond_wait@GLIBC_2.0 (in /lib/tls/i686/cmov/libpthread-2.8.90.so)
==31294==    by 0x4028037: pthread_cond_wait@* (hg_intercepts.c:574)
==31294==    by 0x809EBB7: pthread_cond_wait (PthreadMocks.cc:54)
==31294==    by 0x80AB9B3: wait_sync_completion (mt_adaptor.c:82)
==31294==    by 0x80A1E82: zoo_wget (zookeeper.c:2517)
==31294==    by 0x80A1F13: zoo_get (zookeeper.c:2497)





","[<JIRA Version: name='3.1.1', id='12313649'>, <JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-333,Critical,Patrick D. Hunt,Fixed,2009-03-12T00:31:45.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,helgrind thread issues identified in mt c client code,2009-07-08T20:24:00.000+0000,[],2.0
,"[<JIRA Component: name='c client', id='12312380'>]",2009-02-27T20:04:12.000+0000,Patrick D. Hunt,"Attaching valgrind log files.

1)  getpwuid_r doesn't seem like it's due to us
2) the rest seem to be valid
","[<JIRA Version: name='3.1.1', id='12313649'>, <JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-332,Blocker,Patrick D. Hunt,Invalid,2009-03-04T18:25:40.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,c client issues (memory leaks) reported by valgrind,2009-07-08T20:24:00.000+0000,"[<JIRA Version: name='3.1.0', id='12313381'>]",0.0
Mahadev Konar,[],2009-02-25T01:30:37.000+0000,Mahadev Konar,ZOOKEEPER-326 made a change to zookeeperservermain.java that broke the starting of zookeeperserver with just the port and datadir.,"[<JIRA Version: name='3.1.1', id='12313649'>, <JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-330,Blocker,Mahadev Konar,Fixed,2009-02-27T19:05:59.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper standalone server does not startup with just a port and datadir.,2009-07-08T20:24:00.000+0000,"[<JIRA Version: name='3.1.1', id='12313649'>, <JIRA Version: name='3.2.0', id='12313491'>]",1.0
Chris Darroch,"[<JIRA Component: name='server', id='12312382'>]",2009-02-20T20:12:15.000+0000,Chris Darroch,"When using the ZooKeeper server in standalone mode, it ignores the tickTime setting in the configuration file and uses the DEFAULT_TICK_TIME of 3000 coded into ZooKeeperServer.java.","[<JIRA Version: name='3.1.1', id='12313649'>, <JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-326,Minor,Chris Darroch,Fixed,2009-02-23T23:56:36.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,standalone server ignores tickTime configuration,2009-07-08T20:24:00.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>]",1.0
,"[<JIRA Component: name='quorum', id='12312379'>]",2009-02-20T00:04:55.000+0000,Mahadev Konar,FLENewEpochTest fails quite frequently on my machine. ,"[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-325,Major,Mahadev Konar,Fixed,2009-04-07T23:08:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,FLENewEpoch test fails.,2009-07-08T20:24:00.000+0000,"[<JIRA Version: name='3.1.0', id='12313381'>]",1.0
Chris Darroch,"[<JIRA Component: name='c client', id='12312380'>]",2009-02-17T22:48:01.000+0000,Chris Darroch,"If a client calls zoo_add_auth() with an invalid scheme (e.g., ""foo"") the ZooKeeper server will mark their session expired and close the connection.  However, the C client has returned immediately after queuing the new auth data to be sent with a ZOK return code.

If the client then waits for their auth completion function to be called, they can wait forever, as no session event is ever delivered to that completion function.  All other completion functions are notified of session events by free_completions(), which is called by cleanup_bufs() in handle_error() in handle_socket_error_msg().

In actual fact, what can happen (about 50% of the time, for me) is that the next call by the IO thread to flush_send_queue() calls send() from within send_buffer(), and receives a SIGPIPE signal during this send() call.  Because the ZooKeeper C API is a library, it properly does not catch that signal.  If the user's code is not catching that signal either, they experience an abort caused by an untrapped signal.  If they are ignoring the signal -- which is common in context I'm working in, the Apache httpd server -- then flush_send_queue()'s error return code is EPIPE, which is logged by handle_socket_error_msg(), and all non-auth completion functions are notified of a session event.  However, if the caller is waiting for their auth completion function, they wait forever while the IO thread tries repeatedly to reconnect and is rejected by the server as having an expired session.

So, first of all, it would be useful to document in the C API portion of the programmer's guide that trapping or ignoring SIGPIPE is important, as this signal may be generated by the C API.

Next, the two attached patches call the auth completion function, if any, in free_completions(), which fixes this problem for me.  The second attached patch includes auth lock/unlock function, as per ZOOKEEPER-319.","[<JIRA Version: name='3.1.1', id='12313649'>, <JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-320,Major,Chris Darroch,Fixed,2009-02-27T20:05:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,call auth completion in free_completions(),2009-07-08T20:24:00.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>]",1.0
Chris Darroch,"[<JIRA Component: name='c client', id='12312380'>]",2009-02-17T22:32:27.000+0000,Chris Darroch,"Looking over the zookeeper.c code it appears to me that the zoo_add_auth() function may be called at any time by the user in their ""main"" thread.  This function alters the elements of the auth_info structure in the zhandle_t structure.

Meanwhile, the IO thread may read those elements at any time in such functions as send_auth_info() and auth_completion_func().  It seems important, then, to add a lock which prevents data being read by the IO thread while only partially changed by the user's thread.  The attached patch add such a lock.","[<JIRA Version: name='3.1.1', id='12313649'>, <JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-319,Major,Chris Darroch,Fixed,2009-02-27T19:49:16.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,add locking around auth info in zhandle_t,2009-07-08T20:24:00.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>]",1.0
Chris Darroch,"[<JIRA Component: name='c client', id='12312380'>]",2009-02-17T20:52:02.000+0000,Chris Darroch,"From a review of zk_hashtable.c it appears to me that all functions which manipulate the hashtables are called from the IO thread, and therefore any need for locking is obviated.

If I'm wrong about that, then I think at a minimum collect_keys() should acquire a lock in the same manner as collect_session_watchers().  Both iterate over hashtable contents (in the latter case using copy_table()).

However, from what I can see, the only function (besides the init/destroy functions used when creating a zhandle_t) called from the completion thread is deliverWatchers(), which simply iterates over a ""delivery"" list created from the hashtables by collectWatchers().  The activateWatcher() function contains comments which describe it being called by the completion thread, but in fact it is called by the IO thread in zookeeper_process().

I believe all calls to collectWatchers(), activateWatcher(), and collect_keys() are made by the IO thread in zookeeper_interest(), zookeeper_process(), check_events(), send_set_watches(), and handle_error().  Note that queue_session_event() is aliased as PROCESS_SESSION_EVENT, but appears only in handle_error() and check_events().

Also note that handle_error() is called only in zookeeper_process() and handle_socket_error_msg(), which is used only by the IO thread, so far as I can see.","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-318,Major,Chris Darroch,Fixed,2009-03-06T00:45:04.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,remove locking in zk_hashtable.c or add locking in collect_keys(),2021-06-15T00:53:13.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>]",1.0
,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='tests', id='12312427'>]",2009-02-17T19:02:08.000+0000,Mahadev Konar,configure option --without-cppunit does not work.,[],Bug,ZOOKEEPER-316,Major,Mahadev Konar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,configure option --without-cppunit does not work,2009-07-14T18:14:05.000+0000,"[<JIRA Version: name='3.1.0', id='12313381'>]",0.0
Mahadev Konar,"[<JIRA Component: name='server', id='12312382'>]",2009-02-13T22:20:11.000+0000,Sunanda Bera,"Steps to reproduce:

Create a 3 node cluster . Run some transactions and then stop all clients. Make sure no other clients connect for the duration of the test.

Let L1 be the current leader. Bring down L1. Let L2 be the leader chosen.  Let the third node be N3. Note that this will increase the txn id for N3's snapshot without any  transaction being logged. Now bring up L1 -- same will happen for L1. Now bring down L2.

Both N3 and L1 now have snapshots with a transaction id greater than the last logged transaction. Whoever is elected leader will try to restore its state from the filesystem and fail.

One easy workaround is obviously to change the FileTxnSnapLog not to save a snapshot if zxid > last logged zxid. The correct solution is possibly to log a transaction for leader election as well.","[<JIRA Version: name='3.1.1', id='12313649'>]",Bug,ZOOKEEPER-313,Major,Sunanda Bera,Duplicate,2009-03-04T19:01:27.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Problem with successive leader failures when no client is connected ,2009-03-04T19:01:27.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>]",0.0
Mahadev Konar,"[<JIRA Component: name='c client', id='12312380'>]",2009-02-11T18:16:44.000+0000,Patrick D. Hunt,"Coverity found the following issues in the c code thatwe should look at/resolve:

1) zookeeper.c

Event unterminated_case: This case (value 0) is not terminated by a 'break' statement.
717  	        case 0:
718  	            errno = EHOSTDOWN;
Event fallthrough: The above case falls through to this one.
719  	        case -1:

Event unterminated_case: This case (value 0) is not terminated by a 'break' statement.
739  	        case 0:
740  	            errno = EHOSTDOWN;
Event fallthrough: The above case falls through to this one.
741  	        case -1:

Event negative_return_fn: Called negative-returning function ""socket(2, 1, 0)""
Event var_assign: NEGATIVE return value of ""socket"" assigned to signed variable ""zh->fd""
1099 	            zh->fd = socket(PF_INET, SOCK_STREAM, 0);
Event negative_returns: Tracked variable ""zh->fd"" was passed to a negative sink.
1100 	            setsockopt(zh->fd, IPPROTO_TCP, TCP_NODELAY, &on, sizeof(int));

Event deref_ptr: Directly dereferenced pointer ""cptr->buffer""
1308 	    cptr->buffer->curr_offset = get_buffer_len(oa);
Event check_after_deref: Pointer ""cptr->buffer"" dereferenced before NULL check
1309 	    if (!cptr->buffer) {


cli.c

Event returned_null: Function ""strchr"" returned NULL value (checked 4 out of 5 times)
Event var_assigned: Variable ""ptr"" assigned to NULL return value from ""strchr""
532  	                char *ptr = strchr(buffer, '\n');
Event dereference: Dereferencing NULL value ""ptr"" 


recordio.c

Event alloc_fn: Called allocation function ""malloc""
Event var_assign: Assigned variable ""buff"" to storage returned from ""malloc(12U)""
284  	    struct buff_struct *buff = malloc(sizeof(struct buff_struct));
Event leaked_storage: Variable ""buff"" goes out of scope
At conditional (1): ""!(ia != NULL)"" taking true path 
285  	    if (!ia) return 0;

Event alloc_fn: Called allocation function ""malloc""
Event var_assign: Assigned variable ""buff"" to storage returned from ""malloc(12U)""
301  	    struct buff_struct *buff = malloc(sizeof(struct buff_struct));
Event leaked_storage: Variable ""buff"" goes out of scope
At conditional (1): ""!(oa != NULL)"" taking true path
302  	    if (!oa) return 0;

",[],Bug,ZOOKEEPER-310,Minor,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Coverity report on issues in C client code,2022-02-03T08:50:22.000+0000,"[<JIRA Version: name='3.1.0', id='12313381'>]",0.0
Mahadev Konar,"[<JIRA Component: name='c client', id='12312380'>]",2009-02-10T22:21:50.000+0000,Chris Darroch,"The zookeeper_process() function incorrectly calls the c.acl_result member of the completion_list_t structure when handling the completion from a synchronous zoo_get_acl() request.  The c.acl_result member is set to SYNCHRONOUS_MARKER, which is a null pointer.

The attached patch removes this call.","[<JIRA Version: name='3.1.1', id='12313649'>, <JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-309,Major,Chris Darroch,Fixed,2009-03-12T00:49:32.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,core dump using zoo_get_acl() ,2009-07-08T20:23:57.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>]",1.0
Thomas White,"[<JIRA Component: name='scripts', id='12312384'>]",2009-02-06T14:06:17.000+0000,Thomas White,"Macs don't support the -f option in readlink

{noformat}
$ bin/zkServer.sh start
JMX enabled by default
readlink: illegal option -- f
usage: readlink [-n] [file ...]
{noformat}","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-303,Major,Thomas White,Fixed,2009-02-10T18:41:20.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Bin scripts don't work on Mac,2011-01-06T03:45:21.000+0000,"[<JIRA Version: name='3.1.0', id='12313381'>]",2.0
Patrick D. Hunt,[],2009-02-06T12:40:17.000+0000,Thomas White,"Some characters are not allowed in ObjectName values and need quoting, see http://java.sun.com/javase/6/docs/api/javax/management/ObjectName.html.

This came up with IPv6 addresses which contain a colon character.","[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-302,Major,Thomas White,Fixed,2009-02-06T21:21:47.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Quote values in JMX ObjectNames ,2010-03-23T16:45:01.000+0000,"[<JIRA Version: name='3.1.0', id='12313381'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='jmx', id='12312451'>]",2009-02-05T23:24:03.000+0000,Patrick D. Hunt,"remove printStackTrace from zk jmx code (review the rest of the code at the same time)
","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-300,Major,Patrick D. Hunt,Fixed,2009-05-14T19:16:56.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zk jmx code is calling printStackTrace when creating bean name (should not be),2009-07-08T20:23:57.000+0000,"[<JIRA Version: name='3.1.0', id='12313381'>]",1.0
,[],2009-02-05T23:22:28.000+0000,Mahadev Konar,"JMX enabled by default
readlink: illegal option -- f
usage: readlink [-n] [file ...]

./zkEnv.sh no such file.

if you run bin/zkServer.sh start this is the error i get.
if I run it from the bin directory then it seems to be fine.","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-299,Major,Mahadev Konar,Fixed,2009-02-12T19:05:47.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,server startup scripts fail on a mac.,2009-07-08T20:23:57.000+0000,[],1.0
,"[<JIRA Component: name='build', id='12312383'>]",2009-02-05T22:23:42.000+0000,Patrick D. Hunt,"The configure scripts in src/c, the zkServer.sh in src/c/test, and some of the other scripts are not marked as executable when running ""ant tar"". The build.xml should be updated to mark appropriately.
","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-298,Major,Patrick D. Hunt,Invalid,2009-05-19T22:19:33.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"some excecutables (scripts typ.) are not marked as such in tar generated by ""ant tar""",2009-07-08T20:23:57.000+0000,"[<JIRA Version: name='3.1.0', id='12313381'>]",0.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>]",2009-02-05T18:43:18.000+0000,Patrick D. Hunt,"ZOOKEEPER-255 fixed an issue with zoo_set not providing access to stat structure, however this has broken b/w compatibility with previous releases.

We need to:

1) revert zoo_set to not have stat parameter (keep b/w compat)
2) add zoo_set2 method with stat param added to the function signature
3) add a version.h file to src/c/include that provides zoo version detail to clients

ie.
> #define ZOO_MAJOR_VERSION 3
> #define ZOO_MINOR_VERSION 1
> #define ZOO_PATCH_VERSION 0

(a new jira should be added to centralize version numbering, we now have the version number in 3 places in the source)
","[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-293,Critical,Patrick D. Hunt,Fixed,2009-02-05T21:19:54.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"zoo_set needs to be abi compatible (3.1 changed the signature), fix this by adding zoo_set2",2009-02-13T21:18:54.000+0000,"[<JIRA Version: name='3.1.0', id='12313381'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='java client', id='12312381'>]",2009-02-04T19:23:29.000+0000,Patrick D. Hunt,"In 246 the old constants were deprecated and replace with enum. In the process usage of the orig constants was broken for switch statements, cases require compiletime constants.","[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-291,Major,Patrick D. Hunt,Fixed,2009-02-04T20:01:27.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,regression for legacy code using KeeperException.Code constants (due to 246),2009-02-13T21:18:54.000+0000,[],1.0
,"[<JIRA Component: name='server', id='12312382'>]",2009-02-03T05:26:24.000+0000,Patrick D. Hunt,"a user reported that a long running server, part of a 2 server ensemble, started using 100%cpu (1 server of the ensemble, the other was fine).

mahadev tracked it down to a thread in the server running epoll in a tight loop - the thread was the nio server factory thread that selects on client fds.
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-287,Critical,Patrick D. Hunt,Duplicate,2009-10-14T16:59:57.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,high cpu utilization caused by nioserver factory thread,2010-03-26T17:24:54.000+0000,"[<JIRA Version: name='3.0.1', id='12313500'>]",1.0
Benjamin Reed,[],2009-02-02T11:56:46.000+0000,Benjamin Reed,"the stat command is always returning ""standalone"" it should return the actual mode it is in.","[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-285,Major,Benjamin Reed,Fixed,2009-02-02T22:46:38.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,the stat command does not return the correct mode,2009-02-13T21:18:54.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>]",1.0
Patrick D. Hunt,[],2009-02-02T11:55:40.000+0000,Benjamin Reed,"The client port in JMX is always zero. it should be getting the client port from the cnxnfactory. it's a pretty easy fix. the really problem is that we have a setter for the clientPort as well, which is a bit more complicated to implement correctly. do we need a setter for the clientPort? i think we should make it a readonly attribute.","[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-284,Minor,Benjamin Reed,Fixed,2009-02-02T22:51:33.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,JMX doesn't get the clientPort correctly,2009-02-13T21:18:54.000+0000,"[<JIRA Version: name='3.1.0', id='12313381'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>]",2009-01-29T14:44:23.000+0000,Maxim P. Dementiev,"> autoreconf -i -f -v
autoreconf-2.63: Entering directory `.'
autoreconf-2.63: configure.ac: not using Gettext
autoreconf-2.63: running: aclocal --force 
configure.ac:21: error: AC_SUBST: `DX_FLAG_[]DX_CURRENT_FEATURE' is not a valid shell variable name
acinclude.m4:77: DX_REQUIRE_PROG is expanded from...
acinclude.m4:117: DX_ARG_ABLE is expanded from...
acinclude.m4:178: DX_INIT_DOXYGEN is expanded from...
configure.ac:21: the top level
autom4te-2.63: /usr/bin/m4 failed with exit status: 1
aclocal-1.10: autom4te failed with exit status: 1
autoreconf-2.63: aclocal failed with exit status: 1
>","[<JIRA Version: name='3.1.1', id='12313649'>, <JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-281,Major,Maxim P. Dementiev,Fixed,2009-03-05T22:34:24.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,autoreconf fails for /zookeeper-3.0.1/src/c/,2009-07-08T20:23:56.000+0000,"[<JIRA Version: name='3.0.1', id='12313500'>]",1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='leaderElection', id='12312378'>]",2009-01-15T15:18:00.000+0000,Flavio Paiva Junqueira,"I found an execution in which leader election does not make progress. Here is the problematic scenario:

- We have an ensemble of 3 servers, and we start only 2;
- We let them elect a leader, and then crash the one with lowest id, say S_1 (call the other S_2);
- We restart the crashed server.

Upon restarting S_1, S_2 has its logical clock more advanced, and S_1 has its logical clock set to 1. Once S_1 receives a notification from S_2, it notices that it is in the wrong round and it advances its logical clock to the same value as S_1. Now, the problem comes exactly in this point because in the current code S_1 resets its vote to its initial vote (its own id and zxid). Since S_2 has already notified S_1, it won't do it again, and we are stuck. The patch I'm submitting fixes this problem by setting the vote of S_1 to the one received if it satisfies the total order predicate (""received zxid"" is higher or ""received zxid is the same and received id is higher"").

Related to this problem, I noticed that by trying to avoid unnecessary notification duplicates, there could be scenarios in which a server fails before electing a leader and restarts before leader election succeeds. This could happen, for example, when there isn't enough servers available and one available crashes and restarts. I fixed this problem in the attached patch by allowing a server to send a new batch of notifications if there is at least one outgoing queue of pending notifications empty. This is ok because we space out consecutive batches of notifications. ","[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-275,Major,Flavio Paiva Junqueira,Fixed,2009-01-29T23:11:41.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Bug in FastLeaderElection,2009-02-13T21:18:54.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>]",2009-01-15T00:36:43.000+0000,Runping Qi,"One should be able to build Zookeeper C client libs on a machine without CPPUNIT installation.

A simple fix is to remove from configure.ac the following line:
M_PATH_CPPUNIT(1.10.2)


","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-273,Major,Runping Qi,Fixed,2009-01-26T17:56:45.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Zookeeper c client build should not depend on CPPUNIT,2009-02-17T19:02:54.000+0000,"[<JIRA Version: name='3.1.0', id='12313381'>]",1.0
Mahadev Konar,[],2009-01-13T18:15:32.000+0000,Joshua Tuberville,"Zookeeper allows creation of an abritrary number of children, yet if the String array of children names exceeds 4,194,304 bytes a getChildren will fail because ClientCnxn$SendThread.readLength() throws an exception on line 490.  Mahadev Konar questioned this byte limit's need.  In any case consistency of create children, get children should exist.","[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-272,Major,Joshua Tuberville,Fixed,2009-01-29T23:22:12.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,getChildren can fail for large numbers of children,2009-02-13T21:18:54.000+0000,[],3.0
,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2009-01-08T21:10:20.000+0000,Mahadev Konar,ZOOKEEPER-265 makes some constants deprecated. We should remove the deprecated stuff int the next release.,[],Bug,ZOOKEEPER-270,Major,Mahadev Konar,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,remove NoSyncConnected in KeeperState - it's not used anywhere in the codebase. ,2010-01-25T23:56:54.000+0000,[],0.0
Patrick D. Hunt,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2009-01-07T18:05:01.000+0000,Patrick D. Hunt,"Jute still causing problems with tostring operations on generated code, need to review/cleanup the toCSV code

From user Kevin Burton:
---------------------------------------------
Creating this node with this ACL:
Created /foo
setAcl /foo world:anyone:w

Causes the exception included below.

It's an infinite loop so it's just called over and over again filling my
console.

I'm just doing an exists( path, true ); ... setting a watch still causes the
problem.



java.lang.NullPointerException
        at org.apache.jute.Utils.toCSVBuffer(Utils.java:234)
        at
org.apache.jute.CsvOutputArchive.writeBuffer(CsvOutputArchive.java:101)
        at
org.apache.zookeeper.proto.GetDataResponse.toString(GetDataResponse.java:48)
        at java.lang.String.valueOf(String.java:2827)
        at java.lang.StringBuilder.append(StringBuilder.java:115)
        at
org.apache.zookeeper.ClientCnxn$Packet.toString(ClientCnxn.java:230)
        at java.lang.String.valueOf(String.java:2827)
        at java.lang.StringBuilder.append(StringBuilder.java:115)
        at
org.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:586)
        at
org.apache.zookeeper.ClientCnxn$SendThread.doIO(ClientCnxn.java:626)
        at
org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:852)
java.lang.NullPointerException
        at org.apache.jute.Utils.toCSVBuffer(Utils.java:234)
        at
org.apache.jute.CsvOutputArchive.writeBuffer(CsvOutputArchive.java:101)
        at
org.apache.zookeeper.proto.GetDataResponse.toString(GetDataResponse.java:48)
        at java.lang.String.valueOf(String.java:2827)
        at java.lang.StringBuilder.append(StringBuilder.java:115)
        at
org.apache.zookeeper.ClientCnxn$Packet.toString(ClientCnxn.java:230)
        at java.lang.String.valueOf(String.java:2827)
        at java.lang.StringBuilder.append(StringBuilder.java:115)
        at
org.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:586)
        at
org.apache.zookeeper.ClientCnxn$SendThread.doIO(ClientCnxn.java:626)
        at
org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:852)
","[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-268,Major,Patrick D. Hunt,Fixed,2009-01-26T19:28:37.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,tostring on jute generated objects can cause NPE,2009-02-13T21:18:54.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>]",2009-01-05T22:56:46.000+0000,Patrick D. Hunt,"The java client (and someone should also review the c client) is generating a syncdisconnected even if the client is currently in the disconnected state. We saw this with a user running the java client against a down standalone server (server not running, connection refused) - the ""syncdisconnected"" is generated by the client lib each time a connection attempt (fails) is made. Should only be generated once.","[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-267,Major,Patrick D. Hunt,Fixed,2009-01-28T00:24:25.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,java client incorrectly generating syncdisconnected event when in disconnected state,2009-02-13T21:18:53.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='java client', id='12312381'>]",2009-01-05T20:40:27.000+0000,Patrick D. Hunt,KeeperState is missing documentation of the states. Should provide some basic details and refer users to the appropriate forrest doc for more detail (most likely prog guide).,"[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-266,Major,Patrick D. Hunt,Fixed,2009-01-08T21:22:25.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,KeeperState missing javadoc for values,2009-02-13T21:18:53.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2008-12-16T22:50:09.000+0000,Patrick D. Hunt,"The docs incorrectly state the max client timeout as 60 seconds.
http://hadoop.apache.org/zookeeper/docs/r3.0.1/zookeeperProgrammers.html#ch_zkSessions

the current server code has the following logic:
        if (sessionTimeout < zk.tickTime * 2) {
            sessionTimeout = zk.tickTime * 2;
        }
        if (sessionTimeout > zk.tickTime * 20) {
            sessionTimeout = zk.tickTime * 20;
        }

So really the docs should say max is 20*tickTime
","[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-258,Minor,Patrick D. Hunt,Fixed,2009-01-08T21:41:13.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,docs incorrectly state max client timeout as 60 seconds (it's based on server ticktime),2009-02-13T21:18:53.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2008-12-16T00:49:00.000+0000,Patrick D. Hunt,Some of the WARN/ERROR log messages are incorrectly leveled (too high - many often should be INFO/DEBUG),"[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-257,Major,Patrick D. Hunt,Duplicate,2009-01-07T00:22:53.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Review logging level for WARN/ERROR log messages, some misclassified",2009-02-13T21:18:53.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>]",0.0
Avery Ching,"[<JIRA Component: name='c client', id='12312380'>]",2008-12-12T23:18:21.000+0000,Mahadev Konar,the zoo_set() api does not return the stat datastructure. the java counterpart returns a stat with set api. ,"[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-255,Blocker,Mahadev Konar,Fixed,2008-12-22T21:40:29.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zoo_set() api does not return stat datastructure.,2009-02-13T21:18:53.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>]",1.0
Benjamin Reed,"[<JIRA Component: name='tests', id='12312427'>]",2008-12-12T21:44:22.000+0000,Patrick D. Hunt,"There's currently no way for a user to test session expiration in their code.

We don't  have any unit/system tests that verify our code handles session expiration properly.

There should be a way to test session expiration.

I did notice that we have the ability to terminate a session using JMX mbean interface, however I'm not sure if this is useful in an automated testing context. Even if it is we should provide a wrapper for testing purposes - and add tests to our codebase which uses it.
","[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-254,Major,Patrick D. Hunt,Fixed,2009-01-22T22:39:13.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,there is currently no way for a user to test session expiration in their code,2009-02-13T21:18:53.000+0000,[],1.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2008-12-11T18:45:28.000+0000,Patrick D. Hunt,"the example code has

           case SyncConnected:
               // Everything is happy. Lets kick things off
               // again by checking the existence of the znode
               break;

this is misleading - it should indicate that the watches are automatically reset and therefor no call to exists is necessary

also fix this in the same doc (looks like its old detail, no longer valid), indicate that autoreset will happen on reconnect.

If the client-side ZooKeeper libraries can reestablish the communication channel to ZooKeeper, DataMonitor simply kicks everything off again with the call to ZooKeeper.exists(). If it gets an event for a znode, it calls ZooKeeper.exists() to find out what has changed. 
","[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-253,Minor,Patrick D. Hunt,Fixed,2009-01-28T00:47:31.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,documentation of DataWatcher state transition is misleading regarding auto watch reset on reconnect,2009-02-13T21:18:53.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>]",2.0
Mahadev Konar,"[<JIRA Component: name='server', id='12312382'>]",2008-12-10T23:06:29.000+0000,Patrick D. Hunt,"org.apache.zookeeper.server.PurgeTxnLog class has not been updated to handle the new directory structure imposed by the upgrade from v2 to v3 of ZooKeeper. In particular the dataDir now has a ""version-2"" subdirectory that stores all of the snaps/transactionallogs for version2 of the persistence layer.

I also note that the documentation of this class is particularly poor. I'm working on ZOOKEEPER-229 and would like to point to the API docs for this class regarding usage but they api docs are nonexistent

Also - I think it's important for the user to be able to specify the number of ""backup"" snaps and logs that should be kept -- right now it seems we delete all but the current snaps/txlogs. Either by count or by date -- ie ""remove anything 5 days or older, with a minum of 3 most recents snaps (and accompanying txlogs)"" seems like a pretty common user case (assuming the operator is doing system backups every X days, etc...)

in general this class needs some tlc - the formatting should also be cleaned up.

Also - the API docs for this and LogFormatter are not included in the build.xml ""javadoc"" target. These are user utilities so javadoc for these two classes should be included. I will fix this issue as part of ZOOKEEPER-229. I'm also updateing the forrest documention in 229 so don't worry about that either.

","[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-252,Critical,Patrick D. Hunt,Fixed,2009-01-31T01:19:11.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,PurgeTxnLog is not handling the new dataDir directory structure,2009-02-13T21:18:53.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>]",1.0
Mahadev Konar,"[<JIRA Component: name='server', id='12312382'>]",2008-12-08T21:29:01.000+0000,Thomas Vinod Johnson,"See the following thread for the original report:
http://mail-archives.apache.org/mod_mbox/hadoop-zookeeper-user/200812.mbox/browser
Steps to reproduce:
1) Start a replicated zookeeper service consisting of 3 zookeeper (3.0.1) servers all running on the same host (of course, all using their own ports and log directories)
2) Create one znode in this ensemble (using the zookeeper client console, I issued 'create /node1 node1data').
3) Stop, then restart a single zookeeper server; moving onto the next one a few seconds later. 
4) Go back to 3. After 4-5 iterations, the following should occur, with the failing server exiting:
java.lang.NullPointerException
        at 
org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:447)
        at 
org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.init(FileTxnLog.java:358)
        at 
org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.<init>(FileTxnLog.java:333)
        at 
org.apache.zookeeper.server.persistence.FileTxnLog.read(FileTxnLog.java:250)
        at 
org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:102)
        at 
org.apache.zookeeper.server.ZooKeeperServer.loadData(ZooKeeperServer.java:183)
        at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:245)
        at 
org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:421)
2008-12-08 14:14:24,880 - INFO  
[QuorumPeer:/0:0:0:0:0:0:0:0:2183:Leader@336] - Shutdown called
java.lang.Exception: shutdown Leader! reason: Forcing shutdown
        at 
org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:336)
        at 
org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:427)
Exception in thread ""QuorumPeer:/0:0:0:0:0:0:0:0:2183"" 
java.lang.NullPointerException
        at 
org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:339)
        at 
org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:427)

The inputStream field is null, apparently because next is being called 
at line 358 even after next returns false. Having very little knowledge 
about the implementation, I don't know if the existence of hdr.getZxid() 
 >= zxid is supposed to be an invariant across all invocations of the 
server; however the following change to FileTxnLog.java seems to make 
the problem go away.
diff FileTxnLog.java /tmp/FileTxnLog.java
358c358,359
<                 next();
---
 >               if (!next())
 >                   return;
447c448,450
<                 inputStream.close();
---
 >               if (inputStream != null) {
 >                   inputStream.close();
 >               }
","[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-251,Blocker,Thomas Vinod Johnson,Fixed,2008-12-10T21:55:46.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,NullPointerException stopping and starting Zookeeper servers,2009-02-13T21:18:53.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>]",2.0
Mahadev Konar,[],2008-12-08T18:11:32.000+0000,Mahadev Konar,the isvalidsnapshot will fail with negative seek if 0 snapshot files exist. We should just return false in case the size of the snapshot is less than 5 bytes.,"[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-250,Major,Mahadev Konar,Fixed,2008-12-11T21:17:21.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,isvalidsnapshot should handle the case of 0 snapshot files better.,2009-02-13T21:18:53.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>]",1.0
Nitay Joffe,[],2008-12-08T09:10:31.000+0000,Nitay Joffe,"This was changed in SVN 700690:

http://svn.apache.org/viewvc/hadoop/zookeeper/trunk/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeer.java?r1=700690&r2=700689&pathrev=700690","[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-249,Trivial,Nitay Joffe,Fixed,2008-12-09T05:10:11.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,QuorumPeer.getClientPort() always returns -1,2009-02-13T21:18:50.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2008-12-05T18:22:48.000+0000,Patrick D. Hunt,formatting of C api is wrong in acl section of prog guide,"[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-247,Major,Patrick D. Hunt,Fixed,2008-12-10T00:42:29.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,fix formatting of C API in ACL section of programmer guide,2009-02-13T21:18:50.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2008-12-04T21:08:20.000+0000,Patrick D. Hunt,"Owen O'Malley mentioned:
---------
we need to change both the README and quick start to assume you are working with a release instead of a source tarball. Apache releases are the approved way of getting the project. Documentation that assumes they are getting source themselves doesn't reflect that. 
---------","[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-245,Major,Patrick D. Hunt,Fixed,2008-12-10T00:22:34.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"update readme/quickstart to be release tar, rather than source, based",2009-02-13T21:18:50.000+0000,"[<JIRA Version: name='3.0.1', id='12313500'>]",1.0
,"[<JIRA Component: name='tests', id='12312427'>]",2008-12-03T21:18:19.000+0000,Flavio Paiva Junqueira,Attaching output log file.,"[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-244,Minor,Flavio Paiva Junqueira,Cannot Reproduce,2009-05-19T22:22:55.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,AsyncOpsTest fails when running consecutively,2009-07-08T20:23:56.000+0000,"[<JIRA Version: name='3.0.1', id='12313500'>]",0.0
Mahadev Konar,"[<JIRA Component: name='tests', id='12312427'>]",2008-12-01T18:27:59.000+0000,Mahadev Konar,,"[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-242,Critical,Mahadev Konar,Fixed,2008-12-05T18:25:02.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper c tests are faliling..,2009-02-13T21:18:50.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='build', id='12312383'>]",2008-11-26T19:50:08.000+0000,Nigel Daley,"When the ZooKeeper distro is downloaded, if the clean target is run, it deletes the .revision file:

{quote}
clean:
  [delete] Deleting directory /home/foo/zookeeper-3.0.1/build
  [delete] Deleting directory /home/foo/zookeeper-3.0.1/src/java/generated
  [delete] Deleting directory /home/foo/zookeeper-3.0.1/src/c/generated
  [delete] Deleting directory /home/foo/zookeeper-3.0.1/.revision
{quote}

This causes subsequent builds in the distro to fail with:

{quote}
compile-main:
   [javac] Compiling 73 source files to /home/foo/zookeeper-3.0.1/build/classes
   [javac] /home/foo/zookeeper-3.0.1/src/java/main/org/apache/zookeeper/Version.java:21: package org.apache.zookeeper.version does not exist
   [javac] public class Version implements org.apache.zookeeper.version.Info{
   [javac]                                                             ^
   ...
{quote}


","[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-241,Critical,Nigel Daley,Fixed,2008-12-09T22:16:28.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Build of a distro fails after clean target is run,2009-02-13T21:18:50.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='java client', id='12312381'>]",2008-11-26T16:26:54.000+0000,Flavio Paiva Junqueira,"java.lang.NullPointerException
	at org.apache.jute.Utils.toCSVBuffer(Utils.java:234)
	at org.apache.jute.CsvOutputArchive.writeBuffer(CsvOutputArchive.java:101)
	at org.apache.zookeeper.proto.GetDataResponse.toString(GetDataResponse.java:48)
	at java.lang.String.valueOf(String.java:2827)
	at java.lang.StringBuilder.append(StringBuilder.java:115)
	at org.apache.zookeeper.ClientCnxn$Packet.toString(ClientCnxn.java:230)
	at java.lang.String.valueOf(String.java:2827)
	at java.lang.StringBuilder.append(StringBuilder.java:115)
	at org.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:586)
	at org.apache.zookeeper.ClientCnxn$SendThread.doIO(ClientCnxn.java:626)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:852)
","[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-240,Major,Flavio Paiva Junqueira,Fixed,2009-01-14T00:56:44.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Yet another npe,2009-02-13T21:18:50.000+0000,"[<JIRA Version: name='3.0.1', id='12313500'>]",1.0
Benjamin Reed,"[<JIRA Component: name='server', id='12312382'>]",2008-11-24T21:31:13.000+0000,Benjamin Reed,I think the way the HostAuthenticationProvider is implemented could cause serious performance problems if DNS is slow or broken. The problem is that we need to do a reverse hostname resolution during connection establishment. I suggest it be removed.,"[<JIRA Version: name='3.2.0', id='12313491'>, <JIRA Version: name='3.2.1', id='12314068'>]",Bug,ZOOKEEPER-238,Major,Benjamin Reed,Fixed,2009-08-27T00:05:10.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,HostAuthenicationProvider should be removed,2009-09-05T22:36:18.000+0000,[],1.0
Mahadev Konar,[],2008-11-24T19:48:12.000+0000,Mahadev Konar,,"[<JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-232,Blocker,Mahadev Konar,Fixed,2008-11-24T20:50:00.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,testablezookeeper file is missing apache licence headers.,2009-02-13T21:18:50.000+0000,[],1.0
Mahadev Konar,[],2008-11-18T20:02:06.000+0000,Mahadev Konar,one of the test files is missing apache headers ... ,"[<JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-228,Blocker,Mahadev Konar,Fixed,2008-11-18T20:13:48.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,apache header missing in DBtest,2009-02-13T21:18:50.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Chris Darroch,"[<JIRA Component: name='c client', id='12312380'>]",2008-11-18T19:50:16.000+0000,Chris Darroch,"Recent gcc compilers issue warnings when function declarations for functions with no arguments don't specific ""void"".  The attached patch fixes one such warning for create_buffer_oarchive() in recordio.h.","[<JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-227,Minor,Chris Darroch,Fixed,2008-11-19T20:54:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,gcc warning from recordio.h,2009-02-13T21:18:50.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Mahadev Konar,"[<JIRA Component: name='server', id='12312382'>]",2008-11-18T19:27:26.000+0000,Patrick D. Hunt,"An NPE will be generated on the server (and resulting in client getting MarshallingException) if exists() is called on a node with null data.

workaround is to create the node with non-null data.

be sure to update the tests for this case
","[<JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-226,Blocker,Patrick D. Hunt,Fixed,2008-11-18T23:46:52.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,exists calls fails on server if node has null data,2009-02-13T21:18:50.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='java client', id='12312381'>]",2008-11-12T23:21:29.000+0000,Patrick D. Hunt,"the following code is missing conditional log call based on result of remove call (size > 0)

    // XXX This shouldn't be needed, but just in case
>     synchronized (existWatches) {
>         addTo(existWatches.remove(path), result);
>         LOG.warn(""We are triggering an exists watch for delete! Shouldn't happen!"");
>     }
> ","[<JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-221,Major,Patrick D. Hunt,Fixed,2008-11-19T10:24:43.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,log message in ZkWatchManager.materialize missing conditional,2009-02-13T21:18:49.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",2.0
Patrick D. Hunt,[],2008-11-11T22:01:12.000+0000,Patrick D. Hunt,"in watchertest there are some event.poll calls that have 1milli timeouts

        e = localWatcher.events.poll(1, TimeUnit.MILLISECONDS);

this is showing falure in some cases under hudson (I assume when it's under load from other tests running for other proj)

We should review the poll calls and verify adequate timeouts.
","[<JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-219,Minor,Patrick D. Hunt,Fixed,2008-11-18T00:13:06.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,events.poll timeout in watcher test too short,2009-02-13T21:18:49.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2008-11-10T22:24:08.000+0000,Patrick D. Hunt,"Report from user Murali Vemulapati. The wiki recipe page also should be updated to handle multiple clients using the barrier on the same system (multiple processes, etc...). Currently the docs/example support only a single user of the barrier per host.

------------------

I believe there is a typo in the barrier example given at:

http://hadoop.apache.org/zookeeper/docs/current/zookeeperTutorial.html

With the following fix, the program runs as expected:
==============
83c83
<                 this.name = new String(InetAddress.getLocalHost().getCanonicalHostName().toString());
---
>                 name = new String(InetAddress.getLocalHost().getCanonicalHostName().toString());
100c100
<                     CreateMode.EPHEMERAL_SEQUENTIAL);
---
>                     CreateMode.EPHEMERAL);
==============
The first change assigns the name to the instance variable 'name' of Barrier class (otherwise the 'name' instance variable will have a value of 'null'
when calling zk.create to create the child node under the root barrier node).
The second change lets us run multiple processes on the same machine.

thanks
murali

","[<JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-218,Major,Patrick D. Hunt,Fixed,2008-11-18T23:14:13.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,error in barrier recipe example code,2009-02-13T21:18:49.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2008-10-29T06:02:37.000+0000,Patrick D. Hunt,"see
http://hadoop.apache.org/zookeeper/docs/current/zookeeperProgrammers.html#Zookeeper+C+client+API
and compare with src/c/zookeeper.h
","[<JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-213,Major,Patrick D. Hunt,Fixed,2008-11-17T23:51:11.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,programmer guide C api docs are out of sync with latest zookeeper.h ,2009-02-13T21:18:49.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Mahadev Konar,[],2008-10-28T21:54:57.000+0000,Mahadev Konar,the snapshot in 3.0 is syunchronous. this will affect performance of the system.,"[<JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-212,Critical,Mahadev Konar,Fixed,2008-11-17T23:22:13.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,snapshot is synchronous in 3.0,2009-02-13T21:18:49.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Flavio Paiva Junqueira,[],2008-10-27T23:22:00.000+0000,Mahadev Konar,The quoruom servers throw a nullpointer exception and still keep running. We should atleast have a nice debug message and quit... ,"[<JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-209,Major,Mahadev Konar,Fixed,2008-11-18T17:25:27.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,nullpointerexception if election port is not specified.,2013-05-02T02:29:17.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Austin Bennett,"[<JIRA Component: name='c client', id='12312380'>]",2008-10-27T05:31:11.000+0000,Austin Bennett,"The Zookeeper C client library uses gethostbyname and strtok, both of which are not safe to use from multiple threads.

The problem is resolved by using getaddrinfo and strtok_r in place of the older API.

","[<JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-208,Critical,Austin Bennett,Fixed,2008-11-19T20:49:20.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Zookeeper C client uses API that are not thread safe, causing crashes when multiple instances are active",2009-02-13T21:18:49.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2008-10-23T19:20:56.000+0000,Patrick D. Hunt,"Feedback from Doug Cutting on 3.0.0 documentation:

 - The ""Zookeeper Documentation"" tab should contain the version number.
 - ""Informal Documentation"" might better be named ""Other Documentation"".
 - The ""Other Info"" page should be removed, since it contains nothing. ","[<JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-206,Minor,Patrick D. Hunt,Fixed,2008-11-18T23:36:02.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,documentation  tab should contain the version number (and other small site changes),2009-02-13T21:18:49.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Mahadev Konar,[],2008-10-22T21:46:32.000+0000,Flavio Paiva Junqueira,"When running ant with the code from the release tarball, I get the following messages and stack trace:

{noformat}
svn-revision:
     [exec] svn: '.' is not a working copy

version-info:
     [java] All version-related parameters must be valid integers!
     [java] Exception in thread ""main"" java.lang.NumberFormatException: For input string: """"
     [java]     at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
     [java]     at java.lang.Integer.parseInt(Integer.java:468)
     [java]     at java.lang.Integer.parseInt(Integer.java:497)
     [java]     at org.apache.zookeeper.version.util.VerGen.main(VerGen.java:111)
     [java] Java Result: 1
{noformat}

This seems to be because the code is not coming from svn, so it can't find the version information.","[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-205,Minor,Flavio Paiva Junqueira,Duplicate,2008-12-05T19:48:46.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Error on version-info when compiling from the tarball distribution,2009-02-13T21:18:49.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",0.0
Benjamin Reed,"[<JIRA Component: name='java client', id='12312381'>]",2008-10-22T06:06:58.000+0000,Benjamin Reed,When the ZooKeeper java client  makes a connection it queues a SetWatches  request. The problem is that it puts the request at the end of the outgoing requests. This means that watches for requests that were queued before the connection is made and after the disconnect may improperly get their watches triggered.,"[<JIRA Version: name='3.0.1', id='12313500'>, <JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-204,Major,Benjamin Reed,Fixed,2008-11-19T10:23:06.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,SetWatches needs to be the first message after auth messages to the server,2009-02-13T21:18:49.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2008-10-22T03:43:50.000+0000,Patrick D. Hunt,"typo in releasenotes note on datalog/data dirs.
","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-203,Minor,Patrick D. Hunt,Fixed,2008-10-22T03:47:24.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,fix datadir typo in releasenotes,2008-10-26T01:10:44.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Flavio Paiva Junqueira,[],2008-10-21T21:24:55.000+0000,Flavio Paiva Junqueira,"One of our users has observed that an ephemeral znode had gone away once its creator had disconnected according to the leader, but one follower believed that it existed long after the znode had been deleted. Apparently the follower was never going to delete it. Because the leader wouldn't recognize the znode as an existing one, any attempt to delete the znode failed.  We have to investigate if this is related to any known bug, although, to my knowledge, this is the first time it happens. It is important to note that the user was running an older version of our code.","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-202,Major,Flavio Paiva Junqueira,Fixed,2008-10-22T17:17:44.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Phantom ephemeral node,2010-03-26T17:28:11.000+0000,[],1.0
Mahadev Konar,"[<JIRA Component: name='server', id='12312382'>]",2008-10-21T04:55:16.000+0000,Patrick D. Hunt,"The snapshot and transaction log files are not validating the magic numbers when read.

Mahadev, can you update the code and tests for this? Possible for 3.0 or wait post 3.0? (feel free to fix now or reassign version)

Please add tests for this.
","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-201,Blocker,Patrick D. Hunt,Fixed,2008-10-21T18:55:54.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,validate magic number when reading snapshot and transaction logs,2008-10-26T01:10:44.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2008-10-21T04:39:07.000+0000,Patrick D. Hunt,"the magic number for the snapshot and transaction logs are currently the same - they should be different. Also the magic numbers should also be more indicative of the type of file (currently ""AK47"" for both, not very useful in determining type of file)","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-200,Major,Patrick D. Hunt,Fixed,2008-10-21T17:30:36.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,the magic number for snapshot and log must be different (currently same),2008-10-26T01:10:44.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Mahadev Konar,"[<JIRA Component: name='server', id='12312382'>]",2008-10-20T23:59:08.000+0000,Patrick D. Hunt,"there are 2 log messages during server startup that are misleading:

2008-10-20 16:36:41,135 - INFO  [main:FileTxnLog$FileTxnIterator@441] - EOF excepton java.io.EOFException: Failed to read


2008-10-20 16:36:41,120 - ERROR [main:FileTxnSnapLog@114] - 2(higestZxid) >= 2(next log) for type 1


the first log should be DEBUG and should be changed to say something like eof reached in <file>, reading next file

while the second log seems to indicate an error - however the server is still starting. either this is not an error, or it is an error and it's not being handled correctly.","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-199,Minor,Patrick D. Hunt,Fixed,2008-10-21T04:36:49.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,fix log messages in persistence code,2008-10-26T01:10:44.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2008-10-20T20:46:36.000+0000,Patrick D. Hunt,add license to file & run the rat tool to verify svn,"[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-198,Minor,Patrick D. Hunt,Fixed,2008-10-20T21:09:01.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,apache license header missing from FollowerSyncRequest.java,2008-10-26T01:10:44.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Benjamin Reed,"[<JIRA Component: name='c client', id='12312380'>]",2008-10-20T12:12:56.000+0000,Maxim P. Dementiev,"In zookeeper.h:
 * \param state connection state. If the type is ZOO_SESSION_EVENT, the state value 
 * will be one of the *_STATE constants, otherwise -1.
but for this sequence:
 1. zoo_awexists(name)
 2. zoo_acreate(name)
we've got a watcher callback with type=ZOO_CREATED_EVENT and state!=-1

I think the comment should be altered to underline the difference between zookeeper_init() callback usage and others (""the getter API functions with the ""w"" prefix in their names"") for the new ""watcher object"" style.
It looks like the type and path argument values are useless for the former (because type is always ZOO_SESSION_EVENT, and path is always empty), and the state is useless for the latter (it is considered to be -1).

And more,  the state of the legacy style should be commented - will it be marked as obsolete? Or will it be supported in the future?

I wonder if there are any plans to split current watcher_fn callback to something like:
1. new watcher_fn: typedef void (*watcher_fn)(zhandle_t *zh, int type, const char *path, void *watcherCtx);
2. connection_fn: typedef void (*watcher_fn)(zhandle_t *zh, int state, void *context);
Because, you see, the usage is different and there is no any common set of arguments apart from zh (which is common for API) and context.","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-196,Major,Maxim P. Dementiev,Fixed,2009-06-08T21:31:14.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"doxygen comment for state argument of watcher_fn typedef and implementation differ (""...one of the *_STATE constants, otherwise -1"")",2009-07-08T20:23:56.000+0000,[],1.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2008-10-16T00:14:40.000+0000,Patrick D. Hunt,"In particular the pkg structure has changed.
","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-193,Major,Patrick D. Hunt,Fixed,2008-10-16T20:09:46.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,update java example doc to compile with latest zookeeper,2008-10-26T01:10:44.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2008-10-15T22:49:54.000+0000,Patrick D. Hunt,"a config file with trailing whitespace can cause number format exception

for example a line such as

clientPort=2181

where 2181 is followed by a space character, will fail to parse with number format excetion ""2181 "" (notice the space).

We need to trim whitespace when parsing numbers","[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-192,Minor,Patrick D. Hunt,Fixed,2009-05-22T19:02:44.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,trailing whitespace in config file can cause number format exceptions,2009-07-08T20:23:56.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2008-10-14T23:42:35.000+0000,Robbie Scott,"Right now, on the main documentation page, and also in the left-hand nav, the docs are in a single flat list. Let's reorganize them into related groups.  Maybe, for example, something like this:

Overview
  - Overview
  - Getting Started

Developer Docs
- Programmer's Guide
- Recipes
- Java Example
- Barrier and Queues Tutorial
- API

Administrtor Docs
- Administrators Guide

Contributor Docs
- Zookeeper Internals

Miscellaneous
- FAQ
- Wiki
- Other Information","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-190,Major,Robbie Scott,Fixed,2008-10-15T04:05:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Reorg links to docs and navs to docs into related sections,2008-10-26T01:10:44.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2008-10-14T22:39:52.000+0000,Robbie Scott,It's possible to build the documentation without validating the XML of the source document. ,"[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-189,Major,Robbie Scott,Fixed,2008-10-14T23:45:36.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,forrest build not validated xml of input documents,2008-10-26T01:10:44.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>, <JIRA Component: name='java client', id='12312381'>]",2008-10-14T18:52:10.000+0000,Patrick D. Hunt,"End user API docs are missing for CreateMode. (dev api docs are fine)

","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-187,Minor,Patrick D. Hunt,Fixed,2008-10-15T16:17:55.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,CreateMode api docs missing,2008-10-26T01:10:44.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Maxim P. Dementiev,"[<JIRA Component: name='c client', id='12312380'>]",2008-10-13T11:52:48.000+0000,Maxim P. Dementiev,"Some compilation environments provide implicit inclusion of certain system headers.
But any way it's not a reason to exploit it in platform-independent projects.

TestHashtable.cc and LibCMocks.h from src/c/tests/ use those functions without including corresponding system headers.

Modern versions of GCC are very strict.
You cannot build the code like this with the help of GCC version 4.3.","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-184,Major,Maxim P. Dementiev,Fixed,2008-10-14T20:29:23.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"tests: An explicit include derective is needed for the usage of memcpy(), memset(), strlen(), strdup() and free() functions.",2008-10-26T01:10:43.000+0000,[],1.0
Maxim P. Dementiev,"[<JIRA Component: name='c client', id='12312380'>]",2008-10-10T16:05:24.000+0000,Maxim P. Dementiev,"Having:
        char buf[4096*16];
Present:
        buf[sizeof(buf)]=0;
Must be:
        buf[sizeof(buf)-1]=0;
","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-183,Major,Maxim P. Dementiev,Fixed,2008-10-14T20:17:47.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Array subscript is above array bounds in od_completion(), src/cli.c.",2008-10-26T01:10:43.000+0000,[],1.0
Maxim P. Dementiev,"[<JIRA Component: name='c client', id='12312380'>]",2008-10-10T15:49:42.000+0000,Maxim P. Dementiev,"Please, add this test to src/c/tests/TestZookeeperInit.cc to reproduce this:

    void testEmptyAddressString()
    {
        zh=zookeeper_init("""",0,0,0,0,0);
        CPPUNIT_ASSERT(zh==0);
    }
","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-182,Major,Maxim P. Dementiev,Fixed,2008-10-14T20:10:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper_init accepts empty host-port string and returns valid pointer to zhandle_t.,2008-10-26T01:10:43.000+0000,[],1.0
Patrick D. Hunt,[],2008-10-09T15:45:23.000+0000,Robbie Scott,"Some Source Forge Documents did not get moved over: 

javaExample
zookeeperTutorial
(These two I will move over directly)
 
zookeeperLogging
zookeeperAtomicBroadcast
(These two I will roll up into a new document: zookeeperInternals)

We can expand zookeeperInterals as new sections are needed, maybe one day morphing it into the ""complete guide for zookeeper contributers""","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-181,Major,Robbie Scott,Fixed,2008-10-10T13:58:38.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Some Source Forge Documents did not get moved over: javaExample, zookeeperTutorial, zookeeperInternals",2008-10-26T01:10:43.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='leaderElection', id='12312378'>]",2008-10-06T20:50:15.000+0000,Mahadev Konar,fast leader election test failing .,"[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-178,Major,Mahadev Konar,Fixed,2008-10-10T23:16:55.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,FLE test.,2008-10-26T01:10:43.000+0000,[],1.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2008-10-06T15:24:01.000+0000,Robbie Scott,"There is a chapter in the ZooKeeper Programmer's Guide called: ""Building Blocks: A Guide to ZooKeeper Operations"" that currently contains no content.  The idea behind this section is document all the basic things you can do with the ZooKeeper API -- not every little detail, or even every call in the API, but the basics, like: how to connect to a server, how to read a node, how to write to a node, how to set a watch, etc. I will create a subtask for every section in this chapter.

I don't mind writing up the these sections, if I can get the information, whether its' by phone, email, im, etc. ",[],Bug,ZOOKEEPER-168,Major,Robbie Scott,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,"Programmer's Guide ""Basic Operations"" section is missing content",2008-10-08T18:04:39.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",0.0
,"[<JIRA Component: name='documentation', id='12312422'>]",2008-10-06T14:01:55.000+0000,Robbie Scott,"I'm adding new sections to the ZooKeeper Admin guide, but I don't have the content to fill them out. These sections are: 
- Designing a ZooKeeper Deployment
- Provisioning
- Things to Consider: ZooKeeper Strengths and Limitations
- Administering
- Monitoring
- Logging
- Troubleshooting

I'll add a subtask for each section.  

Note: If the area experts prefer, I can conduct a phone interview or email conversation or IM with them on the topic, and then write up the section so they don't have to. ",[],Bug,ZOOKEEPER-160,Major,Robbie Scott,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Admin / Ops Guide Missing Content,2008-10-08T00:07:02.000+0000,[],0.0
Flavio Paiva Junqueira,"[<JIRA Component: name='leaderElection', id='12312378'>]",2008-10-06T13:15:47.000+0000,Flavio Paiva Junqueira,"In the default implementation of leader election, there are two undesirable cases that need to be covered:

1- If there is a leader elected and this leader is supported by at least  quorum of peers, then it can happen that one peer disconnects from the leader, and initiates a new leader election. As it is a new leader election, we increment the logical clock of this peer, and according to the current implementation, this peer won't accept any vote from a peer with a lower LE turn (corresponds to the value of the logical clock of the voting peer). The attached patch corrects this problem by allowing a peer to go back to a previous epoch in the case a majority votes for a leader, and the peer also receives a vote from the leader. This feature allows a peer to correct a false suspicion of the current leader;

2- If a peer  advances to a new turn before others, then it may end up voting for a peer that either does not have the highest server id or the the highest zxid. The attached patch fixes this problem by resetting the vote a peer when it updates its logical clock upon receiving a notification with a higher turn value.","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-159,Major,Flavio Paiva Junqueira,Fixed,2008-10-08T18:00:53.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Cover two corner cases of leader election,2008-10-26T01:10:43.000+0000,[],1.0
Flavio Paiva Junqueira,[],2008-10-04T22:20:35.000+0000,Flavio Paiva Junqueira,"In the patch of JIRA 127, I forgot to set the state of a peer when this peer is looking for a leader and it receives a message from the current leader. In this patch, I have fixed this problem, and also returned to what we had previously. With this current patch, when a peer joins and there is already a leader elected, the joining peer will only recognize the new leader as the leader once it receives a confirmation from a majority. The alternative is to set the leader once we receive a message from a peer claiming to be the leader (what we have on trunk now, although broken because we don't set the state of the peer), but there could be cases in which a peer believes to be leader, although it is not the leader any longer, and the joining peer would select this false leader to be its leader. Eventually, the false leader would timeout, and both processes would select the correct leader. This small fix gets rid of such problems, though.","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-157,Critical,Flavio Paiva Junqueira,Fixed,2008-10-05T11:11:35.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Peer can't find existing leader,2008-10-26T01:10:43.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2008-10-02T13:28:03.000+0000,Patrick D. Hunt,"In ""the zookeeper project"" section of
http://hadoop.apache.org/zookeeper/docs/current/zookeeperOver.html

we should remove the hod reference (doesn't use zk) and also update the second paragraph, perhaps remove? Since we are already on the apache zk page.
","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-155,Minor,Patrick D. Hunt,Fixed,2008-10-03T23:26:47.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"improve ""the zookeeper project"" section of overview doc",2008-10-26T01:10:43.000+0000,[],1.0
Patrick D. Hunt,"[<JIRA Component: name='documentation', id='12312422'>]",2008-10-02T13:20:45.000+0000,Patrick D. Hunt,"from question on the user mailing list:

the ""Reliability in the Presence of Errors"" graph on http://hadoop.apache.org/zookeeper/docs/current/zookeeperOver.html does not list how many ZooKeeper quorum nodes are in use, or the fraction of reads/writes.

ben mentioned:

Here is the missing text:
To show the behavior of the system over time as failures are injected we
ran a ZooKeeper service made up of 7 machines. We ran the same
saturation benchmark as before, but this time we kept the write
percentage at a constant 30\%, which is a conservative ratio of our
expected workloads.","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-154,Minor,Patrick D. Hunt,Fixed,2008-10-03T23:27:05.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,reliability graph diagram in overview doc needs context,2008-10-26T01:10:43.000+0000,[],1.0
Flavio Paiva Junqueira,[],2008-10-01T20:23:05.000+0000,Flavio Paiva Junqueira,"The patch of jira 127 changed the format of server configuration files, but it didn't change the documentation. ","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-151,Major,Flavio Paiva Junqueira,Fixed,2008-10-16T21:55:42.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Document change to server configuration,2008-10-26T01:10:43.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Mahadev Konar,[],2008-10-01T17:28:34.000+0000,Mahadev Konar,the build is broekn with ZOOKEEPER-38 checked in.,"[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-150,Blocker,Mahadev Konar,Fixed,2008-10-01T19:06:15.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper build broken,2008-10-26T01:10:43.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Flavio Paiva Junqueira,[],2008-09-13T10:02:12.000+0000,Flavio Paiva Junqueira,"Frequently the servers deadlock in QuorumCnxManager:initiateConnection on
s.read(msgBuffer) when reading the challenge from the peer.

Calls to initiateConnection and receiveConnection are synchronized, so only one or the other can be executing at a time. This prevents two connections from opening between the same pair of servers.

However, it seems that this leads to deadlock, as in this scenario:

{noformat}
A (initiate --> B)
B (initiate --> C)
C (initiate --> A)
{noformat}

initiateConnection can only complete when receiveConnection runs on the remote peer and answers the challenge. If all servers are blocked in initiateConnection, receiveConnection never runs and leader election halts.
","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-140,Major,Flavio Paiva Junqueira,Fixed,2008-10-01T09:21:19.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Deadlock in QuorumCnxManager,2008-10-26T01:10:43.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",2.0
Benjamin Reed,"[<JIRA Component: name='c client', id='12312380'>]",2008-09-08T21:03:47.000+0000,Patrick D. Hunt,"if a single watcher (A) is registered on a single node for both a getdata and exists watch the second watch event may be lost:

1) getdata(""node"", A)
2) setdata(""node""...)
3) exists(""node"", A)
4) delete(""node""...)

if watch events for 2 is processed on the client (zookeeper.java, zkwatcher) after 3 completes then the zkwatcher process event method will clear the watch and the subsequent operation's (4) event will be ignored","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-138,Major,Patrick D. Hunt,Fixed,2008-10-22T01:23:13.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,c client watcher objects can lose events,2008-10-26T01:10:42.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='java client', id='12312381'>]",2008-09-08T21:02:48.000+0000,Patrick D. Hunt,"if a single watcher (A) is registered on a single node for both a getdata and exists watch the second watch event may be lost:

1) getdata(""node"", A)
2) setdata(""node""...)
3) exists(""node"", A)
4) delete(""node""...)

if watch events for 2 is processed on the client (zookeeper.java, zkwatcher) after 3 completes then the zkwatcher process event method will clear the watch and the subsequent operation's (4) event will be ignored","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-137,Major,Patrick D. Hunt,Fixed,2008-09-24T21:05:52.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,client watcher objects can lose events,2008-10-26T01:10:42.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Benjamin Reed,[],2008-09-05T18:27:46.000+0000,Patrick D. Hunt,The attached test causes all of the followers of a quorum to hang. Leader continues to function correctly.,"[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-136,Major,Patrick D. Hunt,Fixed,2008-10-03T13:12:35.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,sync causes hang in all followers of quorum,2008-10-26T01:10:42.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Patrick D. Hunt,"[<JIRA Component: name='tests', id='12312427'>]",2008-09-03T20:46:12.000+0000,Patrick D. Hunt,"There is a bug in the ClientTest.java unit test, timing issue in ""withWatcherObj"" test. Patch forthcoming.
","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-133,Major,Patrick D. Hunt,Fixed,2008-09-05T14:39:54.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,hudson tests failing intermittently,2008-10-26T01:10:42.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Benjamin Reed,"[<JIRA Component: name='leaderElection', id='12312378'>]",2008-09-02T17:27:07.000+0000,Benjamin Reed,"I think there is a race condition that is probably easy to get into with the old leader election and a large number of servers:

1) Leader dies
2) Followers start looking for a new leader before all Followers have abandoned the Leader
3) The Followers looking for a new leader see votes of Followers still following the (now dead) Leader and start voting for the dead Leader
4) The dead Leader gets reelected.

For the old leader election a server should not vote for another server that is not nominating himself.","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-131,Major,Benjamin Reed,Fixed,2008-09-19T01:25:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Old leader election can elect a dead leader over and over again,2008-10-26T01:10:42.000+0000,[],2.0
Flavio Paiva Junqueira,"[<JIRA Component: name='quorum', id='12312379'>]",2008-08-27T13:18:34.000+0000,Mark Harwood,"In QuorumCnxManager.toSend there is a call to create a connection as follows:
    channel = SocketChannel.open(new InetSocketAddress(addr, port));

Unfortunately ""addr"" is the ip address of a remote server while ""port"" is the electionPort of *this* server.
As an example, given this configuration (taken from my zoo.cfg)
  server.1=10.20.9.254:2881
  server.2=10.20.9.9:2882
  server.3=10.20.9.254:2883
Server 3 was observed trying to make a connection to host 10.20.9.9 on port 2883 and obviously failing.

In tests where all machines use the same electionPort this bug would not manifest itself.","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-127,Critical,Mark Harwood,Fixed,2008-10-01T08:40:15.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Use of non-standard election ports in config breaks services,2009-11-12T15:23:30.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",3.0
,"[<JIRA Component: name='java client', id='12312381'>]",2008-08-26T21:25:00.000+0000,Patrick D. Hunt,"Moving the hang issue from ZOOKEEPER-63 to here. See 63 for background and potential patch (patch_ZOOKEEPER-63.patch).

specifically (from James): 

""I'm thinking the close() method should not wait() forever on the disconnect packet, just a closeTimeout length - say a few seconds. Afterall blocking and forcing a reconnect just to redeliver the disconnect packet seems a bit silly - when the server has to deal with clients which just have their sockets fail anyway""",[],Bug,ZOOKEEPER-126,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,zookeeper client close operation may block indefinitely,2022-02-03T08:50:23.000+0000,[],4.0
,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>, <JIRA Component: name='tests', id='12312427'>]",2008-08-25T01:00:45.000+0000,Stu Hood,StatCallback appears to be broken in trunk. I'll attach a patch for AsyncTest that triggers the behaviour.,"[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-124,Major,Stu Hood,Invalid,2008-09-26T18:02:33.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,StatCallback is broken in trunk,2008-10-26T01:10:42.000+0000,[],1.0
Jakob Homan,"[<JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='tests', id='12312427'>]",2008-08-24T19:29:13.000+0000,Jakob Homan,"Copy and paste strikes again.  In two logger instantiations, the wrong class is passed to the constructor.  In ClientTest.java and ClientCnxn.java","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-123,Minor,Jakob Homan,Fixed,2008-08-25T18:17:05.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"In two places, the wrong class is specified for the logger",2008-10-26T01:10:42.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Anthony Urso,[],2008-08-19T00:05:24.000+0000,Anthony Urso,"java.lang.NullPointerException
        at org.apache.jute.Utils.toCSVString(Utils.java:128)
        at org.apache.jute.CsvOutputArchive.writeString(CsvOutputArchive.java:94
)
        at org.apache.zookeeper.proto.WatcherEvent.toString(WatcherEvent.java:60
)
        at java.lang.String.valueOf(String.java:2827)
        at java.lang.StringBuilder.append(StringBuilder.java:115)
        at com.liveoffice.mailindex.watchers.SuicidalWatcher.process(SuicidalWat
cher.java:11)
        at org.apache.zookeeper.ZooKeeper.processWatchEvent(ZooKeeper.java:157)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:268)
","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-122,Major,Anthony Urso,Fixed,2008-08-19T18:01:28.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,NPE in jute's Utils.toCSVString,2008-10-26T01:10:42.000+0000,[],1.0
Mahadev Konar,"[<JIRA Component: name='server', id='12312382'>]",2008-08-12T18:57:23.000+0000,Patrick D. Hunt,"The SyncRequestProcessor is not closing log stream during shutdown. 

See FIXMEs with this ID in ClientBase.java -- I've commented out the assertion for the time being (checking for logs being deleted), as part of this fix re-enable these asserts and also verify tests on a Windows system.
","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-121,Blocker,Patrick D. Hunt,Fixed,2010-03-16T22:39:59.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,SyncRequestProcessor is not closing log stream during shutdown,2011-06-24T04:53:24.000+0000,[],2.0
,"[<JIRA Component: name='server', id='12312382'>]",2008-08-12T18:27:36.000+0000,Patrick D. Hunt,"When the NIOServerCnxn is shutdown it doesn't handle gracefully - if shutdown is called we should set internal state. In the run method we print unnecessary error messages as we don't currently know that shutdown (gracefully) was called rather than some other more serious, unexpected, condition.
",[],Bug,ZOOKEEPER-120,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,NIOServerCnxn needs to improve shutdown() handling,2008-08-12T18:27:36.000+0000,[],1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='server', id='12312382'>]",2008-08-11T18:07:56.000+0000,Patrick D. Hunt,"followerrequestprocessor:

is the case statement for SYNC supposed to fall through?
                switch (request.type) {
                case OpCode.sync:
                    zks.pendingSyncs.add(request);
                case OpCode.create:

Please update the docs/code appropriately (if correct note it with comment just after the sync case statement.
","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-118,Major,Patrick D. Hunt,Fixed,2008-09-09T17:56:48.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,findbugs flagged switch statement in followerrequestprocessor.run(),2008-10-26T01:10:42.000+0000,[],1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='server', id='12312382'>]",2008-08-11T18:04:32.000+0000,Patrick D. Hunt,"Leader.lead() creates a new thread that can never terminate (short of restarting vm)

naked notifyall in lead() method - what is the condition variable? Best if set inside the sync block
","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-117,Major,Patrick D. Hunt,Fixed,2008-09-24T21:28:09.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,threading issues in Leader election,2008-10-26T01:10:41.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",2.0
Flavio Paiva Junqueira,"[<JIRA Component: name='quorum', id='12312379'>]",2008-08-07T21:10:41.000+0000,Patrick D. Hunt,"Findbugs flagged this, notice that we are checking for null after using the reference.

               if (senderWorkerMap.get(s.socket().getInetAddress()) != null) {
                    senderWorkerMap.get(s.socket().getInetAddress()).finish();
                }

                /*
                 * Start new worker thread with a clean state.
                 */
                if (s != null) {","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-115,Major,Patrick D. Hunt,Fixed,2008-10-13T14:11:02.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Potential NPE in QuorumCnxManager,2008-10-26T01:10:41.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='tests', id='12312427'>]",2008-08-06T21:01:27.000+0000,Patrick D. Hunt,"src/java/main ZooKeeper.java has a method ""public void disconnect()"" that is not part of the public api but put there for testing purposes (to test disconnection of the client from the server w/o actually shutting down the session)

This method needs to be moved out of the public api. preferably we should have a subclass in the test code itself that provides this method.

","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-112,Major,Patrick D. Hunt,Fixed,2008-09-05T20:31:16.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,src/java/main ZooKeeper.java has test code embedded into it.,2008-10-26T01:10:41.000+0000,[],1.0
Jakob Homan,"[<JIRA Component: name='build', id='12312383'>]",2008-08-05T18:18:25.000+0000,Jakob Homan,"The current build.xml ant script uses svnant to obtain the latest revision number from the repo, however svnant is not compatible with subversion 1.5 (http://subversion.tigris.org/svn_1.5_releasenotes.html), and so the build fails with working copies checked out by this version.  The build fails with ""this version of subversion is too old, please get a newer version...""  This will become more apparent as svn 1.5 trickles out; I'm using a brand new dev environment with both subclipse 1.4 and svn 1.5 client, so I got bit rather quickly.

Those with svn 1.5 can get the code from the trunk, but cannot do an ant build.

svnant hasn't been updated in more than a year and appears to be dead, so it may no longer be a viable tool for the ant build.","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-110,Major,Jakob Homan,Fixed,2008-08-15T18:01:42.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Build script relies on svnant, which is not compatible with subversion 1.5 working copies",2008-10-26T01:10:41.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",3.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2008-08-01T22:01:07.000+0000,Patrick D. Hunt,"NPE/ResourceLeak cleanup for issues found during static analysis.
","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-109,Major,Patrick D. Hunt,Fixed,2008-08-08T19:14:00.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,cleanup of NPE and Resource issue nits found by static analysis,2008-10-26T01:10:41.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='server', id='12312382'>]",2008-08-01T02:48:45.000+0000,Flavio Paiva Junqueira,"The current implementation of sync is broken. There is a race condition that causes a follower to return operations out of order, causing clients to drop their connections to a server.

I'll be attaching a patch to fix this problem shortly.","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-108,Major,Flavio Paiva Junqueira,Fixed,2008-08-09T05:02:52.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,sync implementation reorders operations,2008-10-26T01:10:41.000+0000,[],2.0
,[],2008-07-26T15:03:27.000+0000,Flavio Paiva Junqueira,"This exception seems to be harmless, but I thought it would be a good idea to document it just in case I overlooked anything. I think it is harmless because it is thrown inside the while loop that obtains the keys ready for I/O on Factory.run(). The penalty is skipping all other keys available in the for loop that follows the select call, but since we call again select in the next interation, we don't miss much. An optimizatio, however, would be catching this exception inside the for loop to avoid the extra call to select.  

{noformat}
2008-07-24 21:09:03,894 -  ERROR - [NIOServerCxn.Factory:NIOServerCnxn$Factory@152] - FIXMSG
java.nio.channels.CancelledKeyException
        at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:55)
        at sun.nio.ch.SelectionKeyImpl.readyOps(SelectionKeyImpl.java:69)
        at com.yahoo.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:136)
2008-07-24 21:09:06,912 -  ERROR - [NIOServerCxn.Factory:NIOServerCnxn$Factory@152] - FIXMSG
java.nio.channels.CancelledKeyException
        at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:55)
        at sun.nio.ch.SelectionKeyImpl.readyOps(SelectionKeyImpl.java:69)
        at com.yahoo.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:136)
{noformat}",[],Bug,ZOOKEEPER-106,Major,Flavio Paiva Junqueira,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,CancelledKeyException,2008-07-26T15:03:27.000+0000,[],5.0
Anthony Urso,"[<JIRA Component: name='java client', id='12312381'>]",2008-07-26T03:27:58.000+0000,Anthony Urso,The ZooKeeper java client main loop crashes on KeeperExceptions.  They should be handled when possible.,"[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-105,Minor,Anthony Urso,Fixed,2008-07-28T18:21:13.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ZooKeeper java client main loop crashes on KeeperExceptions,2008-10-26T01:10:41.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
,"[<JIRA Component: name='jmx', id='12312451'>]",2008-07-25T16:55:14.000+0000,Hiram R. Chirino,All the MXBean interfaces that I've looked at are standard MBean interfaces.  The interface names should get renamed to MBean instaead of MXBean. That way the server can also run on a the Java 1.5 Platform.,[],Bug,ZOOKEEPER-99,Major,Hiram R. Chirino,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,All MXBeans interfaces that don't use complex paramters need to be renamed as MBean interaces. ,2009-11-23T12:10:22.000+0000,[],2.0
Patrick D. Hunt,"[<JIRA Component: name='jmx', id='12312451'>]",2008-07-25T14:22:22.000+0000,Hiram R. Chirino,,"[<JIRA Version: name='3.2.0', id='12313491'>]",Bug,ZOOKEEPER-94,Major,Hiram R. Chirino,Fixed,2009-05-14T20:15:22.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,JMX tests are needed to verify that the JMX MBeans work properly,2009-07-08T20:23:56.000+0000,"[<JIRA Version: name='3.1.0', id='12313381'>, <JIRA Version: name='3.1.1', id='12313649'>]",1.0
Germán Blanco,"[<JIRA Component: name='quorum', id='12312379'>]",2008-07-23T18:29:31.000+0000,Mahadev Konar,"Currently, the follower if lagging behind keeps sending pings to the leader it will stay alive and will keep getting further and further behind the leader. The follower should shut itself down if it is not able to keep up to the leader within some limit so that gurantee of updates can be made to the clients connected to different servers.","[<JIRA Version: name='3.4.6', id='12323310'>, <JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-87,Critical,Mahadev Konar,Fixed,2013-09-26T12:37:50.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Follower does not shut itself down if its too far behind the leader.,2014-03-13T18:16:56.000+0000,"[<JIRA Version: name='3.4.5', id='12321883'>, <JIRA Version: name='3.5.0', id='12316644'>]",7.0
James Strachan,"[<JIRA Component: name='tests', id='12312427'>]",2008-07-23T16:59:46.000+0000,James Strachan,Will attach the test output in an attachment...,"[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-86,Major,James Strachan,Fixed,2010-01-15T22:02:45.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,intermittent test failure of org.apache.zookeeper.test.AsyncTest,2010-03-26T17:24:54.000+0000,[],2.0
,"[<JIRA Component: name='jute', id='12312385'>]",2008-07-16T16:22:06.000+0000,Patrick D. Hunt,"As reported by user:

From: Martin Schaaf <ms@101tec.com>
List-Id: <zookeeper-user.lists.sourceforge.net>

FYI today I found this NPE in the logs. After this Exception an Error  
event was thrown.

java.lang.NullPointerException
	at com.yahoo.jute.Utils.toCSVString(Utils.java:128)
	at com.yahoo.jute.CsvOutputArchive.writeString(CsvOutputArchive.java: 
95)
	at com.yahoo.zookeeper.proto.WatcherEvent.toString(WatcherEvent.java: 
60)
	at net.sf.katta.zk.ZKClient.process(ZKClient.java:404)
	at com.yahoo.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:264)
",[],Bug,ZOOKEEPER-77,Major,Patrick D. Hunt,Duplicate,2008-08-27T18:30:48.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,NPE in jute toCSVString code,2008-08-27T18:30:48.000+0000,[],2.0
Patrick D. Hunt,"[<JIRA Component: name='build', id='12312383'>]",2008-07-16T02:30:29.000+0000,Jakob Homan,"Cobertura library removed due to licensing issues, but the code-coverage ant targets were left in.  Have deleted them and created patch.","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-76,Major,Jakob Homan,Fixed,2008-07-16T18:39:17.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"Commit 677109 removed the cobertura library, but not the build targets.",2008-10-26T01:10:40.000+0000,[],1.0
Patrick D. Hunt,"[<JIRA Component: name='build', id='12312383'>]",2008-07-15T22:50:25.000+0000,Owen O'Malley,The cobertura library is GPL and must be removed from the code base. All of the X.jar files should have matching X.license files that contain their license. There is no license for junit. ,"[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-75,Major,Owen O'Malley,Fixed,2008-07-15T23:51:47.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,cleanup the library directory,2008-10-26T01:10:40.000+0000,[],1.0
Mahadev Konar,"[<JIRA Component: name='java client', id='12312381'>]",2008-07-07T22:11:14.000+0000,Jakob Homan,"the parseACLs(String aclString) method attempts to prase ACLs from the form of scheme:id:perm into the three components, delineated by the colons.  The current version calls indexOf for both the first and second colon, receiving the same value for both and failing as if there were only one colon in the string.  I created a one-line patch to call lastIndexOf for the second colon.","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-68,Minor,Jakob Homan,Fixed,2008-07-07T23:03:13.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"parseACLs in ZooKeeper.java fails to parse elements of ACL, should be lastIndexOf rather than IndexOf",2008-10-26T01:10:40.000+0000,[],1.0
Patrick D. Hunt,"[<JIRA Component: name='java client', id='12312381'>]",2008-06-30T22:08:36.000+0000,Patrick D. Hunt,"There is a race condition in the java close operation on ZooKeeper.java.

Client is sending a disconnect request to the server. Server will close any open connections with the client when it receives this. If the client has not yet shutdown it's subthreads (event/send threads for example) these threads may consider the condition an error. We see this alot in the tests where the clients output error logs because they are unaware that a disconnection has been requested by the client.

Ben mentioned: perhaps we just have to change state to closed (on client) before sending disconnect request.
","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-63,Major,Patrick D. Hunt,Fixed,2008-08-27T23:39:25.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Race condition in client close() operation,2008-10-26T01:10:40.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",3.0
Flavio Paiva Junqueira,"[<JIRA Component: name='server', id='12312382'>]",2008-06-30T11:20:15.000+0000,Flavio Paiva Junqueira,"There are two synchronized blocks locking on different objects, and to me they should be guarded by the same object. Here are the parts of the code I'm talking about:

{noformat}
NIOServerCnxn.readRequest@444
...
          synchronized (this) {
                outstandingRequests++;
                // check throttling
                if (zk.getInProcess() > factory.outstandingLimit) {
                    disableRecv();
                    // following lines should not be needed since we are already
                    // reading
                    // } else {
                    // enableRecv();
                }
            } 
{noformat}

{noformat}
NIOServerCnxn.sendResponse@740
...
         synchronized (this.factory) {
                outstandingRequests--;
                // check throttling
                if (zk.getInProcess() < factory.outstandingLimit
                        || outstandingRequests < 1) {
                    sk.selector().wakeup();
                    enableRecv();
                }
            }
{noformat}

I think the second one is correct, and the first synchronized block should be guarded by ""this.factory"". 

This could be related to issue ZOOKEEPER-57, but I have no concrete indication that this is the case so far.","[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-59,Major,Flavio Paiva Junqueira,Fixed,2010-03-09T02:01:14.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Synchronized block in NIOServerCnxn,2010-03-26T17:24:53.000+0000,[],4.0
Benjamin Reed,"[<JIRA Component: name='java client', id='12312381'>]",2008-06-27T15:38:34.000+0000,Flavio Paiva Junqueira,"There is a race condition involving the ByteByffer incomingBuffer, a field of ClientCnxn.SendThread. SendThread reads a packet in two steps: first it reads the length of the packet, followed by a read of the packet itself. Each of these steps corresponds to a call to doIO() from the main loop of run(). If there is an exception or the session times out, then it may leave incomingBuffer in an inconsistent state. 

The attached patch adds code to reset incomingBuffer upon a call to SendThread.cleanup(). This method is called upon an exception on run().","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-58,Major,Flavio Paiva Junqueira,Fixed,2008-07-27T08:07:11.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Race condition on ClientCnxn.java,2008-10-26T01:10:40.000+0000,[],1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='java client', id='12312381'>]",2008-06-27T08:16:11.000+0000,Flavio Paiva Junqueira,"I have observed several connection timeouts with one single client, during periods of inactivity. From reports of other users, it seems that others are observing a similar issue. 

In my case, tickTime is 2000, and the client sets session timeout to be 10000. Client and servers are on different sites. I suspect there is something wrong with the mechanism that issues ping messages.  ",[],Bug,ZOOKEEPER-57,Major,Flavio Paiva Junqueira,Invalid,2009-01-16T06:19:36.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Connection times out when idle,2009-01-16T08:44:50.000+0000,[],3.0
Andrew Kornev,"[<JIRA Component: name='build', id='12312383'>]",2008-06-26T21:13:32.000+0000,Andrew Kornev,"Due to the recent directory layout change,
1) the ant target ""svn-revision"" fails to retrieve the release number,
2) the ant target ""dist"" fails to create the distribution package.","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-55,Major,Andrew Kornev,Fixed,2008-06-26T23:50:28.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,"build.xml failes to retrieve a release number from SVN and the ant target ""dist"" fails",2008-10-26T01:10:40.000+0000,[],1.0
Patrick D. Hunt,"[<JIRA Component: name='tests', id='12312427'>]",2008-06-25T23:30:36.000+0000,Mahadev Konar,remove the sleeps in the tests. ,"[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-54,Major,Mahadev Konar,Fixed,2008-08-25T23:53:48.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,remove sleeps in the tests.,2008-10-26T01:10:40.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",1.0
Mahadev Konar,[],2008-06-25T22:43:46.000+0000,Mahadev Konar,tests are failing on solaris and randomly on my machine as well.,"[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-53,Blocker,Mahadev Konar,Fixed,2008-06-25T23:06:20.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,tests failing on solaris.,2008-10-26T01:10:40.000+0000,[],1.0
Benjamin Reed,"[<JIRA Component: name='java client', id='12312381'>]",2008-06-20T20:36:38.000+0000,Benjamin Reed,"We need to make clear in the documentation and enforce in the code the following watch event rules:

# A watch event will be delivered once to each watcher, even if it is registered multiple times. For example, if the same watch object is used for getChildren(""/foo"", watchObj) and getData(""/foo"", watchObj, stat) and foo is deleted, watchObj will be called once to processed the NodeDeleted event.
# Session events will be delivered to all watchers.

*Note: a watcher is a Watcher object in Java or a (watch function, context) pair in C.*

There is currently a bug in the Java client that causes the session disconnected event to be delivered twice to the default watcher if the default watcher is also used to watch a path. This violates rule 1.","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-50,Major,Benjamin Reed,Fixed,2010-01-15T21:58:54.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Watch event delivery rules,2013-05-02T02:29:16.000+0000,[],4.0
Benjamin Reed,"[<JIRA Component: name='server', id='12312382'>]",2008-06-20T07:23:28.000+0000,Benjamin Reed,"As reported by Shane:

Still exploring the ACL stuff in Zookeeper.  Tried using setACL for a  
path but get InvalidACL error thrown .... looking at pRequest in  
PrepRequestProcessor ... and in particular these lines ...


                 SetACLRequest setAclRequest = new SetACLRequest();
                 if (!fixupACL(request.authInfo,  
setAclRequest.getAcl())) {
                     throw new KeeperException(Code.InvalidACL);
                 }

a new SetACLRequest will return a null when called in fixupACL  
returning false and throwing the exception .... as far as I can see.","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-49,Major,Benjamin Reed,Fixed,2008-08-05T13:46:59.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,SetACL does not work,2013-05-02T02:29:14.000+0000,[],1.0
Benjamin Reed,"[<JIRA Component: name='server', id='12312382'>]",2008-06-20T06:02:47.000+0000,Benjamin Reed,"AUTH_ID is used (usually done using Ids.CREATOR_ALL_ACL ) to represent the id that was used to authenticate with ZooKeeper. Thus, an exception should be raised if there are no authenticated ids present. Currently, the exception is not being raised.","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-48,Major,Benjamin Reed,Fixed,2008-07-31T20:11:03.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,AUTH_ID not handled correctly when no auth ids are present,2013-05-02T02:29:14.000+0000,[],1.0
,"[<JIRA Component: name='c client', id='12312380'>]",2008-06-18T21:45:37.000+0000,Benjamin Reed,"If a session expires, the zhandle_t becomes invalid and useless; however, the io thread keeps going.

do_io in mt_adaptor.c needs to check the return code of zookeeper_interest and zookeeper_process and get out of the loop if the handle is invalid.",[],Bug,ZOOKEEPER-47,Major,Benjamin Reed,Invalid,2008-06-27T14:57:40.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,io thread still around after a EXPIRED_SESSION or AUTH_FAILED event is received,2008-06-27T14:57:40.000+0000,[],0.0
Patrick D. Hunt,"[<JIRA Component: name='java client', id='12312381'>]",2008-06-10T21:25:05.000+0000,Patrick D. Hunt,"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1985723&group_id=209147&atid=1008544","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-21,Major,Patrick D. Hunt,Fixed,2008-09-26T18:09:56.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Improve zk ctor/watcher (state transition) docs,2008-10-26T01:10:37.000+0000,[],1.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2008-06-10T21:23:59.000+0000,Patrick D. Hunt,"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1981340&group_id=209147&atid=1008544","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-20,Major,Patrick D. Hunt,Fixed,2008-08-27T18:36:36.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Child watches are not triggered when the node is deleted,2008-10-26T01:10:37.000+0000,[],1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='jute', id='12312385'>]",2008-06-10T21:22:54.000+0000,Patrick D. Hunt,"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1981288&group_id=209147&atid=1008544",[],Bug,ZOOKEEPER-19,Major,Patrick D. Hunt,Later,2008-09-09T21:06:55.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Vector of Integers with Jute,2008-09-09T21:10:28.000+0000,[],0.0
Patrick D. Hunt,"[<JIRA Component: name='java client', id='12312381'>]",2008-06-10T21:20:06.000+0000,Patrick D. Hunt,"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1979772&group_id=209147&atid=1008544","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-18,Major,Patrick D. Hunt,Fixed,2008-09-25T22:33:39.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,keeper state inconsistency,2008-10-26T01:10:37.000+0000,[],2.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='documentation', id='12312422'>]",2008-06-10T21:18:37.000+0000,Patrick D. Hunt,"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1967467&group_id=209147&atid=1008544","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-17,Major,Patrick D. Hunt,Fixed,2008-10-10T22:56:54.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,zookeeper_init doc needs clarification,2008-10-26T01:10:37.000+0000,[],1.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>, <JIRA Component: name='server', id='12312382'>]",2008-06-10T21:17:18.000+0000,Patrick D. Hunt,"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1963141&group_id=209147&atid=1008544","[<JIRA Version: name='3.1.0', id='12313381'>]",Bug,ZOOKEEPER-16,Major,Patrick D. Hunt,Fixed,2009-01-31T01:27:39.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Need to do path validation,2009-02-13T21:18:48.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>, <JIRA Version: name='3.0.1', id='12313500'>]",2.0
Patrick D. Hunt,"[<JIRA Component: name='build', id='12312383'>]",2008-06-10T21:15:39.000+0000,Patrick D. Hunt,"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1959060&group_id=209147&atid=1008544","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-15,Major,Patrick D. Hunt,Fixed,2008-07-23T04:31:40.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,handle failure better in build.xml:test,2008-10-26T01:10:37.000+0000,[],1.0
Benjamin Reed,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>]",2008-06-10T21:14:24.000+0000,Patrick D. Hunt,"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1948097&group_id=209147&atid=1008544

There should be a better connection between connect timeout, ping timeout, and session timeout. On the Java client it is sessionTimeout/hostsList.length. On the C client it's just the readTimeout. There is also an arbitrary delay when we find that all the hosts are down.

We should come up with a good consistent story for the connect timeout. Since we leave sessionTimeout/3 ms for recovery from a down server, it seems most reasonable to have something like sessionTimeout/(3*hostsList.length).",[],Bug,ZOOKEEPER-14,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Connect timeouts not calculated properly,2012-01-03T14:38:30.000+0000,[],1.0
,"[<JIRA Component: name='server', id='12312382'>]",2008-06-10T21:12:41.000+0000,Patrick D. Hunt,"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1946934&group_id=209147&atid=1008544",[],Bug,ZOOKEEPER-13,Major,Patrick D. Hunt,Duplicate,2008-08-27T18:28:11.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,NPE during normal operations,2008-08-27T18:28:11.000+0000,[],0.0
,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='documentation', id='12312422'>, <JIRA Component: name='java client', id='12312381'>]",2008-06-10T21:11:36.000+0000,Patrick D. Hunt,"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1941120&group_id=209147&atid=1008544",[],Bug,ZOOKEEPER-12,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,getChildren doesn't return a list of paths,2013-05-02T02:29:34.000+0000,[],2.0
Patrick D. Hunt,"[<JIRA Component: name='java client', id='12312381'>]",2008-06-10T21:10:23.000+0000,Patrick D. Hunt,"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1941109&group_id=209147&atid=1008544","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-11,Major,Patrick D. Hunt,Fixed,2008-07-23T04:30:53.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,ArrayList is used instead of List,2008-10-26T01:10:37.000+0000,[],1.0
Patrick D. Hunt,"[<JIRA Component: name='server', id='12312382'>]",2008-06-10T21:09:10.000+0000,Patrick D. Hunt,"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1941108&group_id=209147&atid=1008544","[<JIRA Version: name='3.5.0', id='12316644'>]",Bug,ZOOKEEPER-10,Minor,Patrick D. Hunt,Invalid,2014-03-15T00:20:02.000+0000,"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",Resolved,0.0,Bad error message,2014-03-15T00:20:02.000+0000,[],1.0
,"[<JIRA Component: name='server', id='12312382'>]",2008-06-10T21:07:39.000+0000,Patrick D. Hunt,"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1888835&group_id=209147&atid=1008544",[],Bug,ZOOKEEPER-9,Major,Patrick D. Hunt,,,The issue is open and ready for the assignee to start work on it.,Open,0.0,Set socket linger longer for commands,2011-09-06T16:14:51.000+0000,[],0.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>, <JIRA Component: name='java client', id='12312381'>]",2008-06-10T21:05:10.000+0000,Patrick D. Hunt,"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1886743&group_id=209147&atid=1008544
","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-8,Minor,Patrick D. Hunt,Fixed,2008-10-07T18:52:33.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Stat enchaned to include num of children and size,2008-10-26T01:10:37.000+0000,[],1.0
Jakob Homan,"[<JIRA Component: name='java client', id='12312381'>]",2008-06-10T21:01:06.000+0000,Patrick D. Hunt,"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1831408&group_id=209147&atid=1008544

Would be nice to fix in 3.0 as it's a non-bw compat change, also user feedback show's confusion on state/event/missingjavadoc.
","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-7,Minor,Patrick D. Hunt,Fixed,2008-09-26T21:27:47.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Use enums rather than ints for types and state,2008-10-26T01:10:37.000+0000,[],1.0
Patrick D. Hunt,"[<JIRA Component: name='c client', id='12312380'>]",2008-06-10T20:42:02.000+0000,Patrick D. Hunt,"Submitted on behalf of Jacob. Is is possible for us to address this in the next release? Will cause b/w compatibility issues for c client users but sounds like a good idea to fix now.
--------
I've attached a file with the problem identifiers. All of these
identifiers are unprotected by a ZOO_ (or something similar) prefix.

I've also looked at zookeeper.jute.h and the names of some these structs
are really unfortunate -- theyre sure to collide with some headers e.g.
Stat. There's also some exceptions to the consistent naming scheme --
op_result_t, String_vector, Id_vector are the ones I noticed.

-------- file -------
These enum constants are unprotected:

typedef enum {LOG_LEVEL_ERROR=1,LOG_LEVEL_WARN=2,LOG_LEVEL_INFO=3,LOG_LEVEL_DEBUG=4} ZooLogLevel;

extern ZOOAPI const int PERM_READ;
extern ZOOAPI const int PERM_WRITE;
extern ZOOAPI const int PERM_CREATE;
extern ZOOAPI const int PERM_DELETE;
extern ZOOAPI const int PERM_ADMIN;
extern ZOOAPI const int PERM_ALL;

extern ZOOAPI struct Id ANYONE_ID_UNSAFE;
extern ZOOAPI struct Id AUTH_IDS;

extern ZOOAPI struct ACL_vector OPEN_ACL_UNSAFE;
extern ZOOAPI struct ACL_vector READ_ACL_UNSAFE;
extern ZOOAPI struct ACL_vector CREATOR_ALL_ACL;

extern ZOOAPI const int EPHEMERAL;
extern ZOOAPI const int SEQUENCE;

extern ZOOAPI const int EXPIRED_SESSION_STATE;
extern ZOOAPI const int AUTH_FAILED_STATE;
extern ZOOAPI const int CONNECTING_STATE;
extern ZOOAPI const int ASSOCIATING_STATE;
extern ZOOAPI const int CONNECTED_STATE;

extern ZOOAPI const int CREATED_EVENT;
extern ZOOAPI const int DELETED_EVENT;
extern ZOOAPI const int CHANGED_EVENT;
extern ZOOAPI const int CHILD_EVENT;
extern ZOOAPI const int SESSION_EVENT;
extern ZOOAPI const int NOTWATCHING_EVENT;
","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-6,Major,Patrick D. Hunt,Fixed,2008-09-30T11:15:40.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,List of problem identifiers in zookeeper.h,2008-10-26T01:10:37.000+0000,[],1.0
Flavio Paiva Junqueira,"[<JIRA Component: name='leaderElection', id='12312378'>]",2008-06-09T16:42:38.000+0000,Benjamin Reed," FastLeaderElection.java line 224: The part of the condition after && is not needed: This is the else branch of an if statement, where the condition is exactly the first part. Hence, the part after && *must* be true.","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-4,Major,Benjamin Reed,Fixed,2008-09-09T21:09:01.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Unnecessary condition check in FastLeaderElection,2008-10-14T20:31:43.000+0000,"[<JIRA Version: name='3.0.0', id='12313216'>]",2.0
Mahadev Konar,"[<JIRA Component: name='quorum', id='12312379'>]",2008-06-09T16:39:34.000+0000,Benjamin Reed,syncLimit as documented in QuorumPeer is documented twice with two different aspects of in each instance. It should be better documented and unified. (Probably remove the second instance.),"[<JIRA Version: name='3.3.0', id='12313976'>]",Bug,ZOOKEEPER-3,Trivial,Benjamin Reed,Fixed,2009-11-18T17:48:01.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0," syncLimit has slightly different comments in the class header, and > inline with the variable.",2010-04-09T13:43:11.000+0000,[],3.0
Flavio Paiva Junqueira,"[<JIRA Component: name='leaderElection', id='12312378'>]",2008-06-09T16:34:31.000+0000,Benjamin Reed,"There are a couple of cases of member variables that need to be marked volatile or surrounded in a synchronization block. A couple of examples are:

* QuorumPeer state should be synchronous
* currentVote in QuorumPeer is marked volatile, but when it's members are often accessed individually as if they were in an atomic unit. Such code should be changed to get a reference to the currentVote and they access members through that reference.
* It looks like logicalClock in FastLeaderElection should be volatile. It should either be fixed or commented to explain why it doesn't need to be.","[<JIRA Version: name='3.0.0', id='12313216'>]",Bug,ZOOKEEPER-2,Major,Benjamin Reed,Fixed,2008-08-25T21:13:14.000+0000,"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",Closed,0.0,Synchronization issues in QuorumPeer and FastLeader election,2008-09-09T21:11:22.000+0000,[],2.0
